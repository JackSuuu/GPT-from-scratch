{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0007\n",
      "CPU times: user 549 μs, sys: 928 μs, total: 1.48 ms\n",
      "Wall time: 734 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "# matrix operation\n",
    "zeros = torch.zeros(1, 1)\n",
    "end_times = time.time()\n",
    "\n",
    "elapsed_time = end_times - start_time\n",
    "print(f'{elapsed_time: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3.92982793\n",
      "0.30731297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z0/t_vv9z0908vc5j6k_b12pwn40000gp/T/ipykernel_58807/2431387505.py:16: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  rand = np.multiply(np_rand1, np_rand2)\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand (10000, 10000). to(device)\n",
    "torch_rand2 = torch. rand (10000, 10000). to(device)\n",
    "np_rand1 = torch. rand (10000, 10000)\n",
    "np_rand2 = torch. rand (10000, 10000)\n",
    "\n",
    "start_time - time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time. time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\" {elapsed_time:.8f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time. time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a probability tensor\n",
    "probabilities = torch.tensor([0.1, 0.9])\n",
    "# 10% or 01 => o, 90%Z or 0.9 >= 1, each probability points to the index of the probabilities\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.tril(torch.ones(5,5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.triu(torch.ones(5, 5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.zeros(5,5).masked_fill(torch.tril(torch.ones(5, 5)) == 0, float('-inf'))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.zeros(2, 3, 4)\n",
    "out = input.transpose(0, 2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Stack the tensors along a new dimension\n",
    "# for making a batch\n",
    "stacked_tensor = torch.stack([tensor1, tensor2, tensor3])\n",
    "stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3420, -1.6667,  7.3379], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "sample = torch.tensor([10., 10., 10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "linear(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# creaet a tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# apply softmax using torch.nn.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2392e-02, -2.2733e-01, -3.5570e-02,  4.7562e-01,  9.7112e-02,\n",
       "         -1.1432e-01, -4.5738e-02, -7.8960e-01,  1.1023e+00,  1.0629e+00,\n",
       "          7.4582e-01, -3.8858e-01,  6.2158e-01,  6.3190e-01,  1.3257e+00,\n",
       "         -1.5781e+00,  2.1670e-01, -1.2007e+00, -2.1351e+00,  8.6810e-01,\n",
       "         -2.5998e+00,  1.1835e+00,  9.0926e-01,  7.0172e-02,  4.6598e-01,\n",
       "          4.1228e-01,  1.5461e+00,  7.8193e-01,  1.4526e+00,  6.4001e-02,\n",
       "          2.7878e-01,  1.1129e+00, -1.9279e+00, -4.7179e-01, -3.1902e-01,\n",
       "         -2.4666e-01,  1.7344e-01, -1.1868e+00, -5.5780e-01,  3.0558e-01,\n",
       "          4.3536e-01, -4.0269e-01, -2.5566e-01,  6.1353e-01, -3.4934e-01,\n",
       "          4.1904e-01, -3.2398e-01,  2.7438e-01, -1.5872e-01, -1.4400e+00,\n",
       "         -1.3510e+00,  2.1920e-01,  5.4590e-01,  6.1447e-01, -3.5899e-01,\n",
       "         -9.1615e-01, -6.8322e-01, -2.8986e-01, -5.7049e-01, -1.3215e+00,\n",
       "          1.8155e-02,  9.4294e-01,  2.8181e-01, -5.8216e-01,  3.3327e-01,\n",
       "          1.5380e-01, -1.6047e+00, -1.9507e+00,  1.0872e+00, -5.6582e-01,\n",
       "          1.4163e+00,  7.2261e-01, -9.0013e-01, -1.6060e+00,  1.9969e-03,\n",
       "         -1.6236e-01, -1.7296e+00,  6.2559e-01, -9.6406e-01,  1.1856e+00,\n",
       "         -6.1128e-01,  4.3543e-01, -7.0240e-01,  1.3510e+00,  1.2522e+00,\n",
       "         -1.3510e+00,  2.8032e-01,  1.1796e+00, -1.0558e+00, -2.1322e-01,\n",
       "          5.4402e-01,  1.1925e-01, -2.3818e-01,  1.6861e+00,  2.0772e-01,\n",
       "          7.4644e-01, -3.6593e-02,  3.9343e-01,  1.3590e+00,  3.7079e-01],\n",
       "        [ 8.5137e-01,  1.0414e+00,  2.0624e+00, -1.7329e-01,  4.1886e-01,\n",
       "         -6.1209e-01,  7.4334e-02, -8.3493e-01, -1.4181e+00, -3.2991e-01,\n",
       "         -5.3574e-01, -9.3594e-01,  4.8030e-01, -1.9768e-01, -2.3837e+00,\n",
       "          2.5480e-01,  6.3404e-01,  7.6352e-01, -2.7764e-01, -1.7503e-01,\n",
       "          8.7054e-01, -4.8537e-01,  7.7243e-01,  1.0703e+00, -4.7667e-01,\n",
       "          7.9441e-01, -3.3576e-01,  1.4296e+00,  7.5071e-01, -7.9100e-01,\n",
       "         -2.4359e+00,  2.9361e-01,  2.1416e-01,  6.4496e-01,  2.0313e+00,\n",
       "         -2.1005e-01,  6.7670e-01, -1.0362e+00,  1.3703e+00, -1.6714e-01,\n",
       "          1.1655e+00,  1.8900e+00, -1.2515e-01, -1.7800e+00, -4.3260e-01,\n",
       "         -8.2976e-01,  1.1950e+00, -7.1050e-01, -2.2163e+00, -1.1276e+00,\n",
       "         -1.0168e+00, -2.8768e-01,  1.3011e+00, -1.7092e+00,  4.0016e-01,\n",
       "         -1.2842e-01,  8.6489e-01, -8.8231e-02,  6.4251e-01, -2.8515e-01,\n",
       "          7.7285e-01,  2.7662e-01,  1.2386e+00,  3.7530e-01, -5.7799e-01,\n",
       "          6.2229e-01,  5.4158e-01,  5.5242e-01,  6.2434e-01, -4.3926e-01,\n",
       "          7.9866e-01,  1.9021e-02, -9.6437e-01, -6.0658e-01,  8.2013e-01,\n",
       "          1.2298e+00, -8.3252e-01,  1.0378e-01, -1.7411e-01, -1.8470e+00,\n",
       "          2.1235e+00,  6.3959e-01,  1.2844e-01, -1.9045e-01, -2.6773e-01,\n",
       "         -7.6418e-01,  6.5880e-02, -4.5655e-01, -8.1533e-01, -1.0320e+00,\n",
       "         -1.0501e+00,  1.8546e+00,  1.1758e+00, -6.5534e-01, -7.3405e-01,\n",
       "         -5.1647e-01,  5.0210e-01,  1.7065e+00,  1.8436e-01,  8.2058e-01],\n",
       "        [-1.2209e+00,  7.7530e-01,  6.1599e-01,  8.2476e-01,  7.0771e-01,\n",
       "          1.4383e+00, -1.7957e-02, -1.6840e-01, -2.5813e-01,  6.8526e-02,\n",
       "          6.6469e-01, -1.0773e+00, -1.0009e+00,  2.5538e-01, -5.6805e-01,\n",
       "         -2.0671e-02, -3.6111e-01,  2.6104e+00, -1.2706e+00,  2.7146e-01,\n",
       "         -1.3593e+00,  9.7526e-01,  1.0660e+00,  2.0140e-01, -2.7528e+00,\n",
       "          4.3382e-01, -5.7250e-01, -2.0624e+00,  8.6282e-02,  1.6838e+00,\n",
       "          1.8156e+00, -5.4895e-01,  1.5381e-01, -2.6965e-01,  1.5284e+00,\n",
       "         -1.1906e+00,  8.7847e-01, -2.4833e-02,  1.7754e-01, -1.5335e+00,\n",
       "          4.8200e-01,  5.4378e-01, -1.0421e+00, -9.3335e-01,  1.3602e+00,\n",
       "         -1.3160e+00, -7.2059e-02, -2.3815e-01,  3.9043e-01,  5.9539e-01,\n",
       "         -1.0171e+00, -8.6849e-01,  2.7794e-01,  1.2205e+00,  4.6686e-01,\n",
       "         -1.9775e+00,  4.0840e-01,  6.8367e-03, -1.3668e+00,  8.8687e-01,\n",
       "         -8.2382e-01, -5.2965e-01, -1.2883e+00, -4.6679e-01,  3.3652e-01,\n",
       "          1.1778e+00, -8.8036e-01, -6.9229e-01,  8.3821e-01,  3.2194e-01,\n",
       "         -9.0140e-01, -7.5598e-01,  6.1018e-01, -1.8652e-01, -1.1298e+00,\n",
       "          8.5958e-01,  4.1770e-01,  5.6307e-01,  1.0828e+00, -8.2660e-03,\n",
       "          1.5750e+00, -1.3435e+00, -9.5658e-02, -1.0347e+00,  1.5277e-01,\n",
       "         -2.5731e-01,  8.4161e-01,  1.1016e+00,  1.4494e+00, -1.1822e+00,\n",
       "         -7.0259e-01,  2.7196e+00,  4.2266e-01,  8.1258e-01,  9.0945e-02,\n",
       "         -1.9680e+00, -8.2252e-01, -5.4549e-01,  7.9739e-02,  9.2680e-01],\n",
       "        [ 4.5506e-01,  2.9794e-01, -1.7345e-01, -4.5412e-01, -1.5630e-01,\n",
       "         -4.4805e-01,  9.3090e-01,  1.3458e+00,  9.7130e-01,  3.6539e-01,\n",
       "         -1.2583e+00, -7.2565e-01,  9.7595e-01,  5.0873e-01,  2.8116e-01,\n",
       "          1.3395e+00, -9.3239e-01, -4.2745e-01,  6.2569e-01, -4.5960e-01,\n",
       "         -2.5347e+00, -8.1420e-01, -5.0274e-01,  2.1086e+00, -1.0198e+00,\n",
       "         -1.4265e+00, -7.6097e-01,  1.8231e-01,  1.2301e+00, -7.3647e-02,\n",
       "         -1.4247e-01,  8.3492e-01, -7.8445e-01,  3.8705e-01, -2.9975e-01,\n",
       "          3.1084e-01, -1.1660e+00, -6.1625e-01, -1.6940e+00,  1.0665e+00,\n",
       "         -1.6379e-01, -1.0989e+00,  8.2610e-01,  2.1371e+00, -5.3884e-01,\n",
       "         -3.4972e-02,  1.3405e+00,  1.6127e-01, -1.8255e-01,  3.9453e-01,\n",
       "         -1.9576e-01,  8.2082e-01,  6.5076e-01,  9.4530e-02, -1.2667e+00,\n",
       "          1.3223e+00, -9.1024e-02, -3.3801e-02,  2.5416e+00,  6.9633e-01,\n",
       "          1.1819e+00, -5.2234e-01, -5.3204e-01, -7.1927e-01, -1.9373e+00,\n",
       "          4.6814e-01, -6.3187e-01, -1.7784e+00,  8.6269e-01, -4.8129e-01,\n",
       "         -7.8030e-01, -5.0143e-01, -1.2418e+00,  1.0064e+00, -9.4319e-01,\n",
       "         -1.2435e+00,  2.1632e-01, -2.7610e-01,  3.5768e-01, -2.7904e-01,\n",
       "          4.4922e-01, -2.0250e-01,  2.4115e+00, -2.3953e+00, -1.2206e+00,\n",
       "         -3.3624e-01,  8.3765e-01,  8.6658e-01, -9.8489e-02,  2.3446e+00,\n",
       "          6.8296e-01,  1.8751e+00,  5.7180e-01, -6.3150e-01, -5.9335e-01,\n",
       "          1.9194e+00, -3.0654e-03, -3.1228e-01,  3.1233e-01, -7.3762e-01]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Init an e,bedding layer\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "embedded_output.shape\n",
    "embedded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  30,  33],\n",
       "        [ 61,  68,  75],\n",
       "        [ 95, 106, 117]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "b = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "# dot product\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type int64\n",
    "int_64 = torch.randint(1, (3, 2)).float()\n",
    "#type float32\n",
    "float_32 = torch.rand(2, 3)\n",
    "int_64.dtype\n",
    "result = torch.matmul(int_64, float_32)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3, 5)\n",
    "# unpack the torch tensor\n",
    "x, y, z = a.shape\n",
    "a = a.view(x, y, z)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
