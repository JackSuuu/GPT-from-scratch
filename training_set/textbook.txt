Springer Texts in Statistics
Probability with
Applications in
Engineering,
Science, and
TechnologyMatthew A. Carlton
Jay L. Devore
Second Edition
Springer Texts in Statistics
More information about this series at http://www.springer.com/series/417Series Editors
R. DeVeaux
S.E. Fienberg
I. Olkin
Matthew A. Carlton  Jay L. Devore
Probability with Applications
in Engineering, Science,
and Technology
Second Edition
Editors
Matthew A. Carlton
Department of Statistics
California Polytechnic State University
San Luis Obispo, CA, USAJay L. Devore
Department of Statistics
California Polytechnic State University
San Luis Obispo, CA, USA
ISSN 1431-875X ISSN 2197-4136 (eBook)
Springer Texts in Statistics
ISBN 978-3-319-52400-9 ISBN 978-3-319-52401-6 (eBook)
DOI 10.1007/978-3-319-52401-6
Library of Congress Control Number: 2017932278
#Springer International Publishing AG 2014,    2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or
part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way,
and transmission or information storage and retrieval, electronic adaptation, computer software,
or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are
exempt from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in
this book are believed to be true and accurate at the date of publication. Neither the publisher nor
the authors or the editors give a warranty, express or implied, with respect to the material
contained herein or for any errors or omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandAdditional material to this book can be downloaded from http://extras.springer.com .
Preface
Purpose
Our objective is to provide a post-calculus introduction to the subject of probability that
• Has mathematical integrity and contains some underlying theory
• Shows students a broad range of applications involving real problem scenarios
• Is current in its selection of topics
• Is accessible to a wide audience, including mathematics and statistics majors (yes, there are a few
of the latter, and their numbers are growing), prospective engineers and scientists, and business
and social science majors interested in the quantitative aspects of their disciplines
• Illustrates the importance of software for carrying out simulations when answers to questions
cannot be obtained analytically
A number of currently available probability texts are heavily oriented toward a rigorous mathe-
matical development of probability, with much emphasis on theorems, proofs, and derivations. Even
when applied material is included, the scenarios are often contrived (many examples and exercises
involving dice, coins, cards, and widgets). So in our exposition we have tried to achieve a balance
between mathematical foundations and the application of probability to real-world problems. It is our
belief that the theory of probability by itself is often not enough of a “hook” to get students interested
in further work in the subject. We think that the best way to persuade students to continue their
probabilistic education beyond a ﬁrst course is to show them how the methodology is used in practice.
Let’s ﬁrst seduce them (ﬁguratively speaking, of course) with intriguing problem scenarios and
applications. Opportunities for exposure to mathematical rigor will follow in due course.
Content
The book begins with an Introduction, which contains our attempt to address the following question:
“Why study probability?” Here we are trying to tantalize students with a number of intriguing
problem scenarios—coupon collection, birth and death processes, reliability engineering, ﬁnance,
queuing models, and various conundrums involving the misinterpretation of probabilistic information
(e.g., Benford’s Law and the detection of fraudulent data, birthday problems, and the likelihood of
having a rare disease when a diagnostic test result is positive). Most of the exposition contains
references to recently published results. It is not necessary or even desirable to cover very much of
this motivational material in the classroom. Instead, we suggest that instructors ask their students to
read selectively outside class (a bit of pleasure reading at the very beginning of the term should not be
v
an undue burden!). Subsequent chapters make little reference to the examples herein, and separating
out our “pep talk” should make it easier to cover as little or much as an instructor deems appropriate.
Chapter 1covers sample spaces and events, the axioms of probability and derived properties,
counting, conditional probability, and independence. Discrete random variables and distributions are
the subject of Chap. 2, and Chap. 3introduces continuous random variables and their distributions.
Joint probability distributions are the focus of Chap. 4, including marginal and conditional
distributions, expectation of a function of several variables, correlation, modes of convergence, the
Central Limit Theorem, reliability of systems of components, the distribution of a linear combination,
and some results on order statistics. These four chapters constitute the core of the book.
The remaining chapters build on the core in various ways. Chapter 5introduces methods of
statistical inference—point estimation, the use of statistical intervals, and hypothesis testing. In
Chap. 6we cover basic properties of discrete-time Markov chains. Various other random processes
and their properties, including stationarity and its consequences, Poisson processes, Brownian
motion, and continuous-time Markov chains, are discussed in Chap. 7. The ﬁnal chapter presents
some elementary concepts and methods in the area of signal processing.
One feature of our book that distinguishes it from the competition is a section at the end of almost
every chapter that considers simulation methods for getting approximate answers when exact results
are difﬁcult or impossible to obtain. Both the R software and Matlab are employed for this purpose.
Another noteworthy aspect of the book is the inclusion of roughly 1100 exercises; the ﬁrst four
core chapters together have about 700 exercises. There are numerous exercises at the end of each
section and also supplementary exercises at the end of every chapter. Probability at its heart is
concerned with problem solving. A student cannot hope to really learn the material simply by sitting
passively in the classroom and listening to the instructor. He/she must get actively involved in
working problems. To this end, we have provided a wide spectrum of exercises, ranging from
straightforward to reasonably challenging. It should be easy for an instructor to ﬁnd enough problems
at various levels of difﬁculty to keep students gainfully occupied.
Mathematical Level
The challenge for students at this level should be to master the concepts and methods to a sufﬁcient
degree that problems encountered in the real world can be solved. Most of our exercises are of this
type, and relatively few ask for proofs or derivations. Consequently, the mathematical prerequisites
and demands are reasonably modest. Mathematical sophistication and quantitative reasoning ability
are, of course, crucial to the enterprise. Univariate calculus is employed in the continuous distribution
calculations of Chap. 3as well as in obtaining maximum likelihood estimators in the inference
chapter. But even here the functions we ask students to work with are straightforward—generally
polynomials, exponentials, and logs. A stronger background is required for the signal processing
material at the end of the book (we have included a brief mathematical appendix as a refresher for
relevant properties). Multivariate calculus is used in the section on joint distributions in Chap. 4and
thereafter appears rather rarely. Exposure to matrix algebra is needed for the Markov chain material.
Recommended Coverage
Our book contains enough material for a year-long course, though we expect that many instructors
will use it for a single term (one semester or one quarter). To give a sense of what might be
reasonable, we now brieﬂy describe three courses at our home institution, Cal Poly State University
(in San Luis Obispo, CA), for which this book is appropriate. Syllabi with expanded course outlines
are available for download on the book’s website at Springer.com .vi Preface
Title: Introduction to
Probability and
SimulationIntroduction to Probability
ModelsProbability and Random Processes
for Engineers
Main
audience:Statistics and math
majorsStatistics and math majors Electrical and computer
engineering majors
Prerequisites: Univariate calculus,
computer
programmingUnivariate calculus, computer
programming, matrix algebraMultivariate calculus, continuous-
time signals incl. Fourier analysis
Sections
covered:1.1–1.6 1.1–1.6 1.1–1.5
2.1–2.6, 2.8 2.1–2.5, 2.8 2.1–2.5
3.1–3.4, 3.8 3.1–3.4, 3.8 3.1–3.5
4.1–4.3, 4.5 4.1–4.3, 4.5, 4.8 4.1–4.3, 4.5, 4.7
6.1–6.5 7.1–7.3, 7.5–7.6
7.5 8.1–8.2
Both of the ﬁrst two courses place heavy emphasis on computer simulation of random phenomena;
instructors typically have students work in R. As is evident from the lists of sections covered,
Introduction to Probability Models takes the earlier material at a faster pace in order to leave a few
weeks at the end for Markov chains and some other applications (typically reliability theory and a bit
about Poisson processes). In our experience, the computer programming prerequisite is essential for
students’ success in those two courses.
The third course listed, Probability and Random Processes for Engineers , is our university’s
version of the traditional “random signals and noise” course offered by many electrical engineering
departments. Again, the ﬁrst four chapters are covered at a somewhat accelerated pace, with about
30–40% of the course dedicated to time and frequency representations of random processes (Chaps. 7
and8). Simulation of random phenomena is not emphasized in our course, though we make liberal
use of Matlab for demonstrations.
We are able to cover as much material as indicated on the foregoing syllabi with the aid of a not-
so-secret weapon: we prepare and require that students bring to class a course booklet. The booklet
contains most of the examples we present as well as some surrounding material. A typical example
begins with a problem statement and then poses several questions (as in the exercises in this book).
After each posed question there is some blank space so the student can either take notes as the solution
is developed in class or else work the problem on his/her own if asked to do so. Because students have
a booklet, the instructor does not have to write as much on the board as would otherwise be necessary
and the student does not have to do as much writing to take notes. Both the instructor and the students
beneﬁt.
We also like to think that students can be asked to read an occasional subsection or even section on
their own and then work exercises to demonstrate understanding, so that not everything needs to be
presented in class. For example, we have found that assigning a take-home exam problem that
requires reading about the Weibull and/or lognormal distributions is a good way to acquaint students
with them. But instructors should always keep in mind that there is never enough time in a course of
any duration to teach students all that we’d like them to know. Hopefully students will like the book
enough to keep it after the course is over and use it as a basis for extending their knowledge of
probability!Preface vii
Acknowledgments
We gratefully acknowledge the plentiful feedback provided by the following reviewers: Allan Gut,
Murad Taqqu, Mark Schilling and Robert Heiny.
We very much appreciate the production services provided by the folks at SPi Technologies. Our
production editors, Sasireka. K and Maria David did a ﬁrst-rate job of moving the book through the
production process and were always prompt and considerate in communications with us. Thanks to
our copyeditors at SPi for employing a light touch and not taking us too much to task for our
occasional grammatical and stylistic lapses. The staff at Springer U.S. has been especially supportive
during both the developmental and production stages; special kudos go to Michael Penn and Rebekah
McClure.
A Final Thought
It is our hope that students completing a course taught from this book will feel as passionately about
the subject of probability as we still do after so many years of living with it. Only teachers can really
appreciate how gratifying it is to hear from a student after he/she has completed a course that the
experience had a positive impact and maybe even affected a career choice.
San Luis Obispo, CA Matthew A. Carlton
San Luis Obispo, CA Jay L. Devoreviii Preface
Contents
1 Probability ........................................................ 1
1.1 Sample Spaces and Events ........................................ 1
1.1.1 The Sample Space of an Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1.2 Events . . . .............................................. 3
1.1.3 Some Relations from Set Theory ............................. 4
1.1.4 Exercises: Section 1.1 (1–12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Axioms, Interpretations, and Properties of Probability . . ................... 7
1.2.1 Interpreting Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2.2 More Probability Properties . ................................ 1 1
1.2.3 Determining Probabilities Systematically ....................... 1 3
1.2.4 Equally Likely Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.2.5 Exercises: Section 1.2 (13–30) . . . ............................ 1 4
1.3 Counting Methods . . . . . . ........................................ 1 8
1.3.1 The Fundamental Counting Principle . . . . ...................... 1 8
1.3.2 Tree Diagrams . . . . . . . . . . . . . . ............................. 1 9
1.3.3 Permutations ............................................ 2 0
1.3.4 Combinations ........................................... 2 2
1.3.5 Exercises: Section 1.3 (31–49) . . . ............................ 2 5
1.4 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
1.4.1 The Deﬁnition of Conditional Probability . ...................... 3 0
1.4.2 The Multiplication Rule for P(A\B).......................... 3 2
1.4.3 The Law of Total Probability and Bayes’ Theorem . . . . . . . . . . . . . . . . 34
1.4.4 Exercises: Section 1.4 (50–78) . . . ............................ 3 7
1.5 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
1.5.1 P(A\B) When Events Are Independent ........................ 4 4
1.5.2 Independence of More than Two Events ........................ 4 5
1.5.3 Exercises: Section 1.5 (79–100) . . . ........................... 4 7
1.6 Simulation of Random Events . . . . . . . . . . . . . . ........................ 5 1
1.6.1 The Backbone of Simulation: Random Number Generators . . . . . . . . . . 51
1.6.2 Precision of Simulation .................................... 5 5
1.6.3 Exercises: Section 1.6 (101–120) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
1.7 Supplementary Exercises (121–150) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
2 Discrete Random Variables and Probability Distributions .................... 6 7
2.1 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
2.1.1 Two Types of Random Variables . . ........................... 6 9
2.1.2 Exercises: Section 2.1 (1–10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
ix
2.2 Probability Distributions for Discrete Random Variables . . ................. 7 1
2.2.1 A Parameter of a Probability Distribution ....................... 7 4
2.2.2 The Cumulative Distribution Function . ......................... 7 5
2.2.3 Another View of Probability Mass Functions . . . .................. 7 8
2.2.4 Exercises: Section 2.2 (11–28) . . . ............................ 7 9
2.3 Expected Value and Standard Deviation . .............................. 8 3
2.3.1 The Expected Value of X................................... 8 3
2.3.2 The Expected Value of a Function . ........................... 8 6
2.3.3 The Variance and Standard Deviation of X...................... 8 8
2.3.4 Properties of Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
2.3.5 Exercises: Section 2.3 (29–48) . . . ............................ 9 1
2.4 The Binomial Distribution ......................................... 9 5
2.4.1 The Binomial Random Variable and Distribution .................. 9 7
2.4.2 Computing Binomial Probabilities . . . ......................... 9 9
2.4.3 The Mean and Variance of a Binomial Random Variable . . . . . . . . . . . . 101
2.4.4 Binomial Calculations with Software . . . . ...................... 1 0 2
2.4.5 Exercises: Section 2.4 (49–74) . . . ............................ 1 0 2
2.5 The Poisson Distribution . ......................................... 1 0 7
2.5.1 The Poisson Distribution as a Limit . . . ........................ 1 0 7
2.5.2 The Mean and Variance of a Poisson Random Variable . . . . . . . . . . . . . 110
2.5.3 The Poisson Process ....................................... 1 1 0
2.5.4 Poisson Calculations with Software ............................ 1 1 1
2.5.5 Exercises: Section 2.5 (75–89) . . . ............................ 1 1 1
2.6 Other Discrete Distributions . . ..................................... 1 1 4
2.6.1 The Hypergeometric Distribution ............................. 1 1 4
2.6.2 The Negative Binomial and Geometric Distributions . . . . . . . . . . . . . . . 117
2.6.3 Alternative Deﬁnition of the Negative Binomial Distribution . . . . . . . . . 120
2.6.4 Exercises: Section 2.6 (90–106) . . . ........................... 1 2 0
2.7 Moments and Moment Generating Functions . . ......................... 1 2 3
2.7.1 The Moment Generating Function ............................. 1 2 5
2.7.2 Obtaining Moments from the MGF . . . ......................... 1 2 7
2.7.3 MGFs of Common Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
2.7.4 Exercises: Section 2.7 (107–128) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
2.8 Simulation of Discrete Random Variables . . . . . ........................ 1 3 1
2.8.1 Simulations Implemented in R and Matlab . . . . . . . . . . . . . . . . . . . . . . 134
2.8.2 Simulation Mean, Standard Deviation, and Precision ............... 1 3 5
2.8.3 Exercises: Section 2.8 (129–141) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
2.9 Supplementary Exercises (142–170) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
3 Continuous Random Variables and Probability Distributions ................. 1 4 7
3.1 Probability Density Functions and Cumulative Distribution Functions . . . . . . . . . 147
3.1.1 Probability Distributions for Continuous Variables . ................ 1 4 8
3.1.2 The Cumulative Distribution Function . ......................... 1 5 2
3.1.3 Using F(x) to Compute Probabilities ........................... 1 5 4
3.1.4 Obtaining f(x) from F(x).................................... 1 5 5
3.1.5 Percentiles of a Continuous Distribution . . . . . . .................. 1 5 6
3.1.6 Exercises: Section 3.1 (1–18) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
3.2 Expected Values and Moment Generating Functions ...................... 1 6 2
3.2.1 Expected Values . ........................................ 1 6 2
3.2.2 Moment Generating Functions ............................... 1 6 6
3.2.3 Exercises: Section 3.2(19–38) ................................ 1 6 8x Contents
3.3 The Normal (Gaussian) Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
3.3.1 The Standard Normal Distribution . ........................... 1 7 3
3.3.2 Non-standardized Normal Distributions . . ....................... 1 7 5
3.3.3 The Normal MGF . . ...................................... 1 7 8
3.3.4 The Normal Distribution and Discrete Populations . . . . . . . . . . . . . . . . . 179
3.3.5 Approximating the Binomial Distribution . . ..................... 1 8 0
3.3.6 Normal Distribution Calculations with Software . . . . . . . . . . . . . . . . . . 182
3.3.7 Exercises: Section 3.3 (39–70) . . . ............................ 1 8 2
3.4 The Exponential and Gamma Distributions ............................ 1 8 7
3.4.1 The Exponential Distribution ................................ 1 8 8
3.4.2 The Gamma Distribution ................................... 1 9 0
3.4.3 The Gamma MGF . . . . . ................................... 1 9 3
3.4.4 Gamma and Exponential Calculations with Software ............... 1 9 3
3.4.5 Exercises: Section 3.4 (71–83) . . . ............................ 1 9 4
3.5 Other Continuous Distributions ..................................... 1 9 6
3.5.1 The Weibull Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
3.5.2 The Lognormal Distribution ................................. 1 9 9
3.5.3 The Beta Distribution . ..................................... 2 0 1
3.5.4 Exercises: Section 3.5 (84–100) . . . ........................... 2 0 2
3.6 Probability Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
3.6.1 Sample Percentiles . ....................................... 2 0 5
3.6.2 A Probability Plot . ....................................... 2 0 6
3.6.3 Departures from Normality .................................. 2 0 9
3.6.4 Beyond Normality ........................................ 2 1 1
3.6.5 Probability Plots in Matlab and R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
3.6.6 Exercises: Section 3.6 (101–111) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
3.7 Transformations of a Random Variable ............................... 2 1 6
3.7.1 Exercises: Section 3.7 (112–128) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
3.8 Simulation of Continuous Random Variables . . . . . . . . . . . . . . . . . . . . . ...... 2 2 1
3.8.1 The Inverse CDF Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
3.8.2 The Accept–Reject Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
3.8.3 Built-In Simulation Packages for Matlab and R ................... 2 2 7
3.8.4 Precision of Simulation Results .............................. 2 2 7
3.8.5 Exercises: Section 3.8 (129–139) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
3.9 Supplementary Exercises (140–172) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
4 Joint Probability Distributions and Their Applications ...................... 2 3 9
4.1 Jointly Distributed Random Variables ................................ 2 3 9
4.1.1 The Joint Probability Mass Function for Two Discrete
Random Variables ........................................ 2 4 0
4.1.2 The Joint Probability Density Function for Two Continuous
Random Variables ........................................ 2 4 1
4.1.3 Independent Random Variables . ............................. 2 4 5
4.1.4 More Than Two Random Variables . . . . ....................... 2 4 6
4.1.5 Exercises: Section 4.1 (1–22) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
4.2 Expected Values, Covariance, and Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . 255
4.2.1 Properties of Expected Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
4.2.2 Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
4.2.3 Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259Contents xi
4.2.4 Correlation Versus Causation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
4.2.5 Exercises: Section 4.2 (23–42) . . . ............................ 2 6 2
4.3 Properties of Linear Combinations ................................... 2 6 4
4.3.1 The PDF of a Sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
4.3.2 Moment Generating Functions for Linear Combinations . . ........... 2 7 0
4.3.3 Exercises: Section 4.3 (43–65) . . . ............................ 2 7 2
4.4 Conditional Distributions and Conditional Expectation . . . ................. 2 7 7
4.4.1 Conditional Distributions and Independence . . . . . . . . . . . . . . . . . . . . . 279
4.4.2 Conditional Expectation and Variance .......................... 2 8 0
4.4.3 The Laws of Total Expectation and Variance . . ................... 2 8 1
4.4.4 Exercises: Section 4.4 (66–84) . . . ............................ 2 8 6
4.5 Limit Theorems (What Happens as nGets Large) ........................ 2 9 0
4.5.1 Random Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
4.5.2 The Central Limit Theorem ................................. 2 9 3
4.5.3 Other Applications of the Central Limit Theorem ................. 2 9 7
4.5.4 The Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
4.5.5 Exercises: Section 4.5 (85–102) . . . ........................... 3 0 0
4.6 Transformations of Jointly Distributed Random Variables . . . . . . . . . . . . . . . . . . 302
4.6.1 The Joint Distribution of Two New Random Variables . . . . . . . . . . . . . 303
4.6.2 The Joint Distribution of More Than Two New Variables ........... 3 0 6
4.6.3 Exercises: Section 4.6 (103–110) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
4.7 The Bivariate Normal Distribution ................................... 3 0 9
4.7.1 Conditional Distributions of XandY........................... 3 1 1
4.7.2 Regression to the Mean . . . ................................. 3 1 2
4.7.3 The Multivariate Normal Distribution . . ........................ 3 1 2
4.7.4 Bivariate Normal Calculations with Software . . . . . . . . . . . . . . . . . . . . 313
4.7.5 Exercises: Section 4.7 (111–120) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
4.8 Reliability . . . . . . . . . . . . ........................................ 3 1 5
4.8.1 The Reliability Function . ................................... 3 1 5
4.8.2 Series and Parallel Designs .................................. 3 1 7
4.8.3 Mean Time to Failure ...................................... 3 2 0
4.8.4 Hazard Functions . . . ...................................... 3 2 1
4.8.5 Exercises: Section 4.8 (121–132) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
4.9 Order Statistics ................................................. 3 2 6
4.9.1 The Distributions of YnandY1............................... 3 2 6
4.9.2 The Distribution of the ith Order Statistic . . . . . . . . . . . . . . . . . . . . . . . 328
4.9.3 The Joint Distribution of the nOrder Statistics . . ................. 3 2 9
4.9.4 Exercises: Section 4.9 (133–142) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
4.10 Simulation of Joint Probability Distributions and System Reliability .......... 3 3 2
4.10.1 Simulating Values from a Joint PMF ........................... 3 3 2
4.10.2 Simulating Values from a Joint PDF ........................... 3 3 4
4.10.3 Simulating a Bivariate Normal Distribution . ..................... 3 3 6
4.10.4 Simulation Methods for Reliability . . .......................... 3 3 8
4.10.5 Exercises: Section 4.10 (143–153) . . . ......................... 3 4 0
4.11 Supplementary Exercises (154–192) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
5 The Basics of Statistical Inference ....................................... 3 5 1
5.1 Point Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
5.1.1 Estimates and Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
5.1.2 Assessing Estimators: Accuracy and Precision . . . . . . . . . . . . . . . . . . . . 357
5.1.3 Exercises: Section 5.1 (1–23) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360xii Contents
5.2 Maximum Likelihood Estimation . ................................... 3 6 6
5.2.1 Some Properties of MLEs ................................... 3 7 2
5.2.2 Exercises: Section 5.2 (24–36) . . . ............................ 3 7 3
5.3 Conﬁdence Intervals for a Population Mean ............................ 3 7 5
5.3.1 A Conﬁdence Interval for a Normal Population Mean . . . . . . . . . . . . . . 376
5.3.2 A Large-Sample Conﬁdence Interval for μ...................... 3 8 0
5.3.3 Software for Conﬁdence Interval Calculation ..................... 3 8 1
5.3.4 Exercises: Section 5.3 (37–50) . . . ............................ 3 8 2
5.4 Testing Hypotheses About a Population Mean .......................... 3 8 6
5.4.1 Hypotheses and Test Procedures . . . ........................... 3 8 6
5.4.2 Test Procedures for Hypotheses About a Population Mean μ......... 3 8 8
5.4.3 P-Values and the One-Sample tTest . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
5.4.4 Errors in Hypothesis Testing and the Power of a Test .............. 3 9 2
5.4.5 Software for Hypothesis Test Calculation . . . .................... 3 9 5
5.4.6 Exercises: Section 5.4 (51–76) . . . ............................ 3 9 6
5.5 Inferences for a Population Proportion . . . . . . . . . . . . .................... 4 0 1
5.5.1 Conﬁdence Intervals for p.................................. 4 0 1
5.5.2 Hypothesis Testing for p................................... 4 0 3
5.5.3 Software for Inferences about p.............................. 4 0 5
5.5.4 Exercises: Section 5.5 (77–97) . . . ............................ 4 0 5
5.6 Bayesian Inference . ............................................. 4 0 9
5.6.1 The Posterior Distribution of a Parameter ....................... 4 1 0
5.6.2 Inferences from the Posterior Distribution . . . . . . . . . . . . . . . . . . . . . . . 413
5.6.3 Further Comments on Bayesian Inference ....................... 4 1 3
5.6.4 Exercises: Section 5.6 (98–106) . . . ........................... 4 1 4
5.7 Supplementary Exercises (107–138) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
6 Markov Chains ..................................................... 4 2 3
6.1 Terminology and Basic Properties ................................... 4 2 3
6.1.1 The Markov Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
6.1.2 Exercises: Section 6.1 (1–10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
6.2 The Transition Matrix and the Chapman–Kolmogorov Equations ............ 4 3 1
6.2.1 The Transition Matrix ..................................... 4 3 1
6.2.2 Computation of Multistep Transition Probabilities ................. 4 3 2
6.2.3 Exercises: Section 6.2 (11–22) . . . ............................ 4 3 6
6.3 Specifying an Initial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
6.3.1 A Fixed Initial State ....................................... 4 4 3
6.3.2 Exercises: Section 6.3 (23–30) . . . ............................ 4 4 4
6.4 Regular Markov Chains and the Steady-State Theorem . . . ................. 4 4 6
6.4.1 Regular Chains . ......................................... 4 4 6
6.4.2 The Steady-State Theorem . ................................. 4 4 8
6.4.3 Interpreting the Steady-State Distribution ....................... 4 5 0
6.4.4 Efﬁcient Computation of Steady-State Probabilities ................ 4 5 1
6.4.5 Irreducible and Periodic Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
6.4.6 Exercises: Section 6.4 (31–43) . . . ............................ 4 5 4
6.5 Markov Chains with Absorbing States ................................ 4 5 7
6.5.1 Time to Absorption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
6.5.2 Mean Time to Absorption . . ................................. 4 6 1
6.5.3 Mean First Passage Times . . ................................ 4 6 5
6.5.4 Probabilities of Eventual Absorption . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
6.5.5 Exercises: Section 6.5 (44–58) . . . ............................ 4 6 9Contents xiii
6.6 Simulation of Markov chains ....................................... 4 7 2
6.6.1 Exercises: Section 6.6 (59–66) . . . ............................ 4 7 9
6.7 Supplementary Exercises (67–82) ................................... 4 8 1
7 Random Processes ................................................... 4 8 9
7.1 Types of Random Processes . ...................................... 4 8 9
7.1.1 Classiﬁcation of Processes . . . . . . . . . . ........................ 4 9 3
7.1.2 Random Processes Regarded as Random Variables . . . . . . . . . . . . . . . . 493
7.1.3 Exercises: Section 7.1 (1–10) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
7.2 Properties of the Ensemble: Mean and Autocorrelation Functions ............ 4 9 6
7.2.1 Mean and Variance Functions ................................ 4 9 6
7.2.2 Autocovariance and Autocorrelation Functions . . . . . . . . . . . . . . . . . . . 499
7.2.3 The Joint Distribution of Two Random Processes ................. 5 0 2
7.2.4 Exercises: Section 7.2 (11–24) . . . ............................ 5 0 3
7.3 Stationary and Wide-Sense Stationary Processes . . ...................... 5 0 4
7.3.1 Properties of Wide-Sense Stationary Processes . . . . . . . . . . . . . . . . . . . 508
7.3.2 Ergodic Processes ........................................ 5 1 1
7.3.3 Exercises: Section 7.3 (25–40) . . . ............................ 5 1 4
7.4 Discrete-Time Random Processes ................................... 5 1 6
7.4.1 Special Discrete Sequences . . . ............................... 5 1 8
7.4.2 Exercises: Section 7.4 (41–52) . . . ............................ 5 2 0
7.5 Poisson Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
7.5.1 Relation to Exponential and Gamma Distributions . . . . . . . . . . . . . . . . . 524
7.5.2 Combining and Decomposing Poisson Processes . . . . . . . . . . . . . . . . . . 526
7.5.3 Alternative Deﬁnition of a Poisson Process ...................... 5 2 8
7.5.4 Nonhomogeneous Poisson Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
7.5.5 The Poisson Telegraphic Process . ............................ 5 3 1
7.5.6 Exercises: Section 7.5 (53–72) . . . ............................ 5 3 2
7.6 Gaussian Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
7.6.1 Brownian Motion ......................................... 5 3 6
7.6.2 Brownian Motion as a Limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
7.6.3 Further Properties of Brownian Motion . ........................ 5 3 8
7.6.4 Variations on Brownian Motion .............................. 5 4 1
7.6.5 Exercises: Section 7.6 (73–85) . . . ............................ 5 4 1
7.7 Continuous-Time Markov Chains . . ................................. 5 4 4
7.7.1 Inﬁnitesimal Parameters and Instantaneous Transition Rates . . . . . . . . . . 546
7.7.2 Sojourn Times and Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
7.7.3 Long-Run Behavior of Continuous-Time Markov Chains ............ 5 5 2
7.7.4 Explicit Form of the Transition Matrix . . . . . . . . . . . . . . . . . . . . . . . . . 554
7.7.5 Exercises: Section 7.7 (86–97) . . . ............................ 5 5 6
7.8 Supplementary Exercises (98–114) . ................................. 5 5 9
8 Introduction to Signal Processing ....................................... 5 6 3
8.1 Power Spectral Density . . . . . ...................................... 5 6 3
8.1.1 Properties of the Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . 566
8.1.2 Power in a Frequency Band ................................. 5 6 9
8.1.3 White Noise Processes . .................................... 5 7 0
8.1.4 Power Spectral Density for Two Processes . . . . . . . . . . . . . . . . . . . . . . 572
8.1.5 Exercises: Section 8.1 (1–21) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
8.2 Random Processes and LTI Systems ................................. 5 7 6
8.2.1 Statistical Properties of the LTI System Output . . . . . . . . . . . . . . . . . . . 577xiv Contents
8.2.2 Ideal Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
8.2.3 Signal Plus Noise . ........................................ 5 8 3
8.2.4 Exercises: Section 8.2 (22–38) . . . ............................ 5 8 6
8.3 Discrete-Time Signal Processing .................................... 5 8 9
8.3.1 Random Sequences and LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 591
8.3.2 Random Sequences and Sampling . . . . ......................... 5 9 3
8.3.3 Exercises: Section 8.3 (39–50) . . . ............................ 5 9 5
Appendix A: Statistical Tables ............................................. 5 9 7
Appendix B: Background Mathematics ...................................... 6 0 9
Appendix C: Important Probability Distributions .............................. 6 1 5
Answers to Odd-Numbered Exercises ....................................... 6 2 1
References ............................................................ 6 3 7
Index ................................................................ 6 3 9Contents xv
Introduction: Why Study Probability?
Some of you may enjoy mathematics for its own sake—it is a beautiful subject which provides many
wonderful intellectual challenges. Of course students of philosophy would say the same thing about
their discipline, ditto for students of linguistics, and so on. However, many of us are not satisﬁed just
with aesthetics and mental gymnastics. We want what we’re studying to have some utility, some
applicability to real-world problems. Fortunately, mathematics in general and probability in particu-
lar provide a plethora of tools for answering important professional and societal questions. In this
section, we’ll attempt to provide some preliminary motivation before forging ahead.
The initial development of probability as a branch of mathematics goes back over 300 years, where
it had its genesis in connection with questions involving games of chance. One of the earliest recorded
instances of probability calculation appeared in correspondence between the two very famous
mathematicians, Blaise Pascal and Pierre de Fermat. The issue was which of the following two
outcomes of die-tossing was more favorable to a bettor: (1) getting at least one 6 in four rolls of a fair
die (“fair” here means that each of the six outcomes 1, 2, 3, 4, 5, and 6 is equally likely to occur) or
(2) getting at least one pair of 6s when two fair dice are rolled 24 times in succession. By the end of
Chap. 1, you shouldn’t have any difﬁculty showing that there is a slightly better than 50-50 chance of
(1) occurring, whereas the odds are slightly against (2) occurring.
Games of chance have continued to be a fruitful area for the application of probability methodol-
ogy. Savvy poker players certainly need to know the odds of being dealt various hands, such as a full
house or straight (such knowledge is necessary but not at all sufﬁcient for achieving success in card
games, as such endeavors also involve much psychology). The same holds true for the game of
blackjack. In fact, in 1962 the mathematics professor Edward O. Thorp published the book Beat the
Dealer ; in it he employed probability arguments to show that as cards were dealt sequentially from a
deck, there were situations in which the likelihood of success favored the player rather than the dealer.
Because of this work, casinos changed the way cards were dealt in order to prevent card-counting
strategies from bankrupting them. A recent variant of this is described in the paper “Card Counting in
Continuous Time” ( Journal of Applied Probability , 2012: 184-198), in which the number of decks
utilized is large enough to justify the use of a continuous approximation to ﬁnd an optimal betting
strategy.
In the last few decades, game theory has developed as a signiﬁcant branch of mathematics devoted
to the modeling of competition, cooperation, and conﬂict. Much of this work involves the use of
probability properties, with applications in such diverse ﬁelds as economics, political science, and
biology. However, especially over the course of the last 60 years, the scope of probability applications
has expanded way beyond gambling and games. In this section, we present some contemporary
examples of how probability is being used to solve important problems.
xvii
Software Use in Probability
Modern probability applications often require the use of a calculator or software. Of course, we rely
on machines to perform every conceivable computation from adding numbers to evaluating deﬁnite
integrals. Many calculators and most computer software packages even have built-in functions that
make a number of speciﬁc probability calculations more convenient; we will highlight these through-
out the text. But the real utility of modern software comes from its ability to simulate random
phenomena, which proves invaluable in the analysis of very complicated probability models. We will
introduce the key elements of probability simulation in Sect. 1.7and then revisit simulation in a
variety of settings throughout the book.
Numerous software packages can be used to implement a simulation. We will focus on two:
Matlab and R. Matlab is a powerful engineering software package published by MathWorks; many
universities and technology companies have a license for Matlab. A freeware package called Octave
has been designed to implement the majority of Matlab functions using identical syntax; consult
http://www.gnu.org/software/octave/ . (Readers using Mac OS or Windows rather than GNU/Linux
will ﬁnd links to compatible versions of Octave on this same website.) R is a freeware statistical
software package maintained by a core user group. The R base package and numerous add-ons are
available at http://cran.r-project.org/ .
Throughout this textbook, we will provide side by side Matlab and R code for both probability
computations and simulation. It is not the goal, however, to serve as a primer in either language
(certainly, some prior knowledge of elementary programming is required). Both software packages
have extensive help menus and active online user support groups. Readers interested in a more
thorough treatment of these software packages should consult Matlab Primer by Timothy A. Davis or
The R Book by Michael J. Crawley.
Modern Application of Classic Probability Problems
Thecoupon collector problem has been well known for decades in the probability community. As an
example, suppose each box of a certain type of cereal contains a small toy. The manufacturer of this
cereal has included a total of ten toys in its cereal boxes, with each box being equally likely to yield
one of the ten toys. Suppose you want to obtain a complete set of these toys for a young relative or
friend. Clearly you will have to purchase at least ten boxes, and intuitively it would seem as though
you might have to purchase many more than that. How many boxes would you expect to have to
purchase in order to achieve your goal? Methods from Chap. 4can be used to show that the average
number of boxes required is 10(1 + 1/2 + 1/3 + /C1/C1/C1+ 1/10). If instead there are ntoys, then
nreplaces 10 in this expression. And when nis large, more sophisticated mathematical arguments
yield the approximation n(ln(n) + .58).
The article “A Generalized Coupon Collector Problem” ( Journal of Applied Probability , 2011:
1081-1094) mentions applications of the classic problem to dynamic resource allocation, hashing in
computer science, and the analysis of delays in certain wireless communication channels (in this latter
application, there are nusers, each receiving packets of data from a transmitter). The generalization
considered in the article involves each cereal box containing ddifferent toys with the purchaser then
selecting the least collected toy thus far. The expected number of purchases to obtain a complete
collection is again investigated, with special attention to the case of nbeing quite large. An
application to the wireless communication scenario is mentioned.xviii Introduction: Why Study Probability?
Applications to Business
The article “Newsvendor-Type Models with Decision-Dependent Uncertainty” ( Mathematical
Methods of Operations Research , 2012, published online) begins with an overview of a class of
decision problems involving uncertainty. In the classical newsvendor problem , a seller has to choose
the amount of inventory to obtain at the beginning of a selling season. This ordering decision is made
only once, with no opportunity to replenish inventory during the season. The amount of demand Dis
uncertain (what we will call in Chap. 2arandom variable ). The cost of obtaining inventory is cper
unit ordered, the sale price is rper unit, and any unsold inventory at the end of the season has a
salvage value of vper unit. The optimal policy, that which maximizes expected proﬁt, is easily
characterized in terms of the probability distribution ofD(this distribution speciﬁes how likely it is
that various values of Dwill occur).
In the revenue management problem , there are Sunits of inventory to sell. Each unit is sold for a
price of either r1orr2(r1>r2). During the ﬁrst phase of the selling season, customers arrive who will
buy at the price r2but not at r1. In the second phase, customers arrive who will pay the higher price.
The seller wishes to know how much of the initial inventory should be held in reserve for the second
phase. Again the general form of the optimal policy that maximizes expected proﬁt is easily
determined in terms of the distributions for demands in the two periods. The article cited in the
previous paragraph goes on to consider situations in which the distribution(s) of demand(s) must be
estimated from data and how such estimation affects decision making.
A cornerstone of probabilistic inventory modeling is a general result established more than
50 years ago: Suppose that the amount of inventory of a commodity is reviewed every Ttime periods
to decide whether more should be ordered. Under rather general conditions, it was shown that the
optimal policy—the policy that minimizes the long-run expected cost—is to order nothing if the
current level of inventory is at least an amount sbut to order enough to bring the inventory level up to
an amount Sif the current level is below s. The values of sandSare determined by various costs, the
price of the commodity, and the nature of demand for the commodity (how customer orders and order
amounts occur over time).
The article “A Periodic-Review Base-Stock Inventory System with Sales Rejection” ( Operations
Research , 2011: 742-753) considers a policy appropriate when backorders are possible and lost sales
may occur. In particular, an order is placed every Ttime periods to bring inventory up to some level S.
Demand for the commodity is ﬁlled until the inventory level reaches a sales rejection threshold Mfor
some M<S. Various properties of the optimal values of MandSare investigated.
Applications to the Life Sciences
Examples of the use of probability and probabilistic modeling can be found in many subdisciplines of
the life sciences. For example, Pseudomonas syringae is a bacterium which lives in leaf surfaces. The
article “Stochastic Modeling of Pseudomonas Syringae Growth in the Phyllosphere” ( Mathematical
Biosciences , 2012: 106-116) proposed a probabilistic (synonymous with “stochastic”) model called a
birth and death process with migration to describe the aggregate distribution of such bacteria and
determine the mechanisms which generated experimental data. The topic of birth and death processes
is considered brieﬂy in Chap. 7of our book.
Another example of such modeling appears in the article “Means and Variances in Stochastic
Multistage Cancer Models” ( Journal of Applied Probability , 2012: 590-594). The authors discuss a
widely used model of carcinogenesis in which division of a healthy cell may give rise to a healthy cell
and a mutant cell, whereas division of a mutant cell may result in two mutant cells of the same type or
possibly one of the same types and one with a further mutation. The objective is to obtain anIntroduction: Why Study Probability? xix
expression for the expected number of cells at each stage and also a quantitative assessment of how
much the actual number might deviate from what is expected (that is what “variance” does).
Epidemiology is the branch of medicine and public health that studies the causes and spread of
various diseases. Of particular interest to epidemiologists is how epidemics are propagated in one or
more populations. The general stochastic epidemic model assumes that a newly infected individual is
infectious for a random amount of time having an exponential distribution (this distribution is
discussed in Chap. 3) and during this infectious period encounters other individuals at times
determined by a Poisson process (one of the topics in Chap. 7). The article “The Basic Reproduction
Number and the Probability of Extinction for a Dynamic Epidemic Model” ( Mathematical
Biosciences , 2012: 31-35) considers an extension in which the population of interest consists of a
ﬁxed number of subpopulations. Individuals move between these subpopulations according to a
Markov transition matrix (the subject of Chap. 6) and infectives can only make infectious contact
with members of their current subpopulation. The effect of variation in the infectious period on the
probability that the epidemic ultimately dies out is investigated.
Another approach to the spread of epidemics is based on branching processes . In the simplest such
process, a single individual gives birth to a random number of individuals; each of these in turn gives
birth to a random number of progeny, and so on. The article “The Probability of Containment for
Multitype Branching Process Models for Emerging Epidemics” ( Journal of Applied Probability ,
2011: 173-188) uses a model in which each individual “born” to an existing individual can have one
of a ﬁnite number of severity levels of the disease. The resulting theory is applied to construct a
simulation model of how inﬂuenza spread in rural Thailand.
Applications to Engineering and Operations Research
We want products that we purchase and systems that we rely on (e.g., communication networks,
electric power grids) to be highly reliable—have long lifetimes and work properly during those
lifetimes. Product manufacturers and system designers therefore need to have testing methods that
will assess various aspects of reliability. In the best of all possible worlds, data bearing on reliability
could be obtained under normal operating conditions. However, this may be very time consuming
when investigating components and products that have very long lifetimes. For this reason, there has
been much research on “accelerated” testing methods which induce failure or degradation in a much
shorter time frame. For products that are used only a fraction of the time in a typical day, such as
home appliances and automobile tires, acceleration might entail operating continuously in time but
under otherwise normal conditions. Alternatively, a sample of units could be subjected to stresses
(e.g., temperature, vibration, voltage) substantially more severe than what is usually experienced.
Acceleration can also be applied to entities in which degradation occurs over time—stiffness of
springs, corrosion of metals, and wearing of mechanical components. In all these cases, probability
models must then be developed to relate lifetime behavior under such acceleration to behavior in
more customary situations. The article “Overview of Reliability Testing” ( IEEE Transactions on
Reliability , 2012: 282-291) gives a survey of various testing methodologies and models. The article
“A Methodology for Accelerated Testing by Mechanical Actuation of MEMS Devices” ( Microelec-
tronics Reliability , 2012: 1382-1388) applies some of these ideas in the context of predicting lifetimes
for micro-electro-mechanical systems.
An important part of modern reliability engineering deals with building redundancy into various
systems in order to decrease substantially the likelihood of failure. A k-out-of-n:G system works or is
good only if at least kamongst the nconstituent components work or are good, whereas a k-out-of-n:F
system fails if and only if at least kof the ncomponents fail. The article “Redundancy Issues in
Software and Hardware Systems: An Overview” ( Intl. Journal of Reliability, Quality, and Safety
Engineering , 2011: 61-98) surveys these and various other systems that can improve the performancexx Introduction: Why Study Probability?
of computer software and hardware. The so-called triple modular redundant systems, with 2-out-of-3:
G conﬁguration, are now commonplace (e.g., Hewlett-Packard’s original NonStop server, and a
variety of aero, auto, and rail systems). The article “Reliability of Various 2-Out-of-4:G Redundant
Systems with Minimal Repair” ( IEEE Transactions on Reliability , 2012: 170-179) considers using a
Poisson process with time-varying rate function to model how component failures occur over time so
that the rate of failure increases as a component ages; in addition, a component that fails undergoes
repair so that it can be placed back in service. Several failure modes for combined k-out-of- nsystems
are studied in the article “Reliability of Combined m-Consecutive- k-out-of- n:F and Consecutive- kc-
out-of- n:F Systems” ( IEEE Transactions on Reliability , 2012: 215-219); these have applications in
the areas of infrared detecting and signal processing.
A compelling reason for manufacturers to be interested in reliability information about their
products is that they can establish warranty policies and periods that help control costs. Many
warranties are “one dimensional,” typically characterized by an interval of age (time). However,
some warranties are “two dimensional” in that warranty conditions depend on both age and cumula-
tive usage; these are common in the automotive industry. The article “Effect of Use-Rate on System
Lifetime and Failure Models for 2D Warranty” ( Intl. Journal of Quality and Reliability Management ,
2011: 464-482) describes how certain bivariate probability models for jointly describing the behavior
of time and usage can be used to investigate the reliability of various system conﬁgurations.
The word queue is used chieﬂy by the British to mean “waiting line,” i.e., a line of customers or
other entities waiting to be served or brought into service. The mathematical development of models
for how a waiting line expands and contracts as customers arrive at a service facility, enter service,
and then ﬁnish began in earnest in the middle part of the 1900s and continues unabated today as new
application scenarios are encountered.
For example, the arrival and service of patients at some type of medical unit are often described by
the notation M/M/s , where the ﬁrst Msigniﬁes that arrivals occur according to a Poisson process, the
second Mindicates that the service time of each patient is governed by an exponential probability
distribution, and there are sservers available for the patients. The article “Nurse Stafﬁng in Medical
Units: A Queueing Perspective” ( Operations Research , 2011: 1320-1331) proposes an alternative
closed queueing model in which there are snurses within a single medical unit servicing npatients,
where each patient alternates between requiring assistance and not needing assistance. The perfor-
mance of the unit is characterized by the likelihood that delay in serving a patient needing assistance
will exceed some critical threshold. A stafﬁng rule based on the model and assumptions is developed;
the resulting rule differs signiﬁcantly from the ﬁxed nurse-to-patient stafﬁng ratios mandated by the
state of California.
A variation on the medical unit situation just described occurs in the context of call centers, where
effective management entails a trade-off between operational costs and the quality of service offered
to customers. The article “Stafﬁng Call Centers with Impatient Customers” ( Operations Research ,
2012: 461-474) considers an M/M/squeue in which customers who have to wait for service may
become frustrated and abandon the facility (don’t you sometimes feel like doing that in a doctor’s
ofﬁce?). The behavior of such a system when nis large is investigated, with particular attention to the
stafﬁng principle that relates the number of servers to the square root of the workload offered to the
call center.
The methodology of queueing can also be applied to ﬁnd optimal settings for trafﬁc signals. The
article “Delays at Signalized Intersections with Exhaustive Trafﬁc Control” ( Probability in Engi-
neering and Informational Sciences , 2012: 337-373) utilizes a “polling model,” which entails
multiple queues of customers (corresponding to different trafﬁc ﬂows) served by a single server in
cyclic order. The proposed vehicle-actuated rule is that trafﬁc lights stay green until all lanes within a
group are emptied. The mean trafﬁc delay is studied for a variety of vehicle interarrival-time
distributions in both light-trafﬁc and heavy-trafﬁc situations.Introduction: Why Study Probability? xxi
Suppose two different types of customers, primary and secondary, arrive for service at a facility
where the servers have different service rates. How should customers be assigned to the servers? The
article “Managing Queues with Heterogeneous Servers” ( Journal of Applied Probability , 2011:
435-452) shows that the optimal policy for minimizing mean wait time has a “threshold structure”:
for each server, there is a different threshold such that a primary customer will be assigned to that
server if and only if the queue length of primary customers meets or exceeds the threshold.
Applications to Finance
The most explosive growth in the use of probability theory and methodology over the course of the
last several decades has undoubtedly been in the area of ﬁnance. This has provided wonderful career
opportunities for people with advanced degrees in statistics, mathematics, engineering, and physics
(the son-in-law of one of the authors earned a Ph.D. in mechanical engineering and taught for several
years, but then switched to ﬁnance). Edward O. Thorp, whom we previously met as the man who
ﬁgured out how to beat blackjack, subsequently went on to success in ﬁnance, where he earned much
more money managing hedge funds and giving advice than he could ever have hoped to earn in
academia (those of us in academia love it for the intangible rewards we get—psychic income, if
you will).
One of the central results in mathematical ﬁnance is the Black-Scholes theorem , named after the
two Nobel-prize-winning economists who discovered it. To get the ﬂavor of what is involved here, a
bit of background is needed. Suppose the present price of a stock is $20 per share, and it is known that
at the end of 1 year, the price will either double to $40 or decrease to $10 per share (where those prices
are expressed in current dollars, i.e., taking account of inﬂation over the 1-year period). You can enter
into an agreement, called an option contract, that allows you to purchase yshares of this stock (for any
value y) 1 year from now for the amount cy(again in current dollars). In addition, right now you can
buyxshares of the stock for 20 xwith the objective of possibly selling those shares 1 year from now.
The values xandyare both allowed to be negative; if, for example, xwere negative, then you would
actually be selling shares of the stock now that you would have to purchase at either a cost of $40 per
share or $10 per share 1 year from now. It can then be shown that there is only one value of c,
speciﬁcally 50/3, for which the gain from this investment activity is 0 regardless of the choices of
xandyand the value of the stock 1 year from now. If cis anything other than 50/3, then there is an
arbitrage , an investment strategy involving choices of xand ythat is guaranteed to result in a
positive gain.
A general result called the Arbitrage Theorem speciﬁes conditions under which a collection of
investments (or bets) has expected return 0 as opposed to there being an arbitrage strategy. The basis
for the Black-Sholes theorem is that the variation in the price of an asset over time is described by a
stochastic process called geometric Brownian motion (see Sect. 7.6). The theorem then speciﬁes a fair
price for an option contract on that asset so that no arbitrage is possible.
Modern quantitative ﬁnance is very complex, and many of the basic ideas are unfamiliar to
most novices (like the authors of this text!). It is therefore difﬁcult to summarize the content of
recently published articles as we have done for some other application areas. But a sampling
of recently published titles emphasizes the role of probability modeling. Articles that appeared in
the 2012 Annals of Finance included “Option Pricing Under a Stressed Beta Model” and “Stochastic
Volatility and Stochastic Leverage”; in the 2012 Applied Mathematical Finance , we found
“Determination of Probability Distribution Measures from Market Prices Using the Method of
Maximum Entropy in the Mean” and “On Cross-Currency Models with Stochastic Volatility and
Correlated Interest Rates”; the 2012 Quantitative Finance yielded “Probability Unbiased Value-at-xxii Introduction: Why Study Probability?
Risk Estimators” and “A Generalized Birth-Death Stochastic Model for High-Frequency Order Book
Dynamics.”
If the application of mathematics to problems in ﬁnance is of interest to you, there are now many
excellent masters-level graduate programs in quantitative ﬁnance. Entrance to these programs
typically requires a very solid background in undergraduate mathematics and statistics (including
especially the course for which you are using this book). Be forewarned, though, that not all
ﬁnancially savvy individuals are impressed with the direction in which ﬁnance has recently moved.
Former Federal Reserve Chairman Paul Volcker was quoted not long ago as saying that the ATM
cash machine was the most signiﬁcant ﬁnancial innovation of the last 20 years; he has been a very
vocal critic of the razzle-dazzle of modern ﬁnance.
Probability in Everyday Life
In the hopefully unlikely event that you do not end up using probability concepts and methods in your
professional life, you still need to face the fact that ideas surrounding uncertainty are pervasive in our
world. We now present some amusing and intriguing examples to illustrate this.
The behavioral psychologists Amos Tversky and Daniel Kahneman spent much of their academic
careers carrying out studies to demonstrate that human beings frequently make logical errors when
processing information about uncertainty (Kahneman won a Nobel prize in economics for his work,
and Tversky would surely have also done so had the awards been given posthumously). Consider the
following variant of one Tversky-Kahneman scenario. Which of the following two statements is more
likely?
(A) Dr. D is a former professor.
(B) Dr. D is a former professor who was accused of inappropriate relations with some students,
investigation substantiated the charges, and he was stripped of tenure.
T-K’s research indicated that many people would regard statement B as being more likely, since it
gives a more detailed explanation of why Dr. D is no longer a professor. However, this is incorrect.
Statement B implies statement A. One of our basic probability rules will be that if one event Bis
contained in another event A(i.e., if Bimplies A), then the smaller event Bis less likely to occur or
have occurred than the larger event A. After all, other possible explanations for A are that Dr. D is
deceased or that he is retired or that he deserted academia for investment banking—all of those plus B
would ﬁgure in to the likelihood of A.
The survey article “Judgment under Uncertainty; Heuristics and Biases” ( Science , 1974: 1124-
1131) by T-K described a certain town served by two hospitals. In the larger hospital about 45 babies
are born each day, whereas about 15 are born each day in the smaller one. About 50% of births are
boys, but of course the percentage ﬂuctuates from day to day. For a 1-year period, each hospital
recorded days on which more than 60% of babies born were boys. Each of a number of individuals
was then asked which of the following statements he/she thought was correct: (1) the larger hospital
recorded more such days, (2) the smaller hospital recorded more such days, or (3) the number of such
days was about the same for the two hospitals. Of the 95 participants, 21 chose (1), 21 chose (2), and
53 chose (3). In Chap. 5we present a general result which implies that the correct answer is in fact (2),
because the sample percentage is less likely to stray from the true percentage (in this case about 50%)
when the sample size is larger rather than small.
In case you think that mistakes of this sort are made only by those who are unsophisticated or
uneducated, here is yet another T-K scenario. Each of a sample of 80 physicians was presented with
the following information on treatment for a particular disease:Introduction: Why Study Probability? xxiii
With surgery, 10% will die during treatment, 32% will die within a year, 66% will die within 5 years. With
radiation, 0% will die during treatment, 23% will die within a year, 78% will die within 5 years.
Each of the 87 physicians in a second sample was presented with the following information:
With surgery, 90% will survive the treatment, 68% will survive at least 1 year, and 34% will survive at least
5 years. With radiation, 100% will survive the treatment, 77% will survive at least 1 year, and 22% will survive
at least 5 years.
When each physician was asked to indicate whether he/she would recommend surgery or radiation
based on the supplied information, 50% of those in the ﬁrst group said surgery whereas 84% of those
in the second group said surgery.
The distressing thing about this conclusion is that the information provided to the ﬁrst group of
physicians is identical to that provided to the second group, but described in a slightly different way.
If the physicians were really processing information rationally, there should be no signiﬁcant
difference between the two percentages.
It would be hard to ﬁnd a book containing even a brief exposition of probability that did not
contain examples or exercises involving coin tossing. Many such scenarios involve tossing a “fair”
coin, one that is equally likely to result in H (head side up) or T (tail side up) on any particular toss.
Are real coins actually fair, or is there a bias of some sort? Various analyses have shown that the result
of a coin toss is predicable at least to some degree if initial conditions (position, velocity, angular
momentum) are known. In practice, most people who toss coins (e.g., referees in a football game
trying to determine which team will kick off and which will receive) are not conversant in the physics
of coin tossing. The mathematician and statistician Persi Diaconis, who was a professional magician
for 10 years prior to earning his Ph.D. and mastered many coin and card tricks, has engaged in
ongoing collaboration with other researchers to study coin tossing. One result of these investigations
was the conclusion based on physics that for a caught coin, there is a slight bias toward heads—about
.51 versus .49. It is not, however, clear under which real-world circumstances this or some other bias
will occur.
Simulation of fair-coin tossing can be done using a random number generator available in many
software packages (about which we’ll say more shortly). If the resulting random number is between
0 and .5, we say that the outcome of the toss was H, and if the number is between .5 and 1, then a T
occurred (there is an obvious modiﬁcation of this to incorporate bias). Now consider the following
sequence of 200 Hs and Ts:
THTHTTTHTTTTTHTHTTTHTTHHHTHHTHTHTHTTTTHHTTHHTTHHHT
HHHTTHHHTTTHHHTHHHHTTTHTHTHHHHTHTTTHHHTHHTHTTTHHTH
HHTHHHHTTHTHHTHHHTTTHTHHHTHHTTTHHHTTTTHHHTHTHHHHTH
TTHHTTTTHTHTHTTHTHHTTHTTTHTTTTHHHHTHTHHHTTHHHHHTHH
Did this sequence result from actually tossing a fair coin (equivalently, using computer simulation
as described), or did it come from someone who was asked to write down a sequence of 200 Hs and Ts
that he/she thought would come from tossing a fair coin? One way to address this question is to focus
on the longest run of Hs in the sequence of tosses. This run is of length 4 for the foregoing sequence.
Probability theory tells us that the expected length of the longest run in a sequence of nfair-coin
tosses is approximately log 2(n)/C02/3. For n= 200, this formula gives an expected longest run of
length about 7. It can also be shown that there is less than a 10% chance that the longest run will have
a length of 4 or less. This suggests that the given sequence is ﬁctitious rather than real, as in fact was
the case; see the very nice expository article “The Longest Run of Heads” ( Mathematics Magazine ,
1990, 196-207).
As another example, consider giving a fair coin to each of the two authors of this textbook. Carlton
tosses his coin repeatedly until obtaining the sequence HTT. Devore tosses his coin repeatedly until
the sequence HTH is observed. Is Carlton’s expected number of tosses to obtain his desired sequencexxiv Introduction: Why Study Probability?
the same as Devore’s, or is one expected number of tosses smaller than the other? Most students to
whom we have asked these questions initially answered that the two expected numbers should be the
same. But this is not true. Some rather tricky probability arguments can be used to show that Carlton’s
expected number of tosses is eight, whereas Devore expects to have to make ten tosses. Very
surprising, no? A bit of intuition makes this more plausible. Suppose Carlton merrily tosses away
until at some point he has just gotten HT. So he is very excited, thinking that just one more toss will
enable him to stop tossing the coin and move on to some more interesting pursuit. Unfortunately his
hopes are dashed because the next toss is an H. However, all is not lost, as even though he must
continue tossing, at this point he is partway toward reaching his goal of HTT. If Devore sees HT at
some point and gets excited by light at the end of the tunnel but then is crushed by the appearance of a
T rather than an H, he essentially has to start over again from scratch. The charming nontechnical
book Probabilities :The Little Numbers That Rule Our Lives by Peter Olofsson has more detail on this
and other probability conundrums.
One of the all-time classic probability puzzles that stump most people is called the Birthday
Problem. Consider a group of individuals, all of whom were born in the same year (one that did not
have a February 29). If the group size is 400, how likely is it that at least two members of the group
share the same birthday? Hopefully a moment’s reﬂection will bring you to the realization that a
shared birthday here is a sure thing (100% chance), since there are only 365 possible birthdays for the
400 people. On the other hand, it is intuitively quite unlikely that there will be a shared birthday if the
group size is only ﬁve; in this case we would expect that all ﬁve individuals would have different
birthdays.
Clearly as the group size increases, it becomes more likely that two or more individuals will have
the same birthday. So how large does the group size have to be in order for it to be more likely than
not that at least two people share a birthday (i.e., that the likelihood of a shared birthday is more than
50%)? Which one of the following four group-size categories do you believe contains the correct
answer to this question?
(1) At least 100 (2) At least 50 but less than 100
(3) At least 25 but less than 50 (4) Fewer than 25
When we have asked this of students in our classes, a substantial majority opted for the ﬁrst two
categories. Very surprisingly, the correct answer is category (4). In Chapter 1we will show that with
as few as 23 people in the group, it is a bit more likely than not that at least two group members will
have the same birthday.
Two people having the same birthday implies that they were born within 24 h of one another, but
the converse is not true; e.g., one person might be born just before midnight on a particular day and
another person just after midnight on the next day. This implies that it is more likely that two people
will have been born within 24 h of one another than it is that they have the same birthday. It follows
that a smaller group size than 23 is needed to make it more likely than not that at least two people will
have been born within 24 h of one another. In Sect. 4.9we show how this group size can be
determined.
Two people in a group having the same birthday is an example of a coincidence, an accidental and
seemingly surprising occurrence of events. The fact that even for a relatively small group size it is
more likely than not that this coincidence will occur should suggest that coincidences are often not as
surprising as they might seem. This is because even if a particular coincidence (e.g., “graduated from
the same high school” or “visited the same small town in Croatia”) is quite unlikely, there are so many
opportunities for coincidences that quite a few are sure to occur.Introduction: Why Study Probability? xxv
Back to the follies of misunderstanding medical information: Suppose the incidence rate of a
particular disease in a certain population is 1 in 1000. The presence of the disease cannot be detected
visually, but a diagnostic test is available. The diagnostic test correctly detects 98% of all diseased
individuals (this is the sensitivity of the test, its ability to detect the presence of the disease), and 93%
of non-diseased individuals test negative for the disease (this is the speciﬁcity of the test, an indicator
of how speciﬁc the test is to the disease under consideration). Suppose a single individual randomly
selected from the population is given the test and the test result is positive. In light of this information,
how likely is it that the individual will have the disease?
First note that if the sensitivity and the speciﬁcity were both 100%, then it would be a sure thing
that the selected individual has the disease. The reason this is not a sure thing is that the test
sometimes makes mistakes. Which one of the following ﬁve categories contains the actual likelihood
of having the disease under the described conditions?
1. At least a 75% chance (quite likely)
2. At least 50% but less than 75% (moderately likely)
3. At least 25% but less than 50% (somewhat likely)
4. At least 10% but less than 25% (rather unlikely)
5. Less than 10% (quite unlikely)
Student responses to this question have overwhelmingly been in categories (1) or (2)—another
case of intuition going awry. The correct answer turns out to be category (5). In fact, even in
light of the positive test result, there is still only a bit more than a 1% chance that the individual is
diseased!
What is the explanation for this counterintuitive result? Suppose we start with 100,000 individuals
from the population. Then we’d expect 100 of those, or 100, to be diseased (from the 1 in 1000
incidence rate) and 99,900 to be disease free. From the 100 we expect to be diseased, we’d expect
98 positive test results (98% sensitivity). And from the 99,900 we expect to be disease free, we’d
expect 7% of those or 6993 to yield positive test results. Thus we expect many more false positives
than true positives. This is because the disease is quite rare and the diagnostic test is rather good but
not stunningly so. (In case you think our sensitivity and speciﬁcity are low, consider a certain D-dimer
test for the presence of a coronary embolism; its sensitivity and speciﬁcity are 88% and 75%,
respectively.)
Later in Chapter 1(Example 1.31) we develop probability rules which can be used to show that the
posterior probability of having the disease conditional on a positive test result is .0138—a bit over
1%. This should make you very cautious about interpreting the results of diagnostic tests. Before you
panic in light of a positive test result, you need to know the incidence rate for the condition under
consideration and both the sensitivity and speciﬁcity of the test. There are also implications for
situations involving detection of something other than a disease. Consider airport procedures that are
used to detect the presence of a terrorist. What do you think is the incidence rate of terrorists at a given
airport, and how sensitive and speciﬁc do you think detection procedures are? The overwhelming
number of positive test results will be false, greatly inconveniencing those who test positive!
Here’s one ﬁnal example of probability applied in everyday life: One of the following columns
contains the value of the closing stock index as of August 8, 2012, for each of a number of countries,
and the other column contains fake data obtained with a random number generator. Just by looking at
the numbers, without considering context, can you tell which column is fake and which is real?xxvi Introduction: Why Study Probability?
China 2264 3058
Japan 8881 9546
Britain 5846 7140
Canada 11,781 6519
Euro area 797 511
Austria 2053 4995
France 3438 2097
Germany 6966 4628
Italy 14,665 8461
Spain 722 598
Norway 480 1133
Russia 1445 4100
Sweden 1080 2594
Turkey 64,699 35,027
Hong Kong 20,066 42,182
India 17,601 3388
Pakistan 14,744 10,076
Singapore 3052 5227
Thailand 1214 7460
Argentina 2459 2159
⋮⋮ ⋮
The key to answering this question is a result called Benford’s Law . Suppose you start reading
through a particular issue of a publication like the New York Times orThe Economist , and each time
you encounter any number (the amount of donations to a particular political candidate, the age of an
actor, the number of members of a union, and so on), you record the ﬁrst digit of that number. Possible
ﬁrst digits are 1, 2, 3, ..., or 9. In the long run, how frequently do you think each of these nine possible
ﬁrst digits will be encountered? Your ﬁrst thought might be that each one should have the same long-
run frequency, 1/9 (roughly 11%). But for many sets of numbers this turns out not to be the case.
Instead the long-run frequency is given by the formula log 10[(x+ 1)/ x], which gives .301, .176, .125,
..., .051, .046 for x=1 ,2 ,3 , ..., 8, 9. Thus a leading digit is much more likely to be 1, 2, or 3 than
7, 8, or 9.
Examination of the foregoing lists of numbers shows that the ﬁrst column conforms much more
closely to Benford’s Law than does the second column. In fact, the ﬁrst column is real, whereas the
second one is fake. For Benford’s Law to be valid, it is generally required that the set of numbers
under consideration span several orders of magnitude. It does not work, for example, with batting
averages of major league baseball players, most of which are between .200 and .299, or with fuel
efﬁciency ratings (miles per gallon) for automobiles, most of which are currently between 15 and 30.
Benford’s Law has been employed to detect fraud in accounting reports, and in particular to detect
fraudulent tax returns. So beware when you ﬁle your taxes next year!
This list of amusing probability appetizers could be continued for quite a while. Hopefully what
we have shown thus far has sparked your interest in knowing more about the discipline. So without
further ado ...Introduction: Why Study Probability? xxvii
Probability1
Probability is the subdiscipline of mathematics that focuses on a systematic study of randomness and
uncertainty. In any situation in which one of a number of possible outcomes may occur, the theory of
probability provides methods for quantifying the chances, or likelihoods, associated with the various
outcomes. The language of probability is constantly used in an informal manner in both written and
spoken contexts. Examples include such statements as “It is likely that the Dow Jones Industrial
Average will increase by the end of the year,” “There is a 50–50 chance that the incumbent will seek
reelection,” “There will probably be at least one section of that course offered next year,” “The odds
favor a quick settlement of the strike,” and “It is expected that at least 20,000 concert tickets will be
sold.” In this chapter, we introduce some elementary probability concepts, indicate how probabilities
can be interpreted, and show how the rules of probability can be applied to compute the chances of
many interesting events. The methodology of probability will then permit us to express in precise
language such informal statements as those given above.
1.1 Sample Spaces and Events
In probability, an experiment refers to any action or activity whose outcome is subject to uncertainty.
Although the word experiment generally suggests a planned or carefully controlled laboratory testing
situation, we use it here in a much wider sense. Thus experiments that may be of interest include
tossing a coin once or several times, selecting a card or cards from a deck, weighing a loaf of bread,
measuring the commute time from home to work on a particular morning, determining blood types
from a group of individuals, or calling people to conduct a survey.
1.1.1 The Sample Space of an Experiment
DEFINITION
The sample space of an experiment, denoted by S, is the set of all possible outcomes of that
experiment.
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_11
Example 1.1 The simplest experiment to which probability applies is one with two possible
outcomes. One such experiment consists of examining a single fuse to see whether it is defective.
The sample space for this experiment can be abbreviated as S¼{N,D}, where Nrepresents not
defective, Drepresents defective, and the braces are used to enclose the elements of a set. Another
such experiment would involve tossing a thumbtack and noting whether it landed point up or point
down, with sample space S¼{U,D}, and yet another would consist of observing the gender of the
next child born at the local hospital, with S¼{M,F}. ■
Example 1.2 If we examine three fuses in sequence and note the result of each examination, then an
outcome for the entire experiment is any sequence of Ns and Ds of length 3, so
S¼{NNN ,NND ,NDN ,NDD ,DNN ,DND ,DDN ,DDD }
If we had tossed a thumbtack three times, the sample space would be obtained by replacing NbyUin
Sabove. A similar notational change would yield the sample space for the experiment in which the
genders of three newborn children are observed. ■
Example 1.3 Two gas stations are located at a certain intersection. Each one has six gas pumps.
Consider the experiment in which the number of pumps in use at a particular time of day is observed
for each of the stations. An experimental outcome speciﬁes how many pumps are in use at the ﬁrst
station and how many are in use at the second one. One possible outcome is (2, 2), another is (4, 1),
and yet another is (1, 4). The 49 outcomes in Sare displayed in the accompanying table.
First stationSecond station
0 123456
0 (0, 0) (0, 1) (0, 2) (0, 3) (0, 4) (0, 5) (0, 6)
1 (1, 0) (1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (1, 6)
2 (2, 0) (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (2, 6)
3 (3, 0) (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (3, 6)
4 (4, 0) (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (4, 6)
5 (5, 0) (5, 1) (5, 2) (5, 3) (5, 4) (5, 5) (5, 6)
6 (6, 0) (6, 1) (6, 2) (6, 3) (6, 4) (6, 5) (6, 6)
The sample space for the experiment in which a six-sided die is thrown twice results from deleting the
0 row and 0 column from the table, giving 36 outcomes. ■
Example 1.4 A reasonably large percentage of C++ programs written at a particular company
compile on the ﬁrst run, but some do not. Suppose an experiment consists of selecting and compiling
C++ programs at this location until encountering a program that compiles on the ﬁrst run. Denote a
program that compiles on the ﬁrst run by S(for success) and one that doesn ’td os ob y F(for failure).
Although it may not be very likely, a possible outcome of this experiment is that the ﬁrst 5 (or 10 or
20 or ...) are Fs and the next one is an S. That is, for any positive integer nwe may have to examine
nprograms before seeing the ﬁrst S. The sample space is S¼{S,FS,FFS,FFFS ,...}, which contains
an inﬁnite number of possible outcomes. The same abbreviated form of the sample space is
appropriate for an experiment in which, starting at a speciﬁed time, the gender of each newborn
infant is recorded until the birth of a female is observed. ■2 1 Probability
1.1.2 Events
In our study of probability, we will be interested not only in the individual outcomes of Sbut also in
any collection of outcomes from S.
DEFINITION
Anevent is any collection (subset) of outcomes contained in the sample space S. An event is
said to be simple if it consists of exactly one outcome and compound if it consists of more than
one outcome.
When an experiment is performed, a particular event Ais said to occur if the resulting experimen-
tal outcome is contained in A. In general, exactly one simple event will occur, but many compound
events will occur simultaneously.
Example 1.5 Consider an experiment in which each of three vehicles taking a particular freeway exit
turns left ( L) or right ( R) at the end of the off-ramp. The eight possible outcomes that comprise the
sample space are LLL,RLL,LRL,LLR,LRR,RLR,RRL, and RRR. Thus there are eight simple events,
among which are E1¼{LLL} and E5¼{LRR}. Some compound events include
A¼{RLL,LRL,LLR}¼the event that exactly one of the three vehicles turns right
B¼{LLL,RLL,LRL,LLR}¼the event that at most one of the vehicles turns right
C¼{LLL,RRR}¼the event that all three vehicles turn in the same direction
Suppose that when the experiment is performed, the outcome is LLL. Then the simple event E1has
occurred and so also have the events BandC(but not A). ■
Example 1.6 (Example 1.3 continued) When the number of pumps in use at each of two six-pump
gas stations is observed, there are 49 possible outcomes, so there are 49 simple events: E1¼{(0, 0)},
E2¼{(0, 1)}, ...,E49¼{(6, 6)}. Examples of compound events are
A¼{(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)} ¼the event that the number of pumps in use is
the same for both stations
B¼{(0, 4), (1, 3), (2, 2), (3, 1), (4, 0)} ¼the event that the total number of pumps in use is four
C¼{(0, 0), (0, 1), (1, 0), (1, 1)} ¼the event that at most one pump is in use at each station ■
Example 1.7 (Example 1.4 continued) The sample space for the program compilation experiment
contains an inﬁnite number of outcomes, so there are an inﬁnite number of simple events. Compound
events include
A¼{S,FS,FFS}¼the event that at most three programs are examined
B¼{S,FFS,FFFFS }¼the event that exactly one, three, or ﬁve programs are examined
C¼{FS,FFFS ,FFFFFS ,...}¼the event that an even number of programs are examined ■1.1 Sample Spaces and Events 3
1.1.3 Some Relations from Set Theory
An event is nothing but a set, so relationships and results from elementary set theory can be used to
study events. The following operations will be used to construct new events from given events.
DEFINITION
1. The complement of an event A, denoted by A0, is the set of all outcomes in Sthat are not
contained in A.
2. The intersection of two events AandB, denoted by A\Band read “ A and B ,” is the event
consisting of all outcomes that are in both A and B .
3. The union of two events AandB, denoted by A[Band read “ Ao rB ,” is the event
consisting of all outcomes that are either in A or in B or in both events (so that the union
includes outcomes for which both AandBoccur as well as outcomes for which exactly one
occurs)—that is, all outcomes in at least one of the events.
Example 1.8 (Example 1.3 continued) For the experiment in which the number of pumps in use at a
single six-pump gas station is observed, let A¼{0, 1, 2, 3, 4}, B¼{3, 4, 5, 6}, and C¼{1, 3, 5}.
Then
A[B¼0;1;2;3;4;5;6 fg ¼SA[C¼0;1;2;3;4;5 fg
A\B¼3;4fg A\C¼1;3fg A0¼5;6fg A[CðÞ0¼6fg ■
Example 1.9 (Example 1.4 continued) In the program compilation experiment, deﬁne A,B, and Cas
in Example 1.7. Then
A[B¼{S,FS,FFS,FFFFS }
A\B¼{S,FFS}
A0¼{FFFS ,FFFFS ,FFFFFS ,...}
and
C0¼{S,FFS,FFFFS ,...}¼the event that an odd number of programs are examined ■
The complement, intersection, and union operators from set theory correspond to the not,and, and
oroperators from computer science. Readers with prior programming experience may be aware of an
important relationship between these three operators, ﬁrst discovered by the nineteenth-century
British mathematician Augustus De Morgan.
DE MORGAN ’S LAWS
LetAandBbe two events in the sample space of some experiment. Then
1. (A[B)0¼A0\B0
2. (A\B)0¼A0[B0
De Morgan ’s laws state that the complement of a union is an intersection of complements, and the
complement of an intersection is a union of complements.4 1 Probability
Sometimes AandBhave no outcomes in common, so that the intersection of AandBcontains no
outcomes (see Exercise 11).
DEFINITION
When AandBhave no outcomes in common, they are said to be disjoint ormutually exclusive
events. Mathematicians write this compactly as A\B¼∅, where ∅denotes the event
consisting of no outcomes whatsoever (the “null” or “empty” event).
Example 1.10 A small city has three automobile dealerships: a GM dealer selling Chevrolets and
Buicks; a Ford dealer selling Fords and Lincolns; and a Chrysler dealer selling Jeeps and Chryslers. If
an experiment consists of observing the brand of the next car sold, then the events A¼{Chevrolet,
Buick} and B¼{Ford, Lincoln} are mutually exclusive because the next car sold cannot be both a
GM product and a Ford product. ■
Venn diagrams are often used to visually represent samples spaces and events. To construct a
Venn diagram, draw a rectangle whose interior will represent the sample space S. Then any event Ais
represented as the interior of a closed curve (often a circle) contained in S. Figure 1.1shows examples
of Venn diagrams.
The operations of union and intersection can be extended to more than two events. For any three
events A,B, and C, the event A\B\Cis the set of outcomes contained in allthree events, whereas
A[B[Cis the set of outcomes contained in at least one of the three events. A collection of several
events is said to be mutually exclusive (or pairwise disjoint) if no two events have any outcomes in
common.
1.1.4 Exercises: Section 1.1(1–12)
1. Ann and Bev have each applied for several jobs at a local university. Let Abe the event that Ann
is hired and let Bbe the event that Bev is hired. Express in terms of AandBthe events
(a) Ann is hired but not Bev.
(b) At least one of them is hired.
(c) Exactly one of them is hired.
2. Two voters, Al and Bill, are each choosing between one of three candidates—1, 2, and 3—who
are running for city council. An experimental outcome speciﬁes both Al ’s choice and Bill ’s
choice, e.g., the pair (3,2).
(a) List all elements of S.
(b) List all outcomes in the event Athat Al and Bill make the same choice.
(c) List all outcomes in the event Bthat neither of them votes for candidate 2.AAB AB AB ABabcde
Fig. 1.1 Venn diagrams. ( a) Venn diagram of events AandB(b) Shaded region is A\B(c) Shaded region is A[B
(d) Shaded region is A0(e) Mutually exclusive events1.1 Sample Spaces and Events 5
3. Four universities—1, 2, 3, and 4—are participating in a holiday basketball tournament. In the ﬁrst
round, 1 will play 2 and 3 will play 4. Then the two winners will play for the championship, and
the two losers will also play. One possible outcome can be denoted by 1324: 1 beats 2 and 3 beats
4 in ﬁrst-round games, and then 1 beats 3 and 2 beats 4.
(a) List all outcomes in S.
(b) Let Adenote the event that 1 wins the tournament. List outcomes in A.
(c) Let Bdenote the event that 2 gets into the championship game. List outcomes in B.
(d) What are the outcomes in A[Band in A\B? What are the outcomes in A0?
4. Suppose that vehicles taking a particular freeway exit can turn right ( R), turn left ( L), or go
straight ( S). Consider observing the direction for each of three successive vehicles.
(a) List all outcomes in the event Athat all three vehicles go in the same direction.
(b) List all outcomes in the event Bthat all three vehicles take different directions.
(c) List all outcomes in the event Cthat exactly two of the three vehicles turn right.
(d) List all outcomes in the event Dthat exactly two vehicles go in the same direction.
(e) List the outcomes in D0,C[D, and C\D.
5. Three components are connected to form a system as shown in the accompanying diagram.
Because the components in the 2–3 subsystem are connected in parallel, that subsystem will
function if at least one of the two individual components functions. For the entire system to
function, component 1 must function and so must the 2–3 subsystem.
2
1
3
The experiment consists of determining the condition of each component: S(success) for a
functioning component and F(failure) for a nonfunctioning component.
(a) What outcomes are contained in the event Athat exactly two out of the three components
function?
(b) What outcomes are contained in the event Bthat at least two of the components function?
(c) What outcomes are contained in the event Cthat the system functions?
(d) List outcomes in C0,A[C,A\C,B[C, and B\C.
6. Each of a sample of four home mortgages is classiﬁed as ﬁxed rate ( F) or variable rate ( V).
(a) What are the 16 outcomes in S?
(b) Which outcomes are in the event that exactly three of the selected mortgages are ﬁxed rate?
(c) Which outcomes are in the event that all four mortgages are of the same type?
(d) Which outcomes are in the event that at most one of the four is a variable-rate mortgage?
(e) What is the union of the events in parts (c) and (d), and what is the intersection of these two
events?
(f) What are the union and intersection of the two events in parts (b) and (c)?
7. A family consisting of three persons— A,B, and C—belongs to a medical clinic that always has a
doctor at each of stations 1, 2, and 3. During a certain week, each member of the family visits the
clinic once and is assigned at random to a station. The experiment consists of recording the
station number for each member. One outcome is (1, 2, 1) for Ato station 1, Bto station 2, and
Cto station 1.
(a) List the 27 outcomes in the sample space.
(b) List all outcomes in the event that all three members go to the same station.
(c) List all outcomes in the event that all members go to different stations.
(d) List all outcomes in the event that no one goes to station 2.6 1 Probability
8. A college library has ﬁve copies of a certain text on reserve. Two copies (1 and 2) are ﬁrst
printings, and the other three (3, 4, and 5) are second printings. A student examines these books in
random order, stopping only when a second printing has been selected. One possible outcome is
5, and another is 213.
(a) List the outcomes in S.
(b) Let Adenote the event that exactly one book must be examined. What outcomes are in A?
(c) Let Bbe the event that book 5 is the one selected. What outcomes are in B?
(d) Let Cbe the event that book 1 is not examined. What outcomes are in C?
9. An academic department has just completed voting by secret ballot for a department head. The
ballot box contains four slips with votes for candidate Aand three slips with votes for candidate
B. Suppose these slips are removed from the box one by one.
(a) List all possible outcomes.
(b) Suppose a running tally is kept as slips are removed. For what outcomes does Aremain
ahead of Bthroughout the tally?
10. A construction ﬁrm is currently working on three different buildings. Let Aidenote the event that
theith building is completed by the contract date. Use the operations of union, intersection, and
complementation to describe each of the following events in terms of A1,A2, and A3, draw a Venn
diagram, and shade the region corresponding to each one.
(a) At least one building is completed by the contract date.
(b) All buildings are completed by the contract date.
(c) Only the ﬁrst building is completed by the contract date.
(d) Exactly one building is completed by the contract date.
(e) Either the ﬁrst building or both of the other two buildings are completed by the contract date.
11. Use Venn diagrams to verify De Morgan ’s laws:
(a) ( A[B)0¼A0\B0
(b) ( A\B)0¼A0[B0
12. (a) In Example 1.10, identify three events that are mutually exclusive.
(b) Suppose there is no outcome common to all three of the events A,B, and C. Are these three
events necessarily mutually exclusive? If your answer is yes, explain why; if your answer is
no, give a counterexample using the experiment of Example 1.10.
1.2 Axioms, Interpretations, and Properties of Probability
Given an experiment and its sample space S, the objective of probability is to assign to each event Aa
number P(A), called the probability of the event A, which will give a precise measure of the chance
thatAwill occur. To ensure that the probability assignments will be consistent with our intuitive
notions of probability, all assignments should satisfy the following axioms (basic properties) of
probability.
AXIOM 1
For any event A,P(A)/C210.1.2 Axioms, Interpretations, and Properties of Probability 7
AXIOM 2
P(S)¼1.
AXIOM 3
IfA1,A2,A3,...is an inﬁnite collection of disjoint events, then
PA 1[A2[A3[/C1/C1/C1 ðÞ ¼X1
i¼1PA iðÞ
Axiom 1 reﬂects the intuitive notion that the chance of Aoccurring should be nonnegative. The
sample space is by deﬁnition the event that must occur when the experiment is performed ( Scontains
all possible outcomes), so Axiom 2 says that the maximum possible probability of 1 is assigned to
S. The third axiom formalizes the idea that if we wish the probability that at least one of a number of
events will occur and no two of the events can occur simultaneously, then the chance of at least one
occurring is the sum of the chances of the individual events.
You might wonder why the third axiom contains no reference to a ﬁnite collection of disjoint
events. It is because the corresponding property for a ﬁnite collection can be derived from our three
axioms. We want our axiom list to be as short as possible and not contain any property that can be
derived from others on the list.
PROPOSITION
P(∅)¼0, where ∅is the null event. This, in turn, implies that the property contained in
Axiom 3 is valid for a ﬁnite collection of disjoint events.
Proof First consider the inﬁnite collection A1¼∅,A2¼∅,A3¼∅,.... Since∅\∅¼∅, the
events in this collection are disjoint and [Ai¼∅. Axiom 3 then gives
P∅ðÞ ¼X
P∅ðÞ
This can happen only if P(∅)¼0.
Now suppose that A1,A2,...,Akare disjoint events, and append to these the inﬁnite collection
Ak+1¼∅,Ak+2¼∅,Ak+3¼∅,.... Then the events A1,A2,...,Ak,Ak+1,...are disjoint, since
A\∅¼∅for all events. Again invoking Axiom 3,
P[k
i¼1Ai !
¼P[1
i¼1Ai !
¼X1
i¼1PA iðÞ ¼Xk
i¼1PA iðÞ þX1
i¼kþ1PA iðÞ ¼Xk
i¼1PA iðÞ þX1
i¼kþ10¼Xk
i¼1PA iðÞ
as desired. ■
Example 1.11 Consider tossing a thumbtack in the air. When it comes to rest on the ground, either its
point will be up (the outcome U) or down (the outcome D). The sample space for this event is
therefore S¼{U,D}. The axioms specify P(S)¼1, so the probability assignment will be completed
by determining P(U) and P(D). Since UandDare disjoint and their union is S, the foregoing
proposition implies that8 1 Probability
1¼PSðÞ ¼ PUðÞ þ PDðÞ
It follows that P(D)¼1/C0P(U). One possible assignment of probabilities is P(U)¼.5,
P(D)¼.5, whereas another possible assignment is P(U)¼.75, P(D)¼.25. In fact, letting
prepresent any ﬁxed number between 0 and 1, P(U)¼p,P(D)¼1/C0pis an assignment consistent
with the axioms. ■
Example 1.12 Consider testing batteries coming off an assembly line one by one until a battery
having a voltage within prescribed limits is found. The simple events are E1¼{S},E2¼{FS},
E3¼{FFS},E4¼{FFFS },.... Suppose the probability of any particular battery being satisfactory
is .99. Then it can be shown that the probability assignment P(E1)¼.99, P(E2)¼(.01)(.99),
P(E3)¼(.01)2(.99), ...satisﬁes the axioms. In particular, because the Eis are disjoint and S¼E1[
E2[E3[..., Axioms 2 and 3 require that
1¼PSðÞ ¼ PE 1ðÞ þ PE 2ðÞ þ PE 3ð Þþ/C1/C1/C1
¼:99 1þ:01þ:01ðÞ2þ:01ðÞ3þ/C1/C1/C1hi
This can be veriﬁed using the formula for the sum of a geometric series:
aþarþar2þar3þ/C1/C1/C1¼a
1/C0r
However, another legitimate (according to the axioms) probability assignment of the same
“geometric” type is obtained by replacing.99 by any other number pbetween 0 and 1 (and .01 by
1/C0p). ■
1.2.1 Interpreting Probability
Examples 1.11 and 1.12 show that the axioms do not completely determine an assignment of
probabilities to events. The axioms serve only to rule out assignments inconsistent with our intuitive
notions of probability. In the tack-tossing experiment of Example 1.11, two particular assignments
were suggested. The appropriate or correct assignment depends on the nature of the thumbtack and
also on one ’s interpretation of probability. The interpretation that is most often used and most easily
understood is based on the notion of relative frequencies.
Consider an experiment that can be repeatedly performed in an identical and independent fashion,
and let Abe an event consisting of a ﬁxed set of outcomes of the experiment. Simple examples of such
repeatable experiments include the tack-tossing and die-tossing experiments previously discussed. If
the experiment is performed ntimes, on some of the replications the event Awill occur (the outcome
will be in the set A), and on others, Awill not occur. Let n(A) denote the number of replications on
which Adoes occur. Then the ratio n(A)/nis called the relative frequency of occurrence of the event
Ain the sequence of nreplications.
For example, let Abe the event that a package sent within the state of California for 2-day delivery
actually arrives within 1 day. The results from sending ten such packages (the ﬁrst ten replications)
are as follows.
Package # 1 2 3 456789 1 0
DidAoccur? N Y Y Y NNYYNN
Relative frequency of A 0 .5 .667 .75 .6 .5 .571 .625 .556 .51.2 Axioms, Interpretations, and Properties of Probability 9
Figure 1.2ashows how the relative frequency n(A)/nﬂuctuates rather substantially over the course
of the ﬁrst 50 replications. But as the number of replications continues to increase, Fig. 1.2b illustrates
how the relative frequency stabilizes.
More generally, both empirical evidence and mathematical theory indicate that any relative
frequency of this sort will stabilize as the number of replications nincreases. That is, as ngets
arbitrarily large, n(A)/napproaches a limiting value we refer to as the long-run (orlimiting )relative
frequency of the event A. The objective interpretation of probability identiﬁes this limiting relative
frequency with P(A). A formal justiﬁcation of this interpretation is provided by the Law of Large
Numbers , a theorem we ’ll encounter in Chap. 4.
Suppose that probabilities are assigned to events in accordance with their limiting relative
frequencies. Then a statement such as “the probability of a package being delivered within 1 day
of mailing is .6” means that of a large number of mailed packages, roughly 60% will arrive within
1 day. Similarly, if Bis the event that a certain brand of dishwasher will need service while under
warranty, then P(B)¼.1 is interpreted to mean that in the long run 10% of all such dishwashers will
need warranty service. This does notmean that exactly 1 out of every 10 will need service, or exactly
20 out of 200 will need service, because 10 and 200 are not the long run. Such mis-interpretations of
probability as a guarantee on short-term outcomes are at the heart of the infamous gambler ’s fallacy .
This relative frequency interpretation of probability is said to be objective because it rests on a
property of the experiment rather than on any particular individual concerned with the experiment.
For example, two different observers of a sequence of coin tosses should both use the same
probability assignments since the observers have nothing to do with limiting relative frequency.
In practice, this interpretation is not as objective as it might seem, because the limiting relative
frequency of an event will not be known. Thus we will have to assign probabilities based on our
beliefs about the limiting relative frequency of events under study. Fortunately, there are many
experiments for which there will be a consensus with respect to probability assignments.
When we speak of a fair coin, we shall mean P(H)¼P(T)¼.5, and a fair die is one for which
limiting relative frequencies of the six outcomes are all equal, suggesting probability assignments
P(⚀)¼ /C1/C1/C1 ¼ P(⚅)¼1/6.
Because the objective interpretation of probability is based on the notion of limiting frequency, its
applicability is limited to experimental situations that are repeatable. Yet the language of probability
is often used in connection with situations that are inherently unrepeatable. Examples include:
“The chances are good for a peace agreement”; “It is likely that our company will be awarded the00.2.5.6.7
.4.6.81.0
10 20Relative frequency delivered in one day
Relative frequency delivered in one day
30 40
Number of packagesa b
50 0100200 300 400 500
Number of packages600 700 800 900 1000Relative
frequency==9
15.60
Relative
frequency==5
10.50Approaches .6
Fig. 1.2 Behavior of relative frequency: ( a) Initial ﬂuctuation ( b) Long-run stabilization10 1 Probability
contract”; and “Because their best quarterback is injured, I expect them to score no more than
10 points against us.” In such situations we would like, as before, to assign numerical probabilities
to various outcomes and events (e.g., the probability is .9 that we will get the contract). We must
therefore adopt an alternative interpretation of these probabilities. Because different observers may
have different prior information and opinions concerning such experimental situations, probability
assignments may now differ from individual to individual. Interpretations in such situations are thus
referred to as subjective . The book by Winkler listed in the references gives a very readable survey of
several subjective interpretations. Importantly, even subjective interpretations of probability must
satisfy the three axioms (and all properties that follow from the axioms) in order to be valid.
1.2.2 More Probability Properties
COMPLEMENT RULE
For any event A,P(A)¼1/C0P(A0).
Proof Since by deﬁnition of A0,A[A0¼Swhile AandA0are disjoint, 1 ¼P(S)¼P(A[A0)¼
P(A)+P(A0), from which the desired result follows. ■
This proposition is surprisingly useful because there are many situations in which P(A0) is more easily
obtained by direct methods than is P(A).
Example 1.13 Consider a system of ﬁve identical components connected in series, as illustrated in
Fig.1.3.
Denote a component that fails by Fand one that doesn ’t fail by S(for success). Let Abe the event
that the system fails. For Ato occur, at least one of the individual components must fail. Outcomes in
Ainclude SSFSS (1, 2, 4, and 5 all work, but 3 does not), FFSSS , and so on. There are, in fact,
31 different outcomes in A! However, A0, the event that the system works, consists of the single
outcome SSSSS . We will see in Sect. 1.5that if 90% of all these components do not fail and different
components fail independently of one another, then P(A0)¼.95¼.59. Thus P(A)¼1/C0.59¼.41;
so among a large number of such systems, roughly 41% will fail. ■
In general, the Complement Rule is useful when the event of interest can be expressed as “at least
...,” because the complement “less than ...” may be easier to work with. (In some problems, “more
than ...” is easier to deal with than “at most ...”) When you are having difﬁculty calculating P(A)
directly, think of determining P(A0).
PROPOSITION
For any event A,P(A)/C201.12345
Fig. 1.3 A system of ﬁve components connected in series1.2 Axioms, Interpretations, and Properties of Probability 11
This follows from the previous proposition: 1 ¼P(A)+P(A0)/C21P(A), because P(A0)/C210b y
Axiom 1.
When AandBare disjoint, we know that P(A[B)¼P(A)+P(B). How can this union probabil-
ity be obtained when the events are not disjoint?
ADDITION RULE
For any events AandB,
P(A[B)¼P(A)+P(B)/C0P(A\B).
Notice that the proposition is valid even if AandBare disjoint, since then P(A\B)¼0. The key
idea is that, in adding P(A) and P(B), the probability of the intersection A\Bis actually counted
twice, so P(A\B) must be subtracted out.
Proof Note ﬁrst that A[B¼A[(B\A0), as illustrated in Fig. 1.4. Because Aand ( B\A0) are
disjoint, P(A[B)¼P(A)+P(B\A0). But B¼(B\A)[(B\A0) (the union of that part of Bin
Aand that part of Bnot in A). Furthermore, ( B\A) and ( B\A0) are disjoint, so that P(B)¼
P(B\A)+P(B\A0). Combining these results gives
PA[BðÞ ¼ PAðÞ þ PB\A0ðÞ ¼ PAðÞ þ PBðÞ /C0 PA\BðÞ ½/C138 ¼ PAðÞ þ PBðÞ /C0 PA\BðÞ
Example 1.14 In a certain residential suburb, 60% of all households get internet service from the
local cable company, 80% get television service from that company, and 50% get both services from
the company. If a household is randomly selected, what is the probability that it gets at least one of
these two services from the company, and what is the probability that it gets exactly one of the
services from the company?
With A¼{gets internet service from the cable company} and B¼{gets television service from
the cable company}, the given information implies that P(A)¼.6,P(B)¼.8, and P(A\B)¼.5.
The Addition Rule then applies to give
Pgets at least one of these two services from the companyðÞ ¼
PA[BðÞ ¼ P/C0
A/C1
þP/C0
B/C1
/C0P/C0
A\B/C1
¼:6þ:8/C0:5¼:9
The event that a household gets only television service from the company can be written as A0\B,
i.e., (not internet) and television. Now Fig. 1.4implies that
:9¼PA[BðÞ ¼ PAðÞ þ PA0\B ðÞ ¼ :6þPA0\B ðÞ
from which P(A0\B)¼.3. Similarly, P(A\B0)¼P(A[B)/C0P(B)¼.1. This is all illustrated in
Fig.1.5, from which we see that
Pexactly oneðÞ ¼ PA\B0ðÞ þ PA0\B ðÞ ¼ :1þ:3¼:4AB=
Fig. 1.4 Representing A[Bas a union of disjoint events ■12 1 Probability
The probability of a union of more than two events can be computed analogously. For three events
A,B, and C, the result is
PA[B[C ðÞ ¼ PAðÞ þ PBðÞ þ PCðÞ /C0 PA\BðÞ /C0 PA\CðÞ /C0 PB\CðÞ þ PA\B\C ðÞ
This can be seen by examining a Venn diagram of A[B[C, which is shown in Fig. 1.6. When
P(A),P(B), and P(C) are added, outcomes in certain intersections are double counted and the
corresponding probabilities must be subtracted. But this results in P(A\B\C) being subtracted
once too often, so it must be added back. One formal proof involves applying the Addition Rule to
P((A[B)[C), the probability of the union of the two events A[BandC; see Exercise 30. More
generally, a result concerning P(A1[ /C1/C1/C1 [ Ak) can be proved by induction or by other methods. The
pattern of additions and subtractions (or, equivalently, the method of deriving such union probability
formulas) is often called the inclusion–exclusion principle .
1.2.3 Determining Probabilities Systematically
When the number of possible outcomes (simple events) is large, there will be many compound events.
A simple way to determine probabilities for these events that avoids violating the axioms and derived
properties is to ﬁrst determine probabilities P(Ei) for all simple events. These should satisfy
P(Ei)/C210 and ΣiP(Ei)¼1. Then the probability of any compound event Ais computed by adding
together the P(Ei)s for all Eisi nA:
PAðÞ ¼X
allEisi nAPE iðÞ
Example 1.15 During off-peak hours a commuter train has ﬁve cars. Suppose a commuter is twice as
likely to select the middle car (#3) as to select either adjacent car (#2 or #4), and is twice as likely to
select either adjacent car as to select either end car (#1 or #5). Let pi¼P(cariis selected) ¼P(Ei).
Then we have p3¼2p2¼2p4andp2¼2p1¼2p5¼p4. This gives
1¼X
PE iðÞ ¼ p1þ2p1þ4p1þ2p1þp1¼10p1
implying p1¼p5¼.1,p2¼p4¼.2, and p3¼.4. The probability that one of the three middle cars
is selected (a compound event) is then p2+p3+p4¼.8. ■.5.1 .3P(A   B ′) P(A′   B)
Fig. 1.5 Probabilities for Example 1.14 ■
A B
C
Fig. 1.6 A[B[C1.2 Axioms, Interpretations, and Properties of Probability 13
1.2.4 Equally Likely Outcomes
In many experiments consisting of Noutcomes, it is reasonable to assign equal probabilities to all
Nsimple events. These include such obvious examples as tossing a fair coin or fair die once (or any
ﬁxed number of times), or selecting one or several cards from a well-shufﬂed deck of 52. With
p¼P(Ei) for every i,
1¼XN
i¼1PE iðÞ ¼XN
i¼1p¼p/C1Nsop¼1
N
That is, if there are Npossible outcomes, then the probability assigned to each is 1/ N.
Now consider an event A,w i t h N(A) denoting the number of outcomes contained in A. Then
PAðÞ ¼X
EiinAPE iðÞ ¼X
EiinA1
N¼NAðÞ
N
Once we have counted the number Nof outcomes in the sample space, to compute the probability
of any event we must count the number of outcomes contained in that event and take the ratio of the
two numbers. Thus when outcomes are equally likely, computing probabilities reduces to counting.
Example 1.16 When two dice are rolled separately, there are N¼36 outcomes (delete the ﬁrst row
and column from the table in Example 1.3). If both the dice are fair, all 36 outcomes are equally
likely, so P(Ei)¼1/36. Then the event A¼{sum of two numbers is 8} consists of the ﬁve outcomes
(
⚁,⚅), (
⚂,⚄), (⚃,⚃), (⚄,
⚂), and ( ⚅,
⚁), so
PAðÞ ¼NAðÞ
N¼5
36■
The next section of this book investigates counting methods in depth.
1.2.5 Exercises: Section 1.2(13–30)
13. A mutual fund company offers its customers several different funds: a money-market fund,
three different bond funds (short, intermediate, and long-term), two stock funds (moderate and
high-risk), and a balanced fund. Among customers who own shares in just one fund, the
percentages of customers in the different funds are as follows:
Money-market 20% High-risk stock 18%
Short bond 15% Moderate-risk stock 25%
Intermediate bond 10% Balanced 7%
Long bond 5%
A customer who owns shares in just one fund is randomly selected.
(a) What is the probability that the selected individual owns shares in the balanced fund?
(b) What is the probability that the individual owns shares in a bond fund?
(c) What is the probability that the selected individual does not own shares in a stock fund?
14. Consider randomly selecting a student at a certain university, and let Adenote the event that the
selected individual has a Visa credit card and Bbe the analogous event for a MasterCard.
Suppose that P(A)¼.5,P(B)¼.4, and P(A\B)¼.25.14 1 Probability
(a) Compute the probability that the selected individual has at least one of the two types of
cards (i.e., the probability of the event A[B).
(b) What is the probability that the selected individual has neither type of card?
(c) Describe, in terms of AandB, the event that the selected student has a Visa card but not a
MasterCard, and then calculate the probability of this event.
15. A computer consulting ﬁrm presently has bids out on three projects. Let Ai¼{awarded project i},
fori¼1, 2, 3, and suppose that P(A1)¼.22,P(A2)¼.25,P(A3)¼.28,P(A1\A2)¼.11,
P(A1\A3)¼.05,P(A2\A3)¼.07,P(A1\A2\A3)¼.01. Express in words each of the
following events, and compute the probability of each event:
(a) A1[A2
(b) A10\A20[Hint: Use De Morgan ’s Law.]
(c) A1[A2[A3
(d) A10\A20\A30
(e) A10\A20\A3
(f) ( A10\A20)[A3
16. Suppose that 55% of all adults regularly consume coffee, 45% regularly consume soda, and 70%
regularly consume at least one of these two products.
(a) What is the probability that a randomly selected adult regularly consumes both coffee and
soda?
(b) What is the probability that a randomly selected adult doesn ’t regularly consume either of
these two products?
17. Consider the type of clothes dryer (gas or electric) purchased by each of ﬁve different customers
at a certain store.
(a) If the probability that at most one of these customers purchases an electric dryer is .428,
what is the probability that at least two purchase an electric dryer?
(b) If P(all ﬁve purchase gas) ¼.116 and P(all ﬁve purchase electric) ¼.005, what is the
probability that at least one of each type is purchased?
18. An individual is presented with three different glasses of cola, labeled C,D, and P. He is asked
to taste all three and then list them in order of preference. Suppose the same cola has actually
been put into all three glasses.
(a) What are the simple events in this ranking experiment, and what probability would you
assign to each one?
(b) What is the probability that Cis ranked ﬁrst?
(c) What is the probability that Cis ranked ﬁrst and Dis ranked last?
19. Let Adenote the event that the next request for assistance from a statistical software consultant
relates to the SPSS package, and let Bbe the event that the next request is for help with SAS.
Suppose that P(A)¼.30 and P(B)¼.50.
(a) Why is it not the case that P(A)+P(B)¼1?
(b) Calculate P(A0).
(c) Calculate P(A[B).
(d) Calculate P(A0\B0).
20. A box contains six 40-W bulbs, ﬁve 60-W bulbs, and four 75-W bulbs. If bulbs are selected one
by one in random order, what is the probability that at least two bulbs must be selected to obtain
one that is rated 75 W?
21. Human visual inspection of solder joints on printed circuit boards can be very subjective. Part of
the problem stems from the numerous types of solder defects (e.g., pad nonwetting, knee
visibility, voids) and even the degree to which a joint possesses one or more of these defects.1.2 Axioms, Interpretations, and Properties of Probability 15
Consequently, even highly trained inspectors can disagree on the disposition of a particular
joint. In one batch of 10,000 joints, inspector A found 724 that were judged defective, inspector
B found 751 such joints, and 1159 of the joints were judged defective by at least one of the
inspectors. Suppose that one of the 10,000 joints is randomly selected.
(a) What is the probability that the selected joint was judged to be defective by neither of the
two inspectors?
(b) What is the probability that the selected joint was judged to be defective by inspector B but
not by inspector A?
22. A factory operates three different shifts. Over the last year, 200 accidents have occurred at the
factory. Some of these can be attributed at least in part to unsafe working conditions, whereas the
others are unrelated to working conditions. The accompanying table gives the percentage of
accidents falling in each type of accident–shift category.
Shift Unsafe conditions Unrelated to conditions
Day 10% 35%
Swing 8% 20%
Night 5% 22%
Suppose one of the 200 accident reports is randomly selected from a ﬁle of reports, and the shift
and type of accident are determined.
(a) What are the simple events?
(b) What is the probability that the selected accident was attributed to unsafe conditions?
(c) What is the probability that the selected accident did not occur on the day shift?
23. An insurance company offers four different deductible levels—none, low, medium, and high—
for its homeowner ’s policyholders and three different levels—low, medium, and high—for its
automobile policyholders. The accompanying table gives proportions for the various categories
of policyholders who have both types of insurance. For example, the proportion of individuals
with both low homeowner ’s deductible and low auto deductible is .06 (6% of all such
individuals).
Homeowner ’s
Auto N L M H
L .04 .06 .05 .03
M .07 .10 .20 .10
H .02 .03 .15 .15
Suppose an individual having both types of policies is randomly selected.
(a) What is the probability that the individual has a medium auto deductible and a high
homeowner ’s deductible?
(b) What is the probability that the individual has a low auto deductible? A low homeowner ’s
deductible?
(c) What is the probability that the individual is in the same category for both auto and
homeowner ’s deductibles?
(d) Based on your answer in part (c), what is the probability that the two categories are
different?
(e) What is the probability that the individual has at least one low deductible level?
(f) Using the answer in part (e), what is the probability that neither deductible level is low?
24. The route used by a driver in commuting to work contains two intersections with trafﬁc signals.
The probability that he must stop at the ﬁrst signal is .4, the analogous probability for the second16 1 Probability
signal is .5, and the probability that he must stop at one or more of the two signals is .6. What is
the probability that he must stop
(a) At both signals?
(b) At the ﬁrst signal but not at the second one?
(c) At exactly one signal?
25. The computers of six faculty members in a certain department are to be replaced. Two of the
faculty members have selected laptop machines and the other four have chosen desktop
machines. Suppose that only two of the setups can be done on a particular day, and the two
computers to be set up are randomly selected from the six (implying 15 equally likely outcomes;
if the computers are numbered 1, 2, ..., 6, then one outcome consists of computers 1 and
2, another consists of computers 1 and 3, and so on).
(a) What is the probability that both selected setups are for laptop computers?
(b) What is the probability that both selected setups are desktop machines?
(c) What is the probability that at least one selected setup is for a desktop computer?
(d) What is the probability that at least one computer of each type is chosen for setup?
26. Show that if one event Ais contained in another event B(i.e., Ais a subset of B), then
P(A)/C20P(B). [Hint: For such AandB,AandB\A0are disjoint and B¼A[(B\A0), as
can be seen from a Venn diagram.] For general AandB, what does this imply about the
relationship among P(A\B),P(A), and P(A[B)?
27. The three most popular options on a certain type of new car are a built-in GPS ( A), a sunroof ( B),
and an automatic transmission ( C). If 40% of all purchasers request A, 55% request B, 70%
request C, 63% request AorB, 77% request AorC, 80% request BorC, and 85% request Aor
BorC, compute the probabilities of the following events.
(a) The next purchaser will request at least one of the three options.
(b) The next purchaser will select none of the three options.
(c) The next purchaser will request only an automatic transmission and neither of the other two
options.
(d) The next purchaser will select exactly one of these three options.
[Hint:“AorB” is the event that at least one of the two options is requested; try drawing a Venn
diagram and labeling all regions.]
28. A certain system can experience three different types of defects. Let Ai(i¼1, 2, 3) denote the
event that the system has a defect of type i. Suppose that
P(A1)¼.12 P(A2)¼.07 P(A3)¼.05
P(A1[A2)¼.13 P(A1[A3)¼.14
P(A2[A3)¼.10 P(A1\A2\A3)¼.01
(a) What is the probability that the system does not have a type 1 defect?
(b) What is the probability that the system has both type 1 and type 2 defects?
(c) What is the probability that the system has both type 1 and type 2 defects but not a type
3 defect?
(d) What is the probability that the system has at most two of these defects?
29. In Exercise 7, suppose that any incoming individual is equally likely to be assigned to any of
the three stations irrespective of where other individuals have been assigned. What is the
probability that
(a) All three family members are assigned to the same station?
(b) At most two family members are assigned to the same station?
(c) Every family member is assigned to a different station?
30. Apply the proposition involving the probability of A[Bto the union of the two events ( A[B)
andCin order to verify the result for P(A[B[C).1.2 Axioms, Interpretations, and Properties of Probability 17
1.3 Counting Methods
When the various outcomes of an experiment are equally likely (the same probability is assigned to
each simple event), the task of computing probabilities reduces to counting. Equally likely outcomes
arise in many games, including the six sides of a fair die, the two sides of a fair coin, and the 38 slots
of a fair roulette wheel. As mentioned at the end of the last section, if Nis the number of outcomes in a
sample space and N(A) is the number of outcomes contained in an event A, then
PAðÞ ¼NAðÞ
Nð1:1Þ
If a list of the outcomes is available or easy to construct and Nis small, then the numerator and
denominator of Eq. ( 1.1) can be obtained without the beneﬁt of any general counting principles.
There are, however, many experiments for which the effort involved in constructing such a list is
prohibitive because Nis quite large. By exploiting some general counting rules, it is possible to
compute probabilities of the form ( 1.1) without a listing of outcomes. These rules are also useful in
many problems involving outcomes that are not equally likely. Several of the rules developed here
will be used in studying probability distributions in the next chapter.
1.3.1 The Fundamental Counting Principle
Our ﬁrst counting rule applies to any situation in which an event consists of ordered pairs of objects
and we wish to count the number of such pairs. By an ordered pair, we mean that, if O1andO2are
objects, then the pair ( O1,O2) is different from the pair ( O2,O1). For example, if an individual selects
one airline for a trip from Los Angeles to Chicago and a second one for continuing on to New York,
one possibility is (American, United), another is (United, American), and still another is (United,
United).
PROPOSITION
If the ﬁrst element or object of an ordered pair can be selected in n1ways, and for each of these
n1ways the second element of the pair can be selected in n2ways, then the number of pairs is
n1n2.
Example 1.17 A homeowner doing some remodeling requires the services of both a plumbing
contractor and an electrical contractor. If there are 12 plumbing contractors and 9 electrical
contractors available in the area, in how many ways can the contractors be chosen? If we denote
the plumbers by P1,...,P12and the electricians by Q1,...,Q9, then we wish the number of pairs of
the form ( Pi,Qj). With n1¼12 and n2¼9, the proposition yields N¼(12)(9) ¼108 possible ways
of choosing the two types of contractors. ■
In Example 1.17, the choice of the second element of the pair did not depend on which ﬁrst
element was chosen or occurred. As long as there is the same number of choices of the second
element for each ﬁrst element, the proposition above is valid even when the set of possible second
elements depends on the ﬁrst element.18 1 Probability
Example 1.18 A family has just moved to a new city and requires the services of both an obstetrician
and a pediatrician. There are two easily accessible medical clinics, each having two obstetricians and
three pediatricians. The family will obtain maximum health insurance beneﬁts by joining a clinic and
selecting both doctors from that clinic. In how many ways can this be done? Denote the obstetricians
byO1,O2,O3, and O4and the pediatricians by P1,...,P6. Then we wish the number of pairs ( Oi,Pj)
for which OiandPjare associated with the same clinic. Because there are four obstetricians, n1¼4,
and for each there are three choices of pediatrician, so n2¼3. Applying the proposition gives
N¼n1n2¼12 possible choices. ■
If a six-sided die is tossed ﬁve times in succession, then each possible outcome is an ordered
collection of ﬁve numbers such as ( ⚀,
⚂,⚀,
⚁,⚃)o r(⚅,⚄,
⚁,
⚁,
⚁). We will call an ordered
collection of kobjects a k-tuple (so a pair is a 2-tuple and a triple is a 3-tuple). Each outcome of the
die-tossing experiment is then a 5-tuple. The following theorem, called the Fundamental Counting
Principle, generalizes the previous proposition to k-tuples.
FUNDAMENTAL COUNTING PRINCIPLE
Suppose a set consists of ordered collections of kelements ( k-tuples) and that there are n1
possible choices for the ﬁrst element; for each choice of the ﬁrst element, there are n2possible
choices of the second element; ...; for each possible choice of the ﬁrst k/C01 elements, there are
nkchoices of the kth element. Then there are n1n2/C1/C1/C1nkpossible k-tuples.
Example 1.19 (Example 1.17 continued) Suppose the home remodeling job involves ﬁrst purchasing
several kitchen appliances. They will all be purchased from the same dealer, and there are ﬁve dealers
in the area. With the dealers denoted by D1,...,D5, there are N¼n1n2n3¼(5)(12)(9) ¼540
3-tuples of the form ( Di,Pj,Qk), so there are 540 ways to choose ﬁrst an appliance dealer, then a
plumbing contractor, and ﬁnally an electrical contractor. ■
Example 1.20 (Example 1.18 continued) If each clinic has both three specialists in internal medicine
and two general surgeons, there are n1n2n3n4¼(4)(3)(3)(2) ¼72 ways to select one doctor of each
type such that all doctors practice at the same clinic. ■
1.3.2 Tree Diagrams
In many counting and probability problems, a tree diagram can be used to represent pictorially all
the possibilities. The tree diagram associated with Example 1.18 appears in Fig. 1.7. Starting from a
point on the left side of the diagram, for each possible ﬁrst element of a pair a straight-line segment
emanates rightward. Each of these lines is referred to as a ﬁrst-generation branch. Now for any given
ﬁrst-generation branch we construct another line segment emanating from the tip of the branch for
each possible choice of a second element of the pair. Each such line segment is a second-generation
branch. Because there are four obstetricians, there are four ﬁrst-generation branches, and three
pediatricians for each obstetrician yields three second-generation branches emanating from each
ﬁrst-generation branch.1.3 Counting Methods 19
Generalizing, suppose there are n1ﬁrst-generation branches, and for each ﬁrst-generation branch
there are n2second-generation branches. The total number of second-generation branches is then
n1n2. Since the end of each second-generation branch corresponds to exactly one possible pair
(choosing a ﬁrst element and then a second puts us at the end of exactly one second-generation
branch), there are n1n2pairs, verifying our ﬁrst proposition.
The Fundamental Counting Principle can also be illustrated by a tree diagram; simply construct a
more elaborate diagram by adding third-generation branches emanating from the tip of each second
generation, then fourth-generation branches, and so on, until ﬁnally kth-generation branches are
added.
The construction of a tree diagram does not depend on having the same number of second-
generation branches emanating from each ﬁrst-generation branch. If the second clinic had four
pediatricians, then there would be only three branches emanating from two of the ﬁrst-generation
branches and four emanating from each of the other two ﬁrst-generation branches. A tree diagram can
thus be used to represent experiments for which the Fundamental Counting Principle does not apply.
1.3.3 Permutations
So far the successive elements of a k-tuple were selected from entirely different sets (e.g., appliance
dealers, then plumbers, and ﬁnally electricians). In several tosses of a die, the set from which
successive elements are chosen is always { ⚀,
⚁,
⚂,⚃,⚄,⚅}, but the choices are made “with
replacement” so that the same element can appear more than once. If the die is rolled once, there
are obviously 6 possible outcomes; for two rolls, there are 62¼36 possibilities, since we distinguish
(
⚂,⚄) from ( ⚄,
⚂). In general, if kselections are made with replacement from a set of ndistinct
objects (such as the six sides of a die), then the total number of possible outcomes is nk.
We now consider a ﬁxed set consisting of ndistinct elements and suppose that a k-tuple is formed
by selecting successively from this set without replacement , so that an element can appear in at most
one of the kpositions.O1P1
P4
P4
P6P5P5P3P2P2
P3P1
P6O2
O3
O4Fig. 1.7 Tree diagram for
Example 1.1820 1 Probability
DEFINITION
Any ordered sequence of kobjects taken without replacement from a set of ndistinct objects is
called a permutation of size kof the objects. The number of permutations of size kthat can be
constructed from the nobjects is denoted by nPk.
The number of permutations of size kis obtained immediately from the Fundamental Counting
Principle. The ﬁrst element can be chosen in nways; for each of these nways the second element can
be chosen in n/C01 ways; and so on. Finally, for each way of choosing the ﬁrst k/C01 elements, the kth
element can be chosen in n/C0(k/C01)¼n/C0k+ 1 ways, so
nPk¼nn/C01ðÞ n/C02ð Þ/C1/C1/C1/C1/C1 n/C0kþ2 ðÞ n/C0kþ1 ðÞ
Example 1.21 Ten teaching assistants are available for grading papers in a particular course. The
ﬁrst exam consists of four questions, and the professor wishes to select a different assistant to grade
each question (only one assistant per question). In how many ways can assistants be chosen to grade
the exam? Here n¼the number of assistants ¼10 and k¼the number of questions ¼4. The
number of different grading assignments is then 10P4¼(10)(9)(8)(7) ¼5040. ■
Example 1.22 The Birthday Problem . Disregarding the possibility of a February 29 birthday,
suppose a randomly selected individual is equally likely to have been born on any one of the other
365 days. If ten people are randomly selected, what is the probability that all have different birthdays?
Imagine selecting 10 days, with replacement , from the calendar to represent the birthdays of the
ten randomly selected people. One possible outcome of this selection would be (March 31, December
30, ..., September 27, February 12). There are 36510such outcomes. The number of outcomes among
them with no repeated birthdays is
365ðÞ 364ðÞ /C1 /C1 /C1 356ðÞ ¼ 365P10
(any of the 365 calendar days may be selected ﬁrst; if March 31 is chosen, any of the other 364 days is
acceptable for the second selection; and so on). Hence, the probability all ten randomly selected
people have different birthdays equals 365P10/36510¼.883. Equivalently, there ’s only a .117 chance
that at least two people out of these ten will share a birthday. It ’s worth noting that the ﬁrst probability
can be rewritten as
365P10
36510¼365
365/C1364
365/C1/C1/C1356
365
We may think of each fraction as representing the chance the next birthday selected will be
different from all previous ones. (This is an example of conditional probability , the topic of the next
section.)
Now replace 10 with k(i.e., krandomly selected birthdays); what is the smallest kfor which there
is at least a 50–50 chance that two or more people will have the same birthday? Most people
incorrectly guess that we need a very large group of people for this to be true; the most common
guess is that 183 people are required (half the days on the calendar). But the required value of kis
actually much smaller: the probability that krandomly selected people all have different birthdays
equals 365Pk/365k, which not surprisingly decreases as kincreases. Figure 1.8displays this probability
for increasing values of k. As it turns out, the smallest kfor which this probability falls below .5 is just
k¼23. That is, there is less than a 50–50 chance (.4927, to be precise) of 23 randomly selected
people all having different birthdays, and thus a probability.5073 that at least two people in a random
sample of 23 will share a birthday.1.3 Counting Methods 21
The expression for nPkcan be rewritten with the aid of factorial notation. Recall that 7! (read “7
factorial”) is compact notation for the descending product of integers (7)(6)(5)(4)(3)(2)(1). More
generally, for any positive integer m,m!¼m(m/C01)(m/C02)/C1/C1/C1(2)(1). This gives 1! ¼1, and we
also deﬁne 0! ¼1.
Using factorial notation, (10)(9)(8)(7) ¼(10)(9)(8)(7)(6!)/6! ¼10!/6!. More generally,
nPk¼nn/C01ð Þ/C1/C1/C1/C1/C1/C0
n/C0kþ1/C1
¼nn/C01ð Þ/C1/C1/C1/C1/C1 n/C0kþ1 ðÞ n/C0kðÞ n/C0k/C01 ð Þ/C1/C1/C1/C1/C1 2ðÞ1ðÞ
n/C0kðÞ n/C0k/C01 ð Þ/C1/C1/C1/C1/C1 2ðÞ1ðÞ
which becomes
nPk¼n!
n/C0kðÞ !
For example, 9P3¼9!/(9 /C03)!¼9!/6! ¼9/C18/C17/C16!/6! ¼9/C18/C17. Note also that because
0!¼1,nPn¼n!/(n/C0n)!¼n!/0!¼n!/1¼n!, as it should.
1.3.4 Combinations
Often the objective is to count the number of unordered subsets of size kthat can be formed from a set
consisting of ndistinct objects. For example, in bridge it is only the 13 cards in a hand and not the
order in which they are dealt that is important; in the formation of a committee, the order in which
committee members are listed is frequently unimportant.
DEFINITION
Given a set of ndistinct objects, any unordered subset of size kof the objects is called a
combination . The number of combinations of size kthat can be formed from ndistinct objects
will be denoted byn
k/C18/C19
ornCk.70 60 50 40 30 20 10 11.00
0.75
0.50
0.25
0.00P(no shared birthdays among k people)
k
Fig. 1.8 P(no birthday match) in Example 1.22 ■22 1 Probability
The number of combinations of size kfrom a particular set is smaller than the number of
permutations because, when order is disregarded, some of the permutations correspond to the same
combination. Consider, for example, the set { A,B,C,D,E} consisting of ﬁve elements. There are
5P3¼5!/(5 /C03)!¼60 permutations of size 3. There are six permutations of size 3 consisting of the
elements A,B, and Cbecause these three can be ordered 3 /C12/C11¼3!¼6 ways: ( A,B,C), (A,C,B),
(B,A,C), (B,C,A), (C,A,B), and ( C,B,A). But these six permutations are equivalent to the single
combination {A,B,C}. Similarly, for any other combination of size 3, there are 3! permutations, each
obtained by ordering the three objects. Thus,
60¼5P3¼5
3/C18/C19
/C13! so5
3/C18/C19
¼60
3!¼10
These ten combinations are
A;B;C fg A;B;D fg A;B;E fg A;C;D fg A;C;E fg
A;D;E fg B;C;D fg B;C;E fg B;D;E fg C;D;E fg
When there are ndistinct objects, any permutation of size kis obtained by ordering the kunordered
objects of a combination in one of k! ways, so the number of permutations is the product of k! and the
number of combinations. This gives
nCk¼n
k/C18/C19
¼nPk
k!¼n!
k!n/C0kðÞ !
Notice thatn
n/C18/C19
¼1 andn
0/C18/C19
¼1 because there is only one way to choose a set of (all)
nelements or of no elements, andn
1/C18/C19
¼nsince there are nsubsets of size 1.
Example 1.23 A bridge hand consists of any 13 cards selected from a 52-card deck without regard to
order. There are52
13/C18/C19
¼52!=13!/C139! ðÞ different bridge hands, which works out to approximately
635 billion. Since there are 13 cards in each suit, the number of hands consisting entirely of clubs
and/or spades (no red cards) is26
13/C18/C19
¼26!=13!/C113! ðÞ ¼ 10, 400, 600. One of these26
13/C18/C19
hands
consists entirely of spades, and one consists entirely of clubs, so there are26
13/C18/C19
/C02/C20/C21
hands that
consist entirely of clubs and spades with both suits represented in the hand. Suppose a bridge hand is
dealt from a well-shufﬂed deck (i.e., 13 cards are randomly selected from among the 52 possibilities)
and let
A¼{the hand consists entirely of spades and clubs with both suits represented}
B¼{the hand consists of exactly two suits}
The N¼52
13/C18/C19
possible outcomes are equally likely, so1.3 Counting Methods 23
PAðÞ ¼NAðÞ
N¼26
13/C18/C19
/C02
52
13/C18/C19 ¼:0000164
Since there are4
2/C18/C19
¼6 combinations consisting of two suits, of which spades and clubs is one
such combination,
PBðÞ ¼NBðÞ
N¼626
13/C18/C19
/C02/C20/C21
52
13/C18/C19 ¼:0000983
That is, a hand consisting entirely of cards from exactly two of the four suits will occur roughly
once in every 10,000 hands. If you play bridge only once a month, it is likely that you will never be
dealt such a hand. ■
Example 1.24 A university warehouse has received a shipment of 25 printers, of which 10 are laser
printers and 15 are inkjet models. If 6 of these 25 are selected at random to be checked by a particular
technician, what is the probability that exactly 3 of those selected are laser printers (so that the other
3 are inkjets)?
LetD3¼{exactly 3 of the 6 selected are inkjet printers}. Assuming that any particular set of
6 printers is as likely to be chosen as is any other set, we have equally likely outcomes, so P(D3)¼
N(D3)/N, where Nis the number of ways of choosing 6 printers from the 25 and N(D3) is the number
of ways of choosing 3 laser printers and 3 inkjet models. Thus N¼25
6/C18/C19
. To obtain N(D3), think of
ﬁrst choosing 3 of the 15 inkjet models and then 3 of the laser printers. There are15
3/C18/C19
ways of
choosing the 3 inkjet models, and there are10
3/C18/C19
ways of choosing the 3 laser printers; by the
Fundamental Counting Principle, N(D3) is the product of these two numbers. So
PD 3ðÞ ¼ND 3ðÞ
N¼15
3/C18/C19
10
3/C18/C19
25
6/C18/C19 ¼15!
3!12!/C110!
3!7!
25!
6!19!¼:3083
LetD4¼{exactly 4 of the 6 printers selected are inkjet models} and deﬁne D5andD6in an
analogous manner. Notice that the events D3,D4,D5, and D6are disjoint. Thus, the probability that at
least 3 inkjet printers are selected is
PD 3[D4[D5[D6 ðÞ ¼ PD 3ðÞ þ PD 4ðÞ þ PD 5ðÞ þ PD 6ðÞ
¼15
3/C18/C19
10
3/C18/C19
25
6/C18/C19 þ15
4/C18/C19
10
2/C18/C19
25
6/C18/C19 þ15
5/C18/C19
10
1/C18/C19
25
6/C18/C19 þ15
6/C18/C19
10
0/C18/C19
25
6/C18/C19 ¼:8530
■24 1 Probability
Example 1.25 The article “Does Your iPod Really Play Favorites?” ( The Amer. Statistician , 2009:
263-268) investigated the randomness of the iPod ’s shufﬂing process. One professor ’s iPod playlist
contains 100 songs, of which 10 are by the Beatles. Suppose the shufﬂe feature is used to play the
songs in random order. What is the probability that the ﬁrst Beatles song heard is the ﬁfth song
played?
In order for this event to occur, it must be the case that the ﬁrst four songs played are not Beatles
songs (NBs) and that the ﬁfth song is by the Beatles (B). The total number of ways to select the ﬁrst
ﬁve songs is (100)(99)(98)(97)(96), while the number of ways to select these ﬁve songs so that the
ﬁrst four are NBs and the next is a B is (90)(89)(88)(87)(10). The random shufﬂe assumption implies
that every sequence of 5 songs from amongst the 100 has the same chance of being selected as the ﬁrst
5 played, i.e., each outcome (a list of 5 songs) is equally likely. Therefore the desired probability is
P1stB is the 5thsong played/C0/C1
¼90/C189/C188/C187/C110
100/C199/C198/C197/C196¼90P4/C110
100P5¼:0679
Here is an alternative line of reasoning involving combinations. Rather than focusing on selecting
just the ﬁrst 5 songs, think of playing all 100 songs in random order. The number of ways of choosing
10 of these songs to be the Bs (without regard to the order in which they are played) is100
10/C18/C19
. Now
if we choose 9 of the last 95 songs to be Bs, which can be done in95
9/C18/C19
ways, that leaves four NBs
and one B for the ﬁrst ﬁve songs. Finally, there is only one way for these ﬁrst ﬁve songs to start with
four NBs and then follow with a B (remember that we are considering unordered subsets). Thus
P1stB is the 5thsong played/C0/C1
¼95
9/C18/C19
100
10/C18/C19
It is easily veriﬁed that this latter expression is in fact identical to the previous expression for the
desired probability, so the numerical result is again .0679.
By similar reasoning, the probability that one of the ﬁrst ﬁve songs played is a Beatles song is
P(1st B is the 1st or 2nd or 3rd or 4th or 5th song played)
¼99
9/C18/C19
100
10/C18/C19 þ98
9/C18/C19
100
10/C18/C19 þ97
9/C18/C19
100
10/C18/C19 þ96
9/C18/C19
100
10/C18/C19 þ95
9/C18/C19
100
10/C18/C19 ¼:4162
It is thus rather likely that a Beatles song will be one of the ﬁrst ﬁve songs played. Such a
“coincidence” is not as surprising as might ﬁrst appear to be the case. ■
1.3.5 Exercises: Section 1.3(31–49)
31. An ATM personal identiﬁcation number (PIN) consists of a four-digit sequence.
(a) How many different possible PINs are there if there are no restrictions on the possible
choice of digits?
(b) According to a representative at the authors ’local branch of Chase Bank, there are in fact
restrictions on the choice of digits. The following choices are prohibited: (1) all four digits1.3 Counting Methods 25
identical; (2) sequences of consecutive ascending or descending digits, such as 6543;
(3) any sequence starting with 19 (birth years are too easy to guess). So if one of the
PINs in (a) is randomly selected, what is the probability that it will be a legitimate PIN (i.e.,
not be one of the prohibited sequences)?
(c) Someone has stolen an ATM card and knows the ﬁrst and last digits of the PIN are 8 and
1, respectively. He also knows about the restrictions described in (b). If he gets three
chances to guess the middle two digits before the ATM “eats” the card, what is the
probability the thief gains access to the account?
(d) Recalculate the probability in (c) if the ﬁrst and last digits are 1 and 1.
32. The College of Science Council has one student representative from each of the ﬁve science
departments (biology, chemistry, statistics, mathematics, physics). In how many ways can
(a) Both a council president and a vice president be selected?
(b) A president, a vice president, and a secretary be selected?
(c) Two council members be selected for the Dean ’s Council?
33. A friend of ours is giving a dinner party. Her current wine supply includes 8 bottles of zinfandel,
10 of merlot, and 12 of cabernet (she drinks only red wine), all from different wineries.
(a) If she wants to serve 3 bottles of zinfandel and serving order is important, how many ways
are there to do this?
(b) If 6 bottles of wine are to be randomly selected from the 30 for serving, how many ways are
there to do this?
(c) If 6 bottles are randomly selected, how many ways are there to obtain two bottles of each
variety?
(d) If 6 bottles are randomly selected, what is the probability that this results in two bottles of
each variety being chosen?
(e) If 6 bottles are randomly selected, what is the probability that all of them are the same
variety?
34. (a) Beethoven wrote 9 symphonies and Mozart wrote 27 piano concertos. If a university radio
station announcer wishes to play ﬁrst a Beethoven symphony and then a Mozart concerto,
in how many ways can this be done?
(b) The station manager decides that on each successive night (7 days per week), a Beethoven
symphony will be played, followed by a Mozart piano concerto, followed by a Schubert
string quartet (of which there are 15). For roughly how many years could this policy be
continued before exactly the same program would have to be repeated?
35. A stereo store is offering a special price on a complete set of components (receiver, compact
disc player, speakers, turntable). A purchaser is offered a choice of manufacturer for each
component:
Receiver: Kenwood, Onkyo, Pioneer, Sony, Yamaha
Compact disc player: Onkyo, Pioneer, Sony, Panasonic
Speakers: Boston, Inﬁnity, Polk
Turntable: Onkyo, Sony, Teac, Technics
A switchboard display in the store allows a customer to hook together any selection of
components (consisting of one of each type). Use the Fundamental Counting Principle to answer
the following questions:
(a) In how many ways can one component of each type be selected?
(b) In how many ways can components be selected if both the receiver and the compact disc
player are to be Sony?
(c) In how many ways can components be selected if none is to be Sony?26 1 Probability
(d) In how many ways can a selection be made if at least one Sony component is to be
included?
(e) If someone ﬂips switches on the selection in a completely random fashion, what is the
probability that the system selected contains at least one Sony component? Exactly one
Sony component?
36. In ﬁve-card poker, a straight consists of ﬁve cards in adjacent ranks (e.g., 9 of clubs, 10 of hearts,
jack of hearts, queen of spades, king of clubs). Assuming that aces can be high or low, if you are
dealt a ﬁve-card hand, what is the probability that it will be a straight with high card 10? What is
the probability that it will be a straight? What is the probability that it will be a straight ﬂush (all
cards in the same suit)?
37. A local bar stocks 12 American beers, 8 Mexican beers, and 9 German beers. You ask the
bartender to pick out a ﬁve-beer “sampler” for you. Assume the bartender makes the ﬁve
selections at random and without replacement.
(a) What is the probability you get at least four American beers?
(b) What is the probability you get ﬁve beers from the same country?
38. Computer keyboard failures can be attributed to electrical defects or mechanical defects. A
repair facility currently has 25 failed keyboards, 6 of which have electrical defects and 19 of
which have mechanical defects.
(a) How many ways are there to randomly select 5 of these keyboards for a thorough
inspection (without regard to order)?
(b) In how many ways can a sample of 5 keyboards be selected so that exactly 2 have an
electrical defect?
(c) If a sample of 5 keyboards is randomly selected, what is the probability that at least 4 of
these will have a mechanical defect?
39. The statistics department at the authors ’university participates in an annual volleyball tourna-
ment. Suppose that all 16 department members are willing to play.
(a) How many different six-person volleyball rosters could be generated? (That is, how many
years could the department participate in the tournament without repeating the same
six-person team?)
(b) The statistics department faculty consist of 5 women and 11 men. How many rosters
comprising exactly 2 women and 4 men can be generated?
(c) The tournament ’s rules actually require that each team include at least two women. Under
this rule, how many valid teams could be generated?
(d) Suppose this year the department decides to randomly select its six players. What is the
probability the randomly selected team has exactly two women? At least two women?
40. A production facility employs 20 workers on the day shift, 15 workers on the swing shift, and
10 workers on the graveyard shift. A quality control consultant is to select 6 of these workers for
in-depth interviews. Suppose the selection is made in such a way that any particular group of
6 workers has the same chance of being selected as does any other group (drawing 6 slips without
replacement from among 45).
(a) How many selections result in all 6 workers coming from the day shift? What is the
probability that all 6 selected workers will be from the day shift?
(b) What is the probability that all 6 selected workers will be from the same shift?
(c) What is the probability that at least two different shifts will be represented among the
selected workers?
(d) What is the probability that at least one of the shifts will be unrepresented in the sample of
workers?1.3 Counting Methods 27
41. An academic department with ﬁve faculty members narrowed its choice for department head to
either candidate Aor candidate B. Each member then voted on a slip of paper for one of the
candidates. Suppose there are actually three votes for Aand two for B. If the slips are selected for
tallying in random order, what is the probability that Aremains ahead of Bthroughout the vote
count (e.g., this event occurs if the selected ordering is AABAB , but not for ABBAA )?
42. An experimenter is studying the effects of temperature, pressure, and type of catalyst on yield
from a chemical reaction. Three different temperatures, four different pressures, and ﬁve differ-
ent catalysts are under consideration.
(a) If any particular experimental run involves the use of a single temperature, pressure, and
catalyst, how many experimental runs are possible?
(b) How many experimental runs involve use of the lowest temperature and two lowest
pressures?
(c) Suppose that ﬁve different experimental runs are to be made on the ﬁrst day of experimen-
tation. If the ﬁve are randomly selected from among all the possibilities, so that any group of
ﬁve has the same probability of selection, what is the probability that a different catalyst is
used on each run?
43. A box in a certain supply room contains four 40-W lightbulbs, ﬁve 60-W bulbs, and six 75-W
bulbs. Suppose that three bulbs are randomly selected.
(a) What is the probability that exactly two of the selected bulbs are rated 75 W?
(b) What is the probability that all three of the selected bulbs have the same rating?
(c) What is the probability that one bulb of each type is selected?
(d) Suppose now that bulbs are to be selected one by one until a 75-W bulb is found. What is the
probability that it is necessary to examine at least six bulbs?
44. Fifteen telephones have just been received at an authorized service center. Five of these
telephones are cellular, ﬁve are cordless, and the other ﬁve are corded phones. Suppose that
these components are randomly allocated the numbers 1, 2, ..., 15 to establish the order in which
they will be serviced.
(a) What is the probability that all the cordless phones are among the ﬁrst ten to be serviced?
(b) What is the probability that after servicing ten of these phones, phones of only two of the
three types remain to be serviced?
(c) What is the probability that two phones of each type are among the ﬁrst six serviced?
45. Three molecules of type A, three of type B, three of type C, and three of type Dare to be linked
together to form a chain molecule. One such chain molecule is ABCDABCDABCD , and another
isBCDDAAABDBCC .
(a) How many such chain molecules are there? [ Hint: If the three A’s were distinguishable from
one another— A1,A2,A3—and the B’s,C’s, and D’s were also, how many molecules would
there be? How is this number reduced when the subscripts are removed from the A’s?]
(b) Suppose a chain molecule of the type described is randomly selected. What is the probabil-
ity that all three molecules of each type end up next to each other (such as in
BBBAAADDDCCC )?
46. A popular Dilbert cartoon strip (popular among statisticians, anyway) shows an allegedly
“random” number generator produce the sequence 999999 with the accompanying comment,
“That ’s the problem with randomness: you can never be sure.” Most people would agree that
999999 seems less “random” than, say, 703928, but in what sense is that true? Imagine we
randomly generate a six-digit number, i.e., we make six draws with replacement from the digits
0 through 9.
(a) What is the probability of generating 999999?
(b) What is the probability of generating 703928?28 1 Probability
(c) What is the probability of generating a sequence of six identical digits?
(d) What is the probability of generating a sequence with noidentical digits? (Comparing the
answers to (c) and (d) gives some sense of why some sequences feel intuitively more
random than others.)
(e) Here ’s a real challenge: what is the probability of generating a sequence with exactly one
repeated digit?
47. Three married couples have purchased theater tickets and are seated in a row consisting of just six
seats. If they take their seats in a completely random fashion (random order), what is the
probability that Jim and Paula (husband and wife) sit in the two seats on the far left? What is
the probability that Jim and Paula end up sitting next to one another? What is the probability that
at least one of the wives ends up sitting next to her husband?
48. A starting lineup in basketball consists of two guards, two forwards, and a center.
(a) A certain college team has on its roster ﬁve guards, four forwards, and three centers. How
many different starting lineups can be created?
(b) Their opposing team in one particular game has three centers, four guards, four forwards,
and one individual (X) who can play either guard or forward. How many different starting
lineups can the opposing team create? [ Hint: Consider lineups without X, with X as a guard,
and with X as a forward.]
(c) Now suppose a team has 4 guards, 4 forwards, 2 centers, and two players (X and Y) who can
play either guard or forward. If 5 of the 12 players on this team are randomly selected, what
is the probability that they constitute a legitimate starting lineup?
49. Show thatn
k/C18/C19
¼n
n/C0k/C18/C19
. Give an interpretation involving subsets.
1.4 Conditional Probability
The probabilities assigned to various events depend on what is known about the experimental
situation when the assignment is made. Subsequent to the initial assignment, partial information
about or relevant to the outcome of the experiment may become available. Such information may
cause us to revise some of our probability assignments. For a particular event A, we have used P(A)t o
represent the probability assigned to A; we now think of P(A) as the original or “unconditional”
probability of the event A.
In this section, we examine how the information “an event Bhas occurred” affects the probability
assigned to A. For example, Amight refer to an individual having a particular disease in the presence
of certain symptoms. If a blood test is performed on the individual and the result is negative
(B¼negative blood test), then the probability of having the disease will change (it should decrease,
but not usually to zero, since blood tests are not infallible).
Example 1.26 Complex components are assembled in a plant that uses two different assembly lines,
AandA0. Line Auses older equipment than A0, so it is somewhat slower and less reliable. Suppose on
a given day line Ahas assembled 8 components, of which 2 have been identiﬁed as defective ( B) and
6 as nondefective ( B0), whereas A0has produced 1 defective and 9 nondefective components. This
information is summarized in the accompanying table.1.4 Conditional Probability 29
LineCondition
BB0
A 26
A019
Unaware of this information, the sales manager randomly selects 1 of these 18 components for a
demonstration. Prior to the demonstration
PlineAcomponent selected ðÞ ¼ PAðÞ ¼NAðÞ
N¼8
18¼:444
However, if the chosen component turns out to be defective, then the event Bhas occurred, so the
component must have been one of the 3 in the Bcolumn of the table. Since these 3 components are
equally likely among themselves, the probability the component was selected from line A,given that
event B has occurred ,i s
PA, given B ðÞ ¼2
3¼2=18
3=18¼PA\BðÞ
PBðÞð1:2Þ
■
In Eq. ( 1.2), the conditional probability is expressed as a ratio of unconditional probabilities. The
numerator is the probability of the intersection of the two events, whereas the denominator is the
probability of the conditioning event B. A Venn diagram illuminates this relationship (Fig. 1.9).
Given that Bhas occurred, the relevant sample space is no longer Sbut consists of just outcomes in
B, and Ahas occurred if and only if one of the outcomes in the intersection A\Boccurred. So the
conditional probability of Agiven Bshould, logically, be the ratio of the likelihoods of these two
events.
1.4.1 The Definition of Conditional Probability
Example 1.26 demonstrates that when outcomes are equally likely, computation of conditional
probabilities can be based on intuition. When experiments are more complicated, though intuition
may fail us, we want to have a general deﬁnition of conditional probability that will yield intuitive
answers in simple problems. Figure 1.9and Eq. ( 1.2) suggest the appropriate deﬁnition.A
B“conditioning” on event B
B = new “sample
space”A  B = what remains
of event A
Fig. 1.9 Motivating the deﬁnition of conditional probability30 1 Probability
DEFINITION
For any two events AandBwith P(B)>0, the conditional probability of Agiven that Bhas
occurred , denoted P(A|B), is deﬁned by
PA/C12/C12B/C0/C1
¼PA\BðÞ
PBðÞ: ð1:3Þ
Example 1.27 Suppose that of all individuals buying a certain digital camera, 60% include
an optional memory card in their purchase, 40% include an extra battery, and 30% include both
a card and battery. Consider randomly selecting a buyer and let A¼{memory card purchased} and
B¼{battery purchased}. Then P(A)¼.60,P(B)¼.40, and P(both purchased) ¼P(A\B)¼.30.
Given that the selected individual purchased an extra battery, the probability that an optional card was
also purchased is
PA/C12/C12B/C0/C1
¼PA\BðÞ
PBðÞ¼:30
:40¼:75
That is, of all those purchasing an extra battery, 75% purchased an optional memory card.
Similarly,
Pbattery/C12/C12memory card/C0/C1
¼PB/C12/C12A/C0/C1
¼PA\BðÞ
PAðÞ¼:30
:60¼:50
Notice that P(A|B)6¼P(A) and P(B|A)6¼P(B). Notice also that P(A|B)6¼P(B|A): these represent
two different probabilities computed using difference pieces of “given” information. ■
Example 1.28 A news magazine includes three columns entitled “Art” ( A), “Books” ( B), and
“Cinema” ( C). Reading habits of a randomly selected reader with respect to these columns are
Read regularly A B C A \BA \CB \CA \B\C
Probability .14 .23 .37 .08 .09 .13 .05
(See Fig. 1.10 on the next page.)
We thus have
PA/C12/C12B/C0/C1
¼PA\BðÞ
PBðÞ¼:08
:23¼:348
PA/C12/C12B[C/C0/C1
¼PA\B[CðÞðÞ
PB[CðÞ¼:04þ:05þ:03
:47¼:12
:47¼:255
PA/C12/C12reads at least one/C0/C1
¼PA/C12/C12A[B[C/C0/C1
¼PA\A[B[C ðÞðÞ
PA[B[C ðÞ
¼PAðÞ
PA[B[C ðÞ¼:14
:49¼:286
and
PA[B/C12/C12C/C0/C1
¼PA[BðÞ \ C ðÞ
PCðÞ¼:04þ:05þ:08
:37¼:4591.4 Conditional Probability 31
1.4.2 The Multiplication Rule for P(A\B)
The deﬁnition of conditional probability yields the following result, obtained by multiplying both
sides of Eq. ( 1.3)b yP(B).
MULTIPLICATION RULE
P(A\B)¼P(A|B)/C1P(B)
This rule is important because it is often the case that P(A\B) is desired, whereas both P(B) and
P(A|B) can be speciﬁed from the problem description. By reversing the roles of AandB, the
Multiplication Rule can also be written as P(A\B)¼P(B|A)/C1P(A).
Example 1.29 Four individuals have responded to a request by a blood bank for blood donations.
None of them has donated before, so their blood types are unknown. Suppose only type O+ is desired
and only one of the four actually has this type. If the potential donors are selected in random order for
typing, what is the probability that at least three individuals must be typed to obtain the desired type?
Deﬁne B¼{ﬁrst type not O+} and A¼{second type not O+}. Since three of the four potential
donors are not O+, P(B)¼3/4. Given that the ﬁrst person typed is not O+, two of the three
individuals left are not O+, and so P(A|B)¼2/3. The Multiplication Rule now gives
Pat least three individuals are typedðÞ ¼ Pfirst two typed are not O þ ðÞ
¼PA\BðÞ
¼PA/C12/C12B/C0/C1
/C1PBðÞ
¼2
3/C13
4¼6
12
¼:5 ■
The Multiplication Rule is most useful when the experiment consists of several stages in succes-
sion. The conditioning event Bthen describes the outcome of the ﬁrst stage and Athe outcome of the
second, so that P(A|B)—conditioning on what occurs ﬁrst—will often be known. The rule is easily
extended to experiments involving more than two stages. For example,
PA 1\A2\A3 ðÞ ¼ PA 3/C12/C12A1\A2/C0/C1
/C1P/C0
A1\A2/C1
¼PA 3/C12/C12A1\A2/C0/C1
/C1PA 2/C12/C12A1/C0/C1
/C1PA 1ðÞ
where A1occurs ﬁrst, followed by A2, and ﬁnally A3.
Example 1.30 For the blood typing experiment of Example 1.29,.05.02 .03 .07
.04 .08
.20
.51A B
C
Fig. 1.10 Venn diagram for Example 1.28 ■32 1 Probability
Pthird type is O þ ðÞ ¼ Pthird is/C12/C12first isn ’t\second isn ’t/C0/C1
/C1Psecond isn ’t/C12/C12first isn ’t/C0/C1
/C1Pfirst isn ’t ðÞ
¼1
2/C12
3/C13
4¼1
4¼:25■
When the experiment of interest consists of a sequence of several stages, it is convenient to
represent these with a tree diagram. Once we have an appropriate tree diagram, probabilities and
conditional probabilities can be entered on the various branches; this will make repeated use of the
Multiplication Rule quite straightforward.
Example 1.31 A chain of electronics stores sells three different brands of DVD players. Of its DVD
player sales, 50% are brand 1 (the least expensive), 30% are brand 2, and 20% are brand 3. Each
manufacturer offers a 1-year warranty on parts and labor. It is known that 25% of brand 1 ’s DVD
players require warranty repair work, whereas the corresponding percentages for brands 2 and 3 are
20% and 10%, respectively.
1. What is the probability that a randomly selected purchaser has bought a brand 1 DVD player that
will need repair while under warranty?
2. What is the probability that a randomly selected purchaser has a DVD player that will need repair
while under warranty?
3. If a customer returns to the store with a DVD player that needs warranty repair work, what is the
probability that it is a brand 1 DVD player? A brand 2 DVD player? A brand 3 DVD player?
The ﬁrst stage of the problem involves a customer selecting one of the three brands of DVD player.
LetAi¼{brand iis purchased}, for i¼1, 2, 3. Then P(A1)¼.50,P(A2)¼.30, and P(A3)¼.20.
Once a brand of DVD player is selected, the second stage involves observing whether the selected
DVD player needs warranty repair. With B¼{needs repair} and B0¼{doesn ’t need repair}, the
given information implies that P(B|A1)¼.25,P(B|A2)¼.20, and P(B|A3)¼.10.
Brand 2Brand 1
Brand 3Repair
No repair
Repair
No repairNo repairRepair
P(A1) = .50
P(A2) = .30
P(A3) = .20
P(B  A3) = .10P(B   A2) = .20P(B  A1) = .25 
P(B '   A
1) = .75
P(B '   A
2) = .80
P(B'   A
3) = .90P(B  A1)  P(A1) = P(B    A1) = .125
P(B  A2)  P(A2) = P(B    A2) = .060
P(B  A3)  P(A3) = P(B    A3) = .020
P(B) = .205
Fig. 1.11 Tree diagram for Example 1.311.4 Conditional Probability 33
The tree diagram representing this experimental situation is shown in Fig. 1.11. The initial
branches correspond to different brands of DVD players; there are two second-generation branches
emanating from the tip of each initial branch, one for “needs repair” and the other for “doesn ’t need
repair.” The probability P(Ai) appears on the ith initial branch, whereas the conditional probabilities
P(B|Ai) and P(B0|Ai) appear on the second-generation branches. To the right of each second-
generation branch corresponding to the occurrence of B, we display the product of probabilities on
the branches leading out to that point. This is simply the Multiplication Rule in action. The answer to
question 1 is thus P(A1\B)¼P(B|A1)/C1P(A1)¼.125. The answer to question 2 is
PBðÞ ¼ Pbrand 1 and repairðÞ or brand 2 and repairðÞ or brand 3 and repairðÞ ½/C138
¼PA 1\B ðÞ þ P/C0
A2\B/C1
þP/C0
A3\B/C1
¼:125þ:060þ:020¼:205
Finally,
PA 1/C12/C12B/C0/C1
¼PA 1\B ðÞ
PBðÞ¼:125
:205¼:61
PA 2/C12/C12B/C0/C1
¼PA 2\B ðÞ
PBðÞ¼:060
:205¼:29
and
PA 3/C12/C12B/C0/C1
¼1/C0PA 1/C12/C12B/C0/C1
/C0PA 2/C12/C12B/C0/C1
¼:10
Notice that the initial or prior probability of brand 1 is .50, whereas once it is known that the
selected DVD player needed repair, the posterior probability of brand 1 increases to .61. This is
because brand 1 DVD players are more likely to need warranty repair than are the other brands. The
posterior probability of brand 3 is P(A3|B)¼.10, which is much less than the prior probability
P(A3)¼.20. ■
1.4.3 The Law of Total Probability and Bayes ’Theorem
The computation of a posterior probability P(Aj|B) from given prior probabilities P(Ai) and condi-
tional probabilities P(B|Ai) occupies a central position in elementary probability. The general rule for
such computations, which is really just a simple application of the Multiplication Rule, goes back to
the Reverend Thomas Bayes, who lived in the eighteenth century. To state it we ﬁrst need another
result. Recall that events A1,...,Akare mutually exclusive if no two have any common outcomes. The
events are exhaustive ifA1[ /C1/C1/C1 [ Ak¼S, so that one Aimust occur.
LAW OF TOTAL PROBABILITY
LetA1,...,Akbe mutually exclusive and exhaustive events. Then for any other event B,
PBðÞ ¼ PB/C12/C12A1/C0/C1
/C1P/C0
A1/C1
þ/C1/C1/C1þ P/C0
B/C12/C12Ak/C1
/C1P/C0
Ak/C1
¼Xk
i¼1PB/C12/C12Ai/C0/C1
PA iðÞð1:4Þ34 1 Probability
Proof Because the Ais are mutually exclusive and exhaustive, if Boccurs it must be in conjunction
with exactly one of the Ais. That is, B¼(A1andB)o r ...or (AkandB)¼(A1\B)[ /C1/C1/C1 [ (Ak\B),
where the events ( Ai\B) are mutually exclusive. This “partitioning of B” is illustrated in Fig. 1.12.
Thus
PBðÞ ¼Xk
i¼1PA i\B ðÞ ¼Xk
i¼1PB/C12/C12Ai/C0/C1
PA iðÞ
as desired.
An example of the use of Eq. ( 1.4) appeared in answering question 2 of Example 1.31, where
A1¼{brand 1}, A2¼{brand 2}, A3¼{brand 3}, and B¼{repair}.
Example 1.32 A student has three different e-mail accounts. Most of her messages, in fact 70%,
come into account #1, whereas 20% come into account #2 and the remaining 10% into account #3. Of
the messages coming into account #1, only 1% are spam, compared to 2% and 5% for account #2 and
account #3, respectively. What is the student ’s overall spam rate, i.e., what is the probability a
randomly selected e-mail message received by her is spam?
To answer this question, let ’s ﬁrst establish some notation:
Ai¼{message is from account # i} for i¼1, 2, 3; B¼{message is spam}
The given percentages imply that
PA 1ðÞ ¼ :70,P/C0
A2/C1
¼:20,P/C0
A3/C1
¼:10
PB/C12/C12A1/C0/C1
¼:01,P/C0
B/C12/C12A2/C1
¼:02,P/C0
B/C12/C12A3/C1
¼:05
Now it ’s simply a matter of substituting into the equation for the Law of Total Probability:
PBðÞ ¼ :01ðÞ :70ðÞ þ :02ðÞ :20ðÞ þ :05ðÞ :10ðÞ ¼ :016
In the long run, 1.6% of her messages will be spam. ■
BAYES ’THEOREM
LetA1,...,Akbe a collection of mutually exclusive and exhaustive events with P(Ai)>0 for
i¼1,...,k. Then for any other event Bfor which P(B)>0,
PA j/C12/C12B/C0/C1
¼PA j\B/C0/C1
PBðÞ¼PB/C12/C12Aj/C0/C1
PA j/C0/C1
Xk
i¼1PB/C12/C12Ai/C0/C1
PA iðÞj¼1,...,k ð1:5ÞA1
A2A4A3B
Fig. 1.12 Partition of Bby mutually exclusive and exhaustive Ais ■1.4 Conditional Probability 35
The transition from the second to the third expression in Eq. ( 1.5) rests on using the Multiplication
Rule in the numerator and the Law of Total Probability in the denominator.
The proliferation of events and subscripts in Eq. ( 1.5) can be a bit intimidating to probability
newcomers. When k¼2, so that the partition of Sconsists of just A1¼AandA2¼A0, Bayes ’
Theorem becomes
PA/C12/C12B/C0/C1
¼PAðÞPB/C12/C12A/C0/C1
PAðÞPB/C12/C12A/C0/C1
þPA0/C0/C1
PB/C12/C12A0/C0/C1
As long as there are relatively few events in the partition, a tree diagram (as in Example 1.29) can
be used as a basis for calculating posterior probabilities without ever referring explicitly to Bayes ’
Theorem.
Example 1.33 Incidence of a rare disease . In the book’s Introduction, we presented the following
example as a common misunderstanding of probability in everyday life. Only 1 in 1000 adults is
afﬂicted with a rare disease for which a diagnostic test has been developed. The test is such that when
an individual actually has the disease, a positive result will occur 99% of the time, whereas an
individual without the disease will show a positive test result only 2% of the time. If a randomly
selected individual is tested and the result is positive, what is the probability that the individual has
the disease?
[Note: The sensitivity of this test is 99%, whereas the speciﬁcity —how speciﬁc positive results are
to this disease—is 98%. As an indication of the accuracy of medical tests, an article in the October
29, 2010 New York Times reported that the sensitivity and speciﬁcity for a new DNA test for colon
cancer were 86% and 93%, respectively. The PSA test for prostate cancer has sensitivity 85% and
speciﬁcity about 30%, while the mammogram for breast cancer has sensitivity 75% and speciﬁcity
92%. All tests are less than perfect.]
To use Bayes ’Theorem, let A1¼{individual has the disease}, A2¼{individual does not have
the disease}, and B¼{positive test result}. Then P(A1)¼.001, P(A2)¼.999, P(B|A1)¼.99, and
P(B|A2)¼.02. The tree diagram for this problem is in Fig. 1.13.
A1 = Has disease
A2 = Doesn’t have disease.001
.999.02
.98.01.99
B = +TestB = +Test
B′ = −TestB′ = −TestP(A1   B) = .00099
P(A2   B) = .01998Fig. 1.13 Tree diagram
for the rare-disease
problem36 1 Probability
Next to each branch corresponding to a positive test result, the Multiplication Rule yields the
recorded probabilities. Therefore, P(B)¼.00099 + .01998 ¼.02097, from which we have
PA 1/C12/C12B/C0/C1
¼PA 1\B ðÞ
PBðÞ¼:00099
:02097¼:047
This result seems counterintuitive; because the diagnostic test appears so accurate, we expect
someone with a positive test result to be highly likely to have the disease, whereas the computed
conditional probability is only .047. However, because the disease is rare and the test only moderately
reliable, most positive test results arise from errors rather than from diseased individuals. The
probability of having the disease has increased by a multiplicative factor of 47 (from prior .001 to
posterior .047); but to get a further increase in the posterior probability, a diagnostic test with much
smaller error rates is needed. If the disease were not so rare (e.g., 25% incidence in the population),
then the error rates for the present test would provide good diagnoses.
This example shows why it makes sense to be tested for a rare disease only if you are in a high-risk
group. For example, most of us are at low risk for HIV infection, so testing would not be indicated, but
those who are in a high-risk group should be tested for HIV. For some diseases the degree of risk is
strongly inﬂuenced by age. Young women are at low risk for breast cancer and should not be tested,
but older women do have increased risk and need to be tested. There is some argument about where to
draw the line. If we can ﬁnd the incidence rate for our group and the sensitivity and speciﬁcity for the
test, then we can do our own calculation to see if a positive test result would be informative. ■
1.4.4 Exercises: Section 1.4(50–78)
50. The population of a particular country consists of three ethnic groups. Each individual belongs
to one of the four major blood groups. The accompanying joint probability table gives the
proportions of individuals in the various ethnic group–blood group combinations.
Ethnic groupBlood group
OA BA B
1 .082 .106 .008 .004
2 .135 .141 .018 .006
3 .215 .200 .065 .020
Suppose that an individual is randomly selected from the population, and deﬁne events by
A¼{type A selected}, B¼{type B selected}, and C¼{ethnic group 3 selected}.
(a) Calculate P(A),P(C), and P(A\C).
(b) Calculate both P(A|C) and P(C|A) and explain in context what each of these probabilities
represents.
(c) If the selected individual does not have type B blood, what is the probability that he or she
is from ethnic group 1?
51. Suppose an individual is randomly selected from the population of all adult males living in the
USA. Let Abe the event that the selected individual is over 6 ft in height, and let Bbe the event
that the selected individual is a professional basketball player. Which do you think is larger,
P(A|B)o rP(B|A)? Why?1.4 Conditional Probability 37
52. Return to the credit card scenario of Exercise 14, where A¼{Visa}, B¼{MasterCard},
P(A)¼.5,P(B)¼.4, and P(A\B)¼.25. Calculate and interpret each of the following
probabilities (a Venn diagram might help).
(a) P(B|A)
(b) P(B0|A)
(c) P(A|B)
(d) P(A0|B)
(e) Given that the selected individual has at least one card, what is the probability that he or
she has a Visa card?
53. Reconsider the system defect situation described in Exercise 28.
(a) Given that the system has a type 1 defect, what is the probability that it has a type 2 defect?
(b) Given that the system has a type 1 defect, what is the probability that it has all three types
of defects?
(c) Given that the system has at least one type of defect, what is the probability that it has
exactly one type of defect?
(d) Given that the system has both of the ﬁrst two types of defects, what is the probability that
it does not have the third type of defect?
54. The accompanying table gives information on the type of coffee selected by someone purchas-
ing a single cup at a particular airport kiosk.
Small Medium Large
Regular 14% 20% 26%
Decaf 20% 10% 10%
Consider randomly selecting such a coffee purchaser.
(a) What is the probability that the individual purchased a small cup? A cup of decaf coffee?
(b) If we learn that the selected individual purchased a small cup, what now is the probability
that s/he chose decaf coffee, and how would you interpret this probability?
(c) If we learn that the selected individual purchased decaf, what now is the probability that a
small size was selected, and how does this compare to the corresponding unconditional
probability from (a)?
55. A department store sells sport shirts in three sizes (small, medium, and large), three patterns
(plaid, print, and stripe), and two sleeve lengths (long and short). The accompanying tables give
the proportions of shirts sold in the various category combinations.
Short-sleeved
SizePattern
Plaid Print Stripe
S .04 .02 .05
M .08 .07 .12
L .03 .07 .08
Long-sleeved
SizePattern
Plaid Print Stripe
S .03 .02 .03
M .10 .05 .07
L .04 .02 .08
(a) What is the probability that the next shirt sold is a medium, long-sleeved, print shirt?
(b) What is the probability that the next shirt sold is a medium print shirt?38 1 Probability
(c) What is the probability that the next shirt sold is a short-sleeved shirt? A long-sleeved
shirt?
(d) What is the probability that the size of the next shirt sold is medium? That the pattern of the
next shirt sold is a print?
(e) Given that the shirt just sold was a short-sleeved plaid, what is the probability that its size
was medium?
(f) Given that the shirt just sold was a medium plaid, what is the probability that it was short-
sleeved? Long-sleeved?
56. One box contains six red balls and four green balls, and a second box contains seven red balls and
three green balls. A ball is randomly chosen from the ﬁrst box and placed in the second box. Then
a ball is randomly selected from the second box and placed in the ﬁrst box.
(a) What is the probability that a red ball is selected from the ﬁrst box and a red ball is selected
from the second box?
(b) At the conclusion of the selection process, what is the probability that the numbers of red
and green balls in the ﬁrst box are identical to the numbers at the beginning?
57. A system consists of two identical pumps, #1 and #2. If one pump fails, the system will still
operate. However, because of the added strain, the extra remaining pump is now more likely to
fail than was originally the case. That is, r¼P(#2 fails | #1 fails) >P(#2 fails) ¼q. If at least
one pump fails by the end of the pump design life in 7% of all systems and both pumps fail during
that period in only 1%, what is the probability that pump #1 will fail during the pump design life?
58. A certain shop repairs both audio and video components. Let Adenote the event that the next
component brought in for repair is an audio component, and let Bbe the event that the next
component is a compact disc player (so the event Bis contained in A). Suppose that P(A)¼.6
andP(B)¼.05. What is P(B|A)?
59. In Exercise 15, Ai¼{awarded project i}, for i¼1, 2, 3. Use the probabilities given there to
compute the following probabilities, and explain in words the meaning of each one.
(a) P(A2|A1)
(b) P(A2\A3|A1)
(c) P(A2[A3|A1)
(d) P(A1\A2\A3|A1[A2[A3)
60. Three plants manufacture hard drives and ship them to a warehouse for distribution. Plant I
produces 54% of the warehouse ’s inventory with a 4% defect rate. Plant II produces 35% of the
warehouse ’s inventory with an 8% defect rate. Plant III produces the remainder of the
warehouse ’s inventory with a 12% defect rate.
(a) Draw a tree diagram to represent this information.
(b) A warehouse inspector selects one hard drive at random. What is the probability that it is a
defective hard drive and from Plant II?
(c) What is the probability that a randomly selected hard drive is defective?
(d) Suppose a hard drive is defective. What is the probability that it came from Plant II?
61. For any events AandBwith P(B)>0, show that P(A|B)+P(A0|B)¼1.
62. If P(B|A)>P(B) show that P(B0|A)<P(B0). [Hint: Add P(B0|A) to both sides of the given
inequality and then use the result of the previous exercise.]
63. Show that for any three events A,B, and Cwith P(C)>0,P(A[B|C)¼P(A|C)+P(B|C)/C0
P(A\B|C).
64. At a certain gas station, 40% of the customers use regular gas ( A1), 35% use mid-grade gas ( A2),
and 25% use premium gas ( A3). Of those customers using regular gas, only 30% ﬁll their tanks1.4 Conditional Probability 39
(event B). Of those customers using mid-grade gas, 60% ﬁll their tanks, whereas of those using
premium, 50% ﬁll their tanks.
(a) What is the probability that the next customer will request mid-grade gas and ﬁll the tank
(A2\B)?
(b) What is the probability that the next customer ﬁlls the tank?
(c) If the next customer ﬁlls the tank, what is the probability that regular gas is requested?
mid-grade gas? Premium gas?
65. Suppose a single gene controls the color of hamsters: black ( B) is dominant and brown ( b)i s
recessive. Hence, a hamster will be black unless its genotype is bb. Two hamsters, each with
genotype Bb, mate and produce a single offspring. The laws of genetic recombination state that
each parent is equally likely to donate either of its two alleles ( Borb), so the offspring is equally
likely to be any of BB,Bb,bB,o rbb(the middle two are genetically equivalent).
(a) What is the probability their offspring has black fur?
(b) Given that their offspring has black fur, what is the probability its genotype is Bb?
66. Refer back to the scenario of the previous exercise. In the ﬁgure below, the genotypes of both
members of Generation I are known, as is the genotype of the male member of Generation II. We
know that hamster II2 must be black-colored thanks to her father, but suppose that we don ’t know
her genotype exactly (as indicated by B- in the ﬁgure).
BB
Bb1
1
122
B– Generation II
Generation IIIGeneration I Bb
(a) What are the possible genotypes of hamster II2, and what are the corresponding
probabilities?
(b) If we observe that hamster III1 has a black coat (and hence at least one Bgene), what is the
probability her genotype is Bb?
(c) If we later discover (through DNA testing on poor little hamster III1) that her genotype in
BB, what is the posterior probability that her mom is also BB?
67. Seventy percent of the light aircraft that disappear while in ﬂight in a certain country are
subsequently discovered. Of the aircraft that are discovered, 60% have an emergency locator,
whereas 90% of the aircraft not discovered do not have such a locator. Suppose a light aircraft has
disappeared.
(a) If it has an emergency locator, what is the probability that it will not be discovered?
(b) If it does not have an emergency locator, what is the probability that it will be discovered?
68. Components of a certain type are shipped to a supplier in batches of ten. Suppose that 50% of all
such batches contain no defective components, 30% contain one defective component, and 20%
contain two defective components. Two components from a batch are randomly selected and
tested. What are the probabilities associated with 0, 1, and 2 defective components being in the
batch under each of the following conditions?
(a) Neither tested component is defective.
(b) One of the two tested components is defective.
[Hint: Draw a tree diagram with three ﬁrst-generation branches for the three different types of
batches.]40 1 Probability
69. Show that P(A\B|C)¼P(A|B\C)/C1P(B|C).
70. For customers purchasing a full set of tires at a particular tire store, consider the events
A¼{tires purchased were made in the USA}
B¼{purchaser has tires balanced immediately}
C¼{purchaser requests front-end alignment}
along with A0,B0, and C0. Assume the following unconditional and conditional probabilities:
P(A)¼.75 P(B|A)¼.9 P(B|A0)¼.8 P(C|A\B)¼.8
P(C|A\B0)¼.6 P(C|A0\B)¼.7 P(C|A0\B0)¼.3
(a) Construct a tree diagram consisting of ﬁrst-, second-, and third-generation branches and
place an event label and appropriate probability next to each branch.
(b) Compute P(A\B\C).
(c) Compute P(B\C).
(d) Compute P(C).
(e) Compute P(A|B\C), the probability of a purchase of US tires given that both balancing
and an alignment were requested.
71. A professional organization (for statisticians, of course) sells term life insurance and major
medical insurance. Of those who have just life insurance, 70% will renew next year, and 80%
of those with only a major medical policy will renew next year. However, 90% of policyholders
who have both types of policy will renew at least one of them next year. Of the policy holders,
75% have term life insurance, 45% have major medical, and 20% have both.
(a) Calculate the percentage of policyholders that will renew at least one policy next year.
(b) If a randomly selected policy holder does in fact renew next year, what is the probability that
he or she has both life and major medical insurance?
72. The Reviews editor for a certain scientiﬁc journal decides whether the review for any particular
book should be short (1–2 pages), medium (3–4 pages) or long (5–6 pages). Data on recent
reviews indicate that 60% of them are short, 30% are medium, and the other 10% are long.
Reviews are submitted in either Word or LaTeX. For short reviews, 80% are in Word, whereas
50% of medium reviews and 30% of long reviews are in Word. Suppose a recent review is
randomly selected.
(a) What is the probability that the selected review was submitted in Word?
(b) If the selected review was submitted in Word, what are the posterior probabilities of it being
short, medium, and long?
73. A large operator of timeshare complexes requires anyone interested in making a purchase to ﬁrst
visit the site of interest. Historical data indicates that 20% of all potential purchasers select a day
visit, 50% choose a one-night visit, and 30% opt for a two-night visit. In addition, 10% of day
visitors ultimately make a purchase, 30% of night visitors buy a unit, and 20% of those visiting
for two nights decide to buy. Suppose a visitor is randomly selected and found to have bought a
timeshare. How likely is it that this person made a day visit? A one-night visit? A two-night visit?
74. Consider the following information about travelers (based partly on a recent Travelocity poll):
40% check work e-mail, 30% use a cell phone to stay connected to work, 25% bring a laptop with
them, 23% both check work e-mail and use a cell phone to stay connected, and 51% neither check
work e-mail nor use a cell phone to stay connected nor bring a laptop. Finally, 88 out of every
100 who bring a laptop check work e-mail, and 70 out of every 100 who use a cell phone to stay
connected also bring a laptop.
(a) What is the probability that a randomly selected traveler who checks work e-mail also uses a
cell phone to stay connected?1.4 Conditional Probability 41
(b) What is the probability that someone who brings a laptop on vacation also uses a cell phone
to stay connected?
(c) If a randomly selected traveler checked work e-mail and brought a laptop, what is the
probability that s/he uses a cell phone to stay connected?
75. There has been a great deal of controversy over the last several years regarding what types of
surveillance are appropriate to prevent terrorism. Suppose a particular surveillance system has a
99% chance of correctly identifying a future terrorist and a 99.9% chance of correctly identifying
someone who is not a future terrorist. Imagine there are 1000 future terrorists in a population of
300 million (roughly the US population). If one of these 300 million people is randomly selected
and the system determines him/her to be a future terrorist, what is the probability the system is
correct? Does your answer make you uneasy about using the surveillance system? Explain.
76. At a large university, in the never-ending quest for a satisfactory textbook, the Statistics
Department has tried a different text during each of the last three quarters. During the fall
quarter, 500 students used the text by Professor Mean; during the winter quarter, 300 students
used the text by Professor Median; and during the spring quarter, 200 students used the text by
Professor Mode. A survey at the end of each quarter showed that 200 students were satisﬁed with
Mean ’s book, 150 were satisﬁed with Median ’s book, and 160 were satisﬁed with Mode ’s book.
If a student who took statistics during one of these quarters is selected at random and admits to
having been satisﬁed with the text, is the student most likely to have used the book by Mean,
Median, or Mode? Who is the least likely author? [ Hint: Draw a tree-diagram or use Bayes ’
theorem.]
77. A friend who lives in Los Angeles makes frequent consulting trips to Washington, D.C.; 50% of
the time she travels on airline #1, 30% of the time on airline #2, and the remaining 20% of the
time on airline #3. For airline #1, ﬂights are late into D.C. 30% of the time and late into L.A. 10%
of the time. For airline #2, these percentages are 25% and 20%, whereas for airline #3 the
percentages are 40% and 25%. If we learn that on a particular trip she arrived late at exactly one
of the two destinations, what are the posterior probabilities of having ﬂown on airlines #1, #2, and
#3? Assume that the chance of a late arrival in L.A. is unaffected by what happens on the ﬂight to
D.C. [ Hint: From the tip of each ﬁrst-generation branch on a tree diagram, draw three second-
generation branches labeled, respectively, 0 late, 1 late, and 2 late.]
78. In Exercise 64, consider the following additional information on credit card usage:
70% of all regular ﬁll-up customers use a credit card.
50% of all regular non-ﬁll-up customers use a credit card.
60% of all mid-grade ﬁll-up customers use a credit card.
50% of all mid-grade non-ﬁll-up customers use a credit card.
50% of all premium ﬁll-up customers use a credit card.
40% of all premium non-ﬁll-up customers use a credit card.
Compute the probability of each of the following events for the next customer to arrive (a tree
diagram might help).
(a) {mid-grade and ﬁll-up and credit card}
(b) {premium and non-ﬁll-up and credit card}
(c) {premium and credit card}
(d) {ﬁll-up and credit card}
(e) {credit card}
(f) If the next customer uses a credit card, what is the probability that s/he purchased premium
gasoline?42 1 Probability
1.5 Independence
The deﬁnition of conditional probability enables us to revise the probability P(A) originally assigned
toAwhen we are subsequently informed that another event Bhas occurred; the new probability of Ais
P(A|B). In our examples, it was frequently the case that P(A|B) differed from the unconditional
probability P(A), indicating that the information “ Bhas occurred” resulted in a change in the chance
ofAoccurring. There are other situations, however, in which the chance that Awill occur or has
occurred is not affected by knowledge that Bhas occurred, so that P(A|B)¼P(A). It is then natural to
think of AandBasindependent events, meaning that the occurrence or nonoccurrence of one event
has no bearing on the chance that the other will occur.
DEFINITION
Two events AandBareindependent ifP(A|B)¼P(A) and are dependent otherwise.
The deﬁnition of independence might seem “unsymmetrical” because we do not demand that P(B|A)
¼P(B) also. However, using the deﬁnition of conditional probability and the Multiplication Rule,
PB/C12/C12A/C0/C1
¼PA\BðÞ
PAðÞ¼PA/C12/C12B/C0/C1
PBðÞ
PAðÞð1:6Þ
The right-hand side of Eq. ( 1.6)i sP(B) if and only if P(A|B)¼P(A) (independence), so the
equality in the deﬁnition implies the other equality (and vice versa). It is also straightforward to show
that if AandBare independent, then so are the following pairs of events: (1) A0andB, (2)AandB0,
and (3) A0andB0. See Exercise 82.
Example 1.34 Consider an ordinary deck of 52 cards comprising the four suits spades, hearts,
diamonds, and clubs, with each suit consisting of the 13 ranks ace, king, queen, jack, ten, ..., and
two. Suppose someone randomly selects a card from the deck and reveals to you that it is a picture
card (that is, a king, queen, or jack). What now is the probability that the card is a spade? If we let
A¼{spade} and B¼{face card}, then P(A)¼13/52, P(B)¼12/52 (there are three face cards in
each of the four suits), and P(A\B)¼P(spade and face card) ¼3/52. Thus
PA/C12/C12B/C0/C1
¼PA\BðÞ
PBðÞ¼3=52
12=52¼3
12¼1
4¼13
52¼PAðÞ
Therefore, the likelihood of getting a spade is not affected by knowledge that a face card had been
selected. Intuitively this is because the fraction of spades among face cards (3 out of 12) is the same as
the fraction of spades in the entire deck (13 out of 52). It is also easily veriﬁed that P(B|A)¼P(B), so
knowledge that a spade has been selected does not affect the likelihood of the card being a jack,
queen, or king. ■
Example 1.35 Consider a gas station with six pumps numbered 1, 2, ..., 6 and let Eidenote the
simple event that a randomly selected customer uses pump i. Suppose that
PE 1ðÞ ¼ PE 6ðÞ ¼ :10, PE 2ðÞ ¼ PE 5ðÞ ¼ :15, PE 3ðÞ ¼ PE 4ðÞ ¼ :25
Deﬁne events A,B,Cby1.5 Independence 43
A¼2;4;6 fg ,B¼1;2;3 fg ,C¼2;3;4;5 fg
It is easy to determine that P(A)¼.50,P(A|B)¼.30, and P(A|C)¼.50. Therefore, events Aand
Bare dependent, whereas events AandCare independent. Intuitively, AandCare independent
because the relative division of probability among even- and odd-numbered pumps is the same among
pumps 2, 3, 4, 5 as it is among all six pumps. ■
Example 1.36 LetAandBbe any two mutually exclusive events with P(A)>0. For example, for a
randomly chosen automobile, let A¼{car is blue} and B¼{car is red}. Since the events are
mutually exclusive, if Boccurs, then Acannot possibly have occurred, so P(A|B)¼06¼P(A). The
message here is that if two events are mutually exclusive ,they cannot be independent . When Aand
Bare mutually exclusive, the information that Aoccurred says something about the chance of
B(namely, it cannot have occurred), so independence is precluded. ■
1.5.1 P(A\B) When Events Are Independent
Frequently the nature of an experiment suggests that two events AandBshould be assumed
independent. This is the case, for example, if a manufacturer receives a circuit board from each of
two different suppliers, each board is tested on arrival, and A¼{ﬁrst is defective} and B¼{second
is defective}. If P(A)¼.1, it should also be the case that P(A|B)¼.1; knowing the condition of the
second board shouldn ’t provide information about the condition of the ﬁrst. Our next result shows
how to compute P(A\B) when the events are independent.
PROPOSITION
AandBare independent if and only if
PA\BðÞ ¼ PAðÞ /C1 PBðÞ ð 1:7Þ
Proof By the Multiplication Rule, P(A\B)¼P(A|B)/C1P(B), and this equals P(A)/C1P(B) if and
only if P(A|B)¼P(A). ■
Because of the equivalence of independence with Eq. ( 1.7), the latter can be used as a deﬁnition of
independence.1
Example 1.37 It is known that 30% of a certain company ’s washing machines require service while
under warranty, whereas only 10% of its dryers need such service. If someone purchases both a
washer and a dryer made by this company, what is the probability that both machines need warranty
service?
LetAdenote the event that the washer needs service while under warranty, and let Bbe deﬁned
analogously for the dryer. Then P(A)¼.30 and P(B)¼.10. Assuming that the two machines
function independently of each other, the desired probability is
1However, the multiplication property is satisﬁed if P(B)¼0, yet P(A|B) is not deﬁned in this case. To make the
multiplication property completely equivalent to the deﬁnition of independence, we should append to that deﬁnition
thatAandBare also independent if either P(A)¼0o rP(B)¼0.44 1 Probability
PA\BðÞ ¼ PAðÞ /C1 PBðÞ ¼ :30ðÞ :10ðÞ ¼ :03
The probability that neither machine needs service is
PA0\B0ðÞ ¼ PA0ðÞ /C1 PB0ðÞ ¼ :70ðÞ :90ðÞ ¼ :63
Note that, although the independence assumption is reasonable here, it can be questioned. In
particular, if heavy usage causes a breakdown in one machine, it could also cause trouble for the other
one. ■
Example 1.38 Each day, Monday through Friday, a batch of components sent by a ﬁrst supplier
arrives at a certain inspection facility. Two days a week, a batch also arrives from a second supplier.
Eighty percent of all supplier 1 ’s batches pass inspection, and 90% of supplier 2 ’s do likewise. What
is the probability that, on a randomly selected day, two batches pass inspection? We will answer this
assuming that on days when two batches are tested, whether the ﬁrst batch passes is independent of
whether the second batch does so. Figure 1.14 displays the relevant information.
Ptwo passðÞ ¼ Ptwo received \both pass ðÞ
¼Pboth pass/C12/C12two received/C0/C1
/C1Ptwo receivedðÞ
¼:8ðÞ:9ðÞ½/C138 :4ðÞ ¼ :288 ■
1.5.2 Independence of More than Two Events
The notion of independence can be extended to collections of more than two events. Although it is
possible to extend the deﬁnition for two independent events by working in terms of conditional and
unconditional probabilities, it is more direct and less cumbersome to proceed along the lines of the
last proposition.2 batches1 batch.6
.4 .8
1st passes
.2
1st fails.2
Fails.8
Passes
.9
2nd passes
.1
2nd fails
.9
2nd passes
.1
2nd fails.4    (.8    .9)
Fig. 1.14 Tree diagram for Example 1.381.5 Independence 45
DEFINITION
Events A1,...,Anaremutually independent if for every k(k¼2, 3, ...,n) and every subset of
indices i1,i2,....,ik,
PA i1\Ai2\/C1/C1/C1\ Aik ðÞ ¼ PA i1ðÞ /C1 PA i2ð Þ/C1/C1/C1/C1/C1 PA ikðÞ
To paraphrase the deﬁnition, the events are mutually independent if the probability of the
intersection of any subset of the nevents is equal to the product of the individual probabilities. In
using this multiplication property for more than two independent events, it is legitimate to replace one
or more of the Ais by their complements (e.g., if A1,A2, and A3are independent events, then so are A01,
A02, and A03.) As was the case with two events, we frequently specify at the outset of a problem the
independence of certain events. The deﬁnition can then be used to calculate the probability of an
intersection.
Example 1.39 The article “Reliability Evaluation of Solar Photovoltaic Arrays” ( Solar Energy ,
2002: 129–141) presents various conﬁgurations of solar photovoltaic arrays consisting of crystalline
silicon solar cells. Consider ﬁrst the system illustrated in Fig. 1.15a . There are two subsystems
connected in parallel, each one containing three cells. In order for the system to function, at least one
of the two parallel subsystems must work. Within each subsystem, the three cells are connected in
series, so a subsystem will work only if all cells in the subsystem work. Consider a particular lifetime
value t0, and suppose we want to determine the probability that the system lifetime exceeds t0. Let Ai
denote the event that the lifetime of cell iexceeds t0(i¼1, 2, ..., 6). We assume that the Ais are
independent events (whether any particular cell lasts more than t0hours has no bearing on whether
any other cell does) and that P(Ai)¼.9 for every isince the cells are identical. Then applying the
Addition Rule followed by independence,
Psystem lifetime exceeds t0 ðÞ ¼ P/C2
A1\A2\A3 ðÞ [/C0
A4\A5\A6/C1/C3
¼PA 1\A2\A3 ðÞ þ P/C0
A4\A5\A6/C1
/C0P/C2/C0
A1\A2\A3/C1
\/C0
A4\A5\A6/C1/C3
¼:9ðÞ:9ðÞ:9ðÞ þ :9ðÞ:9ðÞ:9ðÞ
/C0:9ðÞ:9ðÞ:9ðÞ:9ðÞ:9ðÞ:9ðÞ
¼:927
Alternatively,
Psystem lifetime exceeds t0 ðÞ ¼ 1/C0Pboth subsystem lives are /C20t0 ðÞ
¼1/C0Psubsystem life is /C20t0 ðÞ½/C1382
¼1/C01/C0Psubsystem life is >t0 ðÞ ½/C1382
¼1/C01/C0:9ðÞ3hi2
¼:927
123
456123
456b a
Fig. 1.15 System conﬁgurations for Example 1.39: ( a) series–parallel; ( b) total-cross-tied46 1 Probability
Next consider the total-cross-tied system shown in Fig. 1.15b , obtained from the series–parallel
array by connecting ties across each column of junctions. Now the system fails as soon as an entire
column fails, and system lifetime exceeds t0only if the life of every column does so. For this
conﬁguration,
Psystem lifetime exceeds t0 ðÞ ¼ Pcolumn lifetime exceeds t0 ðÞ½/C1383
¼1/C0Pcolumn lifetime is /C20t0 ðÞ ½/C1383
¼1/C0Pboth cells in a column have lifetime /C20t0 ðÞ ½/C1383
¼1/C01/C0:9ðÞ2hi3
¼:970■
Probabilities like those calculated in Example 1.39 are often referred to as the reliability of a
system. In Sect. 4.8, we consider in more detail the analysis of system reliability.
1.5.3 Exercises: Section 1.5(79–100)
79. Reconsider the credit card scenario of Exercise 52, and show that AandBare dependent ﬁrst by
using the deﬁnition of independence and then by verifying that the multiplication property does
not hold.
80. An oil exploration company currently has two active projects, one in Asia and the other in
Europe. Let Abe the event that the Asian project is successful and Bbe the event that the
European project is successful. Suppose that AandBare independent events with P(A)¼.4 and
P(B)¼.7.
(a) If the Asian project is not successful, what is the probability that the European project is
also not successful? Explain your reasoning.
(b) What is the probability that at least one of the two projects will be successful?
(c) Given that at least one of the two projects is successful, what is the probability that only the
Asian project is successful?
81. In Exercise 15, is any Aiindependent of any other Aj? Answer using the multiplication property
for independent events.
82. If AandBare independent events, show that A0andBare also independent. [ Hint: First use a
Venn diagram to establish a relationship among P(A0\B),P(B), and P(A\B).]
83. Suppose that the proportions of blood phenotypes in a particular population are as follows:
A B AB O
.40 .11 .04 .45
Assuming that the phenotypes of two randomly selected individuals are independent of each
other, what is the probability that both phenotypes are O? What is the probability that the
phenotypes of two randomly selected individuals match?
84. The probability that a grader will make a marking error on any particular question of a multiple-
choice exam is .1. If there are ten questions on the exam and questions are marked indepen-
dently, what is the probability that no errors are made? That at least one error is made? If there
arenquestions on the exam and the probability of a marking error is prather than .1, give
expressions for these two probabilities.1.5 Independence 47
85. In October, 1994, a ﬂaw in a certain Pentium chip installed in computers was discovered that
could result in a wrong answer when performing a division. The manufacturer initially claimed
that the chance of any particular division being incorrect was only 1 in 9 billion, so that it would
take thousands of years before a typical user encountered a mistake. However, statisticians are
not typical users; some modern statistical techniques are so computationally intensive that a
billion divisions over a short time period is not unrealistic. Assuming that the 1 in 9 billion ﬁgure
is correct and that results of divisions are independent from one another, what is the probability
that at least one error occurs in 1 billion divisions with this chip?
86. An aircraft seam requires 25 rivets. The seam will have to be reworked if any of these rivets is
defective. Suppose rivets are defective independently of one another, each with the same
probability.
(a) If 20% of all seams need reworking, what is the probability that a rivet is defective?
(b) How small should the probability of a defective rivet be to ensure that only 10% of all seams
need reworking?
87. A boiler has ﬁve identical relief valves. The probability that any particular valve will open on
demand is .95. Assuming independent operation of the valves, calculate P(at least one valve
opens) and P(at least one valve fails to open).
88. Two pumps connected in parallel fail independently of each other on any given day. The
probability that only the older pump will fail is. 10, and the probability that only the newer
pump will fail is .05. What is the probability that the pumping system will fail on any given day
(which happens if both pumps fail)?
89. Consider the system of components connected as in the accompanying picture. Components
1 and 2 are connected in parallel, so that subsystem works iff either 1 or 2 works; since 3 and 4 are
connected in series, that subsystem works iff both 3 and 4 work. If components work indepen-
dently of one another and P(component works) ¼.9, calculate P(system works).
21
3 4
90. Refer back to the series–parallel system conﬁguration introduced in Example 1.39, and suppose
that there are only two cells rather than three in each parallel subsystem [in Fig. 1.15a , eliminate
cells 3 and 6, and renumber cells 4 and 5 as 3 and 4]. Using P(Ai)¼.9, the probability that
system lifetime exceeds t0is easily seen to be .9639. To what value would .9 have to be changed
in order to increase the system lifetime reliability from .9639 to .99? [ Hint: Let P(Ai)¼p,
express system reliability in terms of p, and then let x¼p2.]
91. Consider independently rolling two fair dice, one red and the other green. Let Abe the event that
the red die shows 3 dots, Bbe the event that the green die shows 4 dots, and Cbe the event that the
total number of dots showing on the two dice is 7.
(a) Are these events pairwise independent (i.e., are AandBindependent events, are Aand
Cindependent, and are BandCindependent)?
(b) Are the three events mutually independent?
92. Components arriving at a distributor are checked for defects by two different inspectors (each
component is checked by both inspectors). The ﬁrst inspector detects 90% of all defectives that48 1 Probability
are present, and the second inspector does likewise. At least one inspector fails to detect a defect
on 20% of all defective components. What is the probability that the following occur?
(a) A defective component will be detected only by the ﬁrst inspector? By exactly one of the
two inspectors?
(b) All three defective components in a batch escape detection by both inspectors (assuming
inspections of different components are independent of one another)?
93. Seventy percent of all vehicles examined at a certain emissions inspection station pass the
inspection. Assuming that successive vehicles pass or fail independently of one another, calculate
the following probabilities:
(a) P(all of the next three vehicles inspected pass)
(b) P(at least one of the next three inspected fails)
(c) P(exactly one of the next three inspected passes)
(d) P(at most one of the next three vehicles inspected passes)
(e) Given that at least one of the next three vehicles passes inspection, what is the probability
that all three pass (a conditional probability)?
94. A quality control inspector is inspecting newly produced items for faults. The inspector searches
an item for faults in a series of independent ﬁxations, each of a ﬁxed duration. Given that a ﬂaw is
actually present, let pdenote the probability that the ﬂaw is detected during any one ﬁxation (this
model is discussed in “Human Performance in Sampling Inspection,” Human Factors , 1979: 99–
105).
(a) Assuming that an item has a ﬂaw, what is the probability that it is detected by the end of the
second ﬁxation (once a ﬂaw has been detected, the sequence of ﬁxations terminates)?
(b) Give an expression for the probability that a ﬂaw will be detected by the end of the nth
ﬁxation.
(c) If when a ﬂaw has not been detected in three ﬁxations, the item is passed, what is the
probability that a ﬂawed item will pass inspection?
(d) Suppose 10% of all items contain a ﬂaw [ P(randomly chosen item is ﬂawed) ¼.1]. With the
assumption of part (c), what is the probability that a randomly chosen item will pass
inspection (it will automatically pass if it is not ﬂawed, but could also pass if it is ﬂawed)?
(e) Given that an item has passed inspection (no ﬂaws in three ﬁxations), what is the probability
that it is actually ﬂawed? Calculate for p¼.5.
95. (a) A lumber company has just taken delivery on a lot of 10,000 2 /C24 boards. Suppose that
20% of these boards (2000) are actually too green to be used in ﬁrst-quality construction.
Two boards are selected at random, one after the other. Let A¼{the ﬁrst board is green}
andB¼{the second board is green}. Compute P(A),P(B), and P(A\B) (a tree diagram
might help). Are AandBindependent?
(b) With AandBindependent and P(A)¼P(B)¼.2, what is P(A\B)? How much difference
is there between this answer and P(A\B) in part (a)? For purposes of calculating P(A\B),
can we assume that AandBof part (a) are independent to obtain essentially the correct
probability?
(c) Suppose the lot consists of ten boards, of which two are green. Does the assumption of
independence now yield approximately the correct answer for P(A\B)? What is the
critical difference between the situation here and that of part (a)? When do you think that
an independence assumption would be valid in obtaining an approximately correct answer
toP(A\B)?1.5 Independence 49
96. Refer to the assumptions stated in Exercise 89 and answer the question posed there for the system
in the accompanying picture. How would the probability change if this were a subsystem
connected in parallel to the subsystem pictured in Fig. 1.15a ?
21
53
674
97. Professor Stander Deviation can take one of two routes on his way home from work. On the ﬁrst
route, there are four railroad crossings. The probability that he will be stopped by a train at any
particular one of the crossings is .1, and trains operate independently at the four crossings. The
other route is longer but there are only two crossings, independent of each other, with the same
stoppage probability for each as on the ﬁrst route. On a particular day, Professor Deviation has a
meeting scheduled at home for a certain time. Whichever route he takes, he calculates that he will
be late if he is stopped by trains at at least half the crossings encountered.
(a) Which route should he take to minimize the probability of being late to the meeting?
(b) If he tosses a fair coin to decide on a route and he is late, what is the probability that he took
the four-crossing route?
98. For a customer who test drives three vehicles, deﬁne events Ai¼customer likes vehicle # ifor
i¼1, 2, 3. Suppose that P(A1)¼.55,P(A2)¼.65,P(A3)¼.70,P(A1[A2)¼.80,P(A2\A3)
¼.40, and P(A1[A2[A3)¼.88.
(a) What is the probability that a customer likes both vehicle #1 and vehicle #2?
(b) Determine and interpret P(A2|A3).
(c) Are A2andA3independent events? Answer in two different ways.
(d) If you learn that the customer did not like vehicle #1, what now is the probability that s/he
liked at least one of the other two vehicles?
99. It ’s a commonly held misconception that if you play the lottery ntimes, and the probability of
winning each time is 1/ N, then your chance of winning at least once is n/N.T h a t ’s true if you buy
ntickets in 1 week, but not if you buy a single ticket in each of nindependent weeks. Let ’s
explore further.
(a) Suppose you play a game nindependent times, with P(win) ¼1/Neach time. Find an
expression for the probability you win at least once. [ Hint: Consider the complement.]
(b) How does your answer to (a) compare to n/Nfor the easy task of rolling a ⚃on a fair die
(so 1/ N¼1/6) in n¼3 tries? In n¼6 tries? In n¼10 tries?
(c) How does your answer to (a) compare to n/Nin the setting of Exercise 85: probability ¼1
in 9 billion, number of tries ¼1 billion?
(d) Show that when nis much smaller than N, the fraction n/Nis not a bad approximation to (a).
[Hint: Use the binomial theorem from high school algebra.]
100. Suppose identical tags are placed on both the left ear and the right ear of a fox. The fox is then let
loose for a period of time. Consider the two events C1¼{left ear tag is lost} and C2¼{right
ear tag is lost}. Let p¼P(C1)¼P(C2), and assume C1andC2are independent events. Derive
an expression (involving p) for the probability that exactly one tag is lost, given that at most one
is lost (“Ear Tag Loss in Red Foxes,” J. Wildlife Manag. , 1976: 164–167). [ Hint: Draw a tree
diagram in which the two initial branches refer to whether the left ear tag was lost.]50 1 Probability
1.6 Simulation of Random Events
As probability models in engineering and the sciences have grown in complexity, many problems
have arisen that are too difﬁcult to attack “analytically,” i.e., using mathematical tools such as those in
the previous sections. Instead, computer simulation provides us an effective way to estimate
probabilities of very complicated events (and, in later chapters, of other properties of random
phenomena). Here we introduce the principles of probability simulation, demonstrate a few examples
with Matlab and R code, and discuss the precision of simulated probabilities.
Suppose an investigator wishes to determine P(A), but either the experiment on which Ais deﬁned
or the Aevent itself is so complicated as to preclude the use of probability rules and properties. The
general method for estimating this probability via computer simulation is as follows:
– Write a program that simulates (mimics) the underlying random experiment.
– Run the program many times, with each run independent of all others.
– During each run, record whether or not the event Aof interest occurs.
If the simulation is run a total of nindependent times, then the estimate of P(A), denoted by ^PAðÞ,i s
^PAðÞ ¼number of times Aoccurs
number of runs¼nAðÞ
n
For example, if we run a simulation program 10,000 times and the event of interest Aoccurs in
6174 of those runs, then our estimate of P(A)i s ^PAðÞ ¼ 6174 =10, 000 ¼:6174. Notice that our
deﬁnition is consistent with the long-run relative frequency interpretation of probability discussed in
Sect. 1.2.
1.6.1 The Backbone of Simulation: Random Number Generators
All modern software packages are equipped with a function called a random number generator
(RNG) . A typical call to this function (such as ran or rand) will return a single, supposedly “random”
number, though such functions typically permit the user to request a vector or even a matrix of
“random” numbers. It is more proper to call these results pseudo-random numbers, since there is
actually a deterministic (i.e., non-random) algorithm by which the software generates these values.
We will not discuss the details of such algorithms here; see the book by Law listed in the references.
What will matter to us are the following two characteristics:
1. Each number created by an RNG is as likely to be any particular number in the interval [0, 1) as it
is to be any other number in this interval (up to computer precision, anyway).2
2. Successive values created by RNGs are independent, in the sense that we cannot predict the next
value to be generated from the current value (unless we somehow know the exact parameters of the
underlying algorithm).
2In the language of Chap. 3, the numbers produced by an RNG follow essentially a uniform distribution on the interval
[0, 1).1.6 Simulation of Random Events 51
A typical simulation program manipulates numbers on the interval [0, 1) in a way that mimics the
experiment of interest; several examples are provided below. Arguably the most important building
block for such programs is the ability to simulate a basic event that occurs with a known probability, p.
Since RNGs produce values equally likely to be anywhere in the interval [0, 1), it follows that in the
long run a proportion pof them will lie in the interval [0, p). So, suppose we need to simulate an event
Bwith P(B)¼p. In each run of our simulation program, we can call for a single “random” number,
which we ’ll call u, and apply the following rules:
–I f 0 /C20u<p, then event Bhas occurred on this run of the program.
–I f p/C20u<1, then event Bhasnotoccurred on this run of the program.
Example 1.40 Let ’s begin with an example in which the exact probability can be obtained analyti-
cally, so that we may verify that our simulation method works. Suppose we have two independent
devices which function with probabilities .6 and .7, respectively. What is the probability both devices
function? That at least one device functions?
LetB1andB2denote the events that the ﬁrst and second devices function, respectively; we know
that P(B1)¼.6,P(B2)¼.7, and B1andB2are independent. Our ﬁrst goal is to estimate the
probability of A¼B1\B2, the event that both devices function. The following “pseudo-code”
will allow us to ﬁnd ^PAðÞ.
0. Set a counter for the number of times Aoccurs to zero.
Repeat ntimes:
1. Generate two random numbers, u1andu2. (These will help us determine whether B1andB2occur,
respectively.)
2. If u1<.6 AND u2<.7, then Ahas occurred. Add 1 to the count of occurrences of A.
Once the nruns are complete, then ^PAðÞ ¼ count of the occurrences of A ðÞ =n.
Figure 1.16 shows actual implementation code in both Matlab and R. We ran each program with
n¼10,000 (as in the code); the event Aoccurred 4215 times in Matlab and 4181 times in R,
providing estimated probabilities of ^PAðÞ ¼ :4215 and :4181, respectively. Compare this to the
exact probability of A: by independence, P(A)¼P(B1)P(B2)¼(.6)(.7) ¼.42. Both of our
simulation estimates were “in the ballpark” of the right answer. We ’ll discuss the precision of
these estimates shortly.
A=0;
for i=1:10000
u1=rand; u2=rand;
if u1<.6 && u2<.7
A=A+1;a
end
endbA<-0
for(i in 1:10000){
u1<-runif(1); u2<-runif(1)
if(u1<.6 && u2<.7){
A<-A+1
}
}
Fig. 1.16 Code for Example 1.40: ( a) Matlab; ( b)R52 1 Probability
By replacing the “and” operators && in Fig. 1.16 with “or” operators ||, we can estimate the
probability at least one device functions, P(B1[B2). In one simulation (again with n¼10,000), the
event B1[B2occurred 8802 times, giving the estimate ^PB 1[B2 ðÞ ¼ :8802. This is quite close to
the exact probability:
PB 1[B2 ðÞ ¼ 1/C0PB0
1\B0
2/C16/C17
¼1/C01/C0:6 ðÞ 1/C0:7 ðÞ ¼ :88 ■
Example 1.41 Consider the following game: You ’ll ﬂip a coin 25 times, winning $1 each time it
lands heads ( H) and losing $1 each time it lands tails ( T). Unfortunately for you, the coin is biased in
such a way that P(H)¼.4 and P(T)¼.6. What ’s the probability you come out ahead, i.e., you have
more money at the end of the game than you had at the beginning? We ’ll use simulation to ﬁnd out.
Now each run of the simulation requires 25 “random” objects: the results of the 25 coin tosses.
What ’s more, we need to keep track of how much money we ’ve won or lost at the end of the 25 tosses.
LetA¼{we come out ahead}, and use the following pseudo-code:
0. Set a counter for the number of times Aoccurs to zero.
Repeat ntimes:
1. Set your initial dollar amount to zero.
2. Generate 25 random numbers u1,...,u25.
3. For each ui<.4, heads was tossed, so add 1 to your dollar amount. For each ui/C21.4, the ﬂip was
tails and 1 is deducted.
4. If the ﬁnal dollar amount is positive (i.e., $1 or greater), add 1 to the count of occurrences for A.
Once the nruns are complete, then ^PAðÞ ¼ count of the occurrences of A ðÞ =n.
Matlab and R code for Example 1.41 appear in Fig. 1.17. Our R code gave a ﬁnal count of 1567
occurrences of A, out of 10,000 runs. Thus, the estimated probability that we come out ahead in this
game is ^PAðÞ ¼ 1567 =10, 000 ¼:1567.
ab
A=0;
for i=1:10000
dollar=0;
for j=1:25
u=rand;
if u<.4
dollar=dollar+1;
else
dollar=dollar-1;
end
end
if dollar>0
A=A+1;
end
endA <- 0
for (i in 1:10000){
dollar<-0
for (j in 1:25){
u<-runif(1)
if (u<.4){
dollar<-dollar+1
}
else{dollar<-dollar-1}
}
if (dollar>0){
A<-A+1
}
}
Fig. 1.17 Code for Example 1.41: ( a) Matlab; ( b)R ■1.6 Simulation of Random Events 53
Throughout this textbook, we will illustrate repeated simulation through “for” loops, as in
Figs. 1.16 and1.17. Though this isn ’t necessarily the most efﬁcient way to code these examples,
we do so for clarity ’s sake. Readers familiar with basic programming may realize that such operations
can be sped up by vectorization , i.e., by using a function call that generates all the required random
numbers simultaneously, rather than one at a time. Similarly, the if/else statements used in the
preceding programs to determine whether a random number lies in an interval can be rewritten in
terms of true/false bits, which automatically generate a 1 if a statement is true and a 0 otherwise. For
example, the Matlab code
if u <.5
A¼A+1;
end
can be replaced by the single line of code
A¼A+(u <.5);
If the statement in parentheses is true, Matlab assigns a value 1 to (u<.5), and so 1 is added to
the count A. Similar code works in R.
The previous two examples have both assumed independence of certain events: the functionality
of neighboring devices, or the outcomes of successive coin ﬂips. With the aid of some built-in
packages within Matlab and R, we can also simulate counting experiments similar to those in
Sect. 1.3, even though draws without replacement from a ﬁnite population are not independent. To
illustrate, let ’s use simulation to estimate some of the combinatorial probabilities from Sect. 1.3.
Example 1.42 Consider again the situation presented in Example 1.24: A university warehouse has
received a shipment of 25 printers, of which 10 are laser printers and 15 are inkjet models; a particular
technician will check 6 of these 25 printers, selected at random. Of interest is the probability of the
event D3¼{exactly 3 of the 6 selected are inkjet printers}. Although the initial probability of
selecting an inkjet printer is 15/25, successive selections are not independent (the conditional
probability that the next printer is also an inkjet is not15/25). So, the method of the preceding
examples does not apply.
Instead, we use the sampling tool built into our software, as follows:
0. Set a counter for the number of times D3occurs to zero.
Repeat ntimes:
1. Sample 6 numbers, without replacement , from the integers 1 through 25. (1–15 correspond to the
labels for the 15 inkjet printers and 16–25 identify the 10 laser printers.)
2. Count how many of these 6 numbers fall between 1 and 15, inclusive.
3. If exactly 3 of these 6 numbers fall between 1 and 15, add 1 to the count of occurrences for D3.
Once the nruns are complete, then ^PD 3ðÞ ¼ count of the occurrences of D3 ðÞ =n.
Matlab and R code for this example appear in Fig. 1.18. Vital to the execution of this simulation is
the fact that both software packages have a built-in mechanism for randomly sampling without
replacement from a ﬁnite set of objects (here, the integers 1–25). For more information on these
functions, type help randsample in Matlab or help(sample) in R.54 1 Probability
In both sets of code, the line sum(printers <¼15) performs two actions. First,
printers <¼15converts each of the 6 numbers in the vector printers into a 1 if the entry is
between 1 and 15 (and into a 0 otherwise). Second, sum() adds up the 1s and 0s, which is equivalent
to identifying how many 1s appear (i.e., how many of the 6 numbers fell between 1 and 15).
Our R code resulted in event D3occurring 3054 times, so ^PD 3ðÞ ¼ 3054 =10, 000 ¼:3054, which
is quite close to the “exact” answer of .3083 found in Example 1.24. The other probability of interest,
the chance of randomly selecting at least 3 inkjet printers, can be estimated by modifying one line of
code: change inkjet ¼¼3toinkjet >¼3. One simulation provided a count of 8522 occurrences
in 10,000 trials, for an estimated probability of .8522 (close to the combinatorial solution of .8530). ■
1.6.2 Precision of Simulation
In Example 1.40, we gave two different estimates ^PAðÞ for a probability P(A). Which is more
“correct”? Without knowing P(A) itself, there ’s no way to tell. However, thanks to the theory we will
develop in subsequent chapters, we can quantify the precision of simulated probabilities. Of course,
we must have written code that faithfully simulates the random experiment of interest. Further, we
assume that the results of each single run of our program are independent of the results of all other
runs. (This generally follows from the aforementioned independence of computer-generated random
numbers.)
If this is the case, then a measure of the disparity between the true probability P(A) and the
estimated probability ^PAðÞbased on nruns of the simulation is given by:
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^PAðÞ1/C0^PAðÞ/C2/C3
ns
ð1:8Þ
This measure of precision is called the (estimated) standard error of the estimate ^PAðÞ; see
Sect. 2.4for a derivation. Expression ( 1.8) tells us that the amount by which ^PAðÞtypically differs
from P(A) depends upon two values: ^PAðÞitself, and the number of runs n. You can make sense of the
former this way: if P(A) is very small, then ^PAðÞwill presumably be small as well, in which case
they cannot deviate by very much since both are bounded below by zero. (Standard error quantiﬁes
theabsolute difference between them, not the relative difference.) A similar comment applies if P(A)
is very large, i.e., near 1.
As for the relationship to n, Expression ( 1.8) indicates that the amount by which ^PAðÞ will
typically differ from P(A) is inversely proportional to the square root of n. So, in particular, asab
D=0;
for i=1:10000
printers=randsample(25,6);
inkjet=sum(printers<=15);
if inkjet==3
D=D+1;
end
endD<-0
for (i in 1:10000){
printers<-sample(25,6)
inkjet<-sum(printers<=15)
if (inkjet==3){
D<-D+1
}
}
Fig. 1.18 Matlab and R code for Example 1.421.6 Simulation of Random Events 55
nincreases the tendency is for ^PAðÞto vary less and less. This speaks to the precision of ^PAðÞ: our
estimate becomes more precise as nincreases, but not at a very fast rate.
Let ’s think a bit more about this relationship: suppose your simulation results thus far were too
imprecise for your tastes. By how much would you have to increase the number of runs to gain one
additional decimal place of precision? That ’s equivalent to reducing the estimated standard error by a
factor of 10. Since precision is proportional to 1 =ﬃﬃﬃnp, you would need to increase nby a factor of
100 to achieve the desired improvement, e.g., if using n¼10,000 runs is insufﬁcient for your
purposes, then you ’ll need 1,000,000 runs to get one additional decimal place of precision. Typically,
this will mean running your program 100 times longer—not a big deal if 10,000 runs only take a
nanosecond but prohibitive if they require, say, an hour.
Example 1.43 (Example 1.41 continued) Based on n¼10,000 runs, we estimated the probability of
coming out ahead in a certain game to be ^PAðÞ ¼ :1567. Substituting into Eq. ( 1.8), we get
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:1567 1 /C0:1567 ½/C138
10, 000s
¼:0036
This is the (estimated) standard error of our estimate .1567. We interpret as follows: some
simulation experiments with n¼10,000 will result in an estimated probability that is within .0036
of the actual probability, whereas other such experiments will give an estimated probability that
deviates by more than .0036 from the actual P(A); .0036 is roughly the size of a typical deviation
between the estimate and what it is estimating. ■
In Chap. 5, we will return to the notion of standard error and develop a so-called conﬁdence
interval estimate for P(A): a range of numbers we can be very certain contains P(A).
1.6.3 Exercises: Section 1.6(101–120)
101. Refer to Example 1.40.
(a) Modify the code in Fig. 1.16 to estimate the probability that exactly one of the two devices
functions properly. Then ﬁnd the exact probability using the techniques from earlier
sections of this chapter, and compare it to your estimated probability.
(b) Calculate the estimated standard error for the estimated probability in (a).
102. Imagine you have ﬁve independently operating components, each working properly with
probability .8. Use simulation to estimate the probability that
(a) All ﬁve components work properly.
(b) At least one of the ﬁve components works properly.
[Hints for (a) and (b): You can adapt the code from Example 1.40, but the and/or
statements will become tedious. Consider using the max and min functions instead.]
(c) Calculate the estimated standard errors for your answers in (a) and (b).
103. Consider the system depicted in Exercise 96. Assume the seven components operate indepen-
dently with the following probabilities of functioning properly: .9 for components 1 and 2; .8
for each of components 3, 4, 5, 6; and .95 for component 7. Write a program to estimate the
reliability of the system (i.e., the probability the system functions properly).
104. You have an opportunity to answer six trivia questions about your favorite sports team, and
you will win a pair of tickets to their next game if you can correctly answer at least three of the56 1 Probability
questions. Write a simulation program to estimate the chance you win the tickets under each of
the following assumptions.
(a) You have a 50–50 chance of getting any question right, independent of all others.
(b) Being a true fan, you have a 75% chance of getting any question right, independent of all
others.
(c) The ﬁrst three questions are fairly easy, so you have a .75 chance of getting each of those
right. However, the last three questions are much harder, and you only have a .3
probability of correctly answering each of those.
105. In the game “Now or Then” on the television show The Price is Right , the contestant faces a
wheel with six sectors. Each sector contains a grocery item and a price, and the contestant must
decide whether the price is “now” (i.e., the item ’s price the day of the taping) or “then” (the
price at some speciﬁed past date, such as September 2003). The contestant wins a prize
(bedroom furniture, a Caribbean cruise, etc.) if s/he guesses correctly on three adjacent sectors.
That is, numbering the sectors 1–6 clockwise, correct guesses on sectors 5, 6, and 1 wins the
prize but not on sectors 5, 6, and 3, since the latter are not all adjacent. (The contestant gets to
guess on all six sectors, if need be.)
Write a simulation program to estimate the probability the contestant wins the prize, assuming
her/his guesses are independent from item to item. Provide estimated probabilities under each of
the following assumptions: (1) each guess is “wild” and thus has probability.5 of being correct,
and (2) the contestant is a good shopper, with probability.8 of being correct on any item.
106. Refer to the game in Example 1.41. Under the same settings as in that example, estimate the
probability the player is ahead at any time during the 25 plays. [ Hint: This occurs if the player ’s
dollar amount is positive at any of the 25 steps in the loop. So, you will need to keep track of
every value of the dollar variable, not just the ﬁnal result.]
107. Refer again to Example 1.41. Estimate the probability that the player experiences a “swing” of
at least $5 during the game. That is, estimate the chance that the difference between the largest
and smallest dollar amounts during the game is at least 5. (This would happen, for instance, if
the player was at one point ahead at +$2 but later fell behind to /C0$3.)
108. Each of this book ’s authors has a fair coin. Carlton tosses his coin repeatedly until obtaining the
sequence HTT. Devore tosses his coin until the sequence HTH is obtained.
(a) Write a program to simulate Carlton ’s coin tossing and, separately, Devore ’s. Your
program should keep track of the number of tosses each author requires on each simulation
run to achieve his target sequence.
(b) Estimate the probability that Devore obtains his sequence with fewer tosses than Carlton
requires to obtain his sequence.
109. There ’s a 40-question multiple-choice exam we sometimes administer in our lower-level
statistics classes. The exam has a peculiar feature: 10 of the questions have two options,
13 have three options, 13 have four options, and the other 4 have ﬁve options. (FYI, this is
completely real!) What is the probability that, purely by guessing, a student could get at least
half of these questions correct? Write a simulation program to answer this question.
110. Major League Baseball teams play a 162-game season, during which fans are often excited by
long winning streaks and frustrated by long losing streaks. But how unusual are these streaks,
really? How long a streak would you expect if the team ’s performance were independent from
game to game?1.6 Simulation of Random Events 57
Write a program that simulates a 162-game season, i.e., a string of 162 wins and losses, with
P(win) ¼pfor each game (the value of pto be speciﬁed later). Use your program with at least
10,000 runs to answer the following questions.
(a) Suppose you ’re rooting for a “.500” team—that is, p¼.5. What is the probability of
observing a streak of at least ﬁve wins in a 162-game season? Estimate this probability with
your program, and include a standard error.
(b) Suppose instead your team is quite good: a .600 team overall, so p¼.6. Intuitively, should
the probability of a winning streak of at least ﬁve games be higher or lower? Explain.
(c) Use your program with p¼.6 to estimate the probability alluded to in (b). Is your answer
higher or lower than (a)? Is that what you anticipated?
111. A derangement of the numbers 1 through nis a permutation of all nthose numbers such that
none of them is in the “right place.” For example, 34251 is a derangement of 1 through 5, but
24351 is not because 3 is in the 3rd position. We will use simulation to estimate the number of
derangements of the numbers 1 through 12.
(a) Write a program that generates random permutations of the integers 1, 2, ..., 12. Your
program should determine whether or not each permutation is a derangement.
(b) Based on your program, estimate P(D), where D¼{a permutation of 1–12 is a
derangement}.
(c) From Sect. 1.3, we know the number of permutations of nitems. (How many is that for
n¼12?) Use this information and your answer to part (b) to estimate the number of
derangements of the numbers 1 through 12.
[Hint for part (a): Use random sampling without replacement as in Example 1.42. Alternatively,
therandperm command in Matlab can also be employed.]
112. The book’s Introduction discussed the famous Birthday Problem , which was solved in Example
1.22 of Sect. 1.3. Now suppose you have 500 Facebook friends. Make the same assumptions
here as in the Birthday Problem.
(a) Write a program to estimate the probability that, on at least 1 day during the year,
Facebook tells you three (or more) of your friends share that birthday. Based on your
answer, should you be surprised by this occurrence?
(b) Write a program to estimate the probability that, on at least 1 day during the year,
Facebook tells you ﬁve(or more) of your friends share that birthday. Based on your
answer, should you be surprised by this occurrence?
[Hint: Generate 500 birthdays with replacement, then determine whether any birthday occurs
three or more times (ﬁve or more for part (b)). The table function in R or tabulate in
Matlab may prove useful.]
113. Consider the following game: you begin with $20. You ﬂip a fair coin, winning $10 if the coin
lands heads and losing $10 if the coin lands tails. Play continues until you either go broke or
have $100 (i.e., a net proﬁt of $80). Write a simulation program to estimate:
(a) The probability you win the game.
(b) The probability the game ends within ten coin ﬂips.
[Note: This is a special case of the Gambler ’s Ruin problem, which we ’ll explore in much
greater depth in Exercise 145 and again in Chap. 6.]
114. Consider the Coupon Collector ’s Problem described in the Introduction: 10 different coupons
are distributed into cereal boxes, one per box, so that any randomly selected box is equally likely
to have any of the 10 coupons inside. Write a program to simulate the process of buying cereal
boxes until all 10 distinct coupons have been collected. For each run, keep track of how many58 1 Probability
cereal boxes you purchased to collect the complete set of coupons. Then use your program to
answer the following questions.
(a) What is the probability you collect all 10 coupons with just 10 cereal boxes?
(b) Use counting techniques to determine the exact probability in (a). [ Hint: Relate this to the
Birthday Problem.]
(c) What is the probability you require more than 20 boxes to collect all 10 coupons?
(d) Using techniques from Chap. 4, it can be shown that it takes about 29.3 boxes, on the
average, to collect all 10 coupons. What ’s the probability of collecting all 10 coupons in
fewer than average boxes (i.e., less than 29.3)?
115. In the Introduction we mentioned a famous puzzle from the early days of probability,
investigated by Pascal and Fermat. Which of the following events is more likely: to roll at
least one ⚅in four rolls of a fair die, or to roll at least one ⚅⚅in 24 rolls of two fair dice?
(a) Write a program to simulate a set of four die rolls many times, and use the results to
estimate P(at least one ⚅in 4 rolls).
(b) Now adapt your program to simulate rolling a pair of dice 24 times. Repeat this simulation
many times, and use your results to estimate P(at least one ⚅⚅in 24 rolls).
116. The Problem of the Points . Pascal and Fermat also explored a question concerning how to divide
the stakes in a game that has been interrupted. Suppose two players, Blaise and Pierre, are
playing a game where the winner is the ﬁrst to achieve a certain number of points. The game gets
interrupted at a moment when Blaise needs nmore points to win and Pierre needs mmore to
win. How should the game ’s prize money be divvied up? Fermat argued that Blaise should
receive a proportion of the total stake equal to the chance he would have won if the game hadn ’t
been interrupted (and Pierre receives the remainder).
Assume the game is played in rounds, the winner of each round gets 1 point, rounds are
independent, and the two players are equally likely to win any particular round.
(a) Write a program to simulate the rounds of the game that would have happened after play
was interrupted. A single simulation run should terminate as soon as Blaise has nwins or
Pierre has mwins (equivalently, Blaise has mlosses). Use your program to estimate P
(Blaise gets 10 wins before 15 losses), which is the proportion of the total stake Blaise
should receive if n¼10 and m¼15.
(b) Use your same program to estimate the relevant probability when n¼m¼10. Logically,
what should the answer be? Is your estimated probability close to that?
(c) Finally, let ’s assume Pierre is actually the better player: P(Blaise wins a round) ¼.4.
Again with n¼10 and m¼15, what proportion of the stake should be awarded to Blaise?
117. Twenty faculty members in a certain department have just participated in a department chair
election. Suppose that candidate A has received 12 of the votes and candidate B the other
8 votes. If the ballots are opened one by one in random order and the candidate selected on each
ballot is recorded, use simulation to estimate the probability that candidate A remains ahead of
candidate B throughout the vote count (which happens if, for example, the result is
AA ...AB ...B but not if the result is AABABB ...).
118. Show that the (estimated) standard error for ^PAðÞis at most 1 =ﬃﬃﬃﬃﬃ
4np
.
119. Simulation can be used to estimate numerical constants, such as π. Here ’s one approach:
consider the part of a disk of radius 1 that lies in the ﬁrst quadrant (a quarter-circle). Imagine
two random numbers, xandy, both between 0 and 1. The pair ( x,y) lies somewhere in the ﬁrst
quadrant; let Adenote the event that ( x,y) falls inside the quarter-circle.
(a) Write a program that simulates pairs ( x,y) in order to estimate P(A), the probability that a
randomly selected pair of points in the square [0, 1] /C2[0, 1] lies in the quarter-circle of
radius 1.1.6 Simulation of Random Events 59
(b) Using techniques from Chap. 4, it can be shown that the exact probability of Aisπ/4 (which
makes sense, because that ’s the ratio of the quarter-circle ’s area to the square ’s area). Use
that fact to come up with an estimate of πfrom your simulation. How close is your estimate
to 3.14159 ...?
120. Consider the quadratic equation ax2+bx+c¼0. Suppose that a,b, and care random numbers
between 0 and 1 (like those produced by an RNG). Estimate the probability that the roots of this
quadratic equation are real. [ Hint: Think about the discriminant.] This probability can be
computed exactly using methods from Chap. 4, but a triple integral is required.
1.7 Supplementary Exercises (121–150)
121. A small manufacturing company will start operating a night shift. There are 20 machinists
employed by the company.
(a) If a night crew consists of 3 machinists, how many different crews are possible?
(b) If the machinists are ranked 1, 2, ..., 20 in order of competence, how many of these crews
would not have the best machinist?
(c) How many of the crews would have at least 1 of the 10 best machinists?
(d) If one of these crews is selected at random to work on a particular night, what is the
probability that the best machinist will not work that night?
122. A factory uses three production lines to manufacture cans of a certain type. The accompanying
table gives percentages of nonconforming cans, categorized by type of nonconformance, for
each of the three lines during a particular time period.
Line 1 Line 2 Line 3
Blemish 15 12 20
Crack 50 44 40
Pull-Tab Problem 21 28 24
Surface Defect 10 8 15
Other 4 8 2
During this period, line 1 produced 500 nonconforming cans, line 2 produced 400 such cans,
and line 3 was responsible for 600 nonconforming cans. Suppose that one of these 1,500 cans is
randomly selected.
(a) What is the probability that the can was produced by line 1? That the reason for noncon-
formance is a crack?
(b) If the selected can came from line 1, what is the probability that it had a blemish?
(c) Given that the selected can had a surface defect, what is the probability that it came from
line 1?
123. An employee of the records ofﬁce at a university currently has ten forms on his desk awaiting
processing. Six of these are withdrawal petitions and the other four are course substitution
requests.
(a) If he randomly selects six of these forms to give to a subordinate, what is the probability
that only one of the two types of forms remains on his desk?
(b) Suppose he has time to process only four of these forms before leaving for the day. If these
four are randomly selected one by one, what is the probability that each succeeding form is
of a different type from its predecessor?60 1 Probability
124. One satellite is scheduled to be launched from Cape Canaveral in Florida, and another launching
is scheduled for Vandenberg Air Force Base in California. Let Adenote the event that the
Vandenberg launch goes off on schedule, and let Brepresent the event that the Cape Canaveral
launch goes off on schedule. If AandBare independent events such that P(A)>P(B),
P(A[B)¼.626, and P(A\B)¼.144, determine the values of P(A) and P(B).
125. A transmitter is sending a message by using a binary code, namely, a sequence of 0s and 1s.
Each transmitted bit (0 or 1) must pass through three relays to reach the receiver. At each relay,
the probability is .20 that the bit sent will be different from the bit received (a reversal). Assume
that the relays operate independently of one another.
Transmitter !Relay 1 !Relay 2 !Relay 3 !Receiver
(a) If a 1 is sent from the transmitter, what is the probability that a 1 is sent by all three relays?
(b) If a 1 is sent from the transmitter, what is the probability that a 1 is received by the
receiver? [ Hint: The eight experimental outcomes can be displayed on a tree diagram with
three generations of branches, one generation for each relay.]
(c) Suppose 70% of all bits sent from the transmitter are 1s. If a 1 is received by the receiver,
what is the probability that a 1 was sent?
126. Individual A has a circle of ﬁve close friends (B, C, D, E, and F). A has heard a certain rumor
from outside the circle and has invited the ﬁve friends to a party to circulate the rumor. To begin,
A selects one of the ﬁve at random and tells the rumor to the chosen individual. That individual
then selects at random one of the four remaining individuals and repeats the rumor. Continuing,
a new individual is selected from those not already having heard the rumor by the individual
who has just heard it, until everyone has been told.
(a) What is the probability that the rumor is repeated in the order B, C, D, E, and F?
(b) What is the probability that F is the third person at the party to be told the rumor?
(c) What is the probability that F is the last person to hear the rumor?
127. Refer to the previous exercise. If at each stage the person who currently “has” the rumor does
not know who has already heard it and selects the next recipient at random from all ﬁve possible
individuals, what is the probability that F has still not heard the rumor after it has been told ten
times at the party?
128. According to the article “Optimization of Distribution Parameters for Estimating Probability of
Crack Detection” ( J. of Aircraft , 2009: 2090-2097), the following “Palmberg” equation is
commonly used to determine the probability Pd(c) of detecting a crack of size cin an aircraft
structure:
PdcðÞ ¼c=c*/C0/C1β
1þc=c*ðÞβ
where c* is the crack size that corresponds to a .5 detection probability (and thus is an
assessment of the quality of the inspection process).
(a) Verify that Pd(c*)¼.5.
(b) What is Pd(2c*) when β¼4?
(c) Suppose an inspector inspects two different panels, one with a crack size of c* and the
other with a crack size of 2 c*. Again assuming β¼4 and also that the results of the two
inspections are independent of one another, what is the probability that exactly one of the
two cracks will be detected?
(d) What happens to Pd(c)a sβ!1 ?
129. A sonnet is a 14 line poem in which certain rhyming patterns are followed. The writer Raymond
Queneau published a book containing just 10 sonnets, each on a different page. However, these1.7 Supplementary Exercises (121–150) 61
were such that the ﬁrst line of a sonnet could come from the ﬁrst line on any of the 10 pages, the
second line could come from the second line on any of the ten pages, and so on (successive lines
were perforated for this purpose).
(a) How many sonnets can be created from the 10 in the book?
(b) If one of the sonnets counted in (a) is selected at random, what is the probability that all
14 lines come from exactly two of the ten pages?
130. A chemical engineer is interested in determining whether a certain trace impurity is present in a
product. An experiment has a probability of .80 of detecting the impurity if it is present. The
probability of not detecting the impurity if it is absent is .90. The prior probabilities of the
impurity being present and being absent are .40 and .60, respectively. Three separate
experiments result in only two detections. What is the posterior probability that the impurity
is present?
131. Fasteners used in aircraft manufacturing are slightly crimped so that they lock enough to avoid
loosening during vibration. Suppose that 95% of all fasteners pass an initial inspection. Of the
5% that fail, 20% are so seriously defective that they must be scrapped. The remaining fasteners
are sent to a recrimping operation, where 40% cannot be salvaged and are discarded. The other
60% of these fasteners are corrected by the recrimping process and subsequently pass
inspection.
(a) What is the probability that a randomly selected incoming fastener will pass inspection
either initially or after recrimping?
(b) Given that a fastener passed inspection, what is the probability that it passed the initial
inspection and did not need recrimping?
132. One percent of all individuals in a certain population are carriers of a particular disease. A
diagnostic test for this disease has a 90% detection rate for carriers and a 5% detection rate for
noncarriers. Suppose the test is applied independently to two different blood samples from the
same randomly selected individual.
(a) What is the probability that both tests yield the same result?
(b) If both tests are positive, what is the probability that the selected individual is a carrier?
133. A system consists of two components. The probability that the second component functions in a
satisfactory manner during its design life is .9, the probability that at least one of the two
components does so is .96, and the probability that both components do so is .75. Given that the
ﬁrst component functions in a satisfactory manner throughout its design life, what is the
probability that the second one does also?
134. A certain company sends 40% of its overnight mail parcels via express mail service E1. Of these
parcels, 2% arrive after the guaranteed delivery time (denote the event “late delivery” by L). If a
record of an overnight mailing is randomly selected from the company ’s ﬁle, what is the
probability that the parcel went via E1and was late?
135. Refer to the previous exercise. Suppose that 50% of the overnight parcels are sent via express
mail service E2and the remaining 10% are sent via E3. Of those sent via E2, only 1% arrive late,
whereas 5% of the parcels handled by E3arrive late.
(a) What is the probability that a randomly selected parcel arrived late?
(b) If a randomly selected parcel has arrived on time, what is the probability that it was not sent
viaE1?
136. A company uses three different assembly lines— A1,A2, and A3—to manufacture a particular
component. Of those manufactured by line A1, 5% need rework to remedy a defect, whereas 8%
ofA2’s components need rework and 10% of A3’s need rework. Suppose that 50% of all
components are produced by line A1, 30% are produced by line A2, and 20% come from line62 1 Probability
A3. If a randomly selected component needs rework, what is the probability that it came from line
A1? From line A2? From line A3?
137. Disregarding the possibility of a February 29 birthday, suppose a randomly selected individual
is equally likely to have been born on any one of the other 365 days. If ten people are randomly
selected, what is the probability that either at least two have the same birthday or at least two
have the same last three digits of their Social Security numbers? [ Note: The article “Methods for
Studying Coincidences” (F. Mosteller and P. Diaconis, J. Amer. Statist. Assoc. , 1989: 853–861)
discusses problems of this type.]
138. One method used to distinguish between granitic ( G) and basaltic ( B) rocks is to examine a
portion of the infrared spectrum of the sun ’s energy reﬂected from the rock surface. Let R1,R2,
andR3denote measured spectrum intensities at three different wavelengths; typically, for
granite R1<R2<R3, whereas for basalt R3<R1<R2. When measurements are made
remotely (using aircraft), various orderings of the Ris may arise whether the rock is basalt or
granite. Flights over regions of known composition have yielded the following information:
Granite Basalt
R1<R2<R3 60% 10%
R1<R3<R2 25% 20%
R3<R1<R2 15% 70%
Suppose that for a randomly selected rock in a certain region, P(granite) ¼.25 and
P(basalt) ¼.75.
(a) Show that P(granite| R1<R2<R3)>P(basalt| R1<R2<R3). If measurements yielded
R1<R2<R3, would you classify the rock as granite or basalt?
(b) If measurements yielded R1<R3<R2, how would you classify the rock? Answer the
same question for R3<R1<R2.
(c) Using the classiﬁcation rules indicated in parts (a) and (b), when selecting a rock from this
region, what is the probability of an erroneous classiﬁcation? [ Hint: Either Gcould be
classiﬁed as BorBasG, and P(B) and P(G) are known.]
(d) If P(granite) ¼prather than .25, are there values of p(other than 1) for which a rock would
always be classiﬁed as granite?
139. In a Little League baseball game, team A ’s pitcher throws a strike 50% of the time and a ball
50% of the time, successive pitches are independent of each other, and the pitcher never hits a
batter. Knowing this, team B ’s manager has instructed the ﬁrst batter not to swing at anything.
Calculate the probability that
(a) The batter walks on the fourth pitch.
(b) The batter walks on the sixth pitch (so two of the ﬁrst ﬁve must be strikes), using a counting
argument or constructing a tree diagram.
(c) The batter walks.
(d) The ﬁrst batter up scores while no one is out (assuming that each batter pursues a no-swing
strategy).
140. Consider a woman whose brother is afﬂicted with hemophilia, which implies that the woman ’s
mother has the hemophilia gene on one of her two X chromosomes (almost surely not both,
since that is generally fatal). Thus there is a 50–50 chance that the woman ’s mother has passed
on the bad gene to her. The woman has two sons, each of whom will independently inherit the
gene from one of her two chromosomes. If the woman herself has a bad gene, there is a 50–50
chance she will pass this on to a son. Suppose that neither of her two sons is afﬂicted with
hemophilia. What then is the probability that the woman is indeed the carrier of the hemophilia
gene? What is this probability if she has a third son who is also not afﬂicted?1.7 Supplementary Exercises (121–150) 63
141. A particular airline has 10 a.m. ﬂights from Chicago to New York, Atlanta, and Los Angeles.
LetAdenote the event that the New York ﬂight is full and deﬁne events BandCanalogously for
the other two ﬂights. Suppose P(A)¼.6,P(B)¼.5,P(C)¼.4 and the three events are
independent. What is the probability that
(a) All three ﬂights are full? That at least one ﬂight is not full?
(b) Only the New York ﬂight is full? That exactly one of the three ﬂights is full?
142. Consider four independent events A1,A2,A3, and A4and let pi¼P(Ai) fori¼1, 2, 3, 4. Express
the probability that at least one of these four events occurs in terms of the pis, and do the same
for the probability that at least two of the events occur.
143. A box contains the following four slips of paper, each having exactly the same dimensions:
(1) win prize 1; (2) win prize 2; (3) win prize 3; (4) win prizes 1, 2, and 3. One slip will be
randomly selected. Let A1¼{win prize 1}, A2¼{win prize 2}, and A3¼{win prize 3}. Show
thatA1andA2are independent, that A1andA3are independent, and that A2andA3are also
independent (this is pairwise independence). However, show that P(A1\A2\A3)6¼P(A1)/C1
P(A2)/C1P(A3), so the three events are not mutually independent.
144. Jurors may be a priori biased for or against the prosecution in a criminal trial. Each juror is
questioned by both the prosecution and the defense (the voir dire process), but this may not
reveal bias. Even if bias is revealed, the judge may not excuse the juror for cause because of the
narrow legal deﬁnition of bias. For a randomly selected candidate for the jury, deﬁne events B0,
B1, and B2as the juror being unbiased, biased against the prosecution, and biased against the
defense, respectively. Also let Cbe the event that bias is revealed during the questioning
andDbe the event that the juror is eliminated for cause. Let bi¼P(Bi) for i¼0, 1, 2,
c¼P(C|B1)¼P(C|B2), and d¼P(D|B1\C)¼P(D|B2\C) [“Fair Number of Peremptory
Challenges in Jury Trials,” J. Amer. Statist. Assoc. , 1979: 747–753].
(a) If a juror survives the voir dire process, what is the probability that he/she is unbiased
(in terms of the bis,c, and d)? What is the probability that he/she is biased against the
prosecution? What is the probability that he/she is biased against the defense? [ Hint:
Represent this situation using a tree diagram with three generations of branches.]
(b) What are the probabilities requested in (a) if b0¼.50,b1¼.10,b2¼.40 (all based on
data relating to the famous trial of the Florida murderer Ted Bundy), c¼.85
(corresponding to the extensive questioning appropriate in a capital case), and d¼.7
(a “moderate” judge)?
145. Gambler ’s Ruin . Allan and Beth currently have $2 and $3, respectively. A fair coin is tossed. If
the result of the toss is heads, Allan wins $1 from Beth, whereas if the coin toss results in tails,
then Beth wins $1 from Allan. This process is then repeated, with a coin toss followed by the
exchange of $1, until one of the two players goes broke (one of the two gamblers is ruined). We
wish to determine a2¼P(Allan is the winner jhe starts with $2). To do so, let ’s also consider
ai¼P(Allan wins jhe starts with $ i) for i¼0, 1, 3, 4, and 5.
(a) What are the values of a0anda5?
(b) Use the Law of Total Probability to obtain an equation relating a2toa1anda3.[Hint:
Condition on the result of the ﬁrst coin toss, realizing that if it is heads, then from that point
Allan starts with $3.]
(c) Using the logic described in (b), develop a system of equations relating ai(i¼1, 2, 3, 4) to
ai/C01andai+1. Then solve these equations. [ Hint: Write each equation so that ai/C0ai/C01is on
the left hand side. Then use the result of the ﬁrst equation to express each other ai/C0ai/C01as
a function of a1, and add together all four of these expressions ( i¼2, 3, 4, 5).]64 1 Probability
(d) Generalize the result to the situation in which Allan ’s initial fortune is $ aand Beth ’si s$ b.
[Note: The solution is a bit more complicated if p¼P(Allan wins $1) 6¼.5. We ’ll explore
Gambler ’s Ruin again in Chap. 6.]
146. The Matching Problem . Four friends—Allison, Beth, Carol, and Diane—who have identical
calculators are studying for a statistics exam. They set their calculators down in a pile before
taking a study break and then pick them up in random order when they return from the break.
(a) What is the probability all four friends pick up the correct calculator?
(b) What is the probability that at least one of the four gets her own calculator? [ Hint: Let Abe
the event that Alice gets her own calculator, and deﬁne events B,C, and Danalogously for
the other three students. How can the event {at least one gets her own calculator} be
expressed in terms of the four events A,B,C, and D? Now use a general law of probability.]
(c) Generalize the answer from part (b) to nindividuals. Can you recognize the result when nis
large (the approximation to the resulting series)?
147. An event Ais said to attract event BifP(B|A)>P(B) and repel B ifP(B|A)<P(B). (This reﬁnes
the notion of dependent events by specifying whether Amakes Bmore likely or less likely to
occur.)
(a) Show that if Aattracts B, then Arepels B0.
(b) Show that if Aattracts B, then A0repels B.
(c) Prove the Law of Mutual Attraction : event Aattracts event Bif, and only if, Battracts A.
148. The Secretary Problem . A personnel manager is to interview four candidates for a job. These are
ranked 1, 2, 3, and 4 in order of preference and will be interviewed in random order. However, at
the conclusion of each interview, the manager will know only how the current candidate
compares to those previously interviewed. For example, the interview order 3, 4, 1, 2 generates
no information after the ﬁrst interview, shows that the second candidate is worse than the ﬁrst,
and that the third is better than the ﬁrst two. However, the order 3, 4, 2, 1 would generate the
same information after each of the ﬁrst three interviews. The manager wants to hire the best
candidate but must make an irrevocable hire/no hire decision after each interview. Consider the
following strategy: Automatically reject the ﬁrst scandidates and then hire the ﬁrst subsequent
candidate who is best among those already interviewed (if no such candidate appears, the last
one interviewed is hired).
For example, with s¼2, the order 3, 4, 1, 2 would result in the best being hired, whereas the
order 3, 1, 2, 4 would not. Of the four possible svalues (0, 1, 2, and 3), which one maximizes P
(best is hired)? [ Hint: Write out the 24 equally likely interview orderings: s¼0 means that the
ﬁrst candidate is automatically hired.]
149. Jay and Maurice are playing a tennis match. In one particular game, they have reached deuce,
which means each player won three points. Now in order to ﬁnish the game, one of the two
players must get two points ahead of the other. For example, Jay will win if he wins the next two
points ( JJ), or if Maurice wins the next point and Jay the three points after that ( MJJJ ), or if the
result of the next six points is JMMJJJ , etc.
(a) Suppose that the probability of Jay winning a point is .6 and outcomes of successive points
are independent of one another. What is the probability that Jay wins the game? [ Hint:I n
the law of total probability, let A1¼{Jay wins each of the next two points}, A2¼
{Maurice wins each of the next two points}, and A3¼{each player wins one of the next
two points}. Also let p¼P(Jay wins the game). How does pcompare to P(Jay wins the
game| A3)?]
(b) If Jay wins the game, what is the probability that he needed only two points to do so?1.7 Supplementary Exercises (121–150) 65
150. Here is a variant on one of the puzzles mentioned in the book’s Introduction. A fair coin is
tossed repeatedly until either the sequence TTH or the sequence THT is observed. Let Bbe the
event that stopping occurs because TTH was observed (i.e., that TTH is observed before THT).
Calculate P(B). [Hint: Consider the following partition of the sample space: A1¼{1st toss is
H},A2¼{1st two tosses are TT}, A3¼{1st three tosses are THT}, and A4¼{1st three tosses
are THH}. Also denote P(B)b yp. Apply the Law of Total Probability, and pwill appear on both
sides in various places. The resulting equation is easily solved for p.]66 1 Probability
Discrete Random Variables and Probability
Distributions 2
Suppose a city ’s trafﬁc engineering department monitors a certain intersection during a one-hour
period in the middle of the day. Many characteristics might be of interest to the observers, including
the number of vehicles that enter the intersection, the largest number of vehicles in the left turn lane
during a signal cycle, the speed of the fastest vehicle going through the intersection, the average speed
of all vehicles entering the intersection. The value of each one of the foregoing variable quantities is
subject to uncertainty—we don ’t know a priori how many vehicles will enter, what the maximum
speed will be, etc. So each of these is referred to as a random variable —a variable quantity whose
value is determined by what happens in a chance experiment.
There are two fundamentally different types of random variables, discrete and continuous. In this
chapter we examine the basic properties and introduce the most important examples of discrete
random variables. Chapter 3covers the same territory for continuous random variables.
2.1 Random Variables
In any experiment, numerous characteristics can be observed or measured, but in most cases an
experimenter will focus on some speciﬁc aspect or aspects of a sample. For example, in a study of
commuting patterns in a metropolitan area, each individual in a sample might be asked about
commuting distance and the number of people commuting in the same vehicle, but not about IQ,
income, family size, and other such characteristics. Alternatively, a researcher may test a sample of
components and record only the number that have failed within 1000 hours, rather than record the
individual failure times.
In general, each outcome of an experiment can be associated with a number by specifying a rule of
association (e.g., the number among the sample of ten components that fail to last 1,000 h or the total
weight of baggage for a sample of 25 airline passengers). Such a rule of association is called a
random variable —a variable because different numerical values are possible and random because
the observed value depends on which of the possible experimental outcomes results (Fig. 2.1).
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_267
DEFINITION
For a given sample space Sof some experiment, a random variable (rv) is any rule that
associates a number with each outcome in S. In mathematical language, a random variable is a
function whose domain is the sample space and whose range is some subset of real numbers.
Random variables are customarily denoted by uppercase letters, such as XandY, near the end of
our alphabet. We will use lowercase letters to represent some particular value of the corresponding
random variable. The notation X(s)¼xmeans that xis the value associated with the outcome sby
the rv X.
Example 2.1 When a student attempts to connect to a university computer system, either there is a
failure ( F) or there is a success ( S). With S¼{S,F}, deﬁne an rv XbyX(S)¼1,X(F)¼0. The rv
Xindicates whether (1) or not (0) the student can connect. ■
In Example 2.1, the rv Xwas speciﬁed by explicitly listing each element of Sand the associated
number. If Scontains more than a few outcomes, such a listing is tedious, but it can frequently be
avoided.
Example 2.2 Consider the experiment in which a telephone number in a certain area code is dialed
using a random number dialer (such devices are used extensively by polling organizations), and
deﬁne an rv Yby
Y¼1 if the selected number is unlisted
0 if the selected number is listed in the directory/C26
For example, if 5282966 appears in the telephone directory, then Y(5282966) ¼0, whereas
Y(7727350) ¼1 tells us that the number 7727350 is unlisted. A word description of this sort is
more economical than a complete listing, so we will use such a description whenever possible. ■
In Examples 2.1 and 2.2, the only possible values of the random variable were 0 and 1. Such a
random variable arises frequently enough to be given a special name, after the individual who ﬁrst
studied it.
DEFINITION
Any random variable whose only possible values are 0 and 1 is called a Bernoulli random
variable .
We will often want to deﬁne and study several different random variables from the same sample
space.−2 −1 12 0
Fig. 2.1 A random variable68 2 Discrete Random Variables and Probability Distributions
Example 2.3 Example 1.3 described an experiment in which the number of pumps in use at each of
two gas stations was determined. Deﬁne rvs X,Y, and Uby
X¼the total number of pumps in use at the two stations
Y¼the difference between the number of pumps in use at station 1 and the number in use at station 2
U¼the maximum of the numbers of pumps in use at the two stations
If this experiment is performed and s¼(2, 3) results, then X((2, 3)) ¼2+3¼5, so we say that the
observed value of Xisx¼5. Similarly, the observed value of Ywould be y¼2/C03¼/C01, and the
observed value of Uwould be u¼max(2, 3) ¼3. ■
Each of the random variables of Examples 2.1–2.3 can assume only a ﬁnite number of possible
values. This need not be the case.
Example 2.4 Consider an experiment in which 9-V batteries are examined until one with an
acceptable voltage ( S) is obtained. The sample space is S¼{S,FS,FFS,...}. Deﬁne an rv Xby
X¼the number of batteries examined before the experiment terminates
Then X(S)¼1,X(FS)¼2,X(FFS)¼3,...,X(FFFFFFS )¼7, and so on. Any positive integer is a
possible value of X, so the set of possible values is inﬁnite. ■
Example 2.5 Suppose that in some random fashion, a location (latitude and longitude) in the
continental USA is selected. Deﬁne an rv Yby
Y¼the height, in feet, above sea level at the selected location
For example, if the selected location were (39/C14500N, 98/C14350W), then we might have Y((39/C14500N,
98/C14350W))¼1748.26 ft. The largest possible value of Yis 14,494 (Mt. Whitney), and the smallest
possible value is /C0282 (Death Valley). The set of all possible values of Yis the set of all numbers in
the interval between /C0282 and 14,494; that is, the range of Yis
y:/C0282/C20y/C2014, 494 fg ¼/C0 282, 14, 494½/C138
and there are inﬁnitely-many numbers in this interval. ■
2.1.1 Two Types of Random Variables
Determining the values of variables such as the number of visits to a website during a 24-h period or
the number of patients in an emergency room at a particular time requires only counting. On the other
hand, determining values of variables such as fuel efﬁciency of a vehicle (mpg) or reaction time to a
stimulus necessitates making a measurement of some sort. The following deﬁnition formalizes the
distinction between these two different kinds of variables.2.1 Random Variables 69
DEFINITION
Adiscrete random variable is an rv whose possible values constitute either a ﬁnite set or a
countably inﬁnite set (e.g., the set of all integers, or the set of all positive integers).
A random variable is continuous ifboth of the following apply:
1. Its set of possible values consists either of all numbers in a single interval on the number line
(possibly inﬁnite in extent, e.g., from /C01 to1) or all numbers in a disjoint union of such
intervals (e.g., [0, 10] [[20, 30]).
2. No possible value of the variable has positive probability, that is, P(X¼c)¼0 for any
possible value c.
Although any interval on the number line contains inﬁnitely-many numbers, it can be shown that
there is no way to create a listing of all these values—there are just too many of them. The second
condition describing a continuous random variable is perhaps counterintuitive, since it would seem to
imply a total probability of zero for all possible values. But we shall see in Chap. 3thatintervals of
values have positive probability; the probability of an interval will decrease to zero as the width of the
interval shrinks to zero. In practice, discrete variables virtually always involve counting the number
of something, whereas continuous variables entail making measurements of some sort.
Example 2.6 All random variables in Examples 2.1–2.4 are discrete. As another example, suppose
we select married couples at random and do a blood test on each person until we ﬁnd a husband
and wife who both have the same Rh factor. With X¼the number of blood tests to be performed,
possible values of Xare {2, 4, 6, 8, ...}. Since the possible values have been listed in sequence, Xis a
discrete rv. ■
To study basic properties of discrete rvs, only the tools of discrete mathematics—summation and
differences—are required. The study of continuous variables in Chap. 3will require the continuous
mathematics of the calculus—integrals and derivatives.
2.1.2 Exercises: Section 2.1(1–10)
1. A concrete beam may fail either by shear ( S) or ﬂexure ( F). Suppose that three failed beams are
randomly selected and the type of failure is determined for each one. Let X¼the number of
beams among the three selected that failed by shear. List each outcome in the sample space along
with the associated value of X.
2. Give three examples of Bernoulli rvs (other than those in the text).
3. Using the experiment in Example 2.3, deﬁne two more random variables and list the possible
values of each.
4. Let X¼the number of nonzero digits in a randomly selected zip code. What are the possible
values of X? Give three possible outcomes and their associated Xvalues.
5. If the sample space S is an inﬁnite set, does this necessarily imply that any rv Xdeﬁned from S
will have an inﬁnite set of possible values? If yes, say why. If no, give an example.
6. Starting at a ﬁxed time, each car entering an intersection is observed to see whether it turns left
(L), right ( R), or goes straight ahead ( A). The experiment terminates as soon as a car is observed
to turn left. Let X¼the number of cars observed. What are possible Xvalues? List ﬁve outcomes
and their associated Xvalues.70 2 Discrete Random Variables and Probability Distributions
7. For each random variable deﬁned here, describe the set of possible values for the variable, and
state whether the variable is discrete.
(a) X¼the number of unbroken eggs in a randomly chosen standard egg carton
(b) Y¼the number of students on a class list for a particular course who are absent on the ﬁrst
day of classes
(c) U¼the number of times a duffer has to swing at a golf ball before hitting it
(d) X¼the length of a randomly selected rattlesnake
(e) Z¼the amount of royalties earned from the sale of a ﬁrst edition of 10,000 textbooks
(f)Y¼the acidity level (pH) of a randomly chosen soil sample
(g) X¼the tension (psi) at which a randomly selected tennis racket has been strung
(h) X¼the total number of coin tosses required for three individuals to obtain a match
(HHH or TTT)
8. Each time a component is tested, the trial is a success ( S) or failure ( F). Suppose the component is
tested repeatedly until a success occurs on three consecutive trials. Let Ydenote the number of
trials necessary to achieve this. List all outcomes corresponding to the ﬁve smallest possible
values of Y, and state which Yvalue is associated with each one.
9. An individual named Claudius is located at the point 0 in the accompanying diagram.
A2
A1 A4 B4A3
B3 B1B2
0
Using an appropriate randomization device (such as a tetrahedral die, one having four sides),
Claudius ﬁrst moves to one of the four locations B1,B2,B3,B4. Once at one of these locations, he
uses another randomization device to decide whether he next returns to 0 or next visits one of the
other two adjacent points. This process then continues; after each move, another move to one of
the (new) adjacent points is determined by tossing an appropriate die or coin.
(a) Let X¼the number of moves that Claudius makes before ﬁrst returning to 0. What are
possible values of X?I sXdiscrete or continuous?
(b) If moves are allowed also along the diagonal paths connecting 0 to A1,A2,A3, and A4,
respectively, answer the questions in part (a).
10. The number of pumps in use at both a six-pump station and a four-pump station will be
determined. Give the possible values for each of the following random variables:
(a) T¼the total number of pumps in use
(b) X¼the difference between the numbers in use at stations 1 and 2
(c) U¼the maximum number of pumps in use at either station
(d) Z¼the number of stations having exactly two pumps in use
2.2 Probability Distributions for Discrete Random Variables
When probabilities are assigned to various outcomes in S, these in turn determine probabilities
associated with the values of any particular rv X. The probability distribution of X says how the
total probability of 1 is distributed among (allocated to) the various possible Xvalues.2.2 Probability Distributions for Discrete Random Variables 71
Example 2.7 Six batches of components are ready to be shipped by a supplier. The number of
defective components in each batch is as follows:
Batch #1 #2 #3 #4 #5 #6
Number of defectives 020120
One of these batches is to be randomly selected for shipment to a customer. Let Xbe the number of
defectives in the selected batch. The three possible Xvalues are 0, 1, and 2. Of the six equally likely
simple events, three result in X¼0, one in X¼1, and the other two in X¼2. Let p(0) denote the
probability that X¼0 and p(1) and p(2) represent the probabilities of the other two possible values
ofX.T h e n
p0ðÞ ¼ PX¼0 ðÞ ¼ P/C0
batch 1 or 3 or 6 is sent/C1
¼3
6¼:500
p1ðÞ ¼ PX¼1 ðÞ ¼ P/C0
batch 4 is sent/C1
¼1
6¼:167
p2ðÞ ¼ PX¼2 ðÞ ¼ P/C0
batch 2 or 5 is sent/C1
¼2
6¼:333
That is, a probability of .500 is distributed to the Xvalue 0, a probability of .167 is placed on the
Xvalue 1, and the remaining probability, .333, is associated with the Xvalue 2. The values of Xalong
with their probabilities collectively specify the probability distribution or probability mass function of
X. If this experiment were repeated over and over again, in the long run X¼0 would occur one-half of
the time, X¼1 one-sixth of the time, and X¼2 one-third of the time. ■
DEFINITION
Theprobability distribution orprobability mass function (pmf) of a discrete rv is deﬁned for
every number xby
p(x)¼P(X¼x)¼P(alls2S:X(s)¼x).1
In words, for every possible value xof the random variable, the pmf speciﬁes the probability of
observing that value when the experiment is performed. The conditions p(x)/C210 and Σp(x)¼1, where
the summation is over all possible x, are required of any pmf.
Example 2.8 Consider randomly selecting a student at a large public university, and deﬁne a
Bernoulli rv by X¼1 if the selected student does not qualify for in-state tuition (a success from
the university administration ’s point of view) and X¼0 if the student does qualify. If 20% of all
students do not qualify, the pmf for Xis
p(0)¼P(X¼0)¼P(the selected student does qualify) ¼.8
p(1)¼P(X¼1)¼P(the selected student does not qualify) ¼.2
p(x)¼P(X¼x)¼0 for x6¼0o r1 .
1P(X¼x) is read “the probability that the rv Xassumes the value x.” For example, P(X¼2) denotes the probability that
the resulting Xvalue is 2.72 2 Discrete Random Variables and Probability Distributions
pxðÞ ¼:8i f x¼0
:2i f x¼1
0i f x6¼0o r18
<
:
Figure 2.2is a picture of this pmf, called a line graph .
Example 2.9 Consider a group of ﬁve potential blood donors—A, B, C, D, and E—of whom only A
and B have type O+ blood. Five blood samples, one from each individual, will be typed in random
order until an O+ individual is identiﬁed. Let the rv Y¼the number of typings necessary to identify an
O+individual. Then the pmf of Yis
p1ðÞ ¼ PY¼1ðÞ ¼ P/C0
A or B typed first/C1
¼2
5¼:4
p2ðÞ ¼ PY¼2ðÞ ¼ P/C0
C, D, or E first, and then A or B/C1
¼PC, D, or E firstðÞ /C1 P/C0
A or B next/C12/C12C, D, or E first/C1
¼3
5/C12
4¼:3
p3ðÞ ¼ PY¼3ðÞ ¼ P/C0
C, D, or E first and second, and then A or B/C1
¼3
5/C12
4/C12
3¼:2
p4ðÞ ¼ PY¼4ðÞ ¼ P/C0
C, D, and E all done first/C1
¼3
5/C12
4/C11
3¼:1
pyðÞ ¼ 0 for y6¼1, 2, 3, 4 :
The pmf can be presented compactly in tabular form:
y 1234
p(y) .4 .3 .2 .1
where any yvalue not listed receives zero probability. Figure 2.3shows the line graph for this pmf.1
1x
0p(x)
Fig. 2.2 The line graph for the pmf in Example 2.8 ■
.5
1y
0 234p(y)
Fig. 2.3 The line graph for the pmf in Example 2.9 ■2.2 Probability Distributions for Discrete Random Variables 73
The name “probability mass function” is suggested by a model used in physics for a system of
“point masses.” In this model, masses are distributed at various locations xalong a one-dimensional
axis. Our pmf describes how the total probability mass of 1 is distributed at various points along the
axis of possible values of the random variable (where and how much mass at each x).
Another useful pictorial representation of a pmf is called a probability histogram . Above each
ywith p(y)>0, construct a rectangle centered at y. The height of each rectangle is proportional to
p(y), and the base is the same for all rectangles. When possible values are equally spaced, the base is
frequently chosen as the distance between successive yvalues (though it could be smaller). Figure 2.4
shows two probability histograms.
2.2.1 A Parameter of a Probability Distribution
In Example 2.8, we had p(0)¼.8 and p(1)¼.2. At another university, it may be the case that p(0)¼
.9 and p(1)¼.1. More generally, the pmf of any Bernoulli rv can be expressed in the form p(1)¼α
andp(0)¼1/C0α, where 0 <α<1. Because the pmf depends on the particular value of α, we often
write p(x;α) rather than just p(x):
px;αðÞ ¼1/C0αifx¼0
α ifx¼1
0 otherwise8
<
:ð2:1Þ
Then each choice of αin Expression ( 2.1) yields a different pmf.
DEFINITION
Suppose p(x) depends on a quantity that can be assigned any one of a number of possible values,
with each different value determining a different probability distribution. Such a quantity is
called a parameter of the distribution. The collection of all probability distributions for
different values of the parameter is called a family of probability distributions.
The quantity αin Expression ( 2.1) is a parameter. Each different number αbetween 0 and
1 determines a different member of a family of distributions; two such members are
px;:6ðÞ ¼:4i f x¼0
:6i f x¼1
0 otherwise8
<
:and px;:5ðÞ ¼:5i f x¼0
:5i f x¼1
0 otherwise8
<
:
Every probability distribution for a Bernoulli rv has the form of Expression ( 2.1), so it is called the
family of Bernoulli distributions .01 1234b a
Fig. 2.4 Probability histograms: ( a) Example 2.8; ( b) Example 2.974 2 Discrete Random Variables and Probability Distributions
Example 2.10 Starting at a ﬁxed time, we observe the gender of each newborn child at a certain
hospital until a boy ( B) is born. Let p¼P(B), assume that successive births are independent, and
deﬁne the rv XbyX¼number of births observed. Then
p1ðÞ ¼ PX¼1 ðÞ ¼ P/C0
B/C1
¼p
p2ðÞ ¼ PX¼2 ðÞ ¼ P/C0
GB/C1
¼P/C0
G/C1
/C1P/C0
B/C1
¼/C0
1/C0p/C1
p
and
p3ðÞ ¼ PX¼3 ðÞ ¼ P/C0
GGB/C1
¼P/C0
G/C1
/C1P/C0
G/C1
/C1P/C0
B/C1
¼/C0
1/C0p/C12p
Continuing in this way, a general formula emerges:
pxðÞ ¼1/C0pðÞx/C01px¼1, 2, 3, ...
0 otherwise/C26
ð2:2Þ
The quantity pin Expression ( 2.2) represents a number between 0 and 1 and is a parameter of the
probability distribution. In the gender example, p¼.51 might be appropriate, but if we were looking
for the ﬁrst child with Rh-positive blood, then we might have p¼.85. The random variable Xhas
what is known as a geometric distribution , which we will discuss in Sect. 2.6. ■
2.2.2 The Cumulative Distribution Function
For some ﬁxed value x, we often wish to compute the probability that the observed value of Xwill be
at most x . For example, let Xbe the number of beds occupied in a hospital ’s emergency room at a
certain time of day, and suppose the pmf of Xis given by
x 01234
p(x).20 .25 .30 .15 .10
Then the probability that at most two beds are occupied is P(X/C202)¼p(0) + p(1) + p(2)¼.75.
Furthermore, since X/C202.7 iff X/C202, we also have P(X/C202.7)¼.75, and similarly P(X/C202.999) ¼
.75. Since 0 is the smallest possible Xvalue, P(X/C20/C01.5)¼0,P(X/C20/C010)¼0, and in fact for any
negative number x,P(X/C20x)¼0. And because 4 is the largest possible value of X, P(X/C204)¼1,
P(X/C209.8)¼1, and so on.
Very importantly, P(X<2)¼p(0) + p(1)¼.45<.75¼P(X/C202), because the latter probability
includes the probability mass at the xvalue 2 whereas the former probability does not. More
generally, P(X<x)<P(X/C20x) whenever xis a possible value of X. Furthermore, P(X/C20x)i sa
well-deﬁned and computable probability for anynumber x.
DEFINITION
Thecumulative distribution function (cdf) F(x) of a discrete rv Xwith pmf p(x) is deﬁned for
every number xby
FxðÞ ¼ PX/C20x ðÞ ¼X
y:y/C20xpyðÞ ð 2:3Þ
For any number x,F(x) is the probability that the observed value of Xwill be at most x.2.2 Probability Distributions for Discrete Random Variables 75
Example 2.11 A store carries ﬂash drives with 1, 2, 4, 8, or 16 GB of memory. The accompanying
table gives the distribution of X¼the amount of memory in a purchased drive:
x 1248 1 6
p(x) .05 .10 .35 .40 .10
Let ’s ﬁrst determine F(x) for each of the ﬁve possible values of X:
F1ðÞ ¼ PX/C201 ðÞ ¼ PX¼1 ðÞ ¼ p1ðÞ ¼ :05
F2ðÞ ¼ PX/C202 ðÞ ¼ PX¼1o r2 ðÞ ¼ p1ðÞ þ p2ðÞ ¼ :15
F4ðÞ ¼ PX/C204 ðÞ ¼ PX¼1 or 2 or 4 ðÞ ¼ p1ðÞ þ p2ðÞ þ p4ðÞ ¼ :50
F8ðÞ ¼ PX/C208 ðÞ ¼ p1ðÞ þ p2ðÞ þ p4ðÞ þ p8ðÞ ¼ :90
F16ðÞ ¼ PX/C2016 ðÞ ¼ 1
Now for any other number x,F(x) will equal the value of Fat the closest possible value of Xto the
left of x. For example,
F2:7ðÞ ¼ PX/C202:7 ðÞ ¼ PX/C202 ðÞ ¼ F2ðÞ ¼ :15
F7:999ðÞ ¼ PX/C207:999 ðÞ ¼ PX/C204 ðÞ ¼ F4ðÞ ¼ :50
Ifxis less than 1, F(x)¼0 [e.g., F(.58)¼0], and if xis at least 16, F(x)¼1 [e.g., F(25)¼1]. The
cdf is thus
FxðÞ ¼0 x<1
:05 1 /C20x<2
:15 2 /C20x<4
:50 4 /C20x<8
:90 8 /C20x<16
11 6 /C20x8
>>>>>><
>>>>>>:
A graph of this cdf is shown in Fig. 2.5.
ForXa discrete rv, the graph of F(x) will have a jump at every possible value of Xand will be ﬂat
between possible values. Such a graph is called a step function .20 15 10 5 01.0
0.8
0.6
0.4
0.2
0.0
xF(x)
Fig. 2.5 A graph of the cdf of Example 2.11 ■76 2 Discrete Random Variables and Probability Distributions
Example 2.12 In Example 2.10, any positive integer was a possible Xvalue, and the pmf was
pxðÞ ¼1/C0pðÞx/C01px¼1, 2, 3, ...
0 otherwise/C26
For any positive integer x,
FxðÞ ¼X
y/C20xpyðÞ ¼Xx
y¼11/C0pðÞy/C01p¼pXx/C01
y¼01/C0pðÞyð2:4Þ
To evaluate this sum, we use the fact that the partial sum of a geometric series is
Xk
y¼0ay¼1/C0akþ1
1/C0a
Using this in Eq. ( 2.4), with a¼1/C0pandk¼x/C01, gives
FxðÞ ¼ p/C11/C01/C0pðÞx
1/C01/C0pðÞ¼1/C01/C0pðÞxxa positive integer
Since Fis constant in between positive integers,
FxðÞ ¼0 x<1
1/C01/C0pðÞx½/C138x/C211/C26
ð2:5Þ
where [ x] is the largest integer /C20x(e.g., [2.7] ¼2). Thus if p¼.51 as in the birth example, then the
probability of having to examine at most ﬁve births to see the ﬁrst boy is F(5)¼1/C0(.49)5¼1/C0
.0282 ¼.9718, whereas F(10)/C251.0000. This cdf is graphed in Fig. 2.6.
In our examples thus far, the cdf has been derived from the pmf. This process can be reversed to
obtain the pmf from the cdf whenever the latter function is available. Suppose, for example, that
Xrepresents the number of defective components in a shipment consisting of six components, so that
possible Xvalues are 0, 1, ..., 6. Then012345 5 0 5 11
xF(x)
Fig. 2.6 A graph of F(x) for Example 2.12 ■2.2 Probability Distributions for Discrete Random Variables 77
p3ðÞ ¼ PX¼3 ðÞ
¼p0ðÞ þ p1ðÞ þ p2ðÞ þ p3ðÞ ½/C138 /C0 p0ðÞ þ p1ðÞ þ p2ðÞ ½/C138
¼PX/C203 ðÞ /C0 P/C0
X/C202/C1
¼F3ðÞ /C0 F/C0
2/C1
More generally, the probability that Xfalls in a speciﬁed interval is easily obtained from the cdf.
For example,
P2/C20X/C204 ðÞ ¼ p2ðÞ þ p3ðÞ þ p4ðÞ
¼p0ð Þþ/C1/C1/C1þ p4ðÞ ½/C138 /C0 p0ðÞ þ p1ðÞ ½/C138
¼PX/C204 ðÞ /C0 P/C0
X/C201/C1
¼F4ðÞ /C0 F/C0
1/C1
Notice that P(2/C20X/C204)6¼F(4)/C0F(2). This is because the Xvalue 2 is included in 2 /C20X/C204, so
we do not want to subtract out its probability. However, P(2<X/C204)¼F(4)/C0F(2) because X¼2i s
not included in the interval 2 <X/C204.
PROPOSITION
For any two numbers aandbwith a/C20b,
Pa/C20X/C20b ðÞ ¼ FbðÞ /C0 Fa/C0ðÞ
where “ a/C0” represents the largest possible Xvalue that is strictly less than a. In particular, if the
only possible values are integers and if aandbare integers, then
Pa/C20X/C20b ðÞ ¼ PX¼aoraþ1o r ...orb ðÞ
¼FbðÞ /C0 Fa/C01ðÞ
Taking a¼byields P(X¼a)¼F(a)/C0F(a/C01) in this case.
The reason for subtracting F(a/C0) rather than F(a) is that we want to include P(X¼a);F(b)/C0F(a)
gives P(a<X/C20b). This proposition will be used extensively when computing binomial and Poisson
probabilities in Sects. 2.4and2.5.
Example 2.13 LetX¼the number of days of sick leave taken by a randomly selected employee of a
large company during a particular year. If the maximum number of allowable sick days per year is
14, possible values of Xare 0, 1, ..., 14. With F(0)¼.58,F(1)¼.72,F(2)¼.76,F(3)¼.81,F(4)¼
.88, and F(5)¼.94,
P2/C20X/C205 ðÞ ¼ PX¼2, 3, 4, or 5 ðÞ ¼ F5ðÞ /C0 F1ðÞ ¼ :22
and
PX¼3 ðÞ ¼ F3ðÞ /C0 F2ðÞ ¼ :05 ■
2.2.3 Another View of Probability Mass Functions
It is often helpful to think of a pmf as specifying a mathematical model for a discrete population.78 2 Discrete Random Variables and Probability Distributions
Example 2.14 Consider selecting at random a household in a certain region, and Let X¼the number
of individuals in the selected household. Suppose the pmf of Xis as follows:
x 123456789 1 0
p(x).140 .175 .220 .260 .155 .025 .015 .005 .004 .001
This is very close to the household size distribution for rural Thailand given in the article “The
Probability of Containment for Multitype Branching Process Models for Emerging Epidemics” ( J. of
Applied Probability , 2011: 173–188), which modeled inﬂuenza transmission.
Suppose this is based on one million households. One way to view this situation is to think of the
population as consisting of 1,000,000 households, each one having its own Xvalue; the proportion
with each Xvalue is given by p(x) in the above table. An alternative viewpoint is to forget about the
households and think of the population itself as consisting of Xvalues—14% of these values are
1, 17.5% are 2, and so on. The pmf then describes the distribution of the possible population values
1, 2, ..., 10. ■
Once we have such a population model, we will use it to compute values of various population
characteristics such as the mean , which describes the center of the population distribution, and the
standard deviation , which describes the extent of spread about the center. Both of these are developed
in the next section.
2.2.4 Exercises: Section 2.2(11–28)
11. Let Xbe the number of students who show up at a professor ’s ofﬁce hours on a particular day.
Suppose that the only possible values of Xare 0, 1, 2, 3, and 4, and that p(0)¼.30,p(1)¼.25,
p(2)¼.20, and p(3)¼.15.
(a) What is p(4)?
(b) Draw both a line graph and a probability histogram for the pmf of X.
(c) What is the probability that at least two students come to the ofﬁce hour? What is the
probability that more than two students come to the ofﬁce hour?
(d) What is the probability that the professor shows up for his ofﬁce hour?
12. Airlines sometimes overbook ﬂights. Suppose that for a plane with 50 seats, 55 passengers have
tickets. Deﬁne the random variable Yas the number of ticketed passengers who actually show up
for the ﬂight. The probability mass function of Yappears in the accompanying table.
y 45 46 47 48 49 50 51 52 53 54 55
p(y) .05 .10 .12 .14 .25 .17 .06 .05 .03 .02 .01
(a) What is the probability that the ﬂight will accommodate all ticketed passengers who show
up?
(b) What is the probability that not all ticketed passengers who show up can be
accommodated?
(c) If you are the ﬁrst person on the standby list (which means you will be the ﬁrst one to get on
the plane if there are any seats available after all ticketed passengers have been
accommodated), what is the probability that you will be able to take the ﬂight? What is
this probability if you are the third person on the standby list?2.2 Probability Distributions for Discrete Random Variables 79
13. A mail-order computer business has six telephone lines. Let Xdenote the number of lines in use
at a speciﬁed time. Suppose the pmf of Xis as given in the accompanying table.
x 0123456
p(x).10 .15 .20 .25 .20 .06 .04
Calculate the probability of each of the following events.
(a) {at most three lines are in use}
(b) {fewer than three lines are in use}
(c) {at least three lines are in use}
(d) {between two and ﬁve lines, inclusive, are in use}
(e) {between two and four lines, inclusive, are not in use}
(f) {at least four lines are not in use}
14. A contractor is required by a county planning department to submit one, two, three, four, or ﬁve
forms (depending on the nature of the project) in applying for a building permit. Let Y¼the
number of forms required of the next applicant. The probability that yforms are required is
known to be proportional to y—that is, p(y)¼kyfory¼1,...,5 .
(a) What is the value of k?[Hint:∑y¼15p(y)¼1.]
(b) What is the probability that at most three forms are required?
(c) What is the probability that between two and four forms (inclusive) are required?
(d) Could p(y)¼y2/50 for y¼1,..., 5 be the pmf of Y?
15. Many manufacturers have quality control programs that include inspection of incoming
materials for defects. Suppose a computer manufacturer receives computer boards in lots of
ﬁve. Two boards are selected from each lot for inspection. We can represent possible outcomes
of the selection process by pairs. For example, the pair (1, 2) represents the selection of boards
1 and 2 for inspection.
(a) List the ten different possible outcomes.
(b) Suppose that boards 1 and 2 are the only defective boards in a lot of ﬁve. Two boards are to
be chosen at random. Deﬁne Xto be the number of defective boards observed among those
inspected. Find the probability distribution of X.
(c) Let F(x) denote the cdf of X. First determine F(0)¼P(X/C200),F(1), and F(2), and then
obtain F(x) for all other x.
16. Some parts of California are particularly earthquake-prone. Suppose that in one such area, 25%
of all homeowners are insured against earthquake damage. Four homeowners are to be selected
at random; let Xdenote the number among the four who have earthquake insurance.
(a) Find the probability distribution of X.[Hint: LetSdenote a homeowner who has insurance
andFone who does not. Then one possible outcome is SFSS , with probability (.25)(.75)
(.25)(.25) and associated Xvalue 3. There are 15 other outcomes.]
(b) Draw the corresponding probability histogram.
(c) What is the most likely value for X?
(d) What is the probability that at least two of the four selected have earthquake insurance?
17. A new battery ’s voltage may be acceptable ( A) or unacceptable ( U). A certain ﬂashlight requires
two batteries, so batteries will be independently selected and tested until two acceptable ones
have been found. Suppose that 90% of all batteries have acceptable voltages. Let Ydenote the
number of batteries that must be tested.
(a) What is p(2), that is, P(Y¼2)?
(b) What is p(3)? [ Hint: There are two different outcomes that result in Y¼3.]80 2 Discrete Random Variables and Probability Distributions
(c) To have Y¼5, what must be true of the ﬁfth battery selected? List the four outcomes for
which Y¼5 and then determine p(5).
(d) Use the pattern in your answers for parts (a)–(c) to obtain a general formula for p(y).
18. Two fair six-sided dice are tossed independently. Let M¼the maximum of the two tosses, so
M(1, 5) ¼5,M(3, 3) ¼3, etc.
(a) What is the pmf of M?[Hint: First determine p(1), then p(2), and so on.]
(b) Determine the cdf of Mand graph it.
19. A library subscribes to two different weekly news magazines, each of which is supposed to
arrive in Wednesday ’s mail. In actuality, each one may arrive on Wednesday, Thursday, Friday,
or Saturday. Suppose the two arrive independently of one another, and for each one P(W)¼.3,
P(Th)¼.4,P(F)¼.2, and P(S)¼.1. Let Y¼the number of days beyond Wednesday that it
takes for both magazines to arrive (so possible Yvalues are 0, 1, 2, or 3). Compute the pmf of Y.
[Hint: There are 16 possible outcomes; Y(W, W) ¼0,Y(F, Th) ¼2, and so on.]
20. Three couples and two single individuals have been invited to an investment seminar and have
agreed to attend. Suppose the probability that any particular couple or individual arrives late is .4
(a couple will travel together in the same vehicle, so either both people will be on time or else
both will arrive late). Assume that different couples and individuals are on time or late indepen-
dently of one another. Let X¼the number of people who arrive late for the seminar.
(a) Determine the probability mass function of X.[Hint: label the three couples #1, #2, and #3
and the two individuals #4 and #5.]
(b) Obtain the cumulative distribution function of X, and use it to calculate P(2/C20X/C206).
21. As described in the book’s Introduction, Benford ’s Law arises in a variety of situations as a model
for the ﬁrst digit of a number:
pxðÞ¼P1st digit is x ðÞ ¼log10xþ1
x/C18/C19
,x¼1, 2, ...,9
(a) Without computing individual probabilities from this formula, show that it speciﬁes a
legitimate pmf.
(b) Now compute the individual probabilities and compare to the distribution where 1, 2, ...,
9 are equally likely.
(c) Obtain the cdf of X, a rv following Benford ’s law.
(d) Using the cdf, what is the probability that the leading digit is at most 3? At least 5?
22. Refer to Exercise 13, and calculate and graph the cdf F(x). Then use it to calculate the
probabilities of the events given in parts (a)–(d) of that problem.
23. Let Xdenote the number of vehicles queued up at a bank ’s drive-up window at a particular time
of day. The cdf of Xis as follows:
FxðÞ ¼0 x<0
:06 0 /C20x<1
:19 1 /C20x<2
:39 2 /C20x<3
:67 3 /C20x<4
:92 4 /C20x<5
:97 5 /C20x<6
16 /C20x8
>>>>>>>>>><
>>>>>>>>>>:
Calculate the following probabilities directly from the cdf:
(a) p(2), that is, P(X¼2)
(b) P(X>3)2.2 Probability Distributions for Discrete Random Variables 81
(c) P(2/C20X/C205)
(d) P(2<X<5)
24. An insurance company offers its policyholders a number of different premium payment options.
For a randomly selected policyholder, let X¼the number of months between successive
payments. The cdf of Xis as follows:
FxðÞ ¼0 x<1
:30 1 /C20x<3
:40 3 /C20x<4
:45 4 /C20x<6
:60 6 /C20x<12
11 2 /C20x8
>>>>>><
>>>>>>:
(a) What is the pmf of X?
(b) Using just the cdf, compute P(3/C20X/C206) and P(4/C20X).
25. In Example 2.10, let Y¼the number of girls born before the experiment terminates. With
p¼P(B) and 1 /C0p¼P(G), what is the pmf of Y?[Hint: First list the possible values of Y, starting
with the smallest, and proceed until you see a general formula.]
26. Alvie Singer lives at 0 in the accompanying diagram and has four friends who live at A,B,C, and
D. One day Alvie decides to go visiting, so he tosses a fair coin twice to decide which of the four
to visit. Once at a friend ’s house, he will either return home or else proceed to one of the two
adjacent houses (such as 0, A,o r Cwhen at B), with each of the three possibilities having
probability 1/3. In this way, Alvie continues to visit friends until he returns home.
B
CA
D0
(a) Let X¼the number of times that Alvie visits a friend. Derive the pmf of X.
(b) Let Y¼the number of straight-line segments that Alvie traverses (including those leading to
and from 0). What is the pmf of Y?
(c) Suppose that female friends live at AandCand male friends at BandD.I fZ¼the number
of visits to female friends, what is the pmf of Z?
27. After all students have left the classroom, a statistics professor notices that four copies of the text
were left under desks. At the beginning of the next lecture, the professor distributes the four
books in a completely random fashion to each of the four students (1, 2, 3, and 4) who claim to
have left books. One possible outcome is that 1 receives 2 ’s book, 2 receives 4 ’s book, 3 receives
his or her own book, and 4 receives 1 ’s book. This outcome can be abbreviated as (2, 4, 3, 1).
(a) List the other 23 possible outcomes.
(b) Let Xdenote the number of students who receive their own book. Determine the pmf of X.
28. Show that the cdf F(x) is a nondecreasing function; that is, x1<x2implies that F(x1)/C20F(x2).
Under what condition will F(x1)¼F(x2)?82 2 Discrete Random Variables and Probability Distributions
2.3 Expected Value and Standard Deviation
Consider a university with 15,000 students and let X¼the number of courses for which a randomly
selected student is registered. The pmf of Xfollows. Since p(1)¼.01, we know that (.01) /C1(15,000) ¼
150 of the students are registered for one course, and similarly for the other xvalues.
x 1234567
(2.6)p(x) .01 .03 .13 .25 .39 .17 .02
Number registered 150 450 1950 3750 5850 2550 300
To compute the average number of courses per student, i.e., the average value of Xin the
population, we should calculate the total number of courses and divide by the total number of
students. Since each of 150 students is taking one course, these 150 contribute 150 courses to the
total. Similarly, 450 students contribute 2(450) courses, and so on. The population average value of
Xis then
1 150ðÞ þ 2 450ðÞ þ 3 1950ð Þþ/C1/C1/C1þ 7 300ðÞ
15,000¼4:57 ð2:7Þ
Since 150/15,000 ¼.01¼p(1), 450/15,000 ¼.03¼p(2), and so on, an alternative expression for
Eq. ( 2.7)i s
1/C1p1ðÞ þ 2/C1p2ð Þþ/C1/C1/C1þ 7/C1p7ðÞ ð 2:8Þ
Expression ( 2.8) shows that to compute the population average value of X, we need only the
possible values of Xalong with their probabilities (proportions). In particular, the population size is
irrelevant as long as the pmf is given by (2.6). The average or mean value of Xis then a weighted
average of the possible values 1, ..., 7, where the weights are the probabilities of those values.
2.3.1 The Expected Value of X
DEFINITION
LetXbe a discrete rv with set of possible values Dand pmf p(x). The expected value ormean
value ofX, denoted by E(X)o rμXor just μ,i s
EXðÞ ¼ μX¼μ¼X
x2Dx/C1pxðÞ
Example 2.15 For the pmf of X¼number of courses in (2.6),
μ¼1/C1p1ðÞ þ 2/C1p2ð Þþ/C1/C1/C1þ 7/C1p7ðÞ
¼1ðÞ:01ðÞ þ 2ðÞ:03ð Þþ/C1/C1/C1þ 7ðÞ:02ðÞ
¼:01þ:06þ:39þ1:00þ1:95þ1:02þ:14¼4:57
If we think of the population as consisting of the Xvalues 1, 2, ..., 7, then μ¼4.57 is the population
mean (we will often refer to μas the population mean rather than the mean of Xin the population).
Notice that μhere is not 4, the ordinary average of 1, ..., 7, because the distribution puts more weight
on 4, 5, and 6 than on other Xvalues. ■2.3 Expected Value and Standard Deviation 83
In Example 2.15, the expected value μwas 4.57, which is not a possible value of X. The word
expected should be interpreted with caution because one would not expect to see an Xvalue of 4.57
when a single student is selected.
Example 2.16 Just after birth, each newborn child is rated on a scale called the Apgar scale. The
possible ratings are 0, 1, ..., 10, with the child ’s rating determined by color, muscle tone, respiratory
effort, heartbeat, and reﬂex irritability (the best possible score is 10). Let Xbe the Apgar score of
a randomly selected child born at a certain hospital during the next year, and suppose that the pmf
ofXis
x 0123456789 1 0
p(x) .002 .001 .002 .005 .02 .04 .18 .37 .25 .12 .01
Then the mean value of Xis
EXðÞ ¼ μ¼0ðÞ:002ðÞ þ 1ðÞ:001ðÞ þ 2ðÞ:002ð Þþ/C1/C1/C1þ 8ðÞ:25ðÞ þ 9ðÞ:12ðÞ þ 10ðÞ :01ðÞ
¼7:15
(Again, μis not a possible value of the variable X.) If the stated model is correct, then the mean
Apgar score for the population of all children born at this hospital next year will be 7.15. ■
Example 2.17 LetX¼1 if a randomly selected component needs warranty service and ¼0 other-
wise. If the chance a component needs warranty service is p, then Xis a Bernoulli rv with pmf p(1)¼
pandp(0)¼1/C0p, from which
EXðÞ ¼ 0/C1p0ðÞ þ 1/C1p1ðÞ ¼ 01/C0pðÞ þ 1pðÞ ¼ p
That is, the expected value of Xis just the probability that Xtakes on the value 1. If we
conceptualize a population consisting of 0s in proportion 1 /C0pand 1s in proportion p, then the
population average is μ¼p. ■
There is another frequently used interpretation of μ. Consider observing a ﬁrst value x1ofX, then a
second value x2, a third value x3, and so on. After doing this a large number of times, calculate the
sample average of the observed xis. This average will typically be close to μ; a more rigorous version
of this statement is provided by the Law of Large Numbers in Chap. 4. That is, μcan be interpreted as
the long-run average value of Xwhen the experiment is performed repeatedly. This interpretation is
often appropriate for games of chance, where the “population” is not a concrete set of individuals but
rather the results of all hypothetical future instances of playing the game.
Example 2.18 A standard American roulette wheel has 38 spaces. Players bet on which space a
marble will land in once the wheel has been spun. One of the simplest bets is based on the color of the
space: 18 spaces are black, 18 are red, and 2 are green. So, if a player “bets on black,” s/he has an
18/38 chance of winning. Casinos consider color bets an “even wager,” meaning that a player who
bets $1 on black, say, will proﬁt $1 if the marble lands in a black space (and lose the wagered $1
otherwise).
LetX¼the return on a $1 wager on black. Then the pmf of Xis
x /C0$1 +$1
p(x) 20/38 18/3884 2 Discrete Random Variables and Probability Distributions
and the expected value of XisE(X)¼(/C01)(20/38) + (1)(18/38) ¼/C02/38¼/C0$.0526. If a player
makes $1 bets on black on successive spins of the roulette wheel, in the long run s/he can expect to
lose about 5.26 cents per wager. Since players don ’t necessarily make a large number of wagers, this
long-run average interpretation is perhaps more apt from the casino ’s perspective: in the long run,
they will gain an average of 5.26 cents for every $1 wagered on black at the roulette table. ■
Thus far, we have assumed that the mean of any given distribution exists. If the set of possible
values of Xis unbounded, so that the sum for μXis actually an inﬁnite series, the expected value of
Xmight or might not exist (depending on whether the series converges or diverges).
Example 2.19 From Example 2.10, the general form for the pmf of X¼the number of children born
up to and including the ﬁrst boy is
pxðÞ ¼1/C0pðÞx/C01px¼1, 2, 3, ...
0 otherwise/C26
The expected value of Xtherefore entails evaluating an inﬁnite summation:
EXðÞ ¼X
Dx/C1pxðÞ ¼X1
x¼1xp1/C0pðÞx/C01¼pX1
x¼1x1/C0pðÞx/C01¼pX1
x¼1/C0d
dp1/C0pðÞx/C20/C21
ð2:9Þ
If we interchange the order of taking the derivative and the summation in Eq. ( 2.9), the sum is that
of a geometric series. (In particular, the inﬁnite series converges for 0 <p<1.)
After the sum is computed and the derivative is taken, the ﬁnal result is E(X)¼1/p. That is, the
expected number of children born up to and including the ﬁrst boy is the reciprocal of the chance of
getting a boy. This is actually quite intuitive: if pis near 1, we expect to see a boy very soon, whereas
ifpis near 0, we expect many births before the ﬁrst boy. For p¼.5,E(X)¼2.
Exercise 48 at the end of this section presents an alternative method for computing the mean of this
particular distribution. ■
Example 2.20 LetX, the number of interviews a student has prior to getting a job, have pmf
pxðÞ ¼k=x2x¼1, 2, 3, ...
0 otherwise/C26
where kis such that ∑x¼11(k/x2)¼1. (Because ∑x¼11(1/x2)¼π2/6, the value of kis 6/π2.) The
expected value of Xis
μ¼EXðÞ ¼X1
x¼1xk
x2¼kX1
x¼11
xð2:10Þ
The sum on the right of Eq. ( 2.10) is the famous harmonic series of mathematics and can be shown
to diverge. E(X) is not ﬁnite here because p(x) does not decrease sufﬁciently fast as xincreases;
statisticians say that the probability distribution of Xhas “a heavy tail.” If a sequence of Xvalues is
chosen using this distribution, the sample average will not settle down to some ﬁnite number but will
tend to grow without bound. ■2.3 Expected Value and Standard Deviation 85
2.3.2 The Expected Value of a Function
Often we will be interested in the expected value of some function h(X) rather than Xitself. An easy
way of computing the expected value of h(X) is suggested by the following example.
Example 2.21 The cost of a certain vehicle diagnostic test depends on the number of cylinders Xin
the vehicle ’s engine. Suppose the cost function is h(X)¼20+3X+ .5X2. Since Xis a random
variable, so is Y¼h(X). The pmf of Xand the derived pmf of Yare as follows:
x 468)y 40 56 76
p(x) .5 .3 .2 p(y) .5 .3 .2
With D* denoting possible values of Y,
EYðÞ ¼ E/C2
hXðÞ/C3
¼X
y2D*y/C1pyðÞ ¼ 40ðÞ/C0
:5/C1
þ/C0
56/C1/C0
:3/C1
þ/C0
76/C1/C0
:2/C1
¼$52
¼h4ðÞ /C1/C0
:5/C1
þh/C0
6/C1
/C1/C0
:3/C1
þh/C0
8/C1
/C1/C0
:2/C1
¼X
DhxðÞ /C1 pxðÞ ð 2:11Þ
According to Eq. ( 2.11), it was not necessary to determine the pmf of Yto obtain E(Y); instead, the
desired expected value is a weighted average of the possible h(x) (rather than x) values. ■
PROPOSITION
If the rv Xhas a set of possible values Dand pmf p(x), then the expected value of any function
h(X), denoted by E[h(X)] or μh(X), is computed by
EhXðÞ½/C138 ¼X
DhxðÞ /C1 pxðÞ
This is sometimes referred to as the Law of the Unconscious Statistician .
According to this proposition, E[h(X)] is computed in the same way that E(X) itself is, except that
h(x) is substituted in place of x. That is, E[h(X)] is a weighted average of possible h(X) values, where
the weights are the probabilities of the corresponding original Xvalues.
Example 2.22 A computer store has purchased three computers at $500 apiece. It will sell them for
$1,000 apiece. The manufacturer has agreed to repurchase any computers still unsold after a speciﬁed
period at $200 apiece. Let Xdenote the number of computers sold, and suppose that p(0)¼.1,
p(1)¼.2,p(2)¼.3, and p(3)¼.4. With h(X) denoting the proﬁt associated with selling Xunits, the
given information implies that h(X)¼revenue /C0cost¼1000 X+200(3 /C0X)/C01500¼800X/C0900.
The expected proﬁt is then
EhXðÞ½/C138 ¼ h0ðÞ /C1 p0ðÞ þ h1ðÞ /C1 p1ðÞ þ h2ðÞ /C1 p2ðÞ þ h3ðÞ /C1 p3ðÞ
¼800 0ðÞ /C0 900 ðÞ :1ðÞ þ 800 1ðÞ /C0 900 ðÞ :2ðÞ þ 800 2ðÞ /C0 900 ðÞ :3ðÞ þ 800 3ðÞ /C0 900 ðÞ :4ðÞ
¼/C0 900ðÞ :1ðÞ þ /C0 100ðÞ :2ðÞ þ 700ðÞ :3ðÞ þ 1500ðÞ :4ðÞ ¼ $700 ■
Because an expected value is a sum, it possesses the same properties as any summation; speciﬁ-
cally, the expected value “operator” can be distributed across addition and across multiplication by
constants. This important property is known as linearity of expectation.86 2 Discrete Random Variables and Probability Distributions
LINEARITY OF EXPECTATION
For any functions h1(X) and h2(X) and any constants a1,a2, and b,
Ea 1h1XðÞ þ a2h2XðÞ þ b ½/C138 ¼ a1Eh 1XðÞ½/C138 þ a2Eh 2XðÞ½/C138 þ b
In particular, for any linear function aX+b,
Ea X þb ðÞ ¼ a/C1EXðÞ þ b ð2:12Þ
(or, using alternative notation, μaX+b¼a/C1μX+b).
Proof Leth(X)¼a1h1(X)+a2h2(X)+b, and apply the previous proposition:
E/C2
a1h1XðÞ þ a2h2/C0
X/C1
þb/C3
¼X
Da1h1xðÞ þ a2h2xðÞ þ b/C1
/C1p/C0
x/C0/C1
¼a1X
Dh1xðÞ /C1 pxðÞ þ a2X
Dh2xðÞ /C1 pxðÞ
þbX
DpxðÞ distributive property of addition
¼a1E/C2
h1XðÞ/C3
þa2E/C2
h2/C0
X/C1/C3
þb/C2
1/C3
¼a1E/C2
h1/C0
X/C1/C3
þa2E/C2
h2/C0
X/C1/C3
þb
The special case of aX + b is obtained by setting a1¼a,h1(X)¼X, and a2¼0. ■
By induction, linearity of expectation applies to any ﬁnite number of terms. In Example 2.21, it is
easily computed that E(X)¼4(.5) + 6(.3) + 8(.2) ¼5.4 and E(X2)¼∑x2/C1p(x)¼42(.5) + 62(.3) +
82(.2)¼31.6. Applying linearity of expectation to Y¼h(X)¼20+3X+.5X2, we obtain
μY¼E20þ3Xþ:5X2/C2/C3
¼20þ3EXðÞ þ :5EX2/C0/C1
¼20þ35:4ðÞ þ :53 1:6ðÞ ¼ $52,
which matches the result of Example 2.21.
The special case Eq. ( 2.12) states that the expected value of a linear function equals the linear
function evaluated at the expected value E(X). Since h(X) in Example 2.22 is linear and E(X)¼2,
E[h(X)]¼800(2) /C0900¼$700, as before. Two special cases of Eq. ( 2.12) yield two important rules
of expected value.
1. For any constant a,μaX¼a/C1μX(take b¼0).
2. For any constant b,μX+b¼μX+b¼E(X)+b(take a¼1).
Multiplication of Xby a constant achanges the unit of measurement (from dollars to cents, where
a¼100, inches to cm, where a¼2.54, etc.). Rule 1 says that the expected value in the new units equals
the expected value in the old units multiplied by the conversion factor a. Similarly, if the constant bis
added to each possible value of X, then the expected value will be shifted by that same amount.
One commonly made error is to substitute μXdirectly into the function h(X) when his a nonlinear
function, in which case Eq. ( 2.12) does not apply. Consider Example 2.21: the mean of Xis 5.4, and
it’s tempting to infer that the mean of Y¼h(X) is simply h(5.4). However, since the function h(X)¼
20+3X+.5X2isnotlinear, this does not yield the correct answer:
h5:4ðÞ ¼ 20þ35:4ðÞ þ :55:4ðÞ2¼50:786¼52¼μY
In general, μh(X)does not equal h (μX)unless the function h(x) is linear.2.3 Expected Value and Standard Deviation 87
2.3.3 The Variance and Standard Deviation of X
The expected value of Xdescribes where the probability distribution is centered. Using the physical
analogy of placing point mass p(x) at the value xon a one-dimensional axis, if the axis were then
supported by a fulcrum placed at μ, there would be no tendency for the axis to tilt. This is illustrated
for two different distributions in Fig. 2.7.
Although both distributions pictured in Fig. 2.7have the same mean/fulcrum μ, the distribution of
Fig.2.7b has greater spread or variability or dispersion than does that of Fig. 2.7a. Our goal now is to
obtain a quantitative assessment of the extent to which the distribution spreads out about its mean
value.
DEFINITION
LetXhave pmf p(x) and expected value μ. Then the variance ofX, denoted by Var( X)o rσX2or
justσ2,i s
VarXðÞ¼X
Dx/C0μðÞ2/C1pxðÞhi
¼EX/C0μðÞ2hi
Thestandard deviation (SD) of X, denoted by SD( X)o rσXor just σ,i s
σX¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarXðÞp
The quantity h(X)¼(X/C0μ)2is the squared deviation of Xfrom its mean, and σ2is the expected
squared deviation—i.e., a weighted average of the squared deviations from μ. Taking the square root
of the variance to obtain standard deviation returns us to the original units of the variable, e.g., if Xis
measured in dollars, then both μandσalso have units of dollars. If most of the probability distribution
is close to μ, as in Fig. 2.7a, then σwill typically be relatively small. However, if there are xvalues far
from μthat have large probabilities (as in Fig. 2.7b), then σwill be larger.
Example 2.23 Consider again the distribution of the Apgar score Xof a randomly selected newborn
described in Example 2.16. The mean value of Xwas calculated as μ¼7.15, so
VarXðÞ ¼ σ2¼X10
x¼0x/C07:15 ðÞ2/C1pxðÞ ¼ 0/C07:15 ðÞ2:002ðÞ þ ...þ10/C07:15 ðÞ2:01ðÞ ¼ 1:5815
The standard deviation of Xis SD XðÞ ¼ σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1:5815p
¼1:26. ■.5 .5
123 5x
123 5678b a
p(x) p(x)
x
Fig. 2.7 Two different probability distributions with μ¼488 2 Discrete Random Variables and Probability Distributions
A rough interpretation of σis that its value gives the size of a typical or representative distance
from μ(hence, “standard deviation”). Because σ¼1.26 in the preceding example, we can say that
some of the possible Xvalues differ by more than 1.26 from the mean value 7.15 whereas other
possible Xvalues are closer than this to 7.15; roughly, 1.26 is the size of a typical deviation from the
mean Apgar score.
Example 2.24 (Example 2.18 continued) The variance of X¼the return on a $1 bet on black is
σ2
X¼/C0 1/C0/C0 2=38ðÞ ðÞ2/C120=38ðÞ þ 1/C0/C0 2=38ðÞðÞ2/C118=38¼0:99723
and the standard deviation is σX¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:99723p
¼0:9986/C25$1. The two possible values of Xare/C0$1
and +$1; since betting on black is almost a break-even wager (the mean is quite close to 0), the typical
difference between an actual return Xand the average return μXis roughly one dollar. ■
A natural probability question arises: how often does Xfall within this “typical distance of the
mean”? That is, what ’s the chance that a rv Xlies between μX/C0σXandμX+σX? What about the
likelihood that Xis within two standard deviations of its mean? There are no universal answers:
for different pmfs, varying amounts of probability may lie within one (or two or three) standard
deviation(s) of the expected value. That said, the following theorem, due to Russian mathematician
Pafnuty Chebyshev, partially addresses questions of this sort.
CHEBYSHEV ’S INEQUALITY
LetXbe a discrete rv with mean μand standard deviation σ. Then, for any k/C211,
P/C12/C12X/C0μ/C12/C12/C21kσ/C0/C1
/C201
k2
That is, the probability Xis at least kstandard deviations away from its mean is at most 1/ k2.
An equivalent statement to Chebyshev ’s inequality is that every random variable has a probability
of at least 1 /C01/k2to fall within kstandard deviations of its mean.
Proof LetAdenote the event | X/C0μ|/C21kσ; or, equivalently, the set of values { x:|x/C0μ|/C21kσ}. Begin
by writing out the deﬁnition of Var( X):
VarXðÞ ¼X
Dx/C0μðÞ2/C1pxðÞhi
¼X
Ax/C0μðÞ2/C1pxðÞhi
þX
A0x/C0μðÞ2/C1pxðÞhi
/C21X
Ax/C0μðÞ2/C1pxðÞhi
because the discarded term is /C210
/C21X
AkσðÞ2/C1pxðÞhi
because x/C0μðÞ2/C21/C0
kσ/C12on the set A
¼kσðÞ2X
ApxðÞ ¼/C0
kσ/C12P/C0
A/C1
¼k2σ2P/C0/C12/C12X/C0μ/C12/C12/C21kσ/C1
The Var( X) term on the left-hand side is the same as the σ2term on the right-hand side; cancelling
the two, we are left with 1 /C21k2P(|X/C0μ|/C21kσ), and Chebyshev ’s inequality follows. ■2.3 Expected Value and Standard Deviation 89
Fork¼1, Chebyshev ’s inequality states that P(|X/C0μ|/C21σ)/C201, which isn ’t very informative
since all probabilities are bounded above by 1. In fact, distributions can be constructed for which
100% of the distribution is at least 1 standard deviation from the mean, so that the rv Xhas probability
0 of falling less than one standard deviation from its mean (see Exercise 47). Substituting k¼
2, Chebyshev ’s inequality states that the chance any rv is at least 2 standard deviations from its mean
cannot exceed 1/22¼.25¼25%. Equivalently, every distribution has the property that at least 75% of
its “mass” lies within 2 standard deviations of its mean value (in fact, for many distributions, the exact
probability is much larger than this lower bound).
2.3.4 Properties of Variance
An alternative to the deﬁning formula for Var( X) reduces the computational burden.
PROPOSITION
VarXðÞ ¼ σ2¼EX2/C0/C1
/C0μ2
This equation is referred to as the variance shortcut formula .
In using this formula, E(X2) is computed ﬁrst without any subtraction; then μis computed, squared,
and subtracted (once) from E(X2). This formula is more efﬁcient because it entails only one
subtraction, and E(X2) does not require calculating squared deviations from μ.
Example 2.25 Referring back to the Apgar score scenario of Examples 2.16 and 2.23,
EX2/C0/C1
¼X10
x¼1x2/C1pxðÞ ¼ 02/C0/C1
:002ðÞ þ 12/C0/C1
:001ð Þþ/C1/C1/C1þ 102/C0/C1
:01ðÞ ¼ 52:704
Thus, σ2¼52.704 /C0(7.15)2¼1.5815 as before, and again σ¼1.26. ■
Proof of the Variance Shortcut Formula Expand ( X/C0μ)2in the deﬁnition of Var( X), and then
apply linearity of expectation:
VarXðÞ ¼ E/C2
X/C0μðÞ2/C3
¼E/C2
X2/C02μXþμ2/C3
¼EX2/C0/C1
/C02μE/C0
X/C1
þμ2by linearity of expectation
¼EX2/C0/C1
/C02μ/C1μþμ2¼E/C0
X2/C1
/C02μ2þμ2¼EX2/C0/C1
/C0μ2■
The quantity E(X2) in the variance shortcut formula is called the mean -square value of the
random variable X. Engineers may be familiar with the root-mean-square, or RMS, which is the
square root of E(X2). Do not confuse this with the square of the mean of X, i.e., μ2! For example, if
Xhas a mean of 7.15, the mean-square value of Xisnot(7.15)2, because h(x)¼x2is not linear.
(In Example 2.25, the mean-square value of Xis 52.704.) It helps to look at the two formulas side-
by-side:90 2 Discrete Random Variables and Probability Distributions
EX2/C0/C1
¼X
Dx2/C1pxðÞ versus μ2¼X
Dx/C1pxðÞ !2
The order of operations is clearly different. In fact, it can be shown (see Exercise 46) that
E(X2)/C21μ2for every random variable, with equality if and only if Xis constant.
The variance of a function h(X) is the expected value of the squared difference between h(X) and
its expected value:
VarhXðÞ½/C138 ¼ σ2
hXðÞ¼X
DhxðÞ /C0 μhXðÞ/C16/C172
/C1pxðÞ/C20/C21
¼X
Dh2xðÞ /C1 pxðÞ"#
/C0X
DhxðÞ /C1 pxðÞ"#2
When h(x) is a linear function, Var[ h(X)] has a much simpler expression (see Exercise 43 for
a proof).
PROPOSITION
VaraXþb ðÞ ¼ σ2
aXþb¼a2/C1σ2
Xand σaXþb¼/C12/C12a/C12/C12/C1σX ð2:13Þ
In particular,
σaX¼ajj/C1σXand σXþb¼σX
The absolute value is necessary because amight be negative, yet a standard deviation cannot
be. Usually multiplication by acorresponds to a change in the unit of measurement (e.g., kg to lb or
dollars to euros); the sd in the new unit is just the original sd multiplied by the conversion factor. On
the other hand, the addition of the constant bdoes not affect the variance, which is intuitive, because
the addition of bchanges the location (mean value) but not the spread of values. Together, Eqs. ( 2.12)
and ( 2.13) comprise the rescaling properties of mean and standard deviation.
Example 2.26 In the computer sales scenario of Example 2.22, E(X)¼2 and
EX2/C0/C1
¼02/C0/C1
:1ðÞ þ 12/C0/C1
:2ðÞ þ 22/C0/C1
:3ðÞ þ 32/C0/C1
:4ðÞ ¼ 5
so Var( X)¼5/C0(2)2¼1. The proﬁt function Y¼h(X)¼800X/C0900 is linear, so Eq. ( 2.13) applies
with a¼800 and b¼/C0 900. Hence Yhas variance a2σX2¼(800)2(1)¼640,000 and standard
deviation $800. ■
2.3.5 Exercises: Section 2.3(29–48)
29. The pmf of the amount of memory X(GB) in a purchased ﬂash drive was given in Example 2.11 as
x 12481 6
p(x).05 .10 .35 .40 .10
(a) Compute and interpret E(X).
(b) Compute Var( X) directly from the deﬁnition.
(c) Obtain and interpret the standard deviation of X.
(d) Compute Var( X) using the shortcut formula.2.3 Expected Value and Standard Deviation 91
30. An individual who has automobile insurance from a company is randomly selected. Let Ybe the
number of moving violations for which the individual was cited during the last 3 years. The pmf
ofYis
y 0123
p(y) .60 .25 .10 .05
(a) Compute E(Y).
(b) Suppose an individual with Yviolations incurs a surcharge of $100 Y2. Calculate the
expected amount of the surcharge.
31. Refer to Exercise 12 and calculate Var( Y) and σY. Then determine the probability that Yis
within 1 standard deviation of its mean value.
32. An appliance dealer sells three different models of upright freezers having 13.5, 15.9, and 19.1
cubic feet of storage space, respectively. Let X¼the amount of storage space purchased by the
next customer to buy a freezer. Suppose that Xhas pmf
x 13.5 15.9 19.1
p(x) .2 .5 .3
(a) Compute E(X),E(X2), and Var( X).
(b) If the price of a freezer having capacity Xcubic feet is 17 X+ 180, what is the expected
price paid by the next customer to buy a freezer?
(c) What is the standard deviation of the price 17 X+ 180 paid by the next customer?
(d) Suppose that although the rated capacity of a freezer is X, the actual capacity is h(X)¼
X/C0.01X2. What is the expected actual capacity of the freezer purchased by the next
customer?
33. Let Xbe a Bernoulli rv with pmf as in Example 2.17.
(a) Compute E(X2).
(b) Show that Var( X)¼p(1/C0p).
(c) Compute E(X79).
34. Suppose that the number of plants of a particular type found in a rectangular sampling region
(called a quadrat by ecologists) in a certain geographic area is an rv Xwith pmf
pxðÞ ¼c=x3x¼1, 2, 3, ...
0 otherwise/C26
IsE(X) ﬁnite? Justify your answer. (This is another distribution that statisticians would call
heavy-tailed.)
35. A small market orders copies of a certain magazine for its magazine rack each week. Let X¼
demand for the magazine, with pmf
x 123456
p(x)1
152
153
154
153
152
15
Suppose the store owner actually pays $2.00 for each copy of the magazine and the price to
customers is $4.00. If magazines left at the end of the week have no salvage value, is it better to
order three or four copies of the magazine? [ Hint: For both three and four copies ordered,
express net revenue as a function of demand X, and then compute the expected revenue.]92 2 Discrete Random Variables and Probability Distributions
36. Let Xbe the damage incurred (in $) in a certain type of accident during a given year. Possible
Xvalues are 0, 1000, 5000, and 10,000, with probabilities .8, .1, .08, and .02, respectively.
A particular company offers a $500 deductible policy. If the company wishes its expected proﬁt
to be $100, what premium amount should it charge?
37. The ncandidates for a job have been ranked 1, 2, 3, ...,n. Let X¼the rank of a randomly
selected candidate, so that Xhas pmf
pxðÞ ¼1=nx¼1, 2, 3, ...,n
0 otherwise/C26
(this is called the discrete uniform distribution ). Compute E(X) and Var( X) using the shortcut
formula. [ Hint: The sum of the ﬁrst npositive integers is n(n+1)/2, whereas the sum of their
squares is n(n+1)(2n+1)/6.]
38. Let X¼the outcome when a fair die is rolled once. If before the die is rolled you are offered either
$100 dollars or h(X)¼350/Xdollars, would you accept the guaranteed amount or would you
gamble? [ Hint: Determine E[h(X)], but be careful: the mean of 350/ Xis not 350/ μ.]
39. In the popular game Plinko on The Price Is Right , contestants drop a circular disk (a “chip”) down
a pegged board; the chip bounces down the board and lands in a slot corresponding to one of ﬁve
dollar mounts. The random variable X¼winnings from one chip dropped from the middle slot
has roughly the following distribution.
x $0 $100 $500 $1000 $10,000
p(x) .39 .03 .11 .24 .23
(a) Graph the probability mass function of X.
(b) What is the probability a contestant makes money on a chip?
(c) What is the probability a contestant makes at least $1000 on a chip?
(d) Determine the expected winnings. Interpret this number.
(e) Determine the corresponding standard deviation.
40. A supply company currently has in stock 500 lb of fertilizer, which it sells to customers in 10-lb
bags. Let Xequal the number of bags purchased by a randomly selected customer. Sales data
shows that Xhas the following pmf:
x 123 4
p(x) .2 .4 .3 .1
(a) Compute the average number of bags bought per customer.
(b) Determine the standard deviation for the number of bags bought per customer.
(c) Deﬁne Yto be the amount of fertilizer left in stock, in pounds, after the ﬁrst customer.
Construct the pmf of Y.
(d) Use the pmf of Yto ﬁnd the expected amount of fertilizer left in stock, in pounds, after the
ﬁrst customer.
(e) Write Yas a linear function of X. Then use rescaling properties to ﬁnd the mean and standard
deviation of Y.
(f) The supply company offers a discount to each customer based on the formula W¼(X/C01)2.
Determine the expected discount for a customer.
(g) Does your answer in part (f) equal ( μX/C01)2? Why or why not?
(h) Calculate the standard deviation of W.2.3 Expected Value and Standard Deviation 93
41. Refer back to the roulette scenario in Examples 2.18 and 2.24. Two other ways to wager at
roulette are betting on a single number, or on a four-number “square.” The pmfs for the returns on
a $1 wager on a number and a square are displayed below. (Payoffs for winning are always based
on the odds of losing a wager under the assumption the two green spaces didn ’t exist.)
Single number:
x /C0$1 +$35
p(x) 37/38 1/38
Square:
x /C0$1 +$8
p(x) 34/38 4/38
(a) Determine the expected return from a $1 wager on a single number, and then on a square.
(b) Compare your answers from (a) to Example 2.18. What can be said about the expected
return for a $1 wager? Based on this, does expected return reﬂect most players ’intuition that
betting on black is “safer” and betting on a single number is “riskier”?
(c) Now calculate the standard deviations for the two pmfs above.
(d) How do the standard deviations of the three betting schemes (color, single number, square)
compare? How do these values appear to relate to players ’intuitive sense of risk?
42. (a) Draw a line graph of the pmf of Xin Exercise 35. Then determine the pmf of /C0Xand draw
its line graph. From these two pictures, what can you say about Var( X) and Var( /C0X)?
(b) Use the proposition involving Var( aX + b ) to establish a general relationship between
Var(X) and Var( /C0X).
43. Use the deﬁnition of variance to prove that Var( aX+b)¼a2σX2.[Hint: From Eq. ( 2.12),
μaX+b¼aμX+b.]
44. Suppose E(X)¼5 and E[X(X/C01)]¼27.5.
(a) Determine E(X2). [Hint: E [X(X/C01)]¼E(X2/C0X)¼E(X2)/C0E(X).]
(b) What is Var( X)?
(c) What is the general relationship among the quantities E(X),E[X(X/C01)], and Var( X)?
45. Write a general rule for E(X/C0c) where cis a constant. What happens when you let c¼μ, the
expected value of X?
46. Let Xbe a rv with mean μ. Show that E(X2)/C21μ2, and that E(X2)>μ2unless Xis a constant.
[Hint: Consider variance.]
47. Refer to Chebyshev ’s inequality in this section.
(a) What is the value of the upper bound for k¼2?k¼3?k¼4?k¼5?k¼10?
(b) Compute μandσfor the distribution of Exercise 13. Then evaluate for the values of kgiven
in part (a). What does this suggest about the upper bound relative to the corresponding
probability?
(c) Suppose you will win $ dif a fair coin ﬂips heads and lose $ dif it lands tails. Let Xbe the
amount you get from a single coin ﬂip. Compute E(X) and SD( X). What is the probability
Xwill be less than one standard deviation from its mean value?
(d) Let Xhave three possible values, /C01, 0, and 1, with probabilities1
18,8
9, and1
18respectively.
What is P(|X/C0μ|/C213σ), and how does it compare to the corresponding Chebyshev bound?
(e) Give a distribution for which P(|X/C0μ|/C215σ)¼.04.
48. For a discrete rv Xtaking values in {0, 1, 2, 3, ...}, we shall derive the following alternative
formula for the mean:94 2 Discrete Random Variables and Probability Distributions
μX¼X1
x¼01/C0FxðÞ ½/C138
(a) Suppose for now the range of Xis {0, 1, ...N} for some positive integer N. By regrouping
terms, show that
XN
x¼0x/C1pxðÞ½/C138 ¼ p1ðÞ þ p/C0
2/C1
þp/C0
3/C1
þ/C1/C1/C1þ p/C0
N/C1
þp2ðÞ þ p/C0
3/C1
þ/C1/C1/C1þ p/C0
N/C1
þp3ð Þþ/C1/C1/C1þ p/C0
N/C1
⋮
þpNðÞ
(b) Rewrite each row in the above expression in terms of the cdf of X, and use this to
establish that
XN
x¼0x/C1pxðÞ½/C138 ¼XN/C01
x¼01/C0FxðÞ ½/C138
(c) Let N!1 in part (b) to establish the desired result, and explain why the resulting formula
works even if the maximum value of Xis ﬁnite. [ Hint: If the largest possible value of XisN,
what does 1 /C0F(x) equal for x/C21N?] (This derivation also implies that a discrete rv Xhas a
ﬁnite mean iff the series ∑[1/C0F(x)] converges.)
(d) Let Xhave the pmf from Examples 2.10 and 2.19. Use the cdf of Xand the alternative mean
formula just derived to determine μX.
2.4 The Binomial Distribution
Many experiments conform either exactly or approximately to the following list of requirements:
1. The experiment consists of a sequence of nsmaller experiments called trials , where nis ﬁxed in
advance of the experiment.
2. Each trial can result in one of the same two possible outcomes (dichotomous trials), which we
denote by success ( S) or failure ( F).
3. The trials are independent, so that the outcome on any particular trial does not inﬂuence the
outcome on any other trial.
4. The probability of success is constant from trial to trial (homogeneous trials); we denote this
probability by p.
DEFINITION
An experiment for which Conditions 1–4 are satisﬁed—a ﬁxed number of dichotomous,
independent, homogeneous trials—is called a binomial experiment .2.4 The Binomial Distribution 95
Example 2.27 The same coin is tossed successively and independently ntimes .We arbitrarily use
Sto denote the outcome H (heads) and Fto denote the outcome T (tails). Then this experiment
satisﬁes Conditions 1–4. Tossing a thumbtack ntimes, with S¼point up and F¼point down, also
results in a binomial experiment. ■
Some experiments involve a sequence of independent trials for which there are more than two
possible outcomes on any one trial. A binomial experiment can then be created by dividing the
possible outcomes into two groups.
Example 2.28 The color of pea seeds is determined by a single genetic locus. If the two alleles at this
locus are AA or Aa (the genotype), then the pea will be yellow (the phenotype), and if the allele is aa,
the pea will be green. Suppose we pair off 20 Aa seeds and cross the two seeds in each of the ten pairs
to obtain ten new genotypes. Call each new genotype a success Sif it is aa and a failure otherwise.
Then with this identiﬁcation of SandF, the experiment becomes binomial with n¼10 and p¼
P(aa genotype). If each member of the pair is equally likely to contribute a or A, then p¼P(a)/C1
P(a)¼(1/2)(1/2) ¼.25. ■
Example 2.29 A student has an iPod playlist containing 50 songs, of which 35 were recorded prior to
the year 2015 and the other 15 were recorded more recently. Suppose the random play function is
used to select ﬁve from among these 50 songs, without replacement, for listening during a walk
between classes. Each selection of a song constitutes a trial; we regard a trial as a success if the
selected song was recorded before 2015. Then clearly
PSon first trialðÞ ¼35
50¼:70
It may surprise you that the (unconditional) chance the second song is a success also equals .70!
To see why, apply the Law of Total Probability:
PSon second trialðÞ ¼ PS S[FS ðÞ
¼PSon firstðÞ PSon second jSon first ðÞ
þPFon firstðÞ PSon second jFon first ðÞ
¼35
50/C134
49þ15
50/C135
49¼35
5034
49þ15
49/C18/C19
¼35
50¼:70
Similarly, it can be shown that P(Sonith trial) ¼.70 for i¼3, 4, 5, so the trials are homogeneous
(Condition 4), with p¼.70. However the trials are notindependent (Condition 3), because for
example,
PSon fifth trial/C12/C12SSSS/C0/C1
¼31
46¼:67 whereas PSon fifth trial/C12/C12FFFF/C0/C1
¼35
46¼:76
(This matches our intuitive sense that later song selections “depend on” what was chosen before
them.) The experiment is notbinomial because the trials are not independent. In general, if sampling
is without replacement, the experiment will not yield independent trials. If songs had been selected
with replacement, then trials would have been independent, but this might have resulted in the same
song being listened to more than once. ■
Example 2.30 Suppose a state has 500,000 licensed drivers, of whom 400,000 are insured. A sample
of 10 drivers is chosen without replacement. The ith trial is labeled Sif the ith driver chosen is96 2 Discrete Random Variables and Probability Distributions
insured. Although this situation would seem identical to that of Example 2.29, the important
difference is that the size of the population being sampled is very large relative to the sample size.
In this case
PSon second/C12/C12Son first/C0/C1
¼399, 999
499, 999/C25:80000
and
PSon tenth/C12/C12Son first nine/C0/C1
¼399, 991
499, 991¼:799996 /C25:80000
These calculations suggest that although the trials are not exactly independent, the conditional
probabilities differ so slightly from one another that for practical purposes the trials can be regarded
as independent with constant P(S)¼.8. Thus, to a very good approximation, the experiment is
binomial with n¼10 and p¼.8. ■
We will use the following convention in deciding whether a “without-replacement” experiment
can be treated as being (approximately) binomial.
RULE
Consider sampling without replacement from a dichotomous population of size N. If the sample
size (number of trials) nis at most 5% of the population size, the experiment can be analyzed as
though it were exactly a binomial experiment.
By “analyzed,” we mean that probabilities based on the binomial experiment assumptions will be
quite close to the actual “without-replacement” probabilities, which are typically more difﬁcult to
calculate. In Example 2.29, n/N¼5/50¼.1>.05, so the binomial experiment is not a good
approximation, but in Example 2.30, n/N¼10/500,000 <.05.
2.4.1 The Binomial Random Variable and Distribution
In most binomial experiments, it is the total number of successes, rather than knowledge of exactly
which trials yielded successes, that is of interest.
DEFINITION
Given a binomial experiment consisting of ntrials, the binomial random variable Xassociated
with this experiment is deﬁned as
X¼the number of successes among the ntrials
Suppose, for example, that n¼3. Then there are eight possible outcomes for the experiment:
SSS SSF SFS SFF FSS FSF FFS FFF
From the deﬁnition of X,X(SSF)¼2,X(SFF)¼1, and so on. Possible values for Xin an n-trial
experiment are x¼0, 1, 2, ...,n.2.4 The Binomial Distribution 97
NOTATION
We will write X~Bin(n,p) to indicate that Xis a binomial rv based on ntrials with success
probability p. Because the pmf of a binomial rv Xdepends on the two parameters nandp,w e
denote the pmf by b(x;n,p).
Our next goal is to derive a formula for the binomial pmf. Consider ﬁrst the case n¼4 for which
each outcome, its probability, and corresponding xvalue are listed in Table 2.1. For example,
P SSFSðÞ ¼ PSðÞ /C1 P/C0
S/C1
/C1P/C0
F/C1
/C1P/C0
S/C1
independent trials
¼p/C1p/C11/C0pðÞ /C1 p constant PSðÞ
¼p3/C11/C0pðÞ
In this special case, we wish to determine b(x;4 ,p) for x¼0, 1, 2, 3, and 4. For b(3; 4, p), we
identify which of the 16 outcomes yield an xvalue of 3 and sum the probabilities associated with each
such outcome:
b3;4;pðÞ ¼ P FSSSðÞ þ P SFSSðÞ þ P SSFSðÞ þ P SSSFðÞ ¼ 4p31/C0pðÞ
There are four outcomes with x¼3 and each has probability p3(1/C0p); the probability depends
only on the number of S’s,notthe order of S’s and F’s. So
b3;4;pðÞ ¼number of outcomes
with X¼3/C26/C27
/C1probability of any particular
outcome with X¼3/C26/C27
Similarly, b(2; 4, p)¼6p2(1/C0p)2, which is also the product of the number of outcomes with X¼
2 and the probability of any such outcome.
In general,
bx;n;pðÞ ¼number of sequences of
length nconsisting of xS’s/C26/C27
/C1probability of any
particular such sequence/C26/C27
Since the ordering of S’s and F’s is not important, the second factor in the previous equation is
px(1/C0p)n/C0x(for example, the ﬁrst xtrials resulting in Sand the last n/C0xresulting in F). The ﬁrst
factor is the number of ways of choosing xof the ntrials to be S’s—that is, the number of
combinations of size xthat can be constructed from ndistinct objects (trials here).Table 2.1 Outcomes and probabilities for a binomial experiment with four trials
Outcome x Probability Outcome x Probability
SSSS 4 p4FSSS 3 p3(1/C0p)
SSSF 3 p3(1/C0p) FSSF 2 p2(1/C0p)2
SSFS 3 p3(1/C0p) FSFS 2 p2(1/C0p)2
SSFF 2 p2(1/C0p)2FSFF 1 p(1/C0p)3
SFSS 3 p3(1/C0p) FFSS 2 p2(1/C0p)2
SFSF 2 p2(1/C0p)2FFSF 1 p(1/C0p)3
SFFS 2 p2(1/C0p)2FFFS 1 p(1/C0p)3
SFFF 1 p(1/C0p)3FFFF 0( 1 /C0p)498 2 Discrete Random Variables and Probability Distributions
THEOREM
bx;n;pðÞ ¼n
x/C18/C19
px1/C0pðÞn/C0xx¼0, 1, 2, ...,n
0 otherwise8
<
:
Example 2.31 Each of six randomly selected cola drinkers is given a glass containing cola Sand one
containing cola F. The glasses are identical in appearance except for a code on the bottom to identify
the cola. Suppose there is actually no tendency among cola drinkers to prefer one cola to the other.
Then p¼P(a selected individual prefers S)¼.5, so with X¼the number among the six who prefer S,
X~Bin(6, .5).
Thus
PX¼3 ðÞ ¼ b3;6;:5 ðÞ ¼6
3/C18/C19
:5ðÞ3:5ðÞ3¼20:5ðÞ6¼:313
The probability that at least three prefer Sis
PX/C213 ðÞ ¼X6
x¼3bx;6;:5 ðÞ ¼X6
x¼36
x/C18/C19
:5ðÞx:5ðÞ6/C0x¼:656
and the probability that at most one prefers Sis
PX/C201 ðÞ ¼X1
x¼0bx;6;:5 ðÞ ¼ :109 ■
2.4.2 Computing Binomial Probabilities
Even for a relatively small value of n, the computation of binomial probabilities can be tedious.
Software and statistical tables are both available for this purpose; both are typically in terms of the cdf
F(x)¼P(X/C20x) of the distribution, either in lieu of or in addition to the pmf. Various other
probabilities can then be calculated using the proposition on cdfs from Sect. 2.2.
NOTATION
ForX~Bin(n,p), the cdf will be denoted by
Bx ;n;pðÞ ¼ PX/C20x ðÞ ¼Xx
y¼0by;n;pðÞ x¼0, 1, ...,n
Table 2.2at the end of this section provides the code for performing binomial calculations in both
Matlab and R. In addition, Appendix Table A.1 tabulates the binomial cdf for n¼5, 10, 15, 20, 25 in
combination with selected values of p.
Example 2.32 Suppose that 20% of all copies of a particular textbook fail a binding strength test. Let
Xdenote the number among 15 randomly selected copies that fail the test. Then Xhas a binomial
distribution with n¼15 and p¼.2.2.4 The Binomial Distribution 99
(a) The probability that at most 8 fail the test is
PX/C208 ðÞ ¼X8
y¼0by;15;:2 ðÞ ¼ B8;15;:2 ðÞ
This is found at the intersection of the p¼.2 column and x¼8 row in the n¼15 part of
Table A.1: B(8; 15, .2) ¼.999. In Matlab, we may type binocdf(8,15,.2) ; in R, the
command is pbinom(8,15,.2) .
(b) The probability that exactly 8 fail is PX¼8 ðÞ ¼ b8;15;:2 ðÞ ¼15
8/C18/C19
:2ðÞ8:8ðÞ7¼:0034. We
can calculate this in Matlab or R with binopdf(8,15,.2) anddbinom(8,15,.2) ,
respectively. To use Table A.1, write
PX¼8 ðÞ ¼ PX/C208 ðÞ /C0 PX/C207 ðÞ ¼ B8;15;:2 ðÞ /C0 B7;15;:2 ðÞ
which is the difference between two consecutive entries in the p¼.2 column. The result is
.999/C0.996¼.003.
(c) The probability that at least 8 fail is P(X/C218)¼1/C0P(X/C207)¼1/C0B(7; 15, .2). The cdf
may be evaluated using Matlab or R as above, or by looking up the entry in the x¼7 row of the
p¼.2 column in Table A.1. In any case, we ﬁnd P(X/C218)¼1/C0.996¼.004.
(d) Finally, the probability that between 4 and 7, inclusive, fail is
P4/C20X/C207 ðÞ ¼ PX¼4, 5, 6, or 7 ðÞ ¼ P/C0
X/C207/C1
/C0P/C0
X/C203/C1
¼B7;15;:2 ðÞ /C0 B/C0
3;15,:2/C1
¼:996/C0:648¼:348
Notice that this latter probability is the difference between the cdf values at x¼7 and x¼3,not
x¼7 and x¼4. ■
Example 2.33 An electronics manufacturer claims that at most 10% of its power supply units need
service during the warranty period. To investigate this claim, technicians at a testing laboratory
purchase 20 units and subject each one to accelerated testing to simulate use during the warranty
period. Let pdenote the probability that a power supply unit needs repair during the period (i.e., the
proportion of allsuch units that need repair). The laboratory technicians must decide whether the data
resulting from the experiment supports the claim that p/C20.10. Let Xdenote the number among the
20 sampled that need repair, so X~Bin(20, p). Consider the decision rule
Reject the claim that p/C20.10 in favor of the conclusion that p>.10 if x/C215 (where xis the
observed value of X), and consider the claim plausible if x/C204
The probability that the claim is rejected when p¼.10 (an incorrect conclusion) is
PX/C215 when p¼:10 ðÞ ¼ 1/C0B4;20;:1 ðÞ ¼ 1/C0:957¼:043
The probability that the claim is not rejected when p¼.20 (a different type of incorrect
conclusion) is
PX/C204 when p¼:2 ðÞ ¼ B4;20;:2 ðÞ ¼ :630
The ﬁrst probability is rather small, but the second is intolerably large. When p¼.20, so that the
manufacturer has grossly understated the percentage of units that need service, and the stated decision100 2 Discrete Random Variables and Probability Distributions
rule is used, 63% of all samples of size 20 will result in the manufacturer ’s claim being judged
plausible!
One might recognize that the probability of this second type of erroneous conclusion could be
made smaller by changing the cutoff value 5 in the decision rule to something else. However,
although replacing 5 by a smaller number would indeed yield a probability smaller than .630, the
other probability would then increase. The only way to make both “error probabilities” small is to
base the decision rule on an experiment involving many more units (i.e., to increase n). ■
2.4.3 The Mean and Variance of a Binomial Random Variable
Forn¼1, the binomial distribution becomes the Bernoulli distribution. From Example 2.17, the
mean value of a Bernoulli variable is μ¼p, so the expected number of S’s on any single trial is p.
Since a binomial experiment consists of ntrials, intuition suggests that for X~Bin(n,p),E(X)¼np,
the product of the number of trials and the probability of success on a single trial. The expression for
Var(X) is not so obvious.
PROPOSITION
IfX~Bin(n,p), then E(X)¼np, Var( X)¼np(1/C0p)¼npq, and SD XðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃnpqp(where
q¼1/C0p).
Thus, calculating the mean and variance of a binomial rv does not necessitate evaluating
summations of the sort we employed in Sect. 2.3. The proof of the result for E(X) is sketched in
Exercise 74.
Example 2.34 If 75% of all purchases at a store are made with a credit card and Xis the number
among ten randomly selected purchases made with a credit card, then X~Bin(10, .75). Thus E(X)¼
np¼(10)(.75) ¼7.5, Var( X)¼np(1/C0p)¼10(.75)(.25) ¼1.875, and σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1:875p
¼1:37. Again,
even though Xcan take on only integer values, E(X) need not be an integer. If we perform a large
number of independent binomial experiments, each with n¼10 trials and p¼.75, then the average
number of S’s per experiment will be close to 7.5. ■
An important application of the binomial distribution is to estimating the precision of simulated
probabilities, as in Sect. 1.6. The relative frequency deﬁnition of probability justiﬁed deﬁning an
estimate of a probability P(A)b y ^PAðÞ ¼ X=n, where nis the number of runs of the simulation
program and Xequals the number of runs in which event Aoccurred. Assuming the runs of our
simulation are independent (and they usually are), the rv Xhas a binomial distribution with
parameters nandp¼P(A). From the preceding proposition and the rescaling properties of mean
and standard deviation, we have
E^PAðÞ/C0/C1
¼E1
nX/C18/C19
¼1
n/C1EXðÞ¼1
nnpðÞ¼p¼PAðÞ
Thus we expect the value of our estimate to coincide with the probability being estimated, in the
sense that there is no reason for ^PAðÞto be systematically higher or lower than P(A). Also,2.4 The Binomial Distribution 101
SD ^PAðÞ/C0/C1
¼SD1
nX/C18/C19
¼1
n/C12/C12/C12/C12/C12/C12/C12/C12/C1SDXðÞ ¼1
nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np1/C0pðÞp
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1/C0pðÞ
nr
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
PAðÞ1/C0PAðÞ ½/C138
nr
ð2:14Þ
Expression ( 2.14) is called the standard error of ^PAðÞ (essentially a synonym for standard
deviation) and indicates the amount by which an estimate ^PAðÞ“typically” varies from the true
probability P(A). However, this expression isn ’t of much use in practice: we most often simulate a
probability when P(A) is unknown, which prevents us from using Eq. ( 2.14). As a solution, we simply
substitute the estimate ^P¼^PAðÞinto this expression and get
SD ^PAðÞ/C0/C1
/C25ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^P1/C0^P/C0/C1
ns
This is the estimated standard error formula (1.8) given in Sect. 1.6. Very importantly, this
estimated standard error gets closer to 0 as the number of runs, n, in the simulation increases.
2.4.4 Binomial Calculations with Software
Many software packages, including Matlab and R, have built-in functions to evaluate both the pmf
and cdf of the binomial distribution (and many other named distributions). Table 2.2summarizes the
relevant code in both packages. The use of these functions was illustrated in Example 2.32.
2.4.5 Exercises: Section 2.4(49–74)
49. Determine whether each of the following rvs has a binomial distribution. If it does, identify the
values of the parameters nandp(if possible).
(a) X¼the number of ⚃s in 10 rolls of a fair die
(b) X¼the number of multiple-choice questions a student gets right on a 40-question test,
when each question has four choices and the student is completely guessing
(c) X¼the same as (b), but half the questions have four choices and the other half have three
(d) X¼the number of women in a random sample of 8 students, from a class comprising
20 women and 15 men
(e) X¼the total weight of 15 randomly selected apples
(f)X¼the number of apples, out of a random sample of 15, that weigh more than 150 g
50. Compute the following binomial probabilities directly from the formula for b(x;n,p):
(a) b(3; 8, .6)
(b) b(5; 8, .6)
(c) P(3/C20X/C205) when n¼8 and p¼.6
(d) P(1/C20X) when n¼12 and p¼.1Table 2.2 Binomial probability calculations in Matlab and R
Function: pmf cdf
Notation: b(x;n, p) B(x;n, p)
Matlab: binopdf( x,n,p) binocdf( x,n,p)
R: dbinom( x,n,p) pbinom( x,n,p)102 2 Discrete Random Variables and Probability Distributions
51. Use Appendix Table A.1 or software to obtain the following probabilities:
(a) B(4; 10, .3)
(b) b(4; 10, .3)
(c) b(6; 10, .7)
(d) P(2/C20X/C204) when X~ Bin(10, .3)
(e) P(2/C20X) when X~ Bin(10, .3)
(f)P(X/C201) when X~ Bin(10, .7)
(g) P(2<X<6) when X~ Bin(10, .3)
52. When circuit boards used in the manufacture of DVD players are tested, the long-run percentage
of defectives is 5%. Let X¼the number of defective boards in a random sample of size n¼
25, so X~Bin(25, .05).
(a) Determine P(X/C202).
(b) Determine P(X/C215).
(c) Determine P(1/C20X/C204).
(d) What is the probability that none of the 25 boards is defective?
(e) Calculate the expected value and standard deviation of X.
53. A company that produces ﬁne crystal knows from experience that 10% of its goblets have
cosmetic ﬂaws and must be classiﬁed as “seconds.”
(a) Among six randomly selected goblets, how likely is it that only one is a second?
(b) Among six randomly selected goblets, what is the probability that at least two are seconds?
(c) If goblets are examined one by one, what is the probability that at most ﬁve must be
selected to ﬁnd four that are not seconds?
54. Suppose that only 25% of all drivers come to a complete stop at an intersection having ﬂashing
red lights in all directions when no other cars are visible. What is the probability that, of
20 randomly chosen drivers coming to an intersection under these conditions,
(a) At most 6 will come to a complete stop?
(b) Exactly 6 will come to a complete stop?
(c) At least 6 will come to a complete stop?
55. Refer to the previous exercise.
(a) What is the expected number of drivers among the 20 that come to a complete stop?
(b) What is the standard deviation of the number of drivers among the 20 that come to a
complete stop?
(c) What is the probability that the number of drivers among these 20 that come to a complete
stop differs from the expected number by more than 2 standard deviations?
56. Suppose that 30% of all students who have to buy a text for a particular course want a new copy
(the successes!), whereas the other 70% want a used copy. Consider randomly selecting
25 purchasers.
(a) What are the mean value and standard deviation of the number who want a new copy of the
book?
(b) What is the probability that the number who want new copies is more than two standard
deviations away from the mean value?
(c) The bookstore has 15 new copies and 15 used copies in stock. If 25 people come in one by
one to purchase this text, what is the probability that all 25 will get the type of book they
want from current stock? [ Hint: LetX¼the number who want a new copy. For what values
ofXwill all 25 get what they want?]
(d) Suppose that new copies cost $100 and used copies cost $70. Assume the bookstore has
50 new copies and 50 used copies. What is the expected value of total revenue from the sale
of the next 25 copies purchased? [ Hint: Leth(X)¼the revenue when Xof the 25 purchasers
want new copies. Express this as a linear function.]2.4 The Binomial Distribution 103
57. Exercise 30 (Sect. 2.3) gave the pmf of Y, the number of trafﬁc citations for a randomly selected
individual insured by a company. What is the probability that among 15 randomly chosen such
individuals
(a) At least 10 have no citations?
(b) Fewer than half have at least one citation?
(c) The number that have at least one citation is between 5 and 10, inclusive?
58. A particular type of tennis racket comes in a midsize version and an oversize version. Sixty
percent of all customers at a store want the oversize version.
(a) Among ten randomly selected customers who want this type of racket, what is the probabil-
ity that at least six want the oversize version?
(b) Among ten randomly selected customers, what is the probability that the number who want
the oversize version is within 1 standard deviation of the mean value?
(c) The store currently has seven rackets of each version. What is the probability that all of the
next ten customers who want this racket can get the version they want from current stock?
59. Twenty percent of all telephones of a certain type are submitted for service while under warranty.
Of these, 60% can be repaired, whereas the other 40% must be replaced with new units. If a
company purchases ten of these telephones, what is the probability that exactly two will end up
being replaced under warranty?
60. The College Board reports that 2% of the two million high school students who take the SAT
each year receive special accommodations because of documented disabilities ( Los Angeles
Times , July 16, 2002). Consider a random sample of 25 students who have recently taken the test.
(a) What is the probability that exactly 1 received a special accommodation?
(b) What is the probability that at least 1 received a special accommodation?
(c) What is the probability that at least 2 received a special accommodation?
(d) What is the probability that the number among the 25 who received a special accommoda-
tion is within 2 standard deviations of the number you would expect to be accommodated?
(e) Suppose that a student who does not receive a special accommodation is allowed 3 hours for
the exam, whereas an accommodated student is allowed 4.5 hours. What would you expect
the average time allowed the 25 selected students to be?
61. Suppose that 90% of all batteries from a supplier have acceptable voltages. A certain type of
ﬂashlight requires two type-D batteries, and the ﬂashlight will work only if both its batteries have
acceptable voltages. Among ten randomly selected ﬂashlights, what is the probability that at least
nine will work? What assumptions did you make in the course of answering the question posed?
62. A k-out-of-n system functions provided that at least kof the ncomponents function. Consider
independently operating components, each of which functions (for the needed duration) with
probability .96.
(a) In a 3-component system, what is the probability that exactly two components function?
(b) What is the probability a 2-out-of-3 system works?
(c) What is the probability a 3-out-of-5 system works?
(d) What is the probability a 4-out-of-5 system works?
(e) What does the component probability (previously .96) need to equal so that the 4-out-of-5
system will function with probability at least .9999?
63. Bit transmission errors between computers sometimes occur, where one computer sends a 0 but
the other computer receives a 1 (or vice versa). Because of this, the computer sending a message
repeats each bit three times, so a 0 is sent as 000 and a 1 as 111. The receiving computer
“decodes” each triplet by majority rule: whichever number, 0 or 1, appears more often in a triplet
is declared to be the intended bit. For example, both 000 and 100 are decoded as 0, while 101 and104 2 Discrete Random Variables and Probability Distributions
011 are decoded as 1. Suppose that 6% of bits are switched (0 to 1, or 1 to 0) during transmission
between two particular computers, and that these errors occur independently during transmission.
(a) Find the probability that a triplet is decoded incorrectly by the receiving computer.
(b) Using your answer to part (a), explain how using triplets reduces communication errors.
(c) How does your answer to part (a) change if each bit is repeated ﬁve times (instead of three)?
(d) Imagine a 25 kilobit message (i.e., one requiring 25,000 bits to send). What is the expected
number of errors if there is no bit repetition implemented? If each bit is repeated three
times?
64. A very large batch of components has arrived at a distributor. The batch can be characterized as
acceptable only if the proportion of defective components is at most.10. The distributor decides
to randomly select 10 components and to accept the batch only if the number of defective
components in the sample is at most 2.
(a) What is the probability that the batch will be accepted when the actual proportion of
defectives is .01? .05? .10? .20? .25?
(b) Let pdenote the actual proportion of defectives in the batch. A graph of P(batch is accepted)
as a function of p, with pon the horizontal axis and P(batch is accepted) on the vertical axis,
is called the operating characteristic curve for the acceptance sampling plan. Use the results
of part (a) to sketch this curve for 0 /C20p/C201.
(c) Repeat parts (a) and (b) with “1” replacing “2” in the acceptance sampling plan.
(d) Repeat parts (a) and (b) with “15” replacing “10” in the acceptance sampling plan.
(e) Which of the three sampling plans, that of part (a), (c), or (d), appears most satisfactory, and
why?
65. An ordinance requiring that a smoke detector be installed in all previously constructed houses has
been in effect in a city for 1 year. The ﬁre department is concerned that many houses remain
without detectors. Let p¼the true proportion of such houses having detectors, and suppose that a
random sample of 25 homes is inspected. If the sample strongly indicates that fewer than 80% of
all houses have a detector, the ﬁre department will campaign for a mandatory inspection program.
Because of the costliness of the program, the department prefers not to call for such inspections
unless sample evidence strongly argues for their necessity. Let Xdenote the number of homes
with detectors among the 25 sampled. Consider rejecting the claim that p/C21.8 if X/C2015.
(a) What is the probability that the claim is rejected when the actual value of pis .8?
(b) What is the probability of not rejecting the claim when p¼.7? When p¼.6?
(c) How do the “error probabilities” of parts (a) and (b) change if the value 15 in the decision
rule is replaced by 14?
66. A toll bridge charges $1.00 for passenger cars and $2.50 for other vehicles. Suppose that during
daytime hours, 60% of all vehicles are passenger cars. If 25 vehicles cross the bridge during a
particular daytime period, what is the resulting expected toll revenue? [ Hint: Let X¼the number
of passenger cars; then the toll revenue h(X) is a linear function of X.]
67. A student who is trying to write a paper for a course has a choice of two topics, A and B. If topic A
is chosen, the student will order two books through interlibrary loan, whereas if topic B is chosen,
the student will order four books. The student believes that a good paper necessitates receiving
and using at least half the books ordered for either topic chosen. If the probability that a book
ordered through interlibrary loan actually arrives in time is .9 and books arrive independently of
one another, which topic should the student choose to maximize the probability of writing a good
paper? What if the arrival probability is only .5 instead of .9?
68. Twelve jurors are randomly selected from a large population. Each juror arrives at her or his
conclusion about the case before the jury independently of the other jurors.2.4 The Binomial Distribution 105
(a) In a criminal case, all 12 jurors must agree on a verdict. Let pdenote the probability that a
randomly selected member of the population would reach a guilty verdict based on the
evidence presented (so a proportion 1 /C0pwould reach “not guilty”). What is the probability,
in terms of p, that the jury reaches a unanimous verdict one way or the other?
(b) For what values of pis the probability in part (a) the highest? For what value of pis the
probability in (a) the lowest? Explain why this makes sense.
(c) In most civil cases, only a nine-person majority is required to decide a verdict. That is, if
nine or more jurors favor the plaintiff, then the plaintiff wins; if at least nine jurors side with
the defendant, then the defendant wins. Let pdenote the probability that someone would
side with the plaintiff based on the evidence. What is the probability, in terms of p, that the
jury reaches a verdict one way or the other? How does this compare with your answer to part
(a)?
69. Customers at a gas station pay with a credit card ( A), debit card ( B), or cash ( C). Assume that
successive customers make independent choices, with P(A)¼.5,P(B)¼.2, and P(C)¼.3.
(a) Among the next 100 customers, what are the mean and variance of the number who pay with
a debit card? Explain your reasoning.
(b) Answer part (a) for the number among the 100 who don ’t pay with cash.
70. An airport limousine can accommodate up to four passengers on any one trip. The company will
accept a maximum of six reservations for a trip, and a passenger must have a reservation. From
previous records, 20% of all those making reservations do not appear for the trip. In the following
questions, assume independence, but explain why there could be dependence.
(a) If six reservations are made, what is the probability that at least one individual with a
reservation cannot be accommodated on the trip?
(b) If six reservations are made, what is the expected number of available places when the
limousine departs?
(c) Suppose the probability distribution of the number of reservations made is given in the
accompanying table.
Number of reservations 3456
Probability .1 .2 .3 .4
LetXdenote the number of passengers on a randomly selected trip. Obtain the probability mass
function of X.
71. Let Xbe a binomial random variable with ﬁxed n.
(a) Are there values of p(0/C20p/C201) for which Var( X)¼0? Explain why this is so.
(b) For what value of pis Var( X) maximized? [ Hint: Either graph Var( X) as a function of por
else take a derivative.]
72. (a) Show that b(x;n,1/C0p)¼b(n/C0x;n,p).
(b) Show that B(x;n,1/C0p)¼1/C0B(n/C0x/C01;n,p). [Hint: At most xS ’s is equivalent to at
least ( n/C0x)F’s.]
(c) What do parts (a) and (b) imply about the necessity of including values of pgreater than .5 in
Table A.1?
73. Refer to Chebyshev ’s inequality given in Sect. 2.3. Calculate P(|X/C0μ|/C21kσ) for k¼2 and
k¼3 when X~ Bin(20, .5), and compare to the corresponding upper bounds. Repeat this for
X~Bin(20, .75).
74. Show that E(X)¼npwhen Xis a binomial random variable. [ Hint: Express E(X) as a sum with
lower limit x¼1. Then factor out np, lety¼x/C01 so that the sum is from y¼0t oy¼n/C01, and
show that the sum equals 1.]106 2 Discrete Random Variables and Probability Distributions
2.5 The Poisson Distribution
The binomial distribution was derived by starting with an experiment consisting of trials and applying
the laws of probability to various outcomes of the experiment. There is no simple experiment on
which the Poisson distribution is based, although we will shortly describe how it can be obtained from
the binomial distribution by certain limiting operations.
DEFINITION
A random variable Xis said to have a Poisson distribution with parameter μ(μ>0) if the pmf
ofXis
px;μðÞ ¼e/C0μμx
x!x¼0, 1, 2, ...
We shall see shortly that μis in fact the expected value of X, so the notation here is consistent with
our previous use of the symbol μ. Because μmust be positive, p(x;μ)>0 for all possible xvalues. The
fact that ∑x¼01p(x;μ)¼1 is a consequence of the Maclaurin inﬁnite series expansion of eμ, which
appears in most calculus texts:
eμ¼1þμþμ2
2!þμ3
3!þ/C1/C1/C1¼X1
x¼0μx
x!ð2:15Þ
If the two extreme terms in Eq. ( 2.15) are multiplied by e/C0μand then e/C0μis placed inside the
summation, the result is
1¼X1
x¼0e/C0μμx
x!
which shows that p(x;μ) fulﬁlls the second condition necessary for specifying a pmf.
Example 2.35 LetXdenote the number of creatures of a particular type captured in a trap during a
given time period. Suppose that Xhas a Poisson distribution with μ¼4.5, so on average traps will
contain 4.5 creatures. [The article “Dispersal Dynamics of the Bivalve Gemma gemma in a Patchy
Environment” ( Ecol. Monogr. , 1995: 1–20) suggests this model; the bivalve Gemma gemma is a
small clam.] The probability that a trap contains exactly ﬁve creatures is
PX¼5 ðÞ ¼e/C04:54:5ðÞ5
5!¼:1708
The probability that a trap has at most ﬁve creatures is
PX/C205 ðÞ ¼X5
x¼0e/C04:54:5ðÞx
x!¼e/C04:51þ4:5þ4:52
2!þ/C1/C1/C1þ4:55
5!/C20/C21
¼:7029
■
2.5.1 The Poisson Distribution as a Limit
The rationale for using the Poisson distribution in many situations is provided by the following
proposition.2.5 The Poisson Distribution 107
PROPOSITION
Suppose that in the binomial pmf b(x;n,p) we let n!1 andp!0 in such a way that np
approaches a value μ>0. Then b(x;n,p)!p(x;μ).
Proof Begin with the binomial pmf:
bx;n;pðÞ ¼n
x/C18/C19
px1/C0pðÞn/C0x¼n!
x!n/C0xðÞ !px1/C0pðÞn/C0x
¼n/C1n/C01ð Þ/C1/C1/C1/C1/C1 n/C0xþ1 ðÞ
x!px1/C0pðÞn/C0x
Now multiply both the numerator and denominator by nx:
bx;n;pðÞ ¼n
nn/C01
n/C1/C1/C1n/C0xþ1
n/C1npðÞx
x!/C11/C0pðÞn
1/C0pðÞx
Taking the limit as n!1 andp!0 with np!μ,
lim
n!1bx;n;pðÞ ¼ 1/C11/C1/C1/C11/C1μx
x!/C1lim
n!11/C0np=n ðÞn
1/C18/C19
The limit on the right can be obtained from the calculus theorem that says the limit of (1 /C0an/n)n
ise/C0aifan!a. Because np!μ,
lim
n!1bx;n;pðÞ ¼μx
x!/C1lim
n!11/C0np
n/C16/C17n
¼μxe/C0μ
x!¼px;μðÞ
■
According to the proposition, in any binomial experiment for which the number of trials n is large
and the success probability p is small, b (x;n,p)/C25p(x;μ)where μ¼np.It is interesting to note that
Sime´on Poisson discovered the distribution that bears his name by this approach in the 1830s.
Table 2.3shows the Poisson distribution for μ¼3 along with three binomial distributions with
np¼3, and Fig. 2.8(from R) plots the Poisson along with the ﬁrst two binomial distributions.
The approximation is of limited use for n¼30, but of course the accuracy is better for n¼100 and
much better for n¼300.
Table 2.3 Comparing the Poisson and three binomial distributions
xn ¼30,p¼.1 n¼100, p¼.03 n¼300, p¼.01 Poisson, μ¼3
0 0.042391 0.047553 0.049041 0.049787
1 0.141304 0.147070 0.148609 0.149361
2 0.227656 0.225153 0.224414 0.224042
3 0.236088 0.227474 0.225170 0.224042
4 0.177066 0.170606 0.168877 0.168031
5 0.102305 0.101308 0.100985 0.100819
6 0.047363 0.049610 0.050153 0.050409
7 0.018043 0.020604 0.021277 0.021604
8 0.005764 0.007408 0.007871 0.008102
9 0.001565 0.002342 0.002580 0.002701
10 0.000365 0.000659 0.000758 0.000810108 2 Discrete Random Variables and Probability Distributions
Example 2.36 Suppose you have a 4-megabit modem (4,000,000 bits/s) with bit error probability
10/C08. Assume bit errors occur independently, and assume your bit rate stays constant at 4 Mbps. What
is the probability of exactly 3 bit errors in the next minute? Of at most 3 bit errors in the next minute?
Deﬁne a random variable X¼the number of bit errors in the next minute. From the description,
Xsatisﬁes the conditions of a binomial distribution; speciﬁcally, since a constant bit rate of 4 Mbps
equates to 240,000,000 bits transmitted per minute, X~ Bin(240000000, 10/C08). Hence, the probabil-
ity of exactly three bit errors in the next minute is
PX¼3 ðÞ ¼ b3;240000000 ;10/C08/C0/C1
¼240000000
3/C18/C19
10/C08/C0/C1 31/C010/C08/C0/C1 239999997
For a variety of reasons, some calculators will struggle with this computation. The expression for
the chance of at most 3 bit errors, P(X/C203), is even worse. (The inability to compute such expressions
in the nineteenth century, even with modest values of nandp, was Poisson ’s motive to derive an
easily computed approximation.)
We may approximate these binomial probabilities using the Poisson distribution with μ¼np¼
240000000(10/C08)¼2.4. Then
PX¼3 ðÞ /C25 p3;2:4ðÞ ¼e/C02:42:43
3!¼:20901416
Similarly, the probability of at most 3 bit errors in the next minute is approximated by
PX/C203 ðÞ /C25X3
x¼0px;2:4ðÞ ¼X3
x¼0e/C02:42:4x
x!¼:77872291
Using modern software, the exact probabilities (i.e., using the binomial model) are .2090141655
and .7787229106, respectively. The Poisson approximations agree to eight decimal places and are
clearly more computationally tractable. ■
Many software packages will compute both p(x;μ) and the corresponding cdf P(x;μ) for speciﬁed
values of xandμupon request; the relevant Matlab and R functions appear in Table 2.4at the end ofoxBin(30, .1)
xBin(100,.03)
|Poisson(3)
oxoxox
ox
ox
ox
ox
oxox oxo
x
10 8 6 4 2 00.05.10.15.20.25p(x) Fig. 2.8 Comparing a
Poisson and two binomial
distributions2.5 The Poisson Distribution 109
this section. Appendix Table A.2 exhibits the cdf P(x;μ) for μ¼.1, .2, ...,1 ,2 , ..., 10, 15, and 20.
For example, if μ¼2, then P(X/C203)¼P(3; 2) ¼.857, whereas P(X¼3)¼P(3; 2) /C0P(2; 2) ¼.180.
2.5.2 The Mean and Variance of a Poisson Random Variable
Since b(x;n,p)!p(x;μ)a sn!1 ,p!0,np!μ, one might guess that the mean and variance of a
binomial variable approach those of a Poisson variable. These limits are np!μandnp(1/C0p)!μ.
PROPOSITION
IfXhas a Poisson distribution with parameter μ, then E(X)¼Var(X)¼μ.
These results can also be derived directly from the deﬁnitions of mean and variance (see Exercise
88 for the mean).
Example 2.37 (Example 2.35 continued) Both the expected number of creatures trapped and the
variance of the number trapped equal 4.5, and σX¼ﬃﬃﬃμp¼ﬃﬃﬃﬃﬃﬃﬃ
4:5p
¼2:12. ■
2.5.3 The Poisson Process
A very important application of the Poisson distribution arises in connection with the occurrence of
events of a particular type over time. As an example, suppose that starting from a time point that we
label t¼0, we are interested in counting the number of radioactive pulses recorded by a Geiger
counter. If we make certain assumptions2about the way in which pulses occur—chieﬂy, that the
number of pulses grows roughly linearly with time—then it can be shown that the number of pulses in
any time interval of length tcan be modeled by a Poisson distribution with mean μ¼λtfor an
appropriate positive constant λ. Since the expected number of pulses in an interval of length tisλt, the
expected number in an interval of length 1 is λ.T h u s λis the long run number of pulses per unit
of time.
If we replace “pulse” by “event,” then the number of events occurring during a ﬁxed time interval
of length thas a Poisson distribution with parameter λt. Any process that has this distribution is called
aPoisson process , and λis called the rate of the process . Other examples of situations giving rise to a
Poisson process include monitoring the status of a computer system over time, with breakdowns
constituting the events of interest; recording the number of accidents in an industrial facility over
time; answering 911 calls at a particular location; and observing the number of cosmic-ray showers
from an observatory.
Example 2.36 hints at why this might be reasonable: if we “digitize” time—that is, divide time into
discrete pieces, such as transmitted bits—and look at the number of the resulting time pieces that
include an event, a binomial model is often applicable. If the number of time pieces is very large and
the success probability close to zero, which would occur if we divided a ﬁxed time frame into ever-
smaller pieces, then we may invoke the Poisson approximation from earlier in this section.
2InSect. 7.5 , we present the formal assumptions required in this situation and derive the Poisson distribution that results
from these assumptions.110 2 Discrete Random Variables and Probability Distributions
Example 2.38 Suppose pulses arrive at the Geiger counter at an average rate of 6 per minute, so that
λ¼6. To ﬁnd the probability that in a 30-s interval at least one pulse is received, note that the number
of pulses in such an interval has a Poisson distribution with parameter λt¼6(.5)¼3 (.5 min is used
because λis expressed as a rate per minute). Then with X¼the number of pulses received in the 30-s
interval,
PX/C211 ðÞ ¼ 1/C0PX¼0 ðÞ ¼ 1/C0e/C0330
0!¼:950
In a 1-h interval ( t¼60), the expected number of pulses is μ¼λt¼6(60) ¼360, with a standard
deviation of σ¼ﬃﬃﬃμp¼ﬃﬃﬃﬃﬃﬃﬃﬃ
360p
¼18:97. According to this model, in a typical hour we will observe
360/C619 pulses arrive at the Geiger counter. ■
Instead of observing events over time, consider observing events of some type that occur in a two-
or three-dimensional region. For example, we might select on a map a certain region Rof a forest, go
to that region, and count the number of trees. Each tree would represent an event occurring at a
particular point in space. Under appropriate assumptions (see Sect. 7.5 ), it can be shown that the
number of events occurring in a region Rhas a Poisson distribution with parameter λ/C1a(R), where
a(R) is the area of R. The quantity λis the expected number of events per unit area or volume.
2.5.4 Poisson Calculations with Software
Table 2.4gives the Matlab and R commands for calculating Poisson probabilities.
2.5.5 Exercises: Section 2.5(75–89)
75. Let X, the number of ﬂaws on the surface of a randomly selected carpet of a particular type, have
a Poisson distribution with parameter μ¼5. Use software or Appendix Table A.2 to compute
the following probabilities:
(a) P(X/C208)
(b) P(X¼8)
(c) P(9/C20X)
(d) P(5/C20X/C208)
(e) P(5<X<8)
76. Let Xbe the number of material anomalies occurring in a particular region of an aircraft
gas-turbine disk. The article “Methodology for Probabilistic Life Prediction of Multiple-
Anomaly Materials” ( Amer. Inst. of Aeronautics and Astronautics J. , 2006: 787–793) proposes
a Poisson distribution for X. Suppose μ¼4.
(a) Compute both P(X/C204) and P(X<4).
(b) Compute P(4/C20X/C208).Table 2.4 Poisson
probability calculationsFunction: pmf cdf
Notation: p(x;μ) P(x;μ)
Matlab: poisspdf( x,μ) poisscdf( x,μ)
R: dpois( x,μ) ppois( x,μ)2.5 The Poisson Distribution 111
(c) Compute P(8/C20X).
(d) What is the probability that the observed number of anomalies exceeds the expected
number by no more than one standard deviation?
77. Suppose that the number of drivers who travel between a particular origin and destination during
a designated time period has a Poisson distribution with parameter μ¼20 (suggested in the
article “Dynamic Ride Sharing: Theory and Practice,” J. of Transp. Engr. , 1997: 308–312).
What is the probability that the number of drivers will
(a) Be at most 10?
(b) Exceed 20?
(c) Be between 10 and 20, inclusive? Be strictly between 10 and 20?
(d) Be within 2 standard deviations of the mean value?
78. Consider writing onto a computer disk and then sending it through a certiﬁer that counts the
number of missing pulses. Suppose this number Xhas a Poisson distribution with parameter μ¼
.2. (Suggested in “Average Sample Number for Semi-Curtailed Sampling Using the Poisson
Distribution,” J. Qual. Tech ., 1983: 126–129.)
(a) What is the probability that a disk has exactly one missing pulse?
(b) What is the probability that a disk has at least two missing pulses?
(c) If two disks are independently selected, what is the probability that neither contains a
missing pulse?
79. An article in the Los Angeles Times (Dec. 3, 1993) reports that 1 in 200 people carry the
defective gene that causes inherited colon cancer. In a sample of 1000 individuals, what is the
approximate distribution of the number who carry this gene? Use this distribution to calculate
the approximate probability that
(a) Between 5 and 8 (inclusive) carry the gene.
(b) At least 8 carry the gene.
80. Suppose that only .10% of all computers of a certain type experience CPU failure during the
warranty period. Consider a sample of 10,000 computers.
(a) What are the expected value and standard deviation of the number of computers in the
sample that have the defect?
(b) What is the (approximate) probability that more than 10 sampled computers have the
defect?
(c) What is the (approximate) probability that no sampled computers have the defect?
81. If a publisher of nontechnical books takes great pains to ensure that its books are free of
typographical errors, so that the probability of any given page containing at least one such
error is .005 and errors are independent from page to page, what is the probability that one of its
400-page novels will contain exactly one page with errors? At most three pages with errors?
82. In proof testing of circuit boards, the probability that any particular diode will fail is .01.
Suppose a circuit board contains 200 diodes.
(a) How many diodes would you expect to fail, and what is the standard deviation of the
number that are expected to fail?
(b) What is the (approximate) probability that at least four diodes will fail on a randomly
selected board?
(c) If ﬁve boards are shipped to a particular customer, how likely is it that at least four of them
will work properly? (A board works properly only if all its diodes work.)
83. The article “Expectation Analysis of the Probability of Failure for Water Supply Pipes”
(J. Pipeline Syst. Eng. Pract. 2012.3:36–46) recommends using a Poisson process to model
the number of failures in commercial water pipes. The article also gives estimates of the failure112 2 Discrete Random Variables and Probability Distributions
rateλ, in units of failures per 100 miles of pipe per day, for four different types of pipe and for
many different years.
(a) For PVC pipe in 2008, the authors estimate a failure rate of 0.0081 failures per 100 miles of
pipe per day. Consider a 100-mile-long segment of such pipe. What is the expected number
of failures in 1 year (365 days)? Based on this expectation, what is the probability of at
least one failure along such a pipe in 1 year?
(b) For cast iron pipe in 2005, the authors ’estimate is λ¼0.0864 failures per 100 miles per
day. Suppose a town had 1500 miles of cast iron pipe underground in 2005. What is the
probability of at least one failure somewhere along this pipe system on any given day?
84. Organisms are present in ballast water discharged from a ship according to a Poisson process with
a concentration of 10 organisms/m3(the article “Counting at Low Concentrations: The Statistical
Challenges of Verifying Ballast Water Discharge Standards” ( Ecological Applications , 2013:
339–351) considers using the Poisson process for this purpose).
(a) What is the probability that one cubic meter of discharge contains at least 8 organisms?
(b) What is the probability that the number of organisms in 1.5 m3of discharge exceeds its mean
value by more than one standard deviation?
(c) For what amount of discharge would the probability of containing at least one organism be
.999?
85. Suppose small aircraft arrive at an airport according to a Poisson process with rate λ¼8 per hour,
so that the number of arrivals during a time period of thours is a Poisson rv with parameter
μ¼8t.
(a) What is the probability that exactly 6 small aircraft arrive during a 1-h period? At least 6?
At least 10?
(b) What are the expected value and standard deviation of the number of small aircraft that
arrive during a 90-min period?
(c) What is the probability that at least 20 small aircraft arrive during a 2.5-h period? That at
most 10 arrive during this period?
86. The number of people arriving for treatment at an emergency room can be modeled by a Poisson
process with a rate parameter of ﬁve per hour.
(a) What is the probability that exactly four arrivals occur during a particular hour?
(b) What is the probability that at least four people arrive during a particular hour?
(c) How many people do you expect to arrive during a 45-min period?
87. Suppose that trees are distributed in a forest according to a two-dimensional Poisson process with
rateλ, the expected number of trees per acre, equal to 80.
(a) What is the probability that in a certain quarter-acre plot, there will be at most 16 trees?
(b) If the forest covers 85,000 acres, what is the expected number of trees in the forest?
(c) Suppose you select a point in the forest and construct a circle of radius.1 mile. Let X¼the
number of trees within that circular region. What is the pmf of X?[Hint: 1 sq mile ¼
640 acres.]
88. Let Xhave a Poisson distribution with parameter μ. Show that E(X)¼μdirectly from the
deﬁnition of expected value. [ Hint: The ﬁrst term in the sum equals 0, and then xcan be canceled.
Now factor out μand show that what is left sums to 1.]
89. In some applications the distribution of a discrete rv Xresembles the Poisson distribution except
that zero is not a possible value of X. For example, let X¼the number of tattoos that an individual
wants removed when s/he arrives at a tattoo removal facility. Suppose the pmf of Xis2.5 The Poisson Distribution 113
pxðÞ ¼ ke/C0θθx
x!x¼1, 2, 3, ...
(a) Determine the value of k.[Hint: The sum of all probabilities in the Poisson pmf is 1, and this
pmf must also sum to 1.]
(b) If the mean value of Xis 2.313035, what is the probability that an individual wants at most
5 tattoos removed?
(c) Determine the standard deviation of Xwhen the mean value is as given in (b).
[Note: The article “An Exploratory Investigation of Identity Negotiation and Tattoo Removal”
(Academy of Marketing Science Review , vol. 12, #6, 2008) gave a sample of 22 observations on
the number of tattoos people wanted removed; estimates of μandσcalculated from the data were
2.318182 and 1.249242, respectively.]
2.6 Other Discrete Distributions
This section introduces discrete distributions that are closely related to the binomial distribution.
Whereas the binomial distribution is the approximate probability model for sampling without
replacement from a ﬁnite dichotomous ( S-F) population, the hypergeometric distribution is the
exact probability model for the number of S’s in the sample. The binomial rv Xis the number of
S’s when the number nof trials is ﬁxed, whereas the negative binomial distribution arises from ﬁxing
the number of S’s desired and letting the number of trials be random.
2.6.1 The Hypergeometric Distribution
The assumptions leading to the hypergeometric distribution are as follows:
1. The population or set to be sampled consists of Nindividuals, objects, or elements (a ﬁnite
population).
2. Each individual can be characterized as a success ( S) or a failure ( F), and there are Msuccesses in
the population.
3. A sample of nindividuals is selected without replacement in such a way that each subset of size
nis equally likely to be chosen.
The random variable of interest is X¼the number of S’s in the sample. The probability
distribution of Xdepends on the parameters n,M, and N, so we wish to obtain the pmf P(X¼x)¼
h(x;n,M,N).
Example 2.39 During a particular period a university ’s information technology ofﬁce received
20 service orders for problems with laptops, of which 8 were Macs and 12 were PCs. A sample of
ﬁve of these service orders is to be selected for inclusion in a customer satisfaction survey. Suppose
that the ﬁve are selected in a completely random fashion, so that any particular subset of size 5 has the
same chance of being selected as does any other subset (think of putting the numbers 1, 2, ...,2 0o n
20 identical slips of paper, mixing up the slips, and choosing ﬁve of them). What then is the
probability that exactly 2 of the selected service orders were for PC laptops?114 2 Discrete Random Variables and Probability Distributions
In this example, the population size is N¼20, the sample size is n¼5, and the number of S’s
(PC¼S) and F’s (Mac ¼F) in the population are M¼12 and N/C0M¼8, respectively. Let X¼the
number of PCs among the ﬁve sampled service orders. Because all outcomes (each consisting of ﬁve
particular orders) are equally likely,
PX¼2 ðÞ ¼ h2;5;12;20 ðÞ ¼number of outcomes having X¼2
number of possible outcomes
The number of possible outcomes in the experiment is the number of ways of selecting 5 from
the 20 objects without regard to order—that is,20
5/C18/C19
. To count the number of outcomes having
X¼2, note that there are12
2/C18/C19
ways of selecting two of the PC orders, and for each such way there
are8
3/C18/C19
ways of selecting the three Mac orders to ﬁll out the sample. The Fundamental Counting
Principle from Sect. 1.3then gives12
2/C18/C19
/C18
3/C18/C19
as the number of outcomes with X¼2, so
h2;5;12;20 ðÞ ¼12
2/C18/C19
8
3/C18/C19
20
5/C18/C19 ¼77
323¼:238
■
In general, if the sample size nis smaller than the number of successes in the population ( M),
then the largest possible Xvalue is n. However, if M<n(e.g., a sample size of 25 and only
15 successes in the population), then Xcan be at most M. Similarly, whenever the number of
population failures ( N/C0M) exceeds the sample size, the smallest possible Xvalue is 0 (since all
sampled individuals might then be failures). However, if N/C0M<n, the smallest possible Xvalue is
n/C0(N/C0M). Summarizing, the possible values of Xsatisfy the restriction max(0, n/C0N+M )/C20x/C20
min(n,M). An argument parallel to that of the previous example gives the pmf of X.
PROPOSITION
IfXis the number of S’s in a random sample of size ndrawn from a population consisting of M
S’s and ( N/C0M)F’s, then the probability distribution of X, called the hypergeometric
distribution , is given by
PX¼x ðÞ ¼ hx;n;M;N ðÞ ¼M
x/C18/C19
N/C0M
n/C0x/C18/C19
N
n/C18/C19 ð2:16Þ
forxan integer satisfying max(0, n/C0N+M )/C20x/C20min(n,M).3
3If we deﬁnea
b/C18/C19
¼0 for a<b, then h(x;n, M, N ) may be applied for all integers 0 /C20x/C20n.2.6 Other Discrete Distributions 115
In Example 2.39, n¼5,M¼12, and N¼20, so h(x; 5, 12, 20) for x¼0, 1, 2, 3, 4, 5 can be
obtained by substituting these numbers into Eq. ( 2.16).
Example 2.40 Capture–recapture. Five individuals from an animal population thought to be near
extinction in a region have been caught, tagged, and released to mix into the population. After they
have had an opportunity to mix, a random sample of 10 of these animals is selected. Let X¼the
number of tagged animals in the second sample. If there are actually 25 animals of this type in the
region, what is the probability that (a) X¼2? (b) X/C202?
Application of the hypergeometric distribution here requires assuming that every subset of ten
animals has the same chance of being captured. This in turn implies that released animals are no
easier or harder to catch than are those not initially captured. Then the parameter values are n¼
10,M¼5 (ﬁve tagged animals in the population), and N¼25, so
hx;10;5;25 ðÞ ¼5
x/C18/C19
20
10/C0x/C18/C19
25
10/C18/C19 x¼0, 1, 2, 3, 4, 5
For part (a),
PX¼2 ðÞ ¼ h2;10;5;25 ðÞ ¼5
2/C18/C19
20
8/C18/C19
25
10/C18/C19 ¼:385
For part (b),
PX/C202 ðÞ ¼ PX¼0, 1, or 2 ðÞ ¼X2
x¼0hx;10;5;25 ðÞ
¼:057þ:257þ:385¼:699■
Matlab, R, and other software packages will easily generate hypergeometric probabilities; see
Table 2.5at the end of this section. Comprehensive tables of the hypergeometric distribution are
available, but because the distribution has three parameters, these tables require much more space
than tables for the binomial distribution.
As in the binomial case, there are simple expressions for E(X) and Var( X) for hypergeometric rvs.
PROPOSITION
The mean and variance of the hypergeometric rv Xhaving pmf h(x;n,M,N) are
EXðÞ ¼ n/C1M
NVarXðÞ ¼N/C0n
N/C01/C18/C19
/C1n/C1M
N1/C0M
N/C18/C19
The ratio M/Nis the proportion of S’s in the population. Replacing M/NbypinE(X) and Var( X)
gives116 2 Discrete Random Variables and Probability Distributions
EXðÞ ¼ np ð2:17Þ
VarXðÞ ¼N/C0n
N/C01/C18/C19
/C1np1/C0pðÞ
Expression ( 2.17) shows that the means of the binomial and hypergeometric rvs are equal, whereas
the variances of the two rvs differ by the factor ( N/C0n)/(N/C01), often called the ﬁnite population
correction factor . This factor is less than 1, so the hypergeometric variable has smaller variance than
does the binomial rv. The correction factor can be written (1 /C0n/N)/(1/C01/N), which is approxi-
mately 1 when nis small relative to N.
Example 2.41 (Example 2.40 continued) In the animal-tagging example, n¼10,M¼5, and N¼
25, so p¼5
25¼:2 and
EXðÞ ¼ 10:2ðÞ ¼ 2
VarXðÞ ¼25/C010
25/C0110ðÞ :2ðÞ:8ðÞ ¼ :625ðÞ 1:6ðÞ ¼ 1
If the sampling were carried out with replacement, Var( X)¼1.6.
Suppose the population size Nis not actually known, so the value xis observed and we wish to
estimate N. It is reasonable to equate the observed sample proportion of S’s,x/n, with the population
proportion, M/N, giving the estimate
^N¼M/C1n
x
For example, if M¼100, n¼40, and x¼16, then ^N¼250. ■
Our rule in Sect. 2.4stated that if sampling is without replacement but n/Nis at most .05, then the
binomial distribution can be used to compute approximate probabilities involving the number of S’s
in the sample. A more precise statement is as follows: Let the population size, N, and number of
population S’s,M, get large with the ratio M/Napproaching p. Then h(x;n,M,N) approaches the
binomial pmf b(x;n,p); so for n/Nsmall, the two are approximately equal provided that pis not too
near either 0 or 1. This is the rationale for our rule.
2.6.2 The Negative Binomial and Geometric Distributions
The negative binomial distribution is based on an experiment satisfying the following conditions:
1. The experiment consists of a sequence of independent trials.
2. Each trial can result in either a success ( S) or a failure ( F).
3. The probability of success is constant from trial to trial, so P(Son trial i)¼pfori¼1, 2, 3 ....
4. The experiment continues (trials are performed) until a total of rsuccesses has been observed,
where ris a speciﬁed positive integer.
The random variable of interest is X¼the number of trials required to achieve the rth success, and
Xis called a negative binomial random variable . In contrast to the binomial rv, the number of2.6 Other Discrete Distributions 117
successes is ﬁxed and the number of trials is random. Possible values of Xarer,r+1 ,r+2 , ..., since
it takes at least rtrials to achieve rsuccesses.
Letnb(x;r,p) denote the pmf of X. The event { X¼x} is equivalent to { r/C01S’s in the ﬁrst ( x/C01)
trials and an Son the xth trial}, e.g., if r¼5 and x¼15, then there must be four S’s in the ﬁrst 14 trials
and trial 15 must be an S. Since trials are independent,
nb x ;r;pðÞ ¼ PX¼x ðÞ ¼ Pr/C01S’s on the first x/C01 trials ðÞ /C1 PSðÞ ð 2:18Þ
The ﬁrst probability on the far right of Eq. ( 2.18) is the binomial probability
x/C01
r/C01/C18/C19
pr/C011/C0pðÞx/C01ðÞ /C0 r/C01ðÞwhere PSðÞ ¼ p
Simplifying and then multiplying by the extra factor of pat the end of Eq. ( 2.18) yields the
following.
PROPOSITION
The pmf of the negative binomial rv Xwith parameters r¼desired number of S’s and p¼
P(S)i s
nb x ;r;pðÞ ¼x/C01
r/C01/C18/C19
pr1/C0pðÞx/C0rx¼r,rþ1,rþ2,...
Example 2.42 A pediatrician wishes to recruit four couples, each of whom is expecting their ﬁrst
child, to participate in a new natural childbirth regimen. Let p¼P(a randomly selected couple agrees
to participate). If p¼.2, what is the probability that exactly 15 couples must be asked before 4 are
found who agree to participate? Substituting r¼4,p¼.2, and x¼15 into nb(x;r,p) gives
nb15;4;2 ðÞ ¼15/C01
4/C01/C18/C19
:24:811¼:050
The probability that at most 15 couples need to be asked is
PX/C2015 ðÞ ¼X15
x¼4nb x ;4;:2 ðÞ ¼X15
x¼4x/C01
3/C18/C19
:24:8x/C04¼:352
■
In the special case r¼1, the pmf is
nb x ;1;pðÞ ¼ 1/C0pðÞx/C01px¼1, 2, ... ð2:19Þ
In Example 2.10, we derived the pmf for the number of trials necessary to obtain the ﬁrst S, and the
pmf there is identical to Eq. ( 2.19). The random variable X¼number of trials required to achieve one
success is referred to as a geometric random variable , and the pmf in Eq. ( 2.19) is called the
geometric distribution . The name is appropriate because the probabilities constitute a geometric
series: p,( 1/C0p)p,( 1/C0p)2p,.... To see that the sum of the probabilities is 1, recall that the sum of a
geometric series is a + ar + ar2+...¼a/(1/C0r)i f|r|<1, so for p>0,118 2 Discrete Random Variables and Probability Distributions
pþ1/C0pðÞ pþ1/C0pðÞ2pþ/C1/C1/C1¼p
1/C01/C0pðÞ¼1
In Example 2.19, the expected number of trials until the ﬁrst Swas shown to be 1/ p. Intuitively, we
would then expect to need r/C11/ptrials to achieve the rthS, and this is indeed E(X). There is also a
simple formula for Var( X).
PROPOSITION
IfXis a negative binomial rv with parameters randp, then
EXðÞ ¼r
pVarXðÞ ¼r1/C0pðÞ
p2
Example 2.43 (Example 2.42 continued) With p¼.2, the expected number of couples the doctor
must speak to in order to ﬁnd 4 that will agree to participate is r/p¼4/.2¼20. This makes sense,
since with p¼.2¼1/5 it will take ﬁve attempts, on average, to achieve one success. The
corresponding variance is 4(1 /C0.2)/(.2)2¼80, for a standard deviation of about 8.9. ■
Since they are based on similar experiments, some caution must be taken to distinguish the
binomial and negative binomial models, as seen in the next example.
Example 2.44 In many communication systems, a receiver will send a short signal back to the
transmitter to indicate whether a message has been received correctly or with errors. (These signals
are often called an acknowledgement and a non-acknowledgement , respectively. Bit sum checks and
other tools are used by the receiver to determine the absence or presence of errors.) Assume we are
using such a system in a noisy channel, so that each message is sent error-free with probability .86,
independent of all other messages. What is the probability that in 10 transmissions, exactly 8 will
succeed? What is the probability the system will require exactly 10 attempts to successfully transmit
8 messages?
While these two questions may sound similar, they require two different models for solution.
To answer the ﬁrst question, let Xrepresent the number of successful transmissions out of 10. Then
X~ Bin(10, .86), and the answer is
PX¼8 ðÞ ¼ b8;10;:86 ðÞ ¼10
8/C18/C19
:86ðÞ8:14ðÞ2¼:2639
However, the event {exactly 10 attempts required to successfully transmit 8 messages} is more
restrictive: not only must we observe 8 S’s and 2 F’s in 10 trials, but the last trial must be a success .
Otherwise, it took fewer than 10 tries to send 8 messages successfully. Deﬁne a variable Y¼the
number of transmissions (trials) required to successfully transmit 8 messages. Then Yis negative
binomial, with r¼8 and p¼.86, and the answer to the second question is
PY¼10ðÞ ¼ nb10;8;:86 ðÞ ¼10/C01
8/C01/C18/C19
:86ðÞ8:14ðÞ2¼:2111
Notice this is smaller than the answer to the ﬁrst question, which makes sense because (as we
noted) the second question imposes an additional constraint. In fact, you can think of the “ /C01” terms
in the negative binomial pmf as accounting for this loss of ﬂexibility in the placement of S’s and F’s.2.6 Other Discrete Distributions 119
Similarly, the expected number of successful transmissions in 10 attempts is E(X)¼np¼10(.86)
¼8.6, while the expected number of attempts required to successfully transmit 8 messages is E(Y)¼
r/p¼8/.86¼9.3. In the ﬁrst case, the number of trials ( n¼10) is ﬁxed, while in the second case the
desired number of successes ( r¼8) is ﬁxed. ■
By expanding the binomial coefﬁcient in front of pr(1/C0p)x/C0rand doing some cancellation, it can
be seen that nb(x;r,p) is well-deﬁned even when ris not an integer. This generalized negative
binomial distribution has been found to ﬁt observed data quite well in a wide variety of applications.
2.6.3 Alternative Definition of the Negative Binomial Distribution
There is not universal agreement on the deﬁnition of a negative binomial random variable (or, by
extension, a geometric rv). It is not uncommon in the literature, as well as in some textbooks, to see
the number of failures preceding the rth success called “negative binomial”; in our notation, this
simply equals X/C0r. Possible values of this “number of failures” variable are 0, 1, 2, .... Similarly,
the geometric distribution is sometimes deﬁned in terms of the number of failures preceding the ﬁrst
success in a sequence of independent and identical trials. If one uses these alternative deﬁnitions, then
the pmf and mean formula must be adjusted accordingly. (The variance, however, will stay the same.)
The developers of Matlab and R are among those who have adopted this alternative deﬁnition; as a
result, we must be careful with our inputs to the relevant software functions. The pmf syntax for the
distributions in this section are cataloged in Table 2.5; cdfs may be invoked by changing pdf tocdf
in Matlab or the initial letter dtopin R. Notice the input argument x/C0rfor the negative binomial
functions: both software packages request the number of failures, rather than the number of trials.
For example, suppose Xhas a hypergeometric distribution with n¼10,M¼5,N¼25 as in
Example 2.40. Using Matlab, we may calculate P(X¼2)¼hygepdf(2,25,5,10) andP(X/C202)
¼hygecdf(2,25,5,10) . The corresponding R function calls are dhyper(2,5,20,10) and
phyper(2,5,20,10) , respectively. If Xis the negative binomial variable of Example 2.42 with
parameters r¼4 and p¼.2, then the chance of requiring 15 trials to achieve 4 successes (i.e., 11 total
failures) can be found in Matlab with nbinpdf(11,4, .2) and in R using the command
dnbinom(11,4, .2) .
2.6.4 Exercises: Section 2.6(90–106)
90. An electronics store has received a shipment of 20 table radios that have connections for an iPod
or iPhone. Twelve of these have two slots (so they can accommodate both devices), and the
other eight have a single slot. Suppose that six of the 20 radios are randomly selected to be
stored under a shelf where radios are displayed, and the remaining ones are placed in aTable 2.5 Matlab and R code for hypergeometric and negative binomial calculations
Hypergeometric Negative Binomial
Function: pmf pmf
Notation: h(x;n, M, N ) nb(x;r, p)
Matlab: hygepdf( x,N,M,n) nbinpdf( x/C0r,r,p)
R: dhyper( x,M,N/C0M,n) dnbinom( x/C0r,r,p)120 2 Discrete Random Variables and Probability Distributions
storeroom. Let X¼the number among the radios stored under the display shelf that have
two slots.
(a) What kind of a distribution does Xhave (name and values of all parameters)?
(b) Compute P(X¼2),P(X/C202), and P(X/C212).
(c) Calculate the mean value and standard deviation of X.
91. Each of 12 refrigerators has been returned to a distributor because of an audible, high-pitched,
oscillating noise when the refrigerator is running. Suppose that 7 of these refrigerators have a
defective compressor and the other 5 have less serious problems. If the refrigerators are
examined in random order, let Xbe the number among the ﬁrst 6 examined that have a defective
compressor. Compute the following:
(a) P(X¼5)
(b) P(X/C204)
(c) The probability that Xexceeds its mean value by more than 1 standard deviation.
(d) Consider a large shipment of 400 refrigerators, of which 40 have defective compressors. If
Xis the number among 15 randomly selected refrigerators that have defective compressors,
describe a less tedious way to calculate (at least approximately) P(X/C205) than to use the
hypergeometric pmf.
92. An instructor who taught two sections of statistics last term, the ﬁrst with 20 students and the
second with 30, decided to assign a term project. After all projects had been turned in, the
instructor randomly ordered them before grading. Consider the ﬁrst 15 graded projects.
(a) What is the probability that exactly 10 of these are from the second section?
(b) What is the probability that at least 10 of these are from the second section?
(c) What is the probability that at least 10 of these are from the same section?
(d) What are the mean and standard deviation of the number among these 15 that are from the
second section?
(e) What are the mean and standard deviation of the number of projects not among these ﬁrst
15 that are from the second section?
93. A geologist has collected 10 specimens of basaltic rock and 10 specimens of granite. The
geologist instructs a laboratory assistant to randomly select 15 of the specimens for analysis.
(a) What is the pmf of the number of granite specimens selected for analysis?
(b) What is the probability that all specimens of one of the two types of rock are selected for
analysis?
(c) What is the probability that the number of granite specimens selected for analysis is within
1 standard deviation of its mean value?
94. A personnel director interviewing 11 senior engineers for four job openings has scheduled six
interviews for the ﬁrst day and ﬁve for the second day of interviewing. Assume the candidates
are interviewed in random order.
(a) What is the probability that xof the top four candidates are interviewed on the ﬁrst day?
(b) How many of the top four candidates can be expected to be interviewed on the ﬁrst day?
95. Twenty pairs of individuals playing in a bridge tournament have been seeded 1, ..., 20. In the
ﬁrst part of the tournament, the 20 are randomly divided into 10 east–west pairs and 10 north–
south pairs.
(a) What is the probability that xof the top 10 pairs end up playing east–west?
(b) What is the probability that all of the top ﬁve pairs end up playing the same direction?
(c) If there are 2 npairs, what is the pmf of X¼the number among the top npairs who end up
playing east–west? What are E(X) and Var( X)?2.6 Other Discrete Distributions 121
96. A second-stage smog alert has been called in an area of Los Angeles County in which there are
50 industrial ﬁrms. An inspector will visit 10 randomly selected ﬁrms to check for violations of
regulations.
(a) If 15 of the ﬁrms are actually violating at least one regulation, what is the pmf of the
number of ﬁrms visited by the inspector that are in violation of at least one regulation?
(b) If there are 500 ﬁrms in the area, of which 150 are in violation, approximate the pmf of part
(a) by a simpler pmf.
(c) For X¼the number among the 10 visited that are in violation, compute E(X) and Var( X)
both for the exact pmf and the approximating pmf in part (b).
97. A shipment of 20 integrated circuits (ICs) arrives at an electronics manufacturing site. The site
manager will randomly select 4 ICs and test them to see whether they are faulty. Unknown to the
site manager, 5 of these 20 ICs are faulty.
(a) Suppose the shipment will be accepted if and only if none of the inspected ICs is faulty.
What is the probability this shipment of 20 ICs will be accepted?
(b) Now suppose the shipment will be accepted if and only if at most one of the inspected ICs
is faulty. What is the probability this shipment of 20 ICs will be accepted?
(c) How do your answers to (a) and (b) change if the number of faculty ICs in the shipment is
3 instead of 5? Recalculate (a) and (b) to verify your claim.
98. Suppose that 20% of all individuals have an adverse reaction to a particular drug. A medical
researcher will administer the drug to one individual after another until the ﬁrst adverse reaction
occurs. Deﬁne an appropriate random variable and use its distribution to answer the following
questions.
(a) What is the probability that when the experiment terminates, four individuals have not had
adverse reactions?
(b) What is the probability that the drug is administered to exactly ﬁve individuals?
(c) What is the probability that at most four individuals do not have an adverse reaction?
(d) How many individuals would you expect to not have an adverse reaction, and how many
individuals would you expect to be given the drug?
(e) What is the probability that the number of individuals given the drug is within one standard
deviation of what you expect?
99. Suppose that p¼P(female birth) ¼.5. A couple wishes to have exactly two female children in
their family. They will have children until this condition is fulﬁlled.
(a) What is the probability that the family has xmale children?
(b) What is the probability that the family has four children?
(c) What is the probability that the family has at most four children?
(d) How many children would you expect this family to have? How many male children would
you expect this family to have?
100. A family decides to have children until it has three children of the same gender. Assuming P(B)
¼P(G)¼.5, what is the pmf of X¼the number of children in the family?
101. Three brothers and their wives decide to have children until each family has two female
children. Let X¼the total number of male children born to the brothers. What is E(X), and
how does it compare to the expected number of male children born to each brother?
102. According to the article “Characterizing the Severity and Risk of Drought in the Poudre River,
Colorado” ( J. of Water Res. Planning and Mgmnt. , 2005: 383–393), the drought length Yis the
number of consecutive time intervals in which the water supply remains below a critical value y0
(a deﬁcit), preceded and followed by periods in which the supply exceeds this value (a surplus).
The cited paper proposes a geometric distribution with p¼.409 for this random variable.122 2 Discrete Random Variables and Probability Distributions
(a) What is the probability that a drought lasts exactly 3 intervals? At least 3 intervals?
(b) What is the probability that the length of a drought exceeds its mean value by at least one
standard deviation?
103. Individual A has a red die and B has a green die (both fair). If they each roll until they obtain ﬁve
“doubles” ( ⚀⚀,...,⚅⚅), what is the pmf of X¼the total number of times a die is rolled? What
areE(X) and SD( X)?
104. A carnival game consists of spinning a wheel with 10 slots, nine red and one blue. If you land on
the blue slot, you win a prize. Suppose your signiﬁcant other really wants that prize, so you will
play until you win.
(a) What is the probability you ’ll win on the ﬁrst spin?
(b) What is the probability you ’ll require exactly 5 spins? At least 5 spins? At most ﬁve spins?
(c) What is the expected number of spins required for you to win the prize, and what is the
corresponding standard deviation?
105. A kinesiology professor, requiring volunteers for her study, approaches students one by one at a
campus hub. She will continue until she acquires 40 volunteers. Suppose that 25% of students
are willing to volunteer for the study, that the professor ’s selections are random, and that the
student population is large enough that individual “trials” (asking a student to participate) may
be treated as independent.
(a) What is the expected number of students the kinesiology professor will need to ask in order
to get 40 volunteers? What is the standard deviation?
(b) Determine the probability that the number of students the kinesiology professor will need
to ask is within one standard deviation of the mean.
106. Refer back to the communication system of Example 2.44. Suppose a voice packet can be
transmitted a maximum of 10 times, i.e., if the 10th attempt fails, no 11th attempt is made to
retransmit the voice packet. Let X¼the number of times a message is transmitted. Assuming
each transmission succeeds with probability p, determine the pmf of X. Then obtain an
expression for the expected number of times a packet is transmitted.
2.7 Moments and Moment Generating Functions
The expected values of integer powers of XandX/C0μare often referred to as moments , terminology
borrowed from physics. In this section, we ’ll discuss the general topic of moments and develop a
shortcut for computing them.
DEFINITION
Thekth moment of a random variable XisE(Xk), while the kth moment about the mean
(orkth central moment )o fXisE[(X/C0μ)k], where μ¼E(X).
For example, μ¼E(X) is the “ﬁrst moment” of Xand corresponds to the center of mass of the
distribution of X.Similarly, Var( X)¼E[(X/C0μ)2] is the second moment of Xabout the mean, which
is known in physics as the moment of inertia.2.7 Moments and Moment Generating Functions 123
Example 2.45 A popular brand of dog food is sold in 5, 10, 15, and 20 lb bags. Let Xbe the weight of
the next bag purchased, and suppose the pmf of Xis
x 51 0 1 5 2 0
p(x) .1 .2 .3 .4
The ﬁrst moment of Xis its mean:
μ¼EXðÞ ¼X
x2Dxp xðÞ ¼ 5:1ðÞ þ 10:2ðÞ þ 15:3ðÞ þ 20:4ðÞ ¼ 15lbs
The second moment about the mean is the variance:
σ2¼EX/C0μðÞ2hi
¼X
x2Dx/C0μðÞ2pxðÞ
¼5/C015ðÞ2:1ðÞ þ 10/C015 ðÞ2:2ðÞ þ 15/C015 ðÞ2:3ðÞ þ 20/C015 ðÞ2:4ðÞ ¼ 25,
for a standard deviation of 5 lb. The third central moment of Xis
E/C2
X/C0μðÞ3/C3
¼X
x2Dx/C0μðÞ3pxðÞ
¼5/C015ðÞ3/C0
:1/C1
þ/C0
10/C015/C13/C0
:2/C1
þ/C0
15/C015/C13/C0
:3/C1
þ/C0
20/C015/C13/C0
:4/C1
¼/C075
We ’ll discuss an interpretation of this last number next. ■
It is not difﬁcult to verify that the third moment about the mean is 0 if the pmf of Xis symmetric.
So, we would like to use E[(X/C0μ)3] as a measure of lack of symmetry, but it depends on the scale of
measurement. If we switch the unit of weight in Example 2.45 from pounds to ounces or kilograms,
the value of the third moment about the mean (as well as the values of all the other moments) will
change. But we can achieve scale independence by dividing the third moment about the mean by σ3:
EX/C0μðÞ3hi
σ3¼EX/C0μ
σ/C18/C193"#
ð2:20Þ
Expression ( 2.20) is our measure of departure from symmetry, called the skewness coefﬁcient .
The skewness coefﬁcient for a symmetric distribution is 0 because its third moment about the mean is
0. However, in the foregoing example the skewness coefﬁcient is E[(X/C0μ)3]/σ3¼/C075/53¼/C00.6.
When the skewness coefﬁcient is negative, as it is here, we say that the distribution is negatively
skewed or that it is skewed to the left . Generally speaking, it means that the distribution stretches
farther to the left of the mean than to the right.
If the skewness were positive, then we would say that the distribution is positively skewed or that it
isskewed to the right . For example, reverse the order of the probabilities in the p(x) table above, so the
probabilities of the values 5, 10, 15, 20 are now .4, .3, .2, and .1, (customers now favor much smaller
bags of dog food). Exercise 119 shows that this changes the sign but not the magnitude of the
skewness coefﬁcient, so it becomes +0.6 and the distribution is skewed right. Both distributions are
illustrated in Fig. 2.9.124 2 Discrete Random Variables and Probability Distributions
2.7.1 The Moment Generating Function
Calculation of the mean, variance, skewness coefﬁcient, etc. for a particular discrete rv requires
extensive, sometimes tedious, summation. Mathematicians have developed a tool, the moment
generating function, that will allow us to determine the moments of a distribution with less effort.
Moreover, this function will allow us to derive properties of several of our major probability
distributions here and in subsequent sections of the book.
DEFINITION
Themoment generating function (mgf) of a discrete random variable Xis deﬁned to be
MXtðÞ¼ EetX/C0/C1
¼X
x2DetxpxðÞ
where Dis the set of possible Xvalues. The moment generating function exists iff MX(t)i s
deﬁned for an interval that includes zero as well as positive and negative values of t.
For any random variable X, the mgf evaluated at t¼0i s
MX0ðÞ ¼ Ee0X/C0/C1
¼X
x2De0xpxðÞ ¼X
x2D1pxðÞ ¼ 1
That is, MX(0) is the sum of all the probabilities, so it must always be 1. However, in order for the
mgf to be useful in generating moments, it will need to be deﬁned for an interval of values of
tincluding 0 in its interior. The moment generating function fails to exist in cases when moments
themselves fail to exist (see Example 2.49 below).
Example 2.46 The simplest example of an mgf is for a Bernoulli distribution, where only the
Xvalues 0 and 1 receive positive probability. Let Xbe a Bernoulli random variable with p(0)¼
1/3 and p(1)¼2/3. Then
MXtðÞ¼ EetX/C0/C1
¼X
x2DetxpxðÞ ¼ et/C10/C1ð1=3Þþet/C11/C1ð2=3Þ¼ð 1=3Þþð 2=3Þet
A Bernoulli random variable will always have an mgf of the form p(0)+p(1)et, a well-deﬁned
function for all values of t. ■5 10 15 2000.10.20.30.4
5 10 15 2000.10.20.30.4
x xp(x) p(x)b a
Fig. 2.9 Departures from symmetry: ( a) skewness coefﬁcient <0 (skewed left); ( b) skewness coefﬁcient >0 (skewed
right)2.7 Moments and Moment Generating Functions 125
A key property of the mgf is its “uniqueness,” the fact that it completely characterizes the
underlying distribution.
MGF UNIQUENESS THEOREM
If the mgf exists and is the same for two distributions, then the two distributions are identical.
That is, the moment generating function uniquely speciﬁes the probability distribution; there is
a one-to-one correspondence between distributions and mgfs.
The proof of this theorem, originally due to Laplace, requires some sophisticated mathematics and
is beyond the scope of this textbook.
Example 2.47 LetX, the number of claims submitted on a renter ’s insurance policy on a given
year, have mgf MX(t)¼.7+.2et+.1e2t. It follows that Xmust have the pmf p(0)¼.7,p(1)¼.2, and
p(2)¼.1—because if we use this pmf to obtain the mgf, we get MX(t), and the distribution is uniquely
determined by its mgf. ■
Example 2.48 Consider testing individuals ’blood samples one by one in order to ﬁnd someone
whose blood type is Rh+. Suppose X, the number of tested samples, has a geometric distribution with
p¼.85:
pxðÞ ¼ :85:15ðÞx/C01forx¼1, 2, 3, ::::
Determining the moment generating function here requires using the formula for the sum of a
geometric series: 1 + r+r2+/C1/C1/C1¼ 1/(1/C0r) for | r|<1. The moment generating function is
MXtðÞ¼ EetXðÞ ¼X
x2DetxpxðÞ ¼X1
x¼1etx:85:15ðÞx/C01¼:85etX1
x¼1etx/C01ðÞ:15ðÞx/C01
¼:85etX1
x¼1:15etðÞx/C01¼:85et/C2
1þ:15etþ:15etðÞ2þ/C1/C1/C1/C3
¼:85et
1/C0:15et
The condition on rrequires |.15 et|<1. Dividing by.15 and taking logs, this gives t</C0ln(.15) /C25
1.90, i.e., this function is deﬁned in the interval ( /C01, 1.90). The result is an interval of values that
includes 0 in its interior, so the mgf exists. As a check, MX(0)¼.85/(1 /C0.15)¼1, as required. ■
Example 2.49 Reconsider Example 2.20, where p(x)¼k/x2,x¼1, 2, 3, .... Recall that E(X) does
not exist for this distribution, portending a problem for the existence of the mgf:
MXtðÞ¼ EetX/C0/C1
¼X1
x¼1etxk
x2
With the help of tests for convergence such as the ratio test, we ﬁnd that the series converges if and
only if et/C201, which means that t/C200, i.e., the mgf is only deﬁned on the interval ( /C01, 0]. Because
zero is on the boundary of this interval, not the interior of the interval (the interval must include both
positive and negative values), the mgf of this distribution does not exist. In any case, it could not be
useful for ﬁnding moments, because Xdoes not have even a ﬁrst moment (mean). ■126 2 Discrete Random Variables and Probability Distributions
2.7.2 Obtaining Moments from the MGF
We now turn to the computation of moments from the mgf. For any positive integer r, let MX(r)(t)
denote the rth derivative of MX(t). By computing this and then setting t¼0, we get the rth moment
about 0.
THEOREM
If the mgf of Xexists, then E(Xr) is ﬁnite for all positive integers r, and
EXrðÞ ¼ MrðÞ
X0ðÞ ð 2:21Þ
Proof The proof of the existence of all moments is beyond the scope of this book. We will show that
Eq. ( 2.21) is true for r¼1 and r¼2.A proof by mathematical induction can be used for general r.
Differentiate:
d
dtMXtðÞ¼d
dtX
x2DextpxðÞ ¼X
x2Dd
dtextpxðÞ ¼X
x2DxextpxðÞ
where we have interchanged the order of summation and differentiation. (This is justiﬁed inside the
interval of convergence, which includes 0 in its interior.) Next set t¼0 to obtain the ﬁrst moment:
M0
X0ðÞ ¼ M1ðÞ
X0ðÞ ¼X
x2Dxex0ðÞpxðÞ ¼X
x2Dxp xðÞ ¼ EXðÞ
Differentiating a second time gives
d2
dt2MXtðÞ¼d
dtX
x2DxextpxðÞ¼X
x2Dxd
dtextpxðÞ¼X
x2Dx2extpxðÞ
Sett¼0 to get the second moment:
M00
X0ðÞ ¼ M2ðÞ
X0ðÞ ¼X
x2Dx2pxðÞ ¼ EX2/C0/C1
■
For the pmfs in Examples 2.45 and 2.46, this may seem like needless work—after all, for simple
distributions with just a few values, we can quickly determine the mean, variance, etc. The real utility
of the mgf arises for more complicated distributions.
Example 2.50 (Example 2.48 continued) Recall that p¼.85 is the probability of a person having Rh
+ blood and we keep checking people until we ﬁnd one with this blood type. If Xis the number of
people we need to check, then p(x)¼.85(.15)x/C01,x¼1, 2, 3, ..., and the mgf is
MXtðÞ¼ EetX/C0/C1
¼:85et
1/C0:15et
Differentiating with the help of the quotient rule,
M0
XtðÞ¼:85et
1/C0:15et ðÞ22.7 Moments and Moment Generating Functions 127
Setting t¼0 then gives μ¼E(X)¼MX0(0)¼1/.85 ¼1.176. This corresponds to the formula 1/ p
for a geometric distribution.
To get the second moment, differentiate again:
M00
XtðÞ¼:85et1þ:15etðÞ
1/C0:15et ðÞ3
Setting t¼0,EX2/C0/C1
¼M00
X0ðÞ ¼1:15
:852. Now use the variance shortcut formula:
VarXðÞ ¼ σ2¼EX2/C0/C1
/C0μ2¼1:15
:852/C01
:85/C18/C192
¼:15
:852¼:2076
This matches the variance formula (1 /C0p)/p2given without proof toward the end of Sect. 2.6.■
As mentioned in Sect. 2.3, it is common to transform a rv Xusing a linear function Y¼aX + b .
What happens to the mgf when we do this?
PROPOSITION
LetXhave the mgf MX(t) and let Y¼aX + b . Then MY(t)¼ebtMX(at).
Example 2.51 LetXbe a Bernoulli random variable with p(0)¼20/38 and p(1)¼18/38. Think of
Xas the number of wins, 0 or 1, in a single play of roulette. If you play roulette at an American casino
and bet on red, then your chances of winning are 18/38 because 18 of the 38 possible outcomes are
red. From Example 2.46, MX(t)¼20/38 + et(18/38). Suppose you bet $5 on red, and let Ybe your
winnings. If X¼0 then Y¼/C05, and if X¼1 then Y¼5. The linear equation Y¼10X/C05 gives the
appropriate relationship.
This equation is of the form Y¼aX + b with a¼10 and b¼/C05, so by the foregoing proposition
MYtðÞ¼ ebtMXatðÞ ¼ e/C05tMX/C0
10t/C1
¼e/C05t20
38þe10t18
38/C20/C21
¼e/C05t/C120
38þe5t/C118
38
This implies that the pmf of Yisp(/C05)¼20/38 and p(5)¼18/38; moreover, we can compute the
mean (and other moments) of Ydirectly from this mgf. ■
2.7.3 MGFs of Common Distributions
Several of the distributions presented in this chapter (binomial, Poisson, negative binomial) have
fairly simple expressions for their moment generating functions. These mgfs, in turn, allow us to
determine the means and variances of the distributions without some rather unpleasant summation.
(Additionally, we will use these mgfs to prove some more advanced distributional properties in
Chap. 4.)
To start, determining the moment generating function of a binomial rv requires use of the binomial
theorem: aþbðÞn¼Pn
x¼0n
x/C18/C19
axbn/C0x. Then128 2 Discrete Random Variables and Probability Distributions
MXtðÞ¼ EetXðÞ ¼X
x2Detxbx;n;pðÞ ¼Xn
x¼0etxn
x/C18/C19
px1/C0pðÞn/C0x
¼Xn
x¼0n
x/C18/C19
petðÞx1/C0pðÞn/C0x¼petþ1/C0p ðÞna¼pet;b¼1/C0p ½/C138
The mean and variance can be obtained by differentiating MX(t):
M0
XtðÞ¼ np etþ1/C0p ðÞn/C01pet)μ¼M0
X/C0
0/C1
¼np;
M00
XtðÞ¼ nn/C01ðÞ/C0
petþ1/C0p/C1n/C02petpetþn/C0
petþ1/C0p/C1n/C01pet)
EX2/C0/C1
¼M00
X0ðÞ ¼ n/C0
n/C01/C1
p2þnp)
σ2¼VarXðÞ ¼ E/C0
X2/C1
/C0μ2
¼nn/C01ðÞ p2þnp/C0n2p2¼np/C0np2¼np/C0
1/C0p/C1
,
in accord with the proposition in Sect. 2.4.
Derivation of the Poisson mgf utilizes the series expansion ∑x¼01ux/x!¼eu:
MXtðÞ¼ EetxðÞ ¼X1
x¼0etxe/C0μμx
x!¼e/C0μX1
x¼0μetðÞx
x!¼e/C0μeμet¼eμet/C01ðÞ
Successive differentiation then gives the mean and variance identiﬁed in Sect. 2.5(see Exercise 127).
Finally, derivation of the negative binomial mgf is based on Newton ’s generalization of the
binomial theorem. The result (see Exercise 124) is
MXtðÞ¼pet
1/C01/C0pðÞ et/C18/C19r
The geometric mgf is just the special case r¼1 (cf. Example 2.48 above). There is unfortunately
no simple expression for the mgf of a hypergeometric rv.
2.7.4 Exercises: Section 2.7(107–128)
107. For the entry-level employees of a certain fast food chain, the pmf of X¼highest grade level
completed is speciﬁed by p(9)¼.01,p(10)¼.05,p(11)¼.16, and p(12)¼.78.
(a) Determine the moment generating function of this distribution.
(b) Use (a) to ﬁnd E(X) and SD( X).
108. For a new car the number of defects Xhas the distribution given by the accompanying table.
Find MX(t) and use it to ﬁnd E(X) and SD( X).
x 012345 6
p(x) .04 .20 .34 .20 .15 .04 .03
109. In ﬂipping a fair coin let Xbe the number of tosses to get the ﬁrst head. Then p(x)¼.5xfor
x¼1, 2, 3, .... Find MX(t) and use it to get E(X) and SD( X).
110. If you toss a fair die with outcome X,p(x)¼1/6 for x¼1, 2, 3, 4, 5, 6. Determine MX(t).
111. Find the skewness coefﬁcients of the distributions in the previous four exercises. Do these
agree with the “shape” of each distribution?2.7 Moments and Moment Generating Functions 129
112. Given MX(t)¼.2+.3et+.5e3t, ﬁnd p(x),E(X), Var( X).
113. If MX(t)¼1/(1/C0t2), ﬁnd E(X) and Var( X).
114. Show that g(t)¼tetcannot be a moment generating function.
115. Using a calculation similar to the one in Example 2.48 show that, if Xhas a geometric
distribution with parameter p, then its mgf is
MXtðÞ¼pet
1/C01/C0pðÞ et
Assuming that Yhas mgf MY(t)¼.75et/(1/C0.25et), determine the probability mass function
p(y) with the help of the uniqueness property.
116. (a) Prove the result in the second proposition: MaX+b(t)¼ebtMX(at).
(b) Let Y¼aX + b. Use (a) to establish the relationships between the means and variances of
XandY.
117. Let MXtðÞ¼ e5tþ2t2and let Y¼(X/C05)/2. Find MY(t) and use it to ﬁnd E(Y) and Var( Y).
118. Let Xhave the moment generating function of Example 2.48 and let Y¼X/C01. Recall that Xis
the number of people who need to be checked to get someone who is Rh +,s oYis the number of
people checked before the ﬁrst Rh +person is found. Find MY(t).
119. Let Xbe the number of points earned by a randomly selected student on a 10 point quiz, with
possible values 0, 1, 2, ..., 10 and pmf p(x), and suppose the distribution has a skewness
coefﬁcient of c. Now consider reversing the probabilities in the distribution, so that p(0) is
interchanged with p(10), p(1) is interchanged with p(9), and so on. Show that the skewness
coefﬁcient of the resulting distribution is /C0c.[Hint:L e t Y¼10/C0Xand show that Yhas the
reversed distribution. Use this fact to determine μYand then the value of skewness coefﬁcient for
theYdistribution.]
120. Let MX(t) be the moment generating function of a rv X, and deﬁne a new function by
LXtðÞ¼ lnMXtðÞ½/C138
Show that (a) LX(0)¼0, (b) LX0(0)¼μ, and (c) LX00(0)¼σ2.
121. Refer back to Exercise 120. If MXtðÞ¼ e5tþ2t2then ﬁnd E(X) and Var( X) by differentiating
(a) MX(t)
(b) LX(t)
122. Refer back to Exercise 120. If MXtðÞ¼ e5et/C01ðÞthen ﬁnd E(X) and Var( X) by differentiating
(a) MX(t)
(b) LX(t)
123. Obtain the moment generating function of the number of failures, n/C0X, in a binomial
experiment, and use it to determine the expected number of failures and the variance of the
number of failures. Are the expected value and variance intuitively consistent with the
expressions for E(X) and Var( X)? Explain.
124. Newton ’s generalization of the binomial theorem can be used to show that, for any positive
integer r,
1/C0uðÞ/C0r¼X1
k¼0rþk/C01
r/C01/C18/C19
uk
Use this to derive the negative binomial mgf presented in this section. Then obtain the mean and
variance of a binomial rv using this mgf.130 2 Discrete Random Variables and Probability Distributions
125. If Xis a negative binomial rv, then Y¼X/C0ris the number of failures preceding the rth success.
Obtain the mgf of Yand then its mean value and variance.
126. Refer back to Exercise 120. Obtain the negative binomial mean and variance from LX(t)¼
ln[MX(t)].
127. (a) Use derivatives of MX(t) to obtain the mean and variance for the Poisson distribution.
(b) Obtain the Poisson mean and variance from LX(t)¼ln[MX(t)]. In terms of effort, how does
this method compare with the one in part (a)?
128. Show that the binomial moment generating function converges to the Poisson moment
generating function if we let n!1 andp!0 in such a way that npapproaches a value
μ>0. [Hint: Use the calculus theorem that was used in showing that the binomial pmf
converges to the Poisson pmf.] There is, in fact, a theorem saying that convergence of the
mgf implies convergence of the probability distribution. In particular, convergence of the
binomial mgf to the Poisson mgf implies b(x;n,p)!p(x;μ).
2.8 Simulation of Discrete Random Variables
Probability calculations for complex systems often depend on the behavior of various random
variables. When such calculations are difﬁcult or impossible, simulation is the fallback strategy. In
this section, we give a general method for simulating an arbitrary discrete random variable and
consider implementations in existing software for simulating common discrete distributions.
Example 2.52 Refer back to the distribution of Example 2.11 for the random variable X¼the
amount of memory (GB) in a purchased ﬂash drive, and suppose we wish to simulate X. Recall from
Sect. 1.6that we begin with a “standard uniform” random number generator, i.e., a software function
that generates evenly distributed numbers in the interval [0, 1). Our goal is to convert these decimals
into the values of Xwith the probabilities speciﬁed by its pmf: 5% 1s, 10% 2s, 35% 4s, and so on. To
that end, we partition the interval [0, 1) according to these percentages: [0, .05) has probability .05;
[.05, .15) has probability .1, since the length of the interval is .1; [.15, .50) has probability .50 /C0.15¼
.35; etc. Proceed as follows: given a value ufrom the RNG,
–I f 0 /C20u<.05, assign the value 1 to the variable x.
– If .05 /C20u<.15, assign x¼2.
– If .15 /C20u<.50, assign x¼4.
– If .50 /C20u<.90, assign x¼8.
– If .90 /C20u<1, assign x¼16.
Repeating this algorithm ntimes gives nsimulated values of X. Programs in Matlab and R that
implement this algorithm appear in Fig. 2.10; both return a vector, x, containing n¼10,000
simulated values of the speciﬁed distribution.
Figure 2.11 shows a graph of the results of executing the code, in the form of a histogram : the
height of each rectangle corresponds to the relative frequency of each xvalue in the simulation (i.e.,
the number of times that value occurred, divided by 10,000). The exact pmf of Xis superimposed for
comparison; as expected, simulation results are similar, but not identical, to the theoretical
distribution.2.8 Simulation of Discrete Random Variables 131
Later in this section, we will present a faster, built-in way to simulate discrete distributions in
Matlab and R. The method introduced here will, however, prove useful in adapting to the case of
continuous random variables in Chap. 3. ■
In the preceding example, the selected subintervals of [0, 1) were not our only choices—any ﬁve
intervals with lengths .05, .10, .35, .40, and .10 would produce the desired result. However, those
particular ﬁve subintervals have one desirable feature: the “cut points” for the intervals (i.e., 0, .05,
.15, .50, .90, and 1) are precisely the possible heights of the graph of the cdf, F(x). This permits a
geometric interpretation of the algorithm, which can be seen in Fig. 2.12. The value uprovided by the
RNG corresponds to a position on the vertical axis between 0 and 1; we then “invert” the cdf by
matching this u-value back to one of the gaps in the graph of F(x), denoted by dashed lines in
Fig.2.12. If the gap occurs at horizontal position x, then xis our simulated value of the rv Xfor that
run of the simulation. This is often referred to as the inverse cdf method for simulating discrete
random variables. The general method is spelled out in the accompanying box.x=zeros(10000,1);
for i=1:10000
u=rand; 
if u<.05
x(i)=1;
elseif u<.15
x(i)=2;
elseif u<.50
x(i)=4;
elseif u<.90
x(i)=8;
else
x(i)=16;
end
endx <- NULL
for (i in 1:10000){
u=runif(1)
if (u<.05)
x[i]<-1
else if (u<.15)
x[i]<-2
else if (u<.50)
x[i]<-4
else if (u<.90)
x[i]<-8
else
x[i]<-16
}b a
Fig. 2.10 Simulation code: ( a) Matlab; ( b)R
00.00.10.20.30.4
51 0Probability
15 20
Fig. 2.11 Simulation and exact distribution for Example 2.52132 2 Discrete Random Variables and Probability Distributions
Inverse cdf Method for Simulating Discrete Random Variables
LetXbe a discrete random variable taking on values x1<x2<...with corresponding
probabilities p1,p2,.... Deﬁne F0¼0;F1¼F(x1)¼p1;F2¼F(x2)¼p1+p2; and, in general,
Fk¼F(xk)¼p1+/C1/C1/C1+pk¼Fk/C01+pk. To simulate a value of X, proceed as follows:
1. Use an RNG to produce a value, u, from [0, 1).
2. If Fk/C01/C20u<Fk, then assign x¼xk.
Example 2.53 (Example 2.52 continued): Suppose the prices for the ﬂash drives, in increasing order
of memory size, are $10, $15, $20, $25, and $30. If the store sells 80 ﬂash drives in a week, what ’s the
probability they will make a gross proﬁt of at least $1800?
LetY¼the amount spent on a ﬂash drive, which has the following pmf:
y 10 15 20 25 30
p(y) .05 .10 .35 .40 .10
The gross proﬁt for 80 purchases is the sum of 80 values from this distribution. Let A¼{gross
proﬁt /C21$1800}. We can use simulation to estimate P(A), as follows:
0. Set a counter for the number of times Aoccurs to zero.
Repeat ntimes:
1. Simulate 80 values y1,...,y80from the above pmf (using for example an inverse cdf program
similar to those displayed in Fig. 2.10).
2. Compute the week ’s gross proﬁt, g¼y1+/C1/C1/C1+y80.
3. If g/C211800, add 1 to the count of occurrences for A.
Once the nruns are complete, then ^PAðÞ ¼ count of the occurrences of A ðÞ =n.
Figure 2.13 shows the resulting values of gforn¼10,000 simulations in R. In effect, our program
is simulating a random variable G¼Y1+...+Y80whose pmf is not known (in light of all the possible48 1 6 2100.05.15.50.901
xuF(x) Fig. 2.12 The inverse cdf
method for Example 2.522.8 Simulation of Discrete Random Variables 133
Gvalues, it would not be worthwhile to attempt to determine its pmf analytically). The highlighted
bars in Fig. 2.13 correspond to gvalues of at least $1800; in our simulation, such values
occurred 1940 times. Thus, ^PAðÞ ¼ 1940 =10, 000 ¼:194, with an estimated standard error ofﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:194 1 /C0:194 ðÞ =10, 000p
¼:004. ■
2.8.1 Simulations Implemented in R and Matlab
Earlier in this section, we presented the inverse cdf method as a general way to simulate discrete
distributions applicable in any software. In fact, one can simulate generic discrete rvs in both Matlab
and R by clever use of the built-in randsample andsample functions, respectively. We saw these
functions in the context of probability simulation in Chap. 1. Both are designed to generate a random
sample from any selected set of values (even including text values, if desired); the “clever” part is that
both can accommodate a set of weights. The following short example illustrates their use.
To simulate, say, 35 values from the pmf in Example 2.52, one can use the following code in
Matlab:
randsample([1,2,4,8,16],35,true,[.05, .10, .35, .40, .10])
The function takes four arguments: the list of x-values, the desired number of simulated values (the
“sample size”), whether to sample with replacement (here, true ), and the list of probabilities in the
same order as the x-values. The corresponding call in R is
sample(c(1,2,4,8,16),35,TRUE,c(.05, .10, .35, .40, .10))
Thanks to the ubiquity of the binomial, Poisson, and other distributions in probability modeling,
many software packages have built-in tools for simulating values from these distributions. Table 2.6
summarizes the relevant functions in Matlab and R; the input argument sizerefers to the desired
number of simulated values of the distribution.1600Frequency
050010001500
1650 1700 1750
g1800 1850 1900
Fig. 2.13 Simulated distribution of weekly gross proﬁt for Example 2.53134 2 Discrete Random Variables and Probability Distributions
A word of warning (really, a reminder) about the way software treats the negative binomial distribu-
tion: both Matlab and R deﬁne a negative binomial rv as the number of failures preceding the rth success,
which differs from our deﬁnition. Assuming you want to simulate the number of trials required
to achieve rsuccesses, execute the code in the last line of Table 2.6and then add rto each value.
Example 2.54 The number of customers shipping express mail packages at a certain store during
any particular hour of the day is a Poisson rv with mean 5. Each such customer has 1, 2, 3, or
4 packages with probabilities .4, .3, .2, and .1, respectively. Let ’s carry out a simulation to estimate
the probability that at most 10 packages are shipped during any particular hour.
Deﬁne an event A¼{at most 10 packages shipped in an hour}. Our simulation to estimate P(A)
proceeds as follows.
0. Set a counter for the number of times Aoccurs to zero.
Repeat ntimes:
1. Simulate the number of customers in an hour, C, which is Poisson with μ¼5.
2. For each of the Ccustomers, simulate the number of packages shipped according to the pmf above.
3. If the total number of packages shipped is at most 10, add 1 to the counter for A.
Matlab and R code to implement this simulation appear in Fig. 2.14.
In Matlab, 10,000 simulations resulted in 10 or few er packages 5752 times, f or an estimated probability
of^PAðÞ ¼ :5752, with an estimated standard error ofﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:5752 1 /C0:5752 ðÞ =10,000p
¼:0049. ■
2.8.2 Simulation Mean, Standard Deviation, and Precision
In Sect. 1.6and in the preceding examples, we used simulation to estimate the probability of an event.
But consider the “gross proﬁt” variable Gin Example 2.53: since we have 10,000 simulated values of
this variable, we should be able to estimate its mean μGand its standard deviation σG. More generally,Table 2.6 Functions to simulate major discrete distributions in Matlab and R
Distribution Matlab code R code
Binomial binornd( n,p,size,1) rbinom( size,n,p)
Poisson poissrnd( μ,size,1) rpois( size,μ)
Hypergeometric hygernd( N,M,n,size,1) rhyper( size,M,N/C0M,n)
Negative binomial nbinrnd( r,p,size,1) rnbinom( size,r,p)
A=0;
for i=1:10000
c=poissrnd(5,1); 
packages = randsample([1,2,3,4],c,
true,[.4,.3,.2,.1]);
if sum(packages)<=10
A=A+1;
end
endA <- 0
for (i in 1:10000){
c<-rpois(1,5)
packages <- sample(c(1,2,3,4),c,
TRUE,c(.4,.3,.2,.1))
if (sum(packages)<=10){
A<-A+1
}
}b a
Fig. 2.14 Simulation code for Example 2.54: ( a) Matlab; ( b)R2.8 Simulation of Discrete Random Variables 135
suppose we have simulated nvalues x1,...,xnof a random variable X. Then the following quantities
based on our observed values serve as suitable estimates.
DEFINITION
For a set of numerical values x1,...,xn, the sample mean , denoted by /C22x,i s
/C22x¼x1þ/C1/C1/C1þ xn
n¼1
nXn
i¼1xi
Thesample standard deviation of these numerical values, denoted by s,i s
s¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n/C01Xn
i¼1xi/C0/C22xðÞ2s
Ifx1,...,xnrepresent simulated values of a random variable X, then we may estimate the
expected value and standard deviation of Xby^μ¼/C22xand ^σ¼s, respectively.
The justiﬁcation for the use of the divisor n/C01i nswill be discussed in Chap. 5.
In Sect. 1.6, we introduced the standard error of an estimated probability, which quantiﬁes the
precision of a simulation result ^PAðÞas an estimate of a “true” probability P(A). By analogy, it is
possible to quantify the amount by which a sample mean, /C22x, will generally differ from the
corresponding expected value μ.F o r nsimulated values of a random variable, with sample standard
deviation s, the ( estimated )standard error of the mean is
sﬃﬃﬃnp ð2:22Þ
Expression ( 2.22) will be derived in Chap. 4. As with an estimated probability, the formula
indicates that the precision of /C22xincreases (i.e., its standard error decreases )a snincreases, but not
very quickly. To increase the precision of /C22xas an estimate of μby a factor of 10 (one decimal place)
requires increasing the number of simulation runs, n, by a factor of 100. Unfortunately, there is no
general formula for the standard error of sas an estimate of σ.
Example 2.55 (Ex. 2.53 continued) The 10,000 simulated values of the random variable G, which
we denote by g1,...,g10000, are displayed in the histogram in Fig. 2.13. From these simulated values,
we can estimate both the expected value and standard deviation of G:
^μG¼/C22g¼1
10,000X10,000
i¼1gi¼1759 :62
^σG¼s¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
10,000 /C01X10,000
i¼1gi/C0/C22g ðÞ2vuut¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
9999X10,000
i¼1gi/C01759 :62 ðÞ2vuut¼43:50
We estimate that the average weekly gross proﬁt from ﬂash drive sales is $1759.62, with a standard
deviation of $43.50. Neither of these computations was performed by hand, of course: if the nsimulated
values of a variable are stored in a vector x,t h e n mean(x) andsd(x) in R will provide the sample
mean and standard deviation, respectively. In Matlab, the calls are mean(x) andstd(x) .
Applying Eq. ( 2.22), the (estimated) standard error of /C22giss=ﬃﬃﬃnp¼43:50=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ10,000p¼0:435.
If 10,000 runs are used to simulate G,i t’s estimated that the resulting sample mean will differ from136 2 Discrete Random Variables and Probability Distributions
E(G) by roughly 0.435. (In contrast, the sample standard deviation, s, estimates that the gross proﬁt
for a single week—i.e., a single observation g—typically differs from E(G) by about $43.50.) ■
In Chap. 4, we will see how the expected value and variance of random variables like G, that are
sums of a ﬁxed number of other rvs, can be obtained analytically.
Example 2.56 The “help desk” at a university ’s computer center receives both hardware and
software queries. Let XandYbe the number of hardware and software queries, respectively, in a
given day. Each can be modeled by a Poisson distribution with mean 20. Because computer center
employees need to be allocated efﬁciently, of interest is the difference between the sizes of the two
queues: D¼|X/C0Y|. Let ’s use simulation to estimate (1) the probability the queue sizes differ by
more than 5; (2) the expected difference; (3) the standard deviation of the difference.
Figure 2.15 shows Matlab and R code to simulate this process. In both languages, the code exploits
the built-in Poisson simulator, as well as the fact that 10,000 simulated values may be called
simultaneously.
The line sum((D >5)) performs two operations: ﬁrst, (D>5)determines if each simulated
dvalue exceeds 5, returning a logical vector of bits; second, sum() tallies the “success” bits (1s or
TRUEs) and gives a count of the number of times the event { D>5} occurred in the 10,000
simulations. The results from one run in Matlab were
^PD >5 ðÞ ¼3843
10, 000¼:3843 ^μD¼/C22d¼5:0380 ^σD¼s¼3:8436
A histogram of the simulated values of Dappears in Fig. 2.16.b a
X=poissrnd(20,10000,1);
Y=poissrnd(20,10000,1);
D=abs(X-Y);
sum((D>5))
mean(D)
std(D)X<-rpois(10000,20)
Y<-rpois(10000,20)
D<-abs(X-Y)
sum((D>5))
mean(D)
sd(D)
Fig. 2.15 Simulation code for Example 2.56: ( a) Matlab; ( b)R
05001000150020002500Frequency30003500
10 15x20 25 5 0
Fig. 2.16 Simulation histogram of Din Example 2.56 ■2.8 Simulation of Discrete Random Variables 137
2.8.3 Exercises: Section 2.8(129–141)
129. Consider the pmf given in Exercise 30 for the random variable Y¼the number of moving
violations for which the a randomly selected insured individual was cited during the last
3 years. Write a program to simulate this random variable, then use your simulation to estimate
E(Y) and SD( Y). How do these compare to the exact values of E(Y) and SD( Y)?
130. Consider the pmf given in Exercise 32 for the random variable X¼capacity of a purchased
freezer. Write a program to simulate this random variable, then use your simulation to estimate
E(X) and SD( X). How do these compare to the exact values of E(X) and SD( X)?
131. Suppose person after person is tested for the presence of a certain characteristic. The probabil-
ity that any individual tests positive is .75. Let X¼the number of people who must be tested to
obtain ﬁve consecutive positive test results. Use simulation to estimate P(X/C2025).
132. The matching problem . Suppose that Nitems labeled 1, 2, ...,Nare shufﬂed so that they are in
random order. Of interest is how many of these will be in their “correct” positions (e.g., item #5
situated at the 5th position in the sequence, etc.) after shufﬂing.
(a) Write a program that simulates a permutation of the numbers 1 to Nand then records the
value of the variable X¼number of items in the correct position.
(b) Set N¼5 in your program, and use at least 10,000 simulations to estimate E(X), the
expected number of items in the correct position.
(c) Set N¼52 in your program (as if you were shufﬂing a deck of cards), and use at least
10,000 simulations to estimate E(X). What do you discover? Is this surprising?
133. Exercise 109 of Chap. 1referred to a multiple-choice exam in which 10 of the questions have
two options, 13 have three options, 13 have four options, and the other 4 have ﬁve options. Let
X¼the number of questions a student gets right, assuming s/he is completely guessing.
(a) Write a program to simulate X, and use your program to estimate the mean and standard
deviation of X.
(b) Estimate the probability a student will score at least one standard deviation above
the mean.
134. Example 2.53 of this section considered the gross proﬁt Gresulting from selling ﬂash drives to
80 customers per week. Of course, it isn ’t realistic for the number of customers to remain ﬁxed
from week to week. So, instead, imagine the number of customers buying ﬂash drives in a
week follows a Poisson distribution with mean 80, and that the amount paid by each customer
follows the distribution for Yprovided in that example. Write a program to simulate the
random variable G, and use your simulation to estimate
(a) The probability that weekly gross sales are at least $1,800.
(b) The mean of G.
(c) The standard deviation of G.
135. Exercise 21 (Sect. 2.2) investigated Benford ’s law, a discrete distribution with pmf given by
p(x)¼log10((x+1 ) / x) for x¼1, 2, ..., 9. Use the inverse cdf method to write a program that
simulates the Benford ’s law distribution. Then use your program to estimate the expected
value and variance of this distribution.
136. Recall that a geometric rv has pmf p(x)¼p(1/C0p)x/C01forx¼1, 2, 3, .... In Example 2.12, it
was shown that the cdf of this distribution is given by F(x)¼1/C0(1/C0p)xfor positive integers x.
(a) Write a program that implements the inverse cdf method to simulate a geometric
distribution. Your program should have as inputs the numerical value of pand the desired
sample size.
(b) Use your program to simulate 10,000 values from a geometric rv Xwith p¼.85. From
these values, estimate each of the following: P(X/C202),E(X), SD( X). How do these
compare to the corresponding exact values?138 2 Discrete Random Variables and Probability Distributions
137. Tickets for a particular ﬂight are $250 apiece. The plane seats 120 passengers, but the airline
will knowingly overbook (i.e., sell more than 120 tickets), because not every paid passenger
shows up. Let tdenote the number of tickets the airline sells for this ﬂight, and assume the
number of passengers that actually show up for the ﬂight, X, follows a Bin( t, .85) distribution.
LetB¼the number of paid passengers who show up at the airport but are denied a seat on the
plane, so B¼X/C0120 if X>120 and B¼0 otherwise. If the airline must compensate these
passengers with $500 apiece, then the proﬁt the airline makes on this ﬂight is 250 t/C0500B.
(Notice tis ﬁxed, but Bis random.)
(a) Write a program to simulate this scenario. Speciﬁcally, your program should take in tas
an input and return many values of the proﬁt variable 250 t/C0500B.
(b) The airline wishes to determine the optimal value of t, i.e., the number of tickets to sell
that will maximize their expected proﬁt. Run your program for t¼140, 141, ..., 150, and
record the average proﬁt from many runs under each of these settings. What value of
tappears to return the largest value? [ Note: If a clear winner does not emerge, you might
need to increase the number of runs for each tvalue!]
138. Imagine the following simple game: ﬂip a fair coin repeatedly, winning $1 for every head and
losing $1 for every tail. Your net winnings will potentially oscillate between positive and
negative numbers as play continues. How many times do you think net winnings will change
signs in, say, 1000 coin ﬂips? 5000 ﬂips?
(a) Let X¼the number of sign changes in 1000 coin ﬂips. Write a program to simulate X, and
use your program to estimate the probability of at least 10 sign changes.
(b) Use your program to estimate E(X) and SD( X). Does your estimate for E(X) match your
intuition for the number of sign changes?
(c) Repeat parts (a)–(b) with 5000 ﬂips.
139. Exercise 39 (Sect. 2.3) describes the game Plinko from The Price is Right . Each contestant drops
between one and 5 chips down the Plinko board, depending on how well s/he prices several
small items. Suppose the random variable C¼number of chips earned by a contestant has the
following distribution:
c 12345
p(c) .03 .15 .35 .34 .13
The winnings from each chip follow the distribution presented in Exercise 39. Write a program
to simulate Plinko; you will need to consider both the number of chips a contestant earns and
how much money is won on each of those chips. Use your simulation estimate the answers to the
following questions:
(a) What is the probability a contestant wins more than $11,000?
(b) What is a contestant ’s expected winnings?
(c) What is the corresponding standard deviation?
(d) In fact, a player gets one Plinko chip for free and can earn the other four by guessing the
prices of small items (wafﬂe irons, alarm clocks, etc.). Assume the player has a 50–50
chance of getting each price correct, so we may write C¼1+R, where R~ Bin(4, .5). Use
this revised model for Cto estimate the answers to (a)–(c).
140. Recall the Coupon Collector ’s Problem described in the book’s Introduction and again in
Exercise 114 of Chap. 1. Let X¼the number of cereal boxes purchased in order to obtain all
10 coupons.
(a) Use a simulation program to estimate E(X) and SD( X). Also compute the estimated
standard error of your sample mean.2.8 Simulation of Discrete Random Variables 139
(b) How does your estimate of E(X) compare to the theoretical answer given in the
Introduction?
(c) Repeat (a) with 20 coupons required instead of 10. Does it appear to take roughly twice as
long to collect 20 coupons as 10? More than twice as long? Less?
141. A small high school holds its graduation ceremony in the gym. Because of seating constraints,
students are limited to a maximum of four tickets to graduation for family and friends. Suppose
30% of students want four tickets, 25% want three, 25% want two, 15% want one, and 5%
want none.
(a) Write a simulation for 150 graduates requesting tickets, where students ’requests follow
the distribution described above. In particular, keep track of the variable T¼the total
number of tickets requested by these 150 students.
(b) The gym can seat a maximum of 410 guests. Based on your simulation, estimate the
probability that all students ’requests can be accommodated.
2.9 Supplementary Exercises (142–170)
142. Consider a deck consisting of seven cards, marked 1, 2, ..., 7. Three of these cards are selected
at random. Deﬁne an rv WbyW¼the sum of the resulting numbers, and compute the pmf of
W. Then compute E(W) and Var( W). [Hint: Consider outcomes as unordered, so that (1, 3, 7)
and (3, 1, 7) are not different outcomes. Then there are 35 outcomes, and they can be listed.]
(This type of rv actually arises in connection with Wilcoxon ’s rank-sum test , in which there is
anxsample and a ysample and Wis the sum of the ranks of the x’s in the combined sample.)
143. After shufﬂing a deck of 52 cards, a dealer deals out 5. Let X¼the number of suits represented
in the ﬁve-card hand.
(a) Show that the pmf of Xis
x 123 4
p(x) .002 .146 .588 .264
[Hint:p(1)¼4P(all are spades), p(2)¼6P(only spades and hearts with at least one of
each), and p(4)¼4P(2 spades \one of each other suit).]
(b) Compute E(X) and SD( X).
144. The negative binomial rv Xwas deﬁned as the number of trials necessary to obtain the rthS.
LetY¼the number of F’s preceding the rthS. In the same manner in which the pmf of Xwas
derived, derive the pmf of Y.
145. Of all customers purchasing automatic garage-door openers, 75% purchase a chain-driven
model. Let X¼the number among the next 15 purchasers who select the chain-driven model.
(a) What is the pmf of X?
(b) Compute P(X>10).
(c) Compute P(6/C20X/C2010).
(d) Compute E(X) and SD( X).
(e) If the store currently has in stock 10 chain-driven models and 8 shaft-driven models, what
is the probability that the requests of these 15 customers can all be met from existing
stock?
146. A friend recently planned a camping trip. He has two ﬂashlights, one that required a single 6-V
battery and another that used two size-D batteries. He had previously packed two 6-V and four140 2 Discrete Random Variables and Probability Distributions
size-D batteries in his camper. Suppose the probability that any particular battery works is
pand that batteries work or fail independently of one another. Our friend wants to take just one
ﬂashlight. For what values of pshould he take the 6-V ﬂashlight?
147. Binary data are transmitted over a noisy communication channel. The probability that a
received binary digit is in error due to channel noise is 0.05. Assume that such errors occur
independently within the bit stream.
(a) What is the probability that the 3rd error occurs on the 50th transmitted bit?
(b) On average, how many bits will be transmitted correctly before the ﬁrst error?
(c) Consider a 32-bit “word.” What is the probability of exactly 2 errors in this word?
(d) Consider the next 10,000 bits. What approximating model could we use for X¼the
number of errors in these 10,000 bits? Give both the name of the model and the value(s) of
the parameter(s).
148. A manufacturer of ﬂashlight batteries wishes to control the quality of its product by rejecting
any lot in which the proportion of batteries having unacceptable voltage appears to be too high.
To this end, out of each large lot (10,000 batteries), 25 will be selected and tested. If at least
5 of these generate an unacceptable voltage, the entire lot will be rejected. What is the
probability that a lot will be rejected if
(a) 5% of the batteries in the lot have unacceptable voltages?
(b) 10% of the batteries in the lot have unacceptable voltages?
(c) 20% of the batteries in the lot have unacceptable voltages?
(d) What would happen to the probabilities in parts (a)–(c) if the critical rejection number
were increased from 5 to 6?
149. Of the people passing through an airport metal detector, .5% activate it; let X¼the number
among a randomly selected group of 500 who activate the detector.
(a) What is the (approximate) pmf of X?
(b) Compute P(X¼5).
(c) Compute P(X/C215).
150. An educational consulting ﬁrm is trying to decide whether high school students who have
never before used a handheld calculator can solve a certain type of problem more easily with a
calculator that uses reverse Polish logic or one that does not use this logic. A sample of
25 students is selected and allowed to practice on both calculators. Then each student is asked
to work one problem on the reverse Polish calculator and a similar problem on the other. Let
p¼P(S), where Sindicates that a student worked the problem more quickly using reverse
Polish logic than without, and let X¼number of S’s.
(a) If p¼.5, what is P(7/C20X/C2018)?
(b) If p¼.8, what is P(7/C20X/C2018)?
(c) If the claim that p¼.5 is to be rejected when either X/C207o r X/C2118, what is the
probability of rejecting the claim when it is actually correct?
(d) If the decision to reject the claim p¼.5 is made as in part (c), what is the probability that
the claim is not rejected when p¼.6? When p¼.8?
(e) What decision rule would you choose for rejecting the claim p¼.5 if you wanted the
probability in part (c) to be at most.01?
151. Consider a disease whose presence can be identiﬁed by carrying out a blood test. Let pdenote
the probability that a randomly selected individual has the disease. Suppose nindividuals are
independently selected for testing. One way to proceed is to carry out a separate test on each of
thenblood samples. A potentially more economical approach, group testing, was introduced
during World War II to identify syphilitic men among army inductees. First, take a part of each2.9 Supplementary Exercises (142–170) 141
blood sample, combine these specimens, and carry out a single test. If no one has the disease, the
result will be negative, and only the one test is required. If at least one individual is diseased, the
test on the combined sample will yield a positive result, in which case the nindividual tests are
then carried out. If p¼.1 and n¼3, what is the expected number of tests using this procedure?
What is the expected number when n¼5? [The article “Random Multiple-Access Communi-
cation and Group Testing” ( IEEE Trans. Commun ., 1984: 769–774) applied these ideas to a
communication system in which the dichotomy was active/idle user rather than diseased/
nondiseased.]
152. Let p1denote the probability that any particular code symbol is erroneously transmitted through
a communication system. Assume that on different symbols, errors occur independently of one
another. Suppose also that with probability p2an erroneous symbol is corrected upon receipt.
LetXdenote the number of correct symbols in a message block consisting of nsymbols (after
the correction process has ended). What is the probability distribution of X?
153. The purchaser of a power-generating unit requires cconsecutive successful start-ups before the
unit will be accepted. Assume that the outcomes of individual start-ups are independent of one
another. Let pdenote the probability that any particular start-up is successful. The random
variable of interest is X¼the number of start-ups that must be made prior to acceptance. Give
the pmf of Xfor the case c¼2. If p¼.9, what is P(X/C208)? [ Hint: For x/C215, express p(x)
“recursively” in terms of the pmf evaluated at the smaller values x/C03,x/C04,..., 2.] (This
problem was suggested by the article “Evaluation of a Start-Up Demonstration Test,” J. Qual.
Tech ., 1983: 103–106.)
154. A plan for an executive travelers ’club has been developed by an airline on the premise that 10%
of its current customers would qualify for membership.
(a) Assuming the validity of this premise, among 25 randomly selected current customers,
what is the probability that between 2 and 6 (inclusive) qualify for membership?
(b) Again assuming the validity of the premise, what are the expected number of customers
who qualify and the standard deviation of the number who qualify in a random sample of
100 current customers?
(c) Let Xdenote the number in a random sample of 25 current customers who qualify for
membership. Consider rejecting the company ’s premise in favor of the claim that p>.10 if
x/C217. What is the probability that the company ’s premise is rejected when it is actually
valid?
(d) Refer to the decision rule introduced in part (c). What is the probability that the company ’s
premise is not rejected even though p¼.20 (i.e., 20% qualify)?
155. Forty percent of seeds from maize (modern-day corn) ears carry single spikelets, and the other
60% carry paired spikelets. A seed with single spikelets will produce an ear with single spikelets
29% of the time, whereas a seed with paired spikelets will produce an ear with single spikelets
26% of the time. Consider randomly selecting ten seeds.
(a) What is the probability that exactly ﬁve of these seeds carry a single spikelet and produce
an ear with a single spikelet?
(b) What is the probability that exactly ﬁve of the ears produced by these seeds have single
spikelets? What is the probability that at most ﬁve ears have single spikelets?
156. A trial has just resulted in a hung jury because eight members of the jury were in favor of a
guilty verdict and the other four were for acquittal. If the jurors leave the jury room in random
order and each of the ﬁrst four leaving the room is accosted by a reporter in quest of an
interview, what is the pmf of X¼the number of jurors favoring acquittal among those
interviewed? How many of those favoring acquittal do you expect to be interviewed?142 2 Discrete Random Variables and Probability Distributions
157. A reservation service employs ﬁve information operators who receive requests for information
independently of one another, each according to a Poisson process with rate λ¼2 per minute.
(a) What is the probability that during a given 1-min period, the ﬁrst operator receives no
requests?
(b) What is the probability that during a given 1-min period, exactly four of the ﬁve operators
receive no requests?
(c) Write an expression for the probability that during a given 1-min period, all of the
operators receive exactly the same number of requests.
158. Grasshoppers are distributed at random in a large ﬁeld according to a Poisson process with
parameter λ¼2 per square yard. How large should the radius rof a circular sampling region be
taken so that the probability of ﬁnding at least one grasshopper in the region equals .99?
159. A newsstand has ordered ﬁve copies of a certain issue of a photography magazine. Let X¼the
number of individuals who come in to purchase this magazine. If Xhas a Poisson distribution
with parameter μ¼4, what is the expected number of copies that are sold?
160. Individuals A and B begin to play a sequence of chess games. Let S¼{A wins a game}, and
suppose that outcomes of successive games are independent with P(S)¼pandP(F)¼1/C0
p(they never draw). They will play until one of them wins ten games. Let X¼the number of
games played (with possible values 10, 11, ..., 19).
(a) For x¼10, 11, ..., 19, obtain an expression for p(x)¼P(X¼x).
(b) If a draw is possible, with p¼P(S),q¼P(F), 1/C0p/C0q¼P(draw), what are the possible
values of X? What is P(20/C20X)? [Hint:P(20/C20X)¼1/C0P(X<20).]
161. A test for the presence of a disease has probability .20 of giving a false-positive reading
(indicating that an individual has the disease when this is not the case) and probability.10 of
giving a false-negative result. Suppose that ten individuals are tested, ﬁve of whom have the
disease and ﬁve of whom do not. Let X¼the number of positive readings that result.
(a) Does Xhave a binomial distribution? Explain your reasoning.
(b) What is the probability that exactly three of the ten test results are positive?
162. The generalized negative binomial pmf is given by
nb x ;r;pðÞ ¼ kr;xðÞ /C2 pr1/C0pðÞxx¼0, 1, 2, ...
where
kr;xðÞ ¼xþr/C01 ðÞ xþr/C02 ðÞ ...xþr/C0x ðÞ
x!x¼1, 2, ...
1 x¼08
<
:
LetX, the number of plants of a certain species found in a particular region, have this
distribution with p¼.3 and r¼2.5. What is P(X¼4)? What is the probability that at least
one plant is found?
163. There are two certiﬁed public accountants (CPAs) in a particular ofﬁce who prepare tax returns
for clients. Suppose that for one type of complex tax form, the number of errors made by the ﬁrst
preparer has a Poisson distribution with mean μ1, the number of errors made by the second
preparer has a Poisson distribution with mean μ2, and that each CPA prepares the same number
of forms of this type. Then if one such form is randomly selected, the function
px;μ1;μ2 ðÞ ¼ :5e/C0μ1μx
1
x!þ:5e/C0μ2μx
2
x!x¼0, 1, 2, ...
gives the pmf of X¼the number of errors in the selected form.2.9 Supplementary Exercises (142–170) 143
(a) Verify that p(x;μ1,μ2) is a legitimate pmf ( /C210 and sums to 1).
(b) What is the expected number of errors on the selected form?
(c) What is the standard deviation of the number of errors on the selected form?
(d) How does the pmf change if the ﬁrst CPA prepares 60% of all such forms and the second
prepares 40%?
164. The mode of a discrete random variable Xwith pmf p(x) is that value x* for which p(x) is largest
(the most probable xvalue).
(a) Let X~ Bin( n,p). By considering the ratio b(x+1 ; n,p)/b(x;n,p), show that b(x;n,p)
increases with xas long as x<np/C0(1/C0p). Conclude that the mode x* is the integer
satisfying ( n+1 )p/C01/C20x*/C20(n+1 )p.
(b) Show that if Xhas a Poisson distribution with parameter μ, the mode is the largest integer
less than μ.I fμis an integer, show that both μ/C01 and μare modes.
165. For a particular insurance policy the number of claims by a policy holder in 5 years is Poisson
distributed. If the ﬁling of one claim is four times as likely as the ﬁling of two claims, ﬁnd the
expected number of claims.
166. If Xis a hypergeometric rv, show directly from the deﬁnition that E(X)¼nM/N(consider only
the case n<M). [Hint: Factor nM/Nout of the sum for E(X), and show that the terms inside the
sum are a match to the pmf h(y;n/C01,M/C01,N/C01), where y¼x/C01.]
167. Suppose a store sells two different coffee makers of a particular brand, a basic model selling for
$30 and a fancy one selling for $50. Let Xbe the number of people among the next 25 purchasing
this brand who choose the fancy one. Then h(X)¼revenue ¼50X+30(25 /C0X)¼20X+750, a
linear function. If the choices are independent and have the same probability, then how is
Xdistributed? Find the mean and standard deviation of h(X). Explain why the choices might not
be independent with the same probability.
168. Let Xbe a discrete rv with possible values 0, 1, 2, ...or some subset of these. The function
ψsðÞ¼ EsXðÞ ¼X1
x¼0sx/C1pxðÞis called the probability generating function (pgf)o fX.
(a) Suppose Xis the number of children born to a family, and p(0)¼.2,p(1)¼.5, and p(2)¼
.3. Determine the pgf of X.
(b) Determine the pgf when Xhas a Poisson distribution with parameter μ.
(c) Show that ψ(1)¼1.
(d) Show that ψ0(0)¼p(1). (You ’ll need to assume that the derivative can be brought inside
the summation, which is justiﬁed.) What results from taking the second derivative with
respect to sand evaluating at s¼0? The third derivative? Explain how successive
differentiation of ψ(s) and evaluation at s¼0 “generates the probabilities in the distribu-
tion.” Use this to recapture the probabilities of (a) from the pgf. [ Note: This shows that the
pgf contains all the information about the distribution—knowing ψ(s) is equivalent to
knowing p(x).]
169. Consider a collection A1,...,Akof mutually exclusive and exhaustive events (a partition) and a
random variable Xwhose distribution depends on which of the Ais occurs. (e.g., a commuter
might select one of three possible routes from home to work, with Xrepresenting commute
time.) Let E(X|Ai) denote the expected value of Xgiven that event Aioccurs. Then, analogous to
the Law of Total Probability, it can be shown that the overall mean of Xis given by the weighted
average E(X)¼∑E(X|Ai)P(Ai)
(a) The expected duration of a voice call to a particular ofﬁce telephone number is 3 min,
whereas the expected duration of a data call to that same number is 1 min. If 75% of all
calls are voice calls, what is the expected duration of the next call?144 2 Discrete Random Variables and Probability Distributions
(b) A bakery sells three different types of chocolate chip cookies. The number of chocolate
chips on a type icookie has a Poisson distribution with mean μi¼i+1(i¼1, 2, 3). If 20%
of all customers select a cookie of the ﬁrst type, 50% choose the second type, and 30% opt
for the third type, what is the expected number of chocolate chips in the next customer ’s
cookie?
170. Consider a sequence of identical and independent trials, each of which will be a success Sor
failure F. Let p¼P(S) and q¼P(F).
(a) Let X¼the number of trials necessary to obtain the ﬁrst S, a geometric rv. Here is an
alternative approach to determining E(X). Apply the weighted average formula from the
previous exercise with k¼2,A1¼{Son 1st trial}, and A2¼A0. Show that E(X)¼1/p.
[Hint: Denote E(X)b yμ. Given that the ﬁrst trial is a failure, one trial has been performed
and, starting from the 2nd trial, we are still looking for the ﬁrst S. This implies that E(X|A0)
¼1+μ.]
(b) Now let Y¼the number of trials necessary to obtain two consecutive S ’s. It is not possible
to determine E(Y) directly from the deﬁnition of expected value, because there is no
formula for the pmf of Y; the complication is the word consecutive . Use the weighted
average formula to determine E(Y). [Hint: Consider the partition with k¼3 and A1¼{F},
A2¼{SS},A3¼{SF}.]2.9 Supplementary Exercises (142–170) 145
Continuous Random Variables
and Probability Distributions 3
As emphasized at the beginning of Chap. 2, the two important types of random variables are discrete
and continuous. In this chapter, we study the second general type of random variable that arises in
many applied problems. Sections 3.1and3.2present the basic deﬁnitions and properties of continu-
ous random variables, their probability distributions, and their various expected values. The normal
distribution, arguably the most important and useful model in all of probability and statistics, is
introduced in Sect. 3.3. Sections 3.4and3.5discuss some other continuous distributions that are often
used in applied work. In Sect. 3.6, we introduce a method for assessing whether given sample data is
consistent with a speciﬁed distribution. Section 3.7presents methods for obtaining the distribution of
ar vYfrom the distribution of Xwhen the two are related by some equation Y¼g(X). The last section
of this chapter is dedicated to the simulation of continuous rvs.
3.1 Probability Density Functions and Cumulative Distribution Functions
A discrete random variable (rv) is one whose possible values either constitute a ﬁnite set or else can be
listed in an inﬁnite sequence (a list in which there is a ﬁrst element, a second element, etc.). A random
variable whose set of possible values is an entire interval of numbers is not discrete.
Recall from the beginning of Chap. 2that a random variable Xiscontinuous if (1) its possible
values comprise either a single interval on the number line (for some A<B, any number xbetween
AandBis a possible value) or a union of disjoint intervals, and (2) P(X¼c)¼0 for any number cthat
is a possible value of X.
Example 3.1 If in the study of the ecology of a lake, we make depth measurements at randomly
chosen locations, then X¼the depth at such a location is a continuous rv. Here Ais the minimum
depth in the region being sampled, and Bis the maximum depth. ■
Example 3.2 If a chemical compound is randomly selected and its pH Xis determined, then Xis a
continuous rv because any pH value between 0 and 14 is possible. If more is known about the
compound selected for analysis, then the set of possible values might be a subinterval of [0, 14], such
as 5.5 /C20x/C206.5, but Xwould still be continuous. ■
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_3147
Example 3.3 LetXrepresent the amount of time a randomly selected customer spends waiting for a
haircut. Your ﬁrst thought might be that Xis a continuous random variable, since a measurement is
required to determine its value. However, there are customers lucky enough to have no wait
whatsoever before climbing into the barber or stylist’s chair. So it must be the case that P(X¼0)
>0. Conditional on no chairs being empty, however, the waiting time will be continuous since
Xcould then assume any value between some minimum possible time Aand a maximum possible
time B. This random variable is neither purely discrete nor purely continuous but instead is a mixture
of the two types. ■
One might argue that although in principle variables such as height, weight, and temperature are
continuous, in practice the limitations of our measuring instruments restrict us to a discrete (though
sometimes very ﬁnely subdivided) world. However, continuous models often approximate real-world
situations very well, and continuous mathematics (the calculus) is frequently easier to work with than
the mathematics of discrete variables and distributions.
3.1.1 Probability Distributions for Continuous Variables
Suppose the variable Xof interest is the depth of a lake at a randomly chosen point on the surface. Let
M¼the maximum depth (in meters), so that any number in the interval [0, M] is a possible value of X.
If we “discretize” Xby measuring depth to the nearest meter, then possible values are nonnegative
integers less than or equal to M. The resulting discrete distribution of depth can be pictured using a
probability histogram. If we draw the histogram so that the area of the rectangle above any possible
integer kis the proportion of the lake whose depth is (to the nearest meter) k, then the total area of all
rectangles is 1. A possible histogram appears in Fig. 3.1a.
If depth is measured much more precisely and the same measurement axis as in Fig. 3.1a is used,
each rectangle in the resulting probability histogram is much narrower, although the total area of all
rectangles is still 1. A possible histogram is pictured in Fig. 3.1b; it has a much smoother appearance
than the histogram in Fig. 3.1a. If we continue in this way to measure depth more and more ﬁnely, the
resulting sequence of histograms approaches a smooth curve, as pictured in Fig. 3.1c. Because for
each histogram the total area of all rectangles equals 1, the total area under the smooth curve is also
1. The probability that the depth at a randomly chosen point is between aandbis just the area under
the smooth curve between aandb. It is exactly a smooth curve of the type pictured in Fig. 3.1c that
speciﬁes a continuous probability distribution.
c b a
0 M 0 M 0 M
Fig. 3.1 (a) Probability histogram of depth measured to the nearest meter; ( b) probability histogram of depth
measured to the nearest centimeter; ( c) a limit of a sequence of discrete histograms148 3 Continuous Random Variables and Probability Distributions
DEFINITION
LetXbe a continuous rv. Then a probability distribution orprobability density function
(pdf) of Xis a function f(x) such that for any two numbers aandbwith a/C20b,
Pa/C20X/C20b ðÞ ¼ðb
afðxÞdx
That is, the probability that Xtakes on a value in the interval [ a,b] is the area above this
interval and under the graph of the density function, as illustrated in Fig. 3.2. The graph of f(x)i s
often referred to as the density curve .
Forf(x) to be a legitimate pdf, it must satisfy the following two conditions:
1.f(x)/C210 for all x
2.Ð
/C011f(x)dx¼[area under the entire graph of f(x)]¼1
Example 3.4 The direction of an imperfection with respect to a reference line on a circular object
such as a tire, brake rotor, or ﬂywheel is often subject to uncertainty. Consider the reference line
connecting the valve stem on a tire to the center point, and let Xbe the angle measured clockwise to
the location of an imperfection. One possible pdf for Xis
fðxÞ¼1
3600/C20x<360
0 otherwise8
<
:
The pdf is graphed in Fig. 3.3.C l e a r l y f(x)/C210. The area under the density curve is just the area of a
rectangle: height ðÞ baseðÞ ¼1
360/C18/C19
ð360Þ¼1. The probability that the angle is between 90/C14and 180/C14is
P90/C20X/C20180 ðÞ ¼ð180
901
360dx¼x
360/C12/C12/C12x¼180
x¼90¼1
4¼:25
The probability that the angle of occurrence is within 90/C14of the reference line is
P0/C20X/C2090 ðÞ þ P270/C20X<360 ðÞ ¼ :25þ:25¼:50abxf(x)
Fig. 3.2 P(a/C20X/C20b)¼the area under the density curve between aandb3.1 Probability Density Functions and Cumulative Distribution Functions 149
Because the pdf in Fig. 3.3is completely “level” (i.e., has a uniform height) on the interval
[0, 360], Xis said to have a uniform distribution.
DEFINITION
A continuous rv Xis said to have a uniform distribution on the interval [ A,B] if the pdf of Xis
fx ;A,BðÞ ¼1
B/C0AA/C20x/C20B
0 otherwise8
<
:
The statement that Xhas a uniform distribution on [ A,B] will be denoted X/C24Unif[ A,B].
The graph of any uniform pdf looks like the graph in Fig. 3.3except that the interval of positive
density is [ A,B] rather than [0, 360).
In the discrete case, a probability mass function (pmf) tells us how little “blobs” of probability
mass of various magnitudes are distributed along the measurement axis. In the continuous case,
probability density is “smeared” in a continuous fashion along the interval of possible values. When
density is smeared evenly over the interval, a uniform pdf, as in Fig. 3.3, results.
When Xis a discrete random variable, each possible value is assigned positive probability. This is
not true of a continuous random variable, because the area under a density curve that lies above any
single value is zero:
PX¼cðÞ ¼ Pc/C20X/C20c ðÞ ¼ðc
cfðxÞdx¼0
The fact that P(X¼c)¼0 when Xis continuous has an important practical consequence: The
probability that Xlies in some interval between aandbdoes not depend on whether the lower limit
aor the upper limit bis included in the probability calculation:
Pa/C20X/C20b ðÞ ¼ Pa<X<b ðÞ ¼ Pa<X/C20b ðÞ ¼ Pa/C20X<b ðÞ ð 3:1Þ
In contrast, if Xwere discrete and both aandbwere possible values of X(e.g., X/C24Bin(20, .3) and
a¼5,b¼10), then all four of the probabilities in Eq. ( 3.1) would be different. This also means that
whether we include the endpoints of the range of values for a continuous rv Xis somewhat arbitrary;
for example, the pdf in Example 3.4 could be deﬁned to be positive on (0, 360) or [0, 360] rather than
[0, 360), and the same applies for a uniform distribution on [ A,B] in general.
The zero probability condition has a physical analog. Consider a solid circular rod (with cross-
sectional area of 1 in2for simplicity). Place the rod alongside a measurement axis and suppose that the
density of the rod at any point xis given by the value f(x) of a density function. Then if the rod is slicedx1
360
360 0x
360 270 180 90f(x) f(x)
Shaded area = P(90 ≤ X ≤ 180) 
Fig. 3.3 The pdf and probability for Example 3.4 ■150 3 Continuous Random Variables and Probability Distributions
at points aandband this segment is removed, the amount of mass removed isÐ
abf(x)dx; however, if
the rod is sliced just at the point c, no mass is removed. Mass is assigned to interval segments of the
rod but not to individual points.
So, if P(X¼c)¼0 when Xis a continuous rv, then what does f(c) represent? After all, if Xwere
discrete, its pmf evaluated at x¼c,p(c), would indicate the probability that Xequals c. To help
understand what f(c) means, consider a small window near x¼c—say, [ c,c+Δx]. Using a rectangle
to approximate the area under f(x) between candc+Δx(the usual “Riemann approximation” idea
from calculus), one obtainsÐ
cc+Δxf(x)dx/C25Δx/C1f(c), from which
fðcÞ/C25ðcþΔx
cfðxÞdx
Δx¼Pc/C20X/C20cþΔx ðÞ
Δx
This indicates that f(c) is not a probability, but rather roughly the probability of an interval divided
by the length of the chosen interval . If we associate mass with probability and remember that interval
length is the one-dimensional analog of volume, then frepresents their quotient, mass per volume,
more commonly known as density (hence, the name pdf). The height of the function f(x) at a particular
point reﬂects how “dense” the values of Xare near that point—taller sections of f(x) contain more
probability within a ﬁxed interval length than do shorter sections.
Example 3.5 “Time headway” in trafﬁc ﬂow is the elapsed time between the time that one car
ﬁnishes passing a ﬁxed point and the instant that the next car begins to pass that point. Let X¼the
time headway for two randomly chosen consecutive cars on a freeway during a period of heavy ﬂow.
The following pdf of Xis essentially the one suggested in “The Statistical Properties of Freeway
Trafﬁc” ( Transp. Res ., 11: 221–228):
fðxÞ¼:15e/C0:15x/C0:5ðÞx/C21:5
0 otherwise(
The graph of f(x) is given in Fig. 3.4; there is no density associated with headway times less than .5,
and headway density decreases rapidly (exponentially fast) as xincreases from .5. The fact that the
graph of f(x) is taller near x¼.5 and shorter near, say, x¼10 indicates that time headway values are
more dense near the left boundary, i.e., there is a higher proportion of time headways in the interval
[.5, 1.5] than in [10, 11], even though these two intervals have the same length.
Clearly, f(x)/C210; to show thatð1
/C01f(x)dx¼1 we use the calculus resultð1
ae/C0kxdx¼(1/k)e–ka.
Then
ð1
/C01fðxÞdx¼ð:5
/C010dxþð1
:5:15e/C0:15x/C05ðÞdx
¼:15e:075ð1
:5e/C0:15xdx¼:15e:075/C11
:15e/C0:15ð:5Þ¼1
The probability that headway time is at most 5 seconds is3.1 Probability Density Functions and Cumulative Distribution Functions 151
PX/C205ðÞ ¼ð5
/C01fðxÞdx¼ð5
:5:15e/C0:15x/C0:5ðÞdx¼:15e:075ð5
:5e/C0:15xdx
¼:15e:075/C1/C01
:15e/C0:15x/C12/C12/C12/C12x¼5
x¼:5
¼e:075/C0e/C0:75þe/C0:075/C0/C1
¼1:078/C0:472þ:928 ðÞ ¼ :491
Since Xis a continuous rv, .491 also equals P(X<5), the probability that headway time is (strictly)
less than 5 s. The difference between these two events is { X¼5}, i.e., that headway time is exactly 5 s,
which has probability zero: P(X¼5)¼Ð5
5f(x)dx¼0.
This last statement may feel uncomfortable to you: Is there really zero chance that the headway
time between two cars is exactly 5 s? If time is treated as continuous, then “exactly 5 s” means X¼
5.000 ..., with an endless repetition of 0s. That is to say, Xisn’t rounded to the nearest second (or even
tenth of a second); we are asking for the probability that Xequals one speciﬁc number, 5.000 ...,o u t
of the (uncountably) inﬁnite collection of possible values of X. ■
Unlike discrete distributions such as the binomial, hypergeometric, and negative binomial, the
distribution of any given continuous rv cannot usually be derived using simple probabilistic
arguments. Instead, one must make a judicious choice of pdf based on prior knowledge and available
data. Fortunately, some general pdf families have been found to ﬁt well in a wide variety of
experimental situations; several of these are discussed later in the chapter.
Just as in the discrete case, it is often helpful to think of the population of interest as consisting of
Xvalues rather than individuals or objects. The pdf is then a model for the distribution of values in this
numerical population, and from this model various population characteristics (such as the mean) can
be calculated.
Several of the most important concepts introduced in the study of discrete distributions also play
an important role for continuous distributions. Deﬁnitions analogous to those in Chap. 2involve
replacing summation by integration.
3.1.2 The Cumulative Distribution Function
The cumulative distribution function (cdf) F(x) for a discrete rv Xgives, for any speciﬁed number x,
the probability P(X/C20x). It is obtained by summing the pmf p(y) over all possible values ysatisfying
y/C20x. The cdf of a continuous rv gives the same probabilities P(X/C20x) and is obtained by integrating
the pdf f(y) between the limits /C01 andx.x.15
51 0 1 5f(x)
P(X ≤ 5)
.5
Fig. 3.4 The density curve for headway time in Example 3.5152 3 Continuous Random Variables and Probability Distributions
DEFINITION
The cumulative distribution function F(x) for a continuous rv Xis deﬁned for every number
xby
FðxÞ¼PX/C20xðÞ ¼ðx
/C01fðyÞdy
For each x,F(x) is the area under the density curve to the left of x. This is illustrated in
Fig.3.5, where F(x) increases smoothly as xincreases.
Example 3.6 LetX, the thickness of a membrane, have a uniform distribution on [ A,B]. The density
function is shown in Fig. 3.6.
Forx<A,F(x)¼0, since there is no area under the graph of the density function to the left of such
anx. For x/C21B,F(x)¼1, since all the area is accumulated to the left of such an x. Finally, for A/C20x<B,
FðxÞ¼ðx
/C01fðyÞdy¼ðx
A1
B/C0Ady¼1
B/C0A/C1y/C12/C12/C12/C12y¼x
y¼A¼x/C0A
B/C0A
The entire cdf is
FðxÞ¼0 x<A
x/C0A
B/C0AA/C20x<B
1 x/C21B8
>><
>>:f(x) F(x)
xxF(8)Shaded area =  F(8).5 1.0
.8
.6
.4
.2
0.4
.3
.2
.1
0
56789 1 0 56789 1 0
Fig. 3.5 A pdf and associated cdf
1
AB ABx xf(x)
B − A1
B − AShaded area  = F(x)Fig. 3.6 The pdf for a
uniform distribution3.1 Probability Density Functions and Cumulative Distribution Functions 153
The graph of this cdf appears in Fig. 3.7.
3.1.3 Using F(x) to Compute Probabilities
The importance of the cdf here, just as for discrete rvs, is that probabilities of various intervals can be
computed from a formula or table for F(x).
PROPOSITION
LetXbe a continuous rv with pdf f(x) and cdf F(x). Then for any number a,
PX>aðÞ ¼ 1/C0FðaÞ
and for any two numbers aandbwith a<b,
Pa/C20X/C20b ðÞ ¼ FðbÞ/C0FðaÞ
Figure 3.8illustrates the second part of this proposition; the desired probability is the shaded area
under the density curve between aandb, and it equals the difference between the two shaded
cumulative areas. This is different from what is appropriate for a discrete integer-valued rv (e.g.,
binomial or Poisson): P(a/C20X/C20b)¼F(b)/C0F(a/C01) when aandbare integers.
Example 3.7 Suppose the pdf of the magnitude Xof a dynamic load on a bridge (in newtons) is
given by
fðxÞ¼1
8þ3
8x0/C20x/C202
0 otherwise8
<
:
For any number xbetween 0 and 2,AB x1F(x)
Fig. 3.7 The cdf for a uniform distribution ■
ab b af(x)
= −
Fig. 3.8 Computing P(a/C20X/C20b) from cumulative probabilities154 3 Continuous Random Variables and Probability Distributions
FðxÞ¼ðx
/C01fðyÞdy¼ðx
01
8þ3
8y/C18/C19
dy¼x
8þ3x2
16
Thus
FðxÞ¼0 x<0
x
8þ3x2
160/C20x/C202
12 <x8
>><
>>:
The graphs of f(x) and F(x) are shown in Fig. 3.9. The probability that the load is between 1 and
1.5 N is
P1/C20X/C201:5 ðÞ ¼ Fð1:5Þ/C0Fð1Þ¼1
8ð1:5Þþ3
16ð1:5Þ2/C20/C21
/C01
8ð1Þþ3
16ð1Þ2/C20/C21
¼19
64¼:297
The probability that the load exceeds 1 N is
PX>1ðÞ ¼ 1/C0PX/C201ðÞ ¼ 1/C0Fð1Þ¼1/C01
8ð1Þþ3
16ð1Þ2/C20/C21
¼11
16¼:688
The beauty of the cdf in the continuous case is that once it is available, any probability involving
Xcan easily be calculated without any further integration.
3.1.4 Obtaining f(x) from F(x)
ForXdiscrete, the pmf is obtained from the cdf by taking the difference between two F(x) values. The
continuous analog of a difference is a derivative. The following result is a consequence of the
Fundamental Theorem of Calculus.
PROPOSITION
IfXis a continuous rv with pdf f(x) and cdf F(x), then at every xat which the derivative F0(x)
exists, F0(x)¼f(x).1
87
8
2 0 21
x xf(x) F(x)
Fig. 3.9 The pdf and cdf for Example 3.7 ■3.1 Probability Density Functions and Cumulative Distribution Functions 155
Example 3.8 (Example 3.6 continued) When X/C24Unif[ A,B],F(x) is differentiable except at
x¼Aandx¼B, where the graph of F(x) has sharp corners. Since F(x)¼0 for x<AandF(x)
¼1 for x>B,F0(x)¼0¼f(x) for such x. For A<x<B,
FðxÞ¼d
dxx/C0A
B/C0A/C18/C19
¼1
B/C0A¼fðxÞ■
3.1.5 Percentiles of a Continuous Distribution
When we say that an individual’s test score was at the 85th percentile of the population, we mean that
85% of all population scores were below that score and 15% were above. Similarly, the 40th
percentile is the score that exceeds 40% of all scores and is exceeded by 60% of all scores.
DEFINITION
Letpbe a number between 0 and 1. The (100 p)th percentile of the distribution of a continuous
rvX, denoted by ηp, is deﬁned implicitly by the equation
p¼Fηp/C0/C1
¼ðηp
/C01fðyÞdy ð3:2Þ
Assuming we can ﬁnd the inverse of F(x), this can also be written as
ηp¼F/C01ðpÞ
In particular, the median of a continuous distribution is the 50th percentile, η.5orF/C01(.5).
That is, half the area under the density curve is to the left of the median and half is to the right of
the median. We will occasionally denote the median of a distribution simply as η(i.e., without
the .5 subscript).
According to Expression ( 3.2),ηpis that value on the measurement axis such that 100 p% of the
area under the graph of f(x) lies to the left of ηpand 100(1 /C0p)% lies to the right. Thus η.75, the 75th
percentile, is such that the area under the graph of f(x) to the left of η.75is .75. Figure 3.10 illustrates
the deﬁnition.f(x) F(x)
xxp = F(hp)Shaded area =  p.5 1.0
.8
.6
.4
.2
0.4
.3
.2
.1
0
567 8
hp9 1 0 567 8 9 1 0
hp
Fig. 3.10 The (100 p)th percentile of a continuous distribution156 3 Continuous Random Variables and Probability Distributions
Example 3.9 The distribution of the amount of gravel (in tons) sold by a construction supply
company in a given week is a continuous rv Xwith pdf
fðxÞ¼3
21/C0x2/C0/C1
0/C20x/C201
0 otherwise8
<
:
The cdf of sales for any xbetween 0 and 1 is
FðxÞ¼ðx
03
21/C0y2/C0/C1
dy¼3
2y/C0y3
3/C18/C19/C12/C12/C12/C12y¼x
y¼0¼3
2x/C0x3
3/C18/C19
The graphs of both f(x) and F(x) appear in Fig. 3.11. The (100 p)th percentile of this distribution
satisﬁes the equation
p¼Fηp/C0/C1
¼3
2ηp/C0η3
p
3 !
that is,
η3
p/C03ηpþ2p¼0
For the median, p¼.5 and the equation to be solved is η3/C03η+1¼0; the solution is η¼.347. If
the distribution remains the same from week to week, then in the long run 50% of all weeks will result
in sales of less than .347 tons and 50% in more than .347 tons.
A continuous distribution whose pdf is symmetric —which means that the graph of the pdf to the
left of some point is a mirror image of the graph to the right of that point—has median ηequal to the
point of symmetry, since half the area under the curve lies to either side of this point. Figure 3.12
gives several examples. The amount of error in a measurement of a physical quantity is often assumed
to have a symmetric distribution.f(x) F(x)
21
.5
01 01 .347 xx
Fig. 3.11 The pdf and cdf for Example 3.9 ■
x x x
ABf(x) f(x) f(x)
hh h
Fig. 3.12 Medians of symmetric distributions3.1 Probability Density Functions and Cumulative Distribution Functions 157
3.1.6 Exercises: Section 3.1(1–18)
1. The current in a certain circuit as measured by an ammeter is a continuous random variable
Xwith the following density function:
fðxÞ¼:075xþ:23 /C20x/C205
0 otherwise(
(a) Graph the pdf and verify that the total area under the density curve is indeed 1.
(b) Calculate P(X/C204). How does this probability compare to P(X<4)?
(c) Calculate P(3.5/C20X/C204.5) and P(X>4.5).
2. Suppose the reaction temperature X(in/C14C) in a chemical process has a uniform distribution with
A¼/C05 and B¼5.
(a) Compute P(X<0).
(b) Compute P(/C02.5<X<2.5).
(c) Compute P(/C02/C20X/C203).
(d) For ksatisfying /C05<k<k+4<5, compute P(k<X<k+ 4). Interpret this in words.
3. Suppose the error involved in making a measurement is a continuous rv Xwith pdf
fðxÞ¼:09375 4 /C0x2ðÞ /C0 2/C20x/C202
0 otherwise(
(a) Sketch the graph of f(x).
(b) Compute P(X>0).
(c) Compute P(/C01<X<1).
(d) Compute P(X</C0.5 or X>.5).
4. Let Xdenote the vibratory stress (psi) on a wind turbine blade at a particular wind speed in a wind
tunnel. The article “Blade Fatigue Life Assessment with Application to VAWTS” ( J. Solar
Energy Engr. , 1982: 107–111) proposes the Rayleigh distribution, with pdf
fx ;θðÞ ¼x
θ2/C1e/C0x2=2θ2ðÞx>0
0 otherwise8
<
:
as a model for X, where θis a positive constant.
(a) Verify that f(x;θ) is a legitimate pdf.
(b) Suppose θ¼100 (a value suggested by a graph in the article). What is the probability that
Xis at most 200? Less than 200? At least 200?
(c) What is the probability that Xis between 100 and 200 (again assuming θ¼100)?
(d) Give an expression for the cdf of X.
5. A college professor never ﬁnishes his lecture before the end of the hour and always ﬁnishes his
lectures within 2 min after the hour. Let X¼the time that elapses between the end of the hour and
the end of the lecture and suppose the pdf of Xis158 3 Continuous Random Variables and Probability Distributions
fðxÞ¼kx20/C20x/C202
0 otherwise(
(a) Find the value of kand draw the corresponding density curve. [ Hint: Total area under the
graph of f(x) is 1.]
(b) What is the probability that the lecture ends within 1 min of the end of the hour?
(c) What is the probability that the lecture continues beyond the hour for between 60 and 90 s?
(d) What is the probability that the lecture continues for at least 90 s beyond the end of the hour?
6. The actual tracking weight of a stereo cartridge that is set to track at 3 g on a particular changer
can be regarded as a continuous rv Xwith pdf
fðxÞ¼k1/C0x/C03ðÞ2hi
08
<
:2/C20x/C204
otherwise
(a) Sketch the graph of f(x).
(b) Find the value of k.
(c) What is the probability that the actual tracking weight is greater than the prescribed weight?
(d) What is the probability that the actual weight is within .25 g of the prescribed weight?
(e) What is the probability that the actual weight differs from the prescribed weight by more
than .5 g?
7. The article “Second Moment Reliability Evaluation vs. Monte Carlo Simulations for Weld
Fatigue Strength” ( Quality and Reliability Engr. Intl. , 2012: 887-896) considered the use of a
uniform distribution with A¼.20 and B¼4.25 for the diameter Xof a certain type of weld (mm).
(a) Determine the pdf of Xand graph it.
(b) What is the probability that diameter exceeds 3 mm?
(c) What is the probability that diameter is within 1 mm of the mean diameter?
(d) For any value asatisfying .20 <a<a+1<4.25, what is P(a<X<a+ 1)?
8. Commuting to work requires getting on a bus near home and then transferring to a second bus. If
the waiting time (in minutes) at each stop has a Unif[0, 5] distribution, then it can be shown that
the total waiting time Yhas the pdf
fðyÞ¼1
25y 0/C20y<5
2
5/C01
25y5/C20y/C2010
0 y<0or y >108
>>>><
>>>>:
(a) Sketch the pdf of Y.
(b) Verify thatÐ1
/C01f(y)dy¼1.
(c) What is the probability that total waiting time is at most 3 min?
(d) What is the probability that total waiting time is at most 8 min?
(e) What is the probability that total waiting time is between 3 and 8 min?
(f) What is the probability that total waiting time is either less than 2 min or more than 6 min?3.1 Probability Density Functions and Cumulative Distribution Functions 159
9. Consider again the pdf of X¼time headway given in Example 3.5. What is the probability that
time headway is
(a) At most 6 s?
(b) More than 6 s? At least 6 s?
(c) Between 5 and 6 s?
10. A family of pdfs that has been used to approximate the distribution of income, city population
size, and size of ﬁrms is the Pareto family. The family has two parameters, kandθ, both >0, and
the pdf is
fx ;k,θðÞ ¼k/C1θk
xkþ1x/C21θ
0 x<θ8
<
:
(a) Sketch the graph of f(x;k,θ).
(b) Verify that the total area under the graph equals 1.
(c) If the rv Xhas pdf f(x;k,θ), obtain an expression for the cdf of X.
(d) For θ<a<b, obtain an expression for the probability P(a/C20X/C20b).
(e) Find an expression for the (100 p)th percentile ηp.
11. Let Xdenote the amount of time a book on 2-h reserve is actually checked out, and suppose the
cdf is
FðxÞ¼0 x<0
x2
40/C20x<2
12 /C20x8
>><
>>:
Use this to compute the following:
(a) P(X/C201)
(b) P(.5/C20X/C201)
(c) P(X>1.5)
(d) The median checkout duration η[Hint: Solve F(η)¼.5.]
(e) F0(x) to obtain the density function f(x)
12. The cdf for X¼measurement error of Exercise 3 is
FðxÞ¼0 x</C02
1
2þ3
324x/C0x3
3/C18/C19
/C02/C20x<2
12 /C20x8
>>><
>>>:
(a) Compute P(X<0).
(b) Compute P(/C01<X<1).
(c) Compute P(X>.5).
(d) Verify that f(x) is as given in Exercise 3 by obtaining F0(x).
(e) Verify that η¼0.
13. Example 3.5 introduced the concept of time headway in trafﬁc ﬂow and proposed a particular
distribution for X¼the headway between two randomly selected consecutive car. Suppose that in
a different trafﬁc environment, the distribution of time headway has the form160 3 Continuous Random Variables and Probability Distributions
fðxÞ¼k
x4x>1
0 x/C2018
<
:
(a) Determine the value of kfor which f(x) is a legitimate pdf.
(b) Obtain the cumulative distribution function.
(c) Use the cdf from (b) to determine the probability that headway exceeds 2 s and also the
probability that headway is between 2 and 3 s.
14. Let Xdenote the amount of space occupied by an article placed in a 1-ft3packing container. The
pdf of Xis
fðxÞ¼90x81/C0xðÞ 0<x<1
0 otherwise(
(a) Graph the pdf. Then obtain the cdf of Xand graph it.
(b) What is P(X/C20.5) [i.e., F(.5)]?
(c) Using part (a), what is P(.25<X/C20.5)? What is P(.25/C20X/C20.5)?
(d) What is the 75th percentile of the distribution?
15. Answer parts (a)–(d) of Exercise 14 for the random variable X, lecture time past the hour, given in
Exercise 5.
16. The article “A Model of Pedestrians’ Waiting Times for Street Crossings at Signalized
Intersections” ( Transportation Research , 2013: 17–28) suggested that under some circumstances
the distribution of waiting time Xcould be modeled with the following pdf:
fx ;θ,τðÞ ¼θ
τ1/C0x=τ ðÞθ/C010/C20x<τ
0 otherwise8
<
:
where θ,τ>0.
(a) Graph f(x;θ, 80) for the three cases θ¼4, 1, and .5 (these graphs appear in the cited article)
and comment on their shapes.
(b) Obtain the cumulative distribution function of X.
(c) Obtain an expression for the median of the waiting time distribution.
(d) For the case θ¼4 and τ¼80, calculate P(50/C20X/C2070) without doing any additional
integration.
17. Let Xbe a continuous rv with cdf
FðxÞ¼0 x/C200
x
41þln4
x/C18/C19/C20/C21
0<x/C204
1 x>48
>>><
>>>:
[This type of cdf is suggested in the article “Variability in Measured Bedload-Transport Rates”
(Water Resources Bull. , 1985: 39–48) as a model for a hydrologic variable.] What is
(a) P(X/C201)?
(b) P(1/C20X/C203)?
(c) The pdf of X?3.1 Probability Density Functions and Cumulative Distribution Functions 161
18. Let Xbe the temperature in/C14C at which a chemical reaction takes place, and let Ybe the
temperature in/C14F (so Y¼1.8X+ 32).
(a) If the median of the Xdistribution is η, show that 1.8 η+ 32 is the median of the
Ydistribution.
(b) How is the 90th percentile of the Ydistribution related to the 90th percentile of the
Xdistribution? Verify your conjecture.
(c) More generally, if Y¼aX+b, how is any particular percentile of the Ydistribution related to
the corresponding percentile of the Xdistribution?
3.2 Expected Values and Moment Generating Functions
In Sect. 3.1we saw that the transition from a discrete cdf to a continuous cdf entails replacing
summation by integration. The same thing is true in moving from expected values of discrete
variables to those of continuous variables.
3.2.1 Expected Values
For a discrete random variable X, the mean μXorE(X) was deﬁned as a weighted average and
obtained by summing x/C1p(x) over possible Xvalues. Here we replace summation by integration and
the pmf by the pdf to get a continuous weighted average.
DEFINITION
The expected value ormean value of a continuous rv Xwith pdf f(x)i s
μ¼μX¼EðXÞ¼ð1
/C01x/C1fðxÞdx
Example 3.10 (Example 3.9 continued) The pdf of weekly gravel sales Xwas
fðxÞ¼3
21/C0x2/C0/C1
0/C20x/C201
0 otherwise8
<
:
so
EðXÞ¼ð1
/C01x/C1fðxÞdx¼ð1
0x/C13
21/C0x2/C0/C1
dx¼3
2ð1
0x/C0x3/C0/C1
dx¼3
2x2
2/C0x4
4/C18/C19/C12/C12/C12/C12x¼1
x¼0¼3
8
If gravel sales are determined week after week according to the given pdf, then the long-run
average value of sales per week will be .375 ton. ■
Similar to the interpretation in the discrete case, the mean value μcan be regarded as the balance
point (or fulcrum or center of mass) of a continuous distribution. In Example 3.10, if a piece of
cardboard were cut out in the shape of the region under the density curve f(x), then it would balance if
supported at μ¼3/8 along the bottom edge. When a pdf f(x) is symmetric, then it will balance at its162 3 Continuous Random Variables and Probability Distributions
point of symmetry, which must be the mean μ. Recall from Sect. 3.1that the median is also the point
of symmetry; in general, if a distribution is symmetric and the mean exists, then it is equal to the
median.
Often we wish to compute the expected value of some function h(X) of the rv X. If we think of h(X)
as a new rv Y, methods from Sect. 3.7can be used to derive the pdf of Y, and E(Y) can be computed
from the deﬁnition. Fortunately, as in the discrete case, there is an easier way to compute E[h(X)].
PROPOSITION
IfXis a continuous rv with pdf f(x) and h(X) is any function of X, then
μhðXÞ¼EhðXÞ½/C138 ¼ð1
/C01hðxÞ/C1fðxÞdx
This is sometimes called the Law of the Unconscious Statistician .
Importantly, except in the case where h(x) is a linear function (see later in this section), E[h(X)] is
notequal to h(μX), the function hevaluated at the mean of X.
Example 3.11 The variation in a certain electrical current source X(in milliamps) can be modeled by
the pdf
fðxÞ¼1:25/C0:25x2/C20x/C204
0 otherwise(
The average current from this source is
EðXÞ¼ð4
2x1:25/C0:25x ðÞ dx¼17
6¼2:833mA
If this current passes through a 220- Ωresistor, the resulting power (in microwatts) is given by the
expression h(X)¼(current)2(resistance) ¼220X2. The expected power is given by
EhðXÞðÞ ¼ E220X2/C0/C1
¼ð4
2220x21:25/C0:25x ðÞ dx¼5500
3¼1833 :3μW
Notice that the expected power is notequal to 220(2.833)2, a common error that results from
substituting the mean current μXinto the power formula. ■
Example 3.12 Two species are competing in a region for control of a limited amount of a resource.
LetX¼the proportion of the resource controlled by species 1 and suppose Xhas pdf
fðxÞ¼10 /C20x/C201
0 otherwise(
which is a uniform distribution on [0, 1]. (In her book Ecological Diversity , E. C. Pielou calls this the
“broken-stick” model for resource allocation, since it is analogous to breaking a stick at a randomly
chosen point.) Then the species that controls the majority of this resource controls the amount3.2 Expected Values and Moment Generating Functions 163
hXðÞ ¼ max X,1/C0X ðÞ ¼1/C0Xif 0 /C20X<1
2
X if1
2/C20X/C2018
><
>:
The expected amount controlled by the species having majority control is then
EhðXÞ½/C138 ¼ð1
/C01max x,1/C0x ðÞ /C1 fðxÞdx¼ð1
0max x,1/C0x ðÞ /C1 1dx
¼ð1=2
01/C0xðÞ /C1 1dxþð1
1=2x/C11dx¼3
4 ■
In the discrete case, the variance of Xwas deﬁned as the expected squared deviation from μand
was calculated by summation. Here again integration replaces summation.
DEFINITION
The variance of a continuous random variable Xwith pdf f(x) and mean value μis
σ2
X¼VarðXÞ¼ð1
/C01x/C0μðÞ2/C1fðxÞdx¼EX/C0μðÞ2hi
The standard deviation ofXisσX¼SDðXÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarðXÞp
:
As in the discrete case, σ2
Xis the expected or average squared deviation about the mean μ, and σX
can be interpreted roughly as the size of a representative deviation from the mean value μ. Note that
σXhas the same units as Xitself.
Example 3.13 LetX/C24Unif[ A,B]. Since a uniform distribution is symmetric, the mean of Xis at the
density curve’s point of symmetry, which is clearly the midpoint ( A+B)/2. This can be veriﬁed by
integration:
μ¼ðB
Ax/C11
B/C0Adx¼1
B/C0Ax2
2jB
A¼1
B/C0AB2/C0A2
2¼AþB
2
The variance of Xis then given by
σ2¼ðB
Ax/C0μðÞ2/C11
B/C0Adx¼1
B/C0AðB
Ax/C0AþB
2/C18/C192
dx
¼1
B/C0AðB/C0AðÞ =2
/C0B/C0AðÞ =2u2du substitute u¼x/C0AþB
2
¼2
B/C0AðB/C0AðÞ =2
0u2du symmetry
¼2
B/C0Au3
3jB/C0AðÞ =2
0¼2
B/C0AB/C0AðÞ3
23/C13¼B/C0AðÞ2
12164 3 Continuous Random Variables and Probability Distributions
The standard deviation of Xis the square root of the variance: σ¼B/C0AðÞ =ﬃﬃﬃﬃﬃ
12p
. Notice that the
standard deviation of a Unif[ A, B] distribution is proportional to the length of the interval, B/C0A,
which matches our intuitive notion that a larger standard deviation corresponds to greater “spread” in
a distribution. ■
Section 2.3presented several properties of expected value, variance, and standard deviation for
discrete random variables. Those same properties hold for the continuous case; proofs of these results
are obtained by replacing summation with integration in the proofs presented in Chap. 2.
PROPOSITION
LetXbe a continuous rv with pdf f(x), mean μ, and standard deviation σ. Then the following
properties hold.
1. (variance shortcut) Var( X)¼E(X2)/C0μ2¼ð1
/C01x2/C1f(x)dx/C0ð1
/C01x/C1fðxÞdx/C18/C192
2. (Chebyshev’s inequality) For any constant k/C211,
PX/C0μjj /C21kσ ðÞ /C201
k2
3. (linearity of expectation) For any functions h1(X) and h2(X) and any constants a1,a2, and b,
Ea 1h1ðXÞþa2h2ðXÞþb ½/C138 ¼ a1Eh 1ðXÞ½/C138 þ a2Eh 2ðXÞ½/C138 þ b
4. (rescaling) For any constants aandb,
Ea Xþb ðÞ ¼ aμþb VaraXþb ðÞ ¼ a2σ2σaXþb¼ajjσ
Example 3.14 (Example 3.10 continued) For X¼weekly gravel sales, we computed E(X)¼3/8.
Since
EX2/C0/C1
¼ð1
/C01x2/C1fðxÞdx¼ð1
0x2/C13
21/C0x2/C0/C1
dx¼3
2ð1
0x2/C0x4/C0/C1
dx¼1
5,
VarðXÞ¼1
5/C03
8/C18/C192
¼19
320¼:059 and σX¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:059p
¼:244
Suppose the amount of gravel actually received by customers in a week is h(X)¼X/C0.02X2; the
second term accounts for the small amount that is lost in transport. Then the average weekly amount
received by customers is
EX/C0:02X2/C0/C1
¼EðXÞ/C0:02EX2/C0/C1
¼3
8/C0:02/C11
5¼:371 tons ■
Example 3.15 When a dart is thrown at a circular target, consider the location of the landing point
relative to the bull’s eye. Let Xbe the angle in degrees measured from the horizontal, and assume that
X/C24Unif[0, 360). By Example 3.13, E(X)¼180 and SD ðXÞ¼360=ﬃﬃﬃﬃﬃ
12p
. Deﬁne Yto be the angle3.2 Expected Values and Moment Generating Functions 165
measured in radians between /C0πandπ,s oY¼(2π/360) X/C0π. Then, applying the rescaling properties
with a¼2π/360 and b¼/C0π,
EðYÞ¼2π
360/C1EðXÞ/C0π¼2π
360180/C0π¼0
and
σY¼2π
360/C12/C12/C12/C12/C12/C12/C12/C12/C1σX¼2π
360360ﬃﬃﬃﬃﬃ
12p¼2πﬃﬃﬃﬃﬃ
12p■
3.2.2 Moment Generating Functions
Moments and moment generating functions for discrete random variables were introduced in
Sect. 2.7. These concepts carry over to the continuous case.
DEFINITION
The moment generating function (mgf) of a continuous random variable Xis
MXðtÞ¼EetX/C0/C1
¼ð1
/C01etxfðxÞdx:
As in the discrete case, the moment generating function exists iff MX(t) is deﬁned for an
interval that includes zero as well as positive and negative values of t.
Just as before, when t¼0 the value of the mgf is always 1:
MXð0Þ¼Ee0X/C0/C1
¼ð1
/C01e0xfðxÞdx¼ð1
/C01fðxÞdx¼1:
Example 3.16 At a store, the checkout time Xin minutes has the pdf f(x)¼2e/C02x,x/C210;f(x)¼
0 otherwise. Then
MXðtÞ¼ð1
/C01etxfðxÞdx¼ð1
0etx2e/C02x/C0/C1
dx¼ð1
02e/C02/C0tðÞ xdx
¼/C02
2/C0te/C02/C0tðÞ xj1
0¼2
2/C0t/C02
2/C0tlim
x!1e/C02/C0tðÞ x
The limit above exists (in fact, it equals zero) provided the coefﬁcient on xis negative, i.e.,
/C0(2/C0t)<0. This is equivalent to t<2. The mgf exists because it is deﬁned for an interval of values
including 0 in its interior, speciﬁcally ( /C01, 2). For tin that interval, the mgf of XisMX(t)¼2/(2/C0t).
Notice that MX(0)¼2/(2/C00)¼1. Of course, from the calculation preceding this example we
know that MX(0)¼1 must always be the case, but it is useful as a check to set t¼0 and see if the
result is 1. ■
Recall that in Sect. 2.7we had a uniqueness property for the mgfs of discrete distributions. This
proposition is equally valid in the continuous case: two distributions have the same pdf if and only if
they have the same moment generating function, assuming that the mgf exists. For example, if a166 3 Continuous Random Variables and Probability Distributions
random variable Xis known to have mgf MX(t)¼2/(2/C0t) for t<2, then from Example 3.16 it must
necessarily be the case that the pdf of Xisf(x)¼2e/C02xforx/C210 and f(x)¼0 otherwise.
In the discrete case we also had a theorem on how to get moments from the mgf, and this theorem
applies also in the continuous case: the rth moment of a continuous rv with mgf MX(t) is given by
EXrðÞ ¼ MðrÞ
Xð0Þ,
therth derivative of the mgf with respect to tevaluated at t¼0, if the mgf exists.
Example 3.17 (Example 3.16 continued) The mgf of the rv X¼checkout time at the store was found
to be MX(t)¼2/(2/C0t)¼2(2/C0t)/C01fort<2. To ﬁnd the mean and standard deviation, ﬁrst compute
the derivatives:
M0
XðtÞ¼/C0 22/C0tðÞ/C02/C01ðÞ ¼2
2/C0tðÞ2
M00
XðtÞ¼d
dt22/C0tðÞ/C02hi
¼/C042/C0tðÞ/C03/C01ðÞ ¼4
2/C0tðÞ3
Setting tto 0 in the ﬁrst derivative gives the expected checkout time as
EðXÞ¼Mð1Þ
Xð0Þ¼M0
Xð0Þ¼:5 min :
Setting tto 0 in the second derivative gives the second moment
EX2/C0/C1
¼Mð2Þ
Xð0Þ¼M00
Xð0Þ¼:5,
from which the variance of the checkout time is Var( X)¼σ2¼E(X2)/C0[E(X)]2¼.5/C0.52¼.25 and the
standard deviation is σ¼ﬃﬃﬃﬃﬃﬃﬃ
:25p
¼:5 min : ■
We will sometimes need to transform Xusing a linear function Y¼aX+b. As discussed in the
discrete case, if Xhas the mgf MX(t) and Y¼aX+b, then MY(t)¼ebtMX(at).
Example 3.18 LetX/C24Unif[ A,B]. As veriﬁed in Exercise 32, the moment generating function of
Xis
MXðtÞ¼eBt/C0eAt
B/C0AðÞ tt6¼0
1 t¼08
<
:
In particular, consider the situation in Example 3.15. Let X, the angle measured in degrees, be
uniform on [0, 360], so A¼0 and B¼360. Then
MXðtÞ¼e360t/C01
360tt6¼0,MXð0Þ¼1
Now let Y¼(2π/360) X/C0π,s oYis the angle measured in radians between /C0πandπ. Using the mgf
rule for linear transformations with a¼2π/360 and b¼/C0π, we get3.2 Expected Values and Moment Generating Functions 167
MYðtÞ¼ebtMXðatÞ¼e/C0πtMX2πt
360/C18/C19
¼e/C0πte360 2 π=360ðÞ t/C01
3602πt
360/C18/C19
¼eπt/C0e/C0πt
2πtt6¼0, MYð0Þ¼1
This matches the general form of the moment generating function for a uniform random variable
with A¼/C0πandB¼π. Thus, by the mgf uniqueness property, Y/C24Unif[/C0π,π]. ■
3.2.3 Exercises: Section 3.2(19–38)
19. Reconsider the distribution of checkout duration Xdescribed in Exercise 11. Compute the
following:
(a) E(X)
(b) Var( X) and SD( X)
(c) If the borrower is charged an amount h(X)¼X2when checkout duration is X, compute the
expected charge E[h(X)].
20. The article “Modeling Sediment and Water Column Interactions for Hydrophobic Pollutants”
(Water Res. , 1984: 1169–1174) suggests the uniform distribution on the interval [7.5, 20] as a
model for depth (cm) of the bioturbation layer in sediment in a certain region.
(a) What are the mean and variance of depth?
(b) What is the cdf of depth?
(c) What is the probability that observed depth is at most 10? Between 10 and 15?
(d) What is the probability that the observed depth is within 1 standard deviation of the mean
value?
Within 2 standard deviations?
21. For the distribution of Exercise 14,
(a) Compute E(X) and SD( X).
(b) What is the probability that Xis more than 1 standard deviation from its mean value?
22. Consider the pdf given in Exercise 6.
(a) Obtain and graph the cdf of X.
(b) From the graph of f(x), what is the median, η?
(c) Compute E(X) and Var( X).
23. Let X/C24Unif[ A,B].
(a) Obtain an expression for the (100 p)th percentile.
(b) Obtain an expression for the median, η. How does this compare to the mean μ, and why
does that make sense for this distribution?
(c) For na positive integer, compute E(Xn).
24. Consider the pdf for total waiting time Yfor two buses168 3 Continuous Random Variables and Probability Distributions
fðyÞ¼1
25y 0/C20y<5
2
5/C01
25y 5/C20y/C2010
0 otherwise8
>>>><
>>>>:
introduced in Exercise 8.
(a) Compute and sketch the cdf of Y.[Hint: Consider separately 0 /C20y<5 and 5 /C20y/C2010 in
computing F(y). A graph of the pdf should be helpful.]
(b) Obtain an expression for the (100 p)th percentile. [ Hint: Consider separately 0 <p<.5 and
.5/C20p<1.]
(c) Compute E(Y) and Var( Y). How do these compare with the expected waiting time and
variance for a single bus when the time is uniformly distributed on [0, 5]?
(d) Explain how symmetry can be used to obtain E(Y).
25. An ecologist wishes to mark off a circular sampling region having radius 10 m. However, the
radius of the resulting region is actually a random variable Rwith pdf
fðrÞ¼3
41/C010/C0r ðÞ2hi
9/C20r/C2011
0 otherwise8
<
:
What is the expected area of the resulting circular region?
26. The weekly demand for propane gas (in 1000s of gallons) from a particular facility is an rv
Xwith pdf
fðxÞ¼21/C01
x2/C18/C19
1/C20x/C202
0 otherwise8
<
:
(a) Compute the cdf of X.
(b) Obtain an expression for the (100 p)th percentile. What is the value of the median, η?
(c) Compute E(X). How do the mean and median of this distribution compare?
(d) Compute Var( X) and SD( X).
(e) If 1.5 thousand gallons are in stock at the beginning of the week and no new supply is due
in during the week, how much of the 1.5 thousand gallons is expected to be left at the end
of the week? [ Hint: Let h(x)¼amount left when demand is x.]
27. If the temperature at which a compound melts is a random variable with mean value 120/C14C and
standard deviation 2/C14C, what are the mean temperature and standard deviation measured in/C14F?
[Hint:/C14F¼1.8/C14C + 32.]
28. Let Xhave the Pareto pdf introduced in Exercise 10:
fx ;k,θðÞ ¼k/C1θk
xkþ1x/C21θ
0 x<θ8
<
:
(a) If k>1, compute E(X).
(b) What can you say about E(X)i fk¼1?
(c) If k>2, show that Var( X)¼kθ2(k/C01)/C02(k/C02)/C01.3.2 Expected Values and Moment Generating Functions 169
(d) If k¼2, what can you say about Var( X)?
(e) What conditions on kare necessary to ensure that E(Xn) is ﬁnite?
29. The time (min) between successive visits to a particular Web site has pdf f(x)¼4e/C04x,x/C210;
f(x)¼0 otherwise. Use integration by parts to obtain E(X) and SD( X).
30. Consider the weights, in grams, of walnuts harvested at a nearby farm. Suppose this weight
distribution can be modeled by the following pdf:
fðxÞ¼:5/C0x
80/C20x/C204
0 otherwise8
<
:
(a) Show that E(X)¼4/3 and Var( X)¼8/9.
(b) The skewness coefﬁcient is deﬁned as E[(X/C0μ)3]/σ3. Show that its value for the given pdf is
.566. What would the skewness be for a perfectly symmetric pdf?
31. The delta method provides approximations to the mean and variance of a nonlinear function h(X)
of a rv X. These approximations are based on a ﬁrst-order Taylor series expansion of h(x) about
x¼μ, the mean of X:
hðXÞ/C25h1ðXÞ¼hμðÞ þ h0μðÞX/C0μðÞ
(a) Show that E[h1(X)]¼h(μ). (This is the delta method approximation to E[h(X)].)
(b) Show that Var[ h1(X)]¼[h0(μ)]2Var(X). (This is the delta method approximation to
Var[h(X)].)
(c) If the voltage vacross a medium is ﬁxed but current Iis random, then resistance will also be
a random variable related to IbyR¼v/I.I fμI¼20 and σI¼.5, calculate approximations to
μRandσR.
(d) Let Rhave the distribution in Exercise 25, whose mean and variance are 10 and 1/5,
respectively. Let h(R)¼πR2, the area of the ecologist’s sampling region. How does
E[h(R)] from Exercise 25 compare to the delta method approximation h(10)?
(e) It can be shown that Var[ h(R)]¼14008 π2/175. Compute the delta method approximation to
Var[h(R)] using the formula in (b). How good is the approximation?
32. Let X/C24Unif[ A,B], so its pdf is f(x)¼1/(B/C0A),A/C20x/C20B,f(x)¼0 otherwise. Show that the
moment generating function of Xis
MXðtÞ¼eBt/C0eAt
B/C0AðÞ tt6¼0
1 t¼08
><
>:
33. Let X/C24Unif[0, 1]. Find a linear function Y¼g(X) such that the interval [0, 1] is transformed into
[/C05, 5]. Use the relationship for linear functions MaX+b(t)¼ebtMX(at) to obtain the mgf of Yfrom
the mgf of X. Compare your answer with the result of Exercise 32, and use this to obtain the pdf
ofY.
34. If the pdf of a measurement error Xisf(x)¼.5e/C0|x|,/C01 <x<1, show that MXðtÞ¼1=1/C0t2ðÞ
for |t|<1.
35. Consider the rv X¼time headway in Example 3.5.
(a) Find the moment generating function and use it to ﬁnd the mean and variance.
(b) Now consider a random variable whose pdf is170 3 Continuous Random Variables and Probability Distributions
fðxÞ¼:15e/C0:15xx/C210
0 otherwise(
Find the moment generating function and use it to ﬁnd the mean and variance. Compare with
(a), and explain the similarities and differences.
(c) Let Y¼X/C0.5 and use the relationship for linear functions MaX+b(t)¼ebtMX(at) to obtain
the mgf of Yfrom (a). Compare with the result of (b) and explain.
36. Deﬁne LX(t)¼ln[MX(t)]. It was shown in Exercise 120 of Chap. 2thatLX0(0)¼E(X) and
LX00(0)¼Var(X).
(a) Determine MX(t) for the pdf in Exercise 29, and use this mgf to obtain E(X) and Var( X).
How does this compare, in terms of difﬁculty, with the integration by parts required in that
exercise?
(b) Determine LX(t) for this same distribution, and use LX(t) to obtain E(X) and Var( X). How
does the computational effort here compare with that of (a)?
37. Let Xbe a nonnegative, continuous rv with pdf f(x) and cdf F(x).
(a) Show that, for any constant t>0,
ð1
tx/C1fðxÞdx/C21t/C1PX>tðÞ ¼ t/C11/C0FðtÞ ½/C138
(b) Assume the mean of Xis ﬁnite (i.e., the integral deﬁning μconverges). Use part (a) to show
that
lim
t!1t/C11/C0FðtÞ ½/C138 ¼ 0
[Hint: Write the integral for μas the sum of two other integrals, one from 0 to tand another
from tto1.]
38. Let Xbe a nonnegative, continuous rv with cdf F(x).
(a) Assuming the mean μofXis ﬁnite, show that
μ¼ð1
01/C0FðxÞ ½/C138 dx
[Hint: Apply integration by parts to the integral above, and use the result of the previous
exercise.] This is the continuous analog of the result established in Exercise 48 of Chap. 2.
(b) A similar argument can be used to show that the kth moment of Xis given by
EXk/C0/C1
¼kð1
0xk/C011/C0FðxÞ ½/C138 dx
and that E(Xk) exists iff tk[1/C0F(t)]!0a st!1 . (This was the topic of a 2012 article in The
American Statistician .) Suppose the lifetime X, in weeks, of a low-grade transistor under
continuous use has cdf F(x)¼1/C0(x+1 )/C03forx>0. Without ﬁnding the pdf of X, determine
its mean and its standard deviation.3.2 Expected Values and Moment Generating Functions 171
3.3 The Normal (Gaussian) Distribution
The normal distribution, often called the Gaussian distribution by engineers, is the most important
one in all of probability and statistics. Many numerical populations have distributions that can be ﬁt
very closely by an appropriate normal curve. Examples include heights, weights, and other physical
characteristics, measurement errors in scientiﬁc experiments, measurements on fossils, reaction times
in psychological experiments, measurements of intelligence and aptitude, scores on various tests, and
numerous economic measures and indicators. Even when the underlying distribution is discrete, the
normal curve often gives an excellent approximation. In addition, even when individual variables
themselves are not normally distributed, sums and averages of the variables will, under suitable
conditions, have approximately a normal distribution; this is the content of the Central Limit Theorem
discussed in Chap. 4.
DEFINITION
A continuous rv Xis said to have a normal distribution (orGaussian distribution ) with
parameters μandσ, where /C01 <μ<1andσ>0, if the pdf of Xis
fx ;μ,σðÞ ¼1
σﬃﬃﬃﬃﬃ
2πp e/C0x/C0μðÞ2=2σ2ðÞ/C01 <x<1ð 3:3Þ
The statement that Xis normally distributed with parameters μandσis often abbreviated
X/C24N(μ,σ).
Figure 3.13 presents graphs of f(x;μ,σ) for several different ( μ,σ) pairs. Each resulting density
curve is symmetric about μand bell-shaped, so the center of the bell (point of symmetry) is both the
mean of the distribution and the median. The value of σis the distance from μto the inﬂection points
of the curve (the points at which the curve changes between turning downward to turning upward).
Large values of σyield density curves that are quite spread out about μ, whereas small values of σ
yield density curves with a high peak above μand most of the area under the density curve quite close
toμ. Thus a large σimplies that a value of Xfar from μmay well be observed, whereas such a value is
quite unlikely when σis small.
Clearly f(x;μ,σ)/C210, but a somewhat complicated calculus argument is required to prove thatÐ1
/C01f(x;μ,σ)dx¼1( s e eE x e r c i s e6 6 ) .I tc a nb es h o w nu s i ng calculus (Exercise 67) or moment
generating functions (Exercise 68) that E(X)¼μand Var( X)¼σ2,s ot h ep a r a m e t e r s μandσare the
mean and the standard deviation, respectively, of X.mm  + s m  m + s m m + s
Fig. 3.13 Normal density curves172 3 Continuous Random Variables and Probability Distributions
3.3.1 The Standard Normal Distribution
To compute P(a/C20X/C20b) when X/C24N(μ,σ), we must evaluate
ðb
a1
σﬃﬃﬃﬃﬃ
2πp e/C0x/C0μðÞ2=2σ2ðÞdx ð3:4Þ
None of the standard integration techniques can be used here, and there is no closed-form
expression for the integral. Table 3.1at the end of this section provides the code for performing
such normal distribution calculations in both Matlab and R. For the purpose of hand calculation of
normal distribution probabilities, we now introduce a special normal distribution.
DEFINITION
The normal distribution with parameter values μ¼0 and σ¼1 is called the standard normal
distribution . A random variable that has a standard normal distribution is called a standard
normal random variable and will be denoted by Z. The pdf of Zis
fz ;0, 1ðÞ ¼1ﬃﬃﬃﬃﬃ
2πp e/C0z2=2/C01 <z<1
The cdf of ZisPZ/C20zðÞ ¼ðz
/C011ﬃﬃﬃﬃﬃ
2πp e/C0y2=2dy, which we will denote by Φ(z).
The standard normal distribution does not frequently serve as a model for a naturally arising
population, since few variables have mean 0 and standard deviation 1. Instead, it is a reference
distribution from which information about other normal distributions can be obtained. Appendix
Table A.3 gives values of Φ(z) for z¼/C03.49,/C03.48, ..., 3.48, 3.49 and is referred to as the standard
normal table orz table . Figure 3.14 illustrates the type of cumulative area (probability) tabulated in
Table A.3. From this table, various other probabilities involving Zcan be calculated.
Example 3.19 Here we demonstrate how the ztable is used to calculate various probabilities
involving a standard normal rv.
(a) P(Z/C201.25)¼Φ(1.25), a probability that is tabulated in Table A.3 at the intersection of the row
marked 1.2 and the column marked .05. The number there is .8944, so P(Z/C201.25)¼.8944. See
Fig.3.15a. In Matlab, we may type normcdf(1.25,0,1) ; in R, use pnorm(1.25,0,1)
or just pnorm(1.25).
(b) P(Z>1.25)¼1/C0P(Z/C201.25)¼1/C0Φ(1.25), the area under the standard normal curve to the
right of 1.25 (an upper-tail area). Since Φ(1.25) ¼.8944, it follows that P(Z>1.25) ¼.1056.
Since Zis a continuous rv, P(Z/C211.25) also equals .1056. See Fig. 3.15b.0zStandard normal ( z) curveShaded area = Φ(z) Fig. 3.14 Standard
normal cumulative areas
tabulated in Appendix
Table A.33.3 The Normal (Gaussian) Distribution 173
(c) P(Z/C20/C01.25)¼Φ(/C01.25), a lower-tail area. Directly from the ztable, Φ(/C01.25)¼.1056. By
symmetry of the normal curve, this is identical to the probability in (b).
(d) P(/C0.38/C20Z/C201.25) is the area under the standard normal curve above the interval [ /C0.38, 1.25].
From Sect. 3.1,i fZis a continuous rv with cdf F(z), then P(a/C20Z/C20b)¼F(b)/C0F(a). This gives
P(/C0.38/C20Z/C201.25)¼Φ(1.25) /C0Φ(/C0.38)¼.8944 /C0.3520 ¼.5424. (See Fig. 3.16.) To evaluate
this probability in Matlab, type normcdf(1.25,0,1)-normcdf(.38,0,1) ;i nR ,
typepnorm(1.25,0,1)-pnorm(-.38,0,1) or just pnorm(1.25)-pnorm(-.38) .
From Sect. 3.1, we have that the (100 p)th percentile of the standard normal distribution, for any
pbetween 0 and 1, is the solution to the equation Φ(z)¼p. So, we may write the (100 p)th percentile
of the standard normal distribution as ηp¼Φ/C01(p). Matlab, R, or the ztable can be used to obtain this
percentile.
Example 3.20 The 99th percentile of the standard normal distribution, Φ/C01(.99), is the value on the
horizontal axis such that the area under the curve to the left of the value is .9900, as illustrated in
Fig.3.17. To solve the “inverse” problem Φ(z)¼p, the standard normal table is used in an inverse
fashion: Find in the middle of the table .9900; the row and column in which it lies identify the 99th
zpercentile. Here .9901 lies in the row marked 2.3 and column marked .03, so Φ(2.33) ¼.9901 /C25.99
and the 99th percentile is approximately z¼2.33. By symmetry, the ﬁrst percentile is the negative of
the 99th percentile, so it equals /C02.33 (1% lies below the ﬁrst and above the 99th). See Fig. 3.18.z curve z curve
0b a
1.25 1.25 0Shaded area = Φ(1.25) 
P(Z > 1.25)
Fig. 3.15 Normal curve areas (probabilities) for Example 3.19
0 1.25 −.38 0 −.38− =
0 1.25z curve
Fig. 3.16 P(/C0.38/C20Z/C201.25) as the difference between two cumulative areas ■
z curve
99th percentile0Shaded area = .9900 Fig. 3.17 Finding the 99th
percentile174 3 Continuous Random Variables and Probability Distributions
To ﬁnd the 99th percentile of the standard normal distribution in Matlab, use the command
norminv(.99,0,1) ;i nR , qnorm(.99,0,1) or just qnorm(.99) produces that same
value of roughly z¼2.33. ■
3.3.2 Non-standardized Normal Distributions
When X/C24N(μ,σ), probabilities involving Xmay be computed by “standardizing.” A standardized
variable has the form ( X/C0μ)/σ. Subtracting μshifts the mean from μto zero, and then dividing by σ
scales the variable so that the standard deviation is 1 rather than σ.
Standardizing amounts to nothing more than calculating a distance from the mean and then
reexpressing the distance as some number of standard deviations. For example, if μ¼100 and σ¼
15, then x¼130 corresponds to z¼(130/C0100)/15 ¼30/15 ¼2.00. That is, 130 is 2 standard
deviations above (to the right of) the mean value. Similarly, standardizing 85 gives (85 /C0100)/15 ¼
/C01.00, so 85 is 1 standard deviation below the mean. According to the next proposition, the ztable
applies to anynormal distribution provided that we think in terms of number of standard deviations
away from the mean value.
PROPOSITION
IfX/C24N(μ,σ), then the “standardized” rv Zdeﬁned by
Z¼X/C0μ
σ
has a standard normal distribution. Thus
Pa/C20X/C20b ðÞ ¼ Pa/C0μ
σ/C20Z/C20b/C0μ
σ/C18/C19
¼Φb/C0μ
σ/C18/C19
/C0Φa/C0μ
σ/C16/C17
,
PX/C20aðÞ ¼ Φa/C0μ
σ/C16/C17
, PX/C21bðÞ ¼ 1/C0Φb/C0μ
σ/C18/C19
,
and the (100 p)th percentile of the N(μ,σ) distribution is given by
ηp¼μþΦ/C01ðpÞ/C1σ:
Conversely, if Z/C24N(0, 1) and μandσare constants (with σ>0), then the “un-standardized”
rvX¼μ+σZhas a normal distribution with mean μand standard deviation σ.
Proof LetX/C24N(μ,σ) and deﬁne Z¼(X/C0μ)/σas in the statement of the proposition. Then the cdf
ofZis given byShaded area = .01z curve
0
−2.33 = 1st percentile 2.33 = 99th percentileFig. 3.18 The relationship
between the 1st and 99th
percentiles3.3 The Normal (Gaussian) Distribution 175
FzðzÞ¼PZ/C20zðÞ
¼PX/C0μ
σ/C20z/C18/C19
¼PX/C20μþzσ ðÞ ¼ðμþzσ
/C01fx ;μ,σðÞ dx¼ðμþzσ
/C011
σﬃﬃﬃﬃﬃ
2πp e/C0x/C0μðÞ2=2σ2ðÞdx
Now make the substitution u¼(x/C0μ)/σ. The new limits of integration become /C01 toz, and the
differential dxis replaced by σdu, resulting in
FzðzÞ¼ðz
/C011
σﬃﬃﬃﬃﬃ
2πp e/C0u2=2σdu¼ðz
/C011ﬃﬃﬃﬃﬃ
2πp e/C0u2=2du¼ΦðzÞ
Thus, the cdf of ( X/C0μ)/σis the standard normal cdf, which establishes that ( X/C0μ)/σ/C24N(0, 1).
The probability formulas in the statement of the proposition follow directly from this main result,
as does the formula for the (100 p)th percentile:
p¼PX/C20ηp/C0/C1
¼PX/C0μ
σ/C20ηp/C0μ
σ/C18/C19
¼Φηp/C0μ
σ/C16/C17
)ηp/C0μ
σ¼Φ/C01ðpÞ)
ηp¼μþΦ/C01ðpÞ/C1σ
The converse statement Z/C24N(0, 1) )μ+σZ/C24N(μ,σ) is derived similarly. ■
The key idea of this proposition is that by standardizing, any probability involving Xcan be
expressed as a probability involving a standard normal rv Z, so that the ztable can be used. This is
illustrated in Fig. 3.19.
Software eliminates the need for standardizing X, although the standard normal distribution is still
important in its own right. Table 3.1at the end of this section details the relevant R and Matlab
commands, which are also illustrated in the following examples.
Example 3.21 The time that it takes a driver to react to the brake lights on a decelerating vehicle is
critical in avoiding rear-end collisions. The article “Fast-Rise Brake Lamp as a Collision-Prevention
Device” ( Ergonomics , 1993: 391–395) suggests that reaction time for an in-trafﬁc response to a brake
signal from standard brake lights can be modeled with a normal distribution having mean value 1.25 s
and standard deviation of .46 s. What is the probability that reaction time is between 1.00 s and 1.75 s?
If we let Xdenote reaction time, then standardizing gives 1.00 /C20X/C201.75 if and only if
1:00/C01:25
:46/C20X/C01:25
:46/C201:75/C01:25
:46
The middle expression, by the previous proposition, is a standard normal rv. Thusx m 0
(x− m)/sN(m,s)N(0,1)
=Fig. 3.19 Equality of
nonstandard and standard
normal curve areas176 3 Continuous Random Variables and Probability Distributions
P1:00/C20X/C201:75 ðÞ ¼ P1:00/C01:25
:46/C20Z/C201:75/C01:25
:46/C18/C19
¼P/C0:54/C20Z/C201:09 ðÞ ¼ Φð1:09Þ/C0Φ/C0:54ðÞ
¼:8621/C0:2946¼:5675
This is illustrated in Fig. 3.20. The same answer may be produced in Matlab with the command
normcdf(1.75,1.25,.46)-normcdf(1.00, 1.25,.46) ; Matlab gives the answer .5681,
which is more accurate than the value .5675 above (due to rounding the z-values to two decimal
places). The analogous R command is pnorm(1.75,1.25,.46)-pnorm(1.00,1.25,.46) .
Similarly, if we view 2 s as a critically long reaction time, the probability that actual reaction time
will exceed this value is
PX>2ðÞ ¼ PZ >2/C01:25
:46/C18/C19
¼PZ>1:63 ðÞ ¼ 1/C0Φð1:63Þ¼:0516
This probability is determined in Matlab and R by executing the commands
1-normcdf(2,1.25,.46) and1-pnorm(2,1.25,.46) , respectively. ■
Example 3.22 The amount of distilled water dispensed by a machine is normally distributed with
mean value 64 oz and standard deviation .78 oz. What container size cwill ensure that overﬂow
occurs only .5% of the time? If Xdenotes the amount dispensed, the desired condition is that P(X>c)
¼.005, or, equivalently, that P(X/C20c)¼.995. Thus cis the 99.5th percentile of the normal
distribution with μ¼64 and σ¼.78. The 99.5th percentile of the standard normal distribution is
Φ/C01(.995) /C252.58, so
c¼η:995¼64þð2:58Þð:78Þ¼64þ2:0¼66:0oz
This is illustrated in Fig. 3.21.1.25
1.75 1.000
1.09 −.54z curveNormal, m = 1.25, s = .46 P(1.00 ≤ X ≤ 1.75) Fig. 3.20 Normal curves
for Example 3.21
c = 99.5th percentile = 66.0Shaded area = .995
m = 64Fig. 3.21 Distribution of
amount dispensed for
Example 3.223.3 The Normal (Gaussian) Distribution 177
The Matlab and R commands to calculate this percentile are norminv(.995,64,.78) and
qnorm(.995,64,.78) , respectively. ■
Example 3.23 The return on a diversiﬁed investment portfolio is normally distributed. What is the
probability that the return is within 1 standard deviation of its mean value? This question can be
answered without knowing either μorσ, as long as the distribution is known to be normal; in other
words, the answer is the same for anynormal distribution. Going one standard deviation below μ
lands us at μ/C0σ, while μ+σis one standard deviation above the mean. Thus
PXis within one standard
deviation of its mean/C18/C19
¼Pμ/C0σ/C20X/C20μþσ ðÞ
¼P/C18μ/C0σ/C0μ
σ/C20Z/C20μþσ/C0μ
σ/C19
¼P/C01/C20Z/C201 ðÞ
¼Φ1ðÞ /C0 Φð/C01/C1
¼:6826
The probability that Xis within 2 standard deviations of the mean is P(/C02/C20Z/C202)¼.9544 and
the probability that Xis within 3 standard deviations of the mean is P(/C03/C20Z/C203)¼.9973. ■
The results of Example 3.23 are often reported in percentage form and referred to as the empirical
rule (because empirical evidence has shown that histograms of real data can very frequently be
approximated by normal curves).
EMPIRICAL RULE
If the population distribution of a variable is (approximately) normal, then
1. Roughly 68% of the values are within 1 SD of the mean.
2. Roughly 95% of the values are within 2 SDs of the mean.
3. Roughly 99.7% of the values are within 3 SDs of the mean.
3.3.3 The Normal MGF
The moment generating function provides a straightforward way to establish several important results
concerning the normal distribution.
PROPOSITION
The moment generating function of a normally distributed random variable Xis
MXðtÞ¼eμtþσ2t2=2
Proof Consider ﬁrst the special case of a standard normal rv Z. Then
MZðtÞ¼EetZ/C0/C1
¼ð1
/C01etz1ﬃﬃﬃﬃﬃ
2πp e/C0z2=2dz¼ð1
/C011ﬃﬃﬃﬃﬃ
2πp e/C0z2/C02tzðÞ =2dz
Completing the square in the exponent, we have178 3 Continuous Random Variables and Probability Distributions
MZðtÞ¼et2=2ð1
/C011ﬃﬃﬃﬃﬃ
2πp e/C0z2/C02tzþt2ðÞ =2dz¼et2=2ð1
/C011ﬃﬃﬃﬃﬃ
2πp e/C0z/C0tðÞ2=2dz
The last integral is the area under a normal density curve with mean tand standard deviation 1, so
the value of the integral is 1. Therefore, MZðtÞ¼et2=2.
Now let Xbe any normal rv with mean μand standard deviation σ. Then, by the proposition earlier
in this section, ( X/C0μ)/σ¼Z, where Zis standard normal. Rewrite this relationship as X¼μ+σZ, and
use the property MaY+b(t)¼ebtMY(at):
MXðtÞ¼MμþσZðtÞ¼eμtMZσtðÞ ¼ eμteσ2t2=2¼eμtþσ2t2=2 ■
The normal mgf can be used to establish that μandσare indeed the mean and standard deviation of
X, as claimed earlier (Exercise 68). Also, by the mgf uniqueness property, any rv Xwhose moment
generating function has the form speciﬁed above is necessarily normally distributed. For example,
if it is known that the mgf of XisMXðtÞ¼e8t2, then Xmust be a normal rv with mean μ¼0 and
standard deviation σ¼4 (since the N(0, 4) distribution has e8t2as its mgf).
It was established earlier in this section that if X/C24N(μ,σ)a n d Z¼(X/C0μ)/σ, then Z/C24N(0, 1), and vice
versa. This standardizing transformation is actually a special case of a much more general property.
PROPOSITION
LetX/C24N(μ,σ). Then for any constants aandbwith a6¼0,aX+bis also normally distributed.
That is, any linear rescaling of a normal rv is normal.
The proof of this proposition uses mgfs and is left as an exercise (Exercise 70). This proposition
provides a much easier proof of the earlier relationship between XandZ. The rescaling formulas and
this proposition combine to give the following statement: if Xis normally distributed and Y¼aX+
b(with a6¼0), then Yis also normal, with mean μY¼aμX+band standard deviation σY¼|a|σX.
3.3.4 The Normal Distribution and Discrete Populations
The normal distribution is often used as an approximation to the distribution of values in a discrete
population. In such situations, extra care must be taken to ensure that probabilities are computed in an
accurate manner.
Example 3.24 IQ (as measured by a standard test) is known to be approximately normally
distributed with μ¼100 and σ¼15. What is the probability that a randomly selected individual
has an IQ of at least 125? Letting X¼the IQ of a randomly chosen person, we wish P(X/C21125). The
temptation here is to standardize X/C21125 immediately as in previous examples. However, the IQ
population is actually discrete, since IQs are integer-valued, so the normal curve is an approximation
to a discrete probability histogram, as pictured in Fig. 3.22.
The rectangles of the histogram are centered at integers, so IQs of at least 125 correspond to
rectangles beginning at 124.5, as shaded in Fig. 3.22. Thus we really want the area under the
approximating normal curve to the right of 124.5. Standardizing this value gives P(Z/C211.63) ¼
.0516. If we had standardized X/C21125, we would have obtained P(Z/C211.67)¼.0475. The difference
is not great, but the answer .0516 is more accurate. Similarly, P(X¼125) would be approximated by
the area between 124.5 and 125.5, since the area under the normal curve above the single value
125 is zero.3.3 The Normal (Gaussian) Distribution 179
The correction for discreteness of the underlying distribution in Example 3.24 is often called a
continuity correction ; it adjusts for the use of a continuous distribution in approximating a proba-
bility involving a discrete rv. It is useful in the following application of the normal distribution to the
computation of binomial probabilities. The normal distribution was actually created as an approxi-
mation to the binomial distribution (by Abraham de Moivre in the 1730s).
3.3.5 Approximating the Binomial Distribution
Recall that the mean value and standard deviation of a binomial random variable Xareμ¼npand
σ¼ﬃﬃﬃﬃﬃﬃﬃﬃnpqp, respectively. Figure 3.23a displays a probability histogram for the binomial distribution
with n¼20,p¼.6 [so μ¼20(.6) ¼12 and σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
20ð:6Þð:4Þp
¼2:19]. A normal curve with mean
value and standard deviation equal to the corresponding values for the binomial distribution has been
superimposed on the probability histogram. Although the probability histogram is a bit skewed
(because p6¼.5), the normal curve gives a very good approximation, especially in the middle part
of the picture. The area of any rectangle (probability of any particular Xvalue) except those in the
extreme tails can be accurately approximated by the corresponding normal curve area. For example,
PX¼10ðÞ ¼20
10/C18/C19
ð:6Þ10ð:4Þ10¼:117, whereas the area under the normal curve between 9.5 and
10.5 is P(/C01.14/C20Z/C20/C0.68)¼.120.
On the other hand, a normal distribution is a poor approximation to a discrete distribution that is
heavily skewed. For example, Figure 3.23b shows a probability histogram for the Bin(20, .1)125
Fig. 3.22 A normal approximation to a discrete distribution ■
201816141210864200.30
0.25
0.20
0.15
0.10
0.05
0.00normal curve,
µ = 12, s = 2.19
201816141210864200.30
0.25
0.20
0.15
0.10
0.05
0.00normal curve,
µ = 2, s = 1.34ab
Fig. 3.23 Binomial probability histograms with normal approximation curves superimposed: ( a)n¼20 and p¼.6
(a good ﬁt); ( b)n¼20 and p¼.1 (a poor ﬁt)180 3 Continuous Random Variables and Probability Distributions
distribution and the normal pdf with the same mean and standard deviation ( μ¼2 and σ¼1.34).
Clearly, we would not want to use this normal curve to approximate binomial probabilities, even with
a continuity correction.
PROPOSITION
LetXbe a binomial rv based on ntrials with success probability p. Then if the binomial
probability histogram is not too skewed, Xhas approximately a normal distribution with μ¼np
andσ¼ﬃﬃﬃﬃﬃﬃﬃﬃnpqp. In particular, for x¼a possible value of X,
P(X/C20x)¼B(x;n,p)/C25(area under the normal curve to the left of x+ .5)
¼Φxþ:5/C0npﬃﬃﬃﬃﬃﬃﬃﬃnpqp/C18/C19
In practice, the approximation is adequate provided that both np/C2110 and nq/C2110.
If either np<10 or nq<10, the binomial distribution may be too skewed for the (symmetric)
normal curve to give accurate approximations.
Example 3.25 Suppose that 25% of all licensed drivers in a state do not have insurance. Let Xbe the
number of uninsured drivers in a random sample of size 50 (somewhat perversely, a success is an
uninsured driver), so that p¼.25. Then μ¼12.5 and σ¼3.062. Since np¼50(.25) ¼12.5/C2110 and
nq¼37.5/C2110, the approximation can safely be applied:
PX/C2010ðÞ ¼ B10;50,:25 ðÞ /C25 Φ10þ:5/C012:5
3:062/C18/C19
¼Φ/C0:6532ðÞ ¼ :2568
Similarly, the probability that between 5 and 15 (inclusive) of the selected drivers are uninsured is
P5/C20X/C2015 ðÞ ¼ B15;50,:25 ðÞ /C0 B4;50,:25 ðÞ
/C25Φ15:5/C012:5
3:062/C18/C19
/C0Φ4:5/C012:5
3:062/C18/C19
¼:8319
The exact probabilities are .2622 and .8348, respectively, so the approximations are quite good.
In the last calculation, the probability P(5/C20X/C2015) is being approximated by the area under
the normal curve between 4.5 and 15.5—the continuity correction is used for both the upper and
lower limits. ■
The wide availability of software for doing binomial probability calculations, even for large values
ofn, has considerably diminished the importance of the normal approximation. However, it is
important for another reason. When the objective of an investigation is to make an inference about
a population proportion p, interest will focus on the sample proportion of successes bP¼X=nrather
than on Xitself. Because this proportion is just Xmultiplied by the constant 1/ n, the earlier rescaling
proposition tells us that bPwill also have approximately a normal distribution (with mean μ¼pand
standard deviation σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=np
) provided that both np/C2110 and nq/C2110. This normal approximation
is the basis for several inferential procedures to be discussed in Chap. 5.3.3 The Normal (Gaussian) Distribution 181
It is quite difﬁcult to give a direct proof of the validity of this normal approximation (the ﬁrst one
goes back about 270 years to de Moivre). In Chap. 4, we’ll see that it is a consequence of an important
general result called the Central Limit Theorem.
3.3.6 Normal Distribution Calculations with Software
Many software packages, including Matlab and R, have built-in functions to determine both
probabilities under a normal curve and quantiles (aka percentiles) of any given normal distribution.
Table 3.1summarizes the relevant code in both packages.
In the special case of a standard normal distribution, R (but not Matlab) will allow the user to drop
the last two arguments, μandσ. That is, the R commands pnorm(x) andpnorm(x,0,1) yield the
same result for any number x, and a similar comment applies to qnorm . Both software packages also
have built-in function calls for the normal pdf: normpdf( x,μ,σ)anddnorm (x,μ,σ), respectively.
However, these two commands are generally only used when one desires to graph a normal density
curve ( xvs.f(x;μ,σ)), since the pdf evaluated at particular xdoes not represent a probability, as
discussed in Sect. 3.1.
3.3.7 Exercises: Section 3.3(39–70)
39. Let Zbe a standard normal random variable and obtain each of the following probabilities,
drawing pictures wherever appropriate.
(a) P(0/C20Z/C202.17)
(b) P(0/C20Z/C201)
(c) P(/C02.50/C20Z/C200)
(d) P(/C02.50/C20Z/C202.50)
(e) P(Z/C201.37)
(f)P(/C01.75/C20Z)
(g) P(/C01.50/C20Z/C202.00)
(h) P(1.37/C20Z/C202.50)
(i)P(1.50/C20Z)
(j)P(|Z|/C202.50)
40. In each case, determine the value of the constant cthat makes the probability statement correct.
(a)Φ(c)¼.9838
(b) P(0/C20Z/C20c)¼.291
(c) P(c/C20Z)¼.121
(d) P(/C0c/C20Z/C20c)¼.668
(e) P(c/C20|Z|)¼.016Table 3.1 Normal probability and quantile calculations in Matlab and R
Function: cdf quantile, i.e., the (100 p)th percentile
Notation: Φx/C0μ
σ/C0/C1ηp¼μ+Φ/C01(p)/C1σ
Matlab: normcdf( x,μ,σ) norminv( p,μ,σ)
R: pnorm( x,μ,σ) qnorm( p,μ,σ)182 3 Continuous Random Variables and Probability Distributions
41. Find the following percentiles for the standard normal distribution. Interpolate where
appropriate.
(a) 91st
(b) 9th
(c) 75th
(d) 25th
(e) 6th
42. Suppose the force acting on a column that helps to support a building is a normally distributed
random variable Xwith mean value 15.0 kips and standard deviation 1.25 kips. Compute the
following probabilities.
(a) P(X/C2015)
(b) P(X/C2017.5)
(c) P(X/C2110)
(d) P(14/C20X/C2018)
(e) P(|X/C015|/C203)
43. Mopeds (small motorcycles with an engine capacity below 50 cc) are very popular in Europe
because of their mobility, ease of operation, and low cost. The article “Procedure to Verify the
Maximum Speed of Automatic Transmission Mopeds in Periodic Motor Vehicle Inspections”
(J. of Automobile Engr. , 2008: 1615-1623) described a rolling bench test for determining
maximum vehicle speed. A normal distribution with mean value 46.8 km/h and standard
deviation 1.75 km/h is postulated. Consider randomly selecting a single such moped.
(a) What is the probability that maximum speed is at most 50 km/h?
(b) What is the probability that maximum speed is at least 48 km/h?
(c) What is the probability that maximum speed differs from the mean value by at most 1.5
standard deviations?
44. Let Xbe the birth weight, in grams, of a randomly selected full-term baby. The article “Fetal
Growth Parameters and Birth Weight: Their Relationship to Neonatal Body Composition”
(Ultrasound in Obstetrics and Gynecology , 2009: 441–446) suggests that Xis normally
distributed with mean 3500 and standard deviation 600.
(a) Sketch the relevant density curve, including tick marks on the horizontal scale.
(b) What is P(3000 <X<4500), and how does this compare to P(3000 /C20X/C204500)?
(c) What is the probability that the weight of such a newborn is less than 2500 g?
(d) What is the probability that the weight of such a newborn exceeds 6000 g (roughly
13.2 lb)?
(e) How would you characterize the most extreme .1% of all birth weights?
(f) Use the rescaling proposition from this section to determine the distribution of birth weight
expressed in pounds (shape, mean, and standard deviation), and then recalculate the
probability from part (c). How does this compare to your previous answer?
45. Based on extensive data from an urban freeway near Toronto, Canada, “it is assumed that free
speeds can best be represented by a normal distribution” (“Impact of Driver Compliance on the
Safety and Operational Impacts of Freeway Variable Speed Limit Systems,” J. of Transp. Engr. ,
2011: 260–268). The mean and standard deviation reported in the article were 119 km/h and
13.1 km/h, respectively.
(a) What is the probability that the speed of a randomly selected vehicle is between 100 and
120 km/h?
(b) What speed characterizes the fastest 10% of all speeds?3.3 The Normal (Gaussian) Distribution 183
(c) The posted speed limit was 100 km/h. What percentage of vehicles was traveling at speeds
exceeding this posted limit?
(d) If ﬁve vehicles are randomly and independently selected, what is the probability that at
least one is not exceeding the posted speed limit?
(e) What is the probability that the speed of a randomly selected vehicle exceeds 70 miles/h?
46. The defect length of a corrosion defect in a pressurized steel pipe is normally distributed with
mean value 30 mm and standard deviation 7.8 mm (suggested in the article “Reliability
Evaluation of Corroding Pipelines Considering Multiple Failure Modes and Time-Dependent
Internal Pressure,” J. of Infrastructure Systems , 2011: 216–224).
(a) What is the probability that defect length is at most 20 mm? Less than 20 mm?
(b) What is the 75th percentile of the defect length distribution, i.e., the value that separates the
smallest 75% of all lengths from the largest 25%?
(c) What is the 15th percentile of the defect length distribution?
(d) What values separate the middle 80% of the defect length distribution from the smallest
10% and the largest 10%?
47. The plasma cholesterol level (mg/dL) for patients with no prior evidence of heart disease who
experience chest pain is normally distributed with mean 200 and standard deviation 35.
Consider randomly selecting an individual of this type. What is the probability that the plasma
cholesterol level
(a) Is at most 250?
(b) Is between 300 and 400?
(c) Differs from the mean by at least 1.5 standard deviations?
48. Suppose the diameter at breast height (in.) of trees of a certain type is normally distributed with
μ¼8.8 and σ¼2.8, as suggested in the article “Simulating a Harvester-Forwarder Softwood
Thinning” ( Forest Products J. , May 1997: 36–41).
(a) What is the probability that the diameter of a randomly selected tree will be at least 10 in.?
Will exceed 10 in.?
(b) What is the probability that the diameter of a randomly selected tree will exceed 20 in.?
(c) What is the probability that the diameter of a randomly selected tree will be between 5 and
10 in.?
(d) What value cis such that the interval (8.8 /C0c, 8.8 + c) includes 98% of all diameter values?
(e) If four trees are independently selected, what is the probability that at least one has a
diameter exceeding 10 in.?
49. There are two machines available for cutting corks intended for use in wine bottles. The ﬁrst
produces corks with diameters that are normally distributed with mean 3 cm and standard
deviation .1 cm. The second machine produces corks with diameters that have a normal
distribution with mean 3.04 cm and standard deviation .02 cm. Acceptable corks have diameters
between 2.9 and 3.1 cm. Which machine is more likely to produce an acceptable cork?
50. Human body temperatures for healthy individuals have approximately a normal distribution with
mean 98.25/C14F and standard deviation .75/C14F. (The past accepted value of 98.6/C14F was obtained by
converting the Celsius value of 37/C14, which is correct to the nearest integer.)
(a) Find the 90th percentile of the distribution.
(b) Find the 5th percentile of the distribution.
(c) What temperature separates the coolest 25% from the others?
51. The article “Monte Carlo Simulation—Tool for Better Understanding of LRFD” ( J. Struct. Engr. ,
1993: 1586–1599) suggests that yield strength (ksi) for A36 grade steel is normally distributed
with μ¼43 and σ¼4.5.184 3 Continuous Random Variables and Probability Distributions
(a) What is the probability that yield strength is at most 40? Greater than 60?
(b) What yield strength value separates the strongest 75% from the others?
52. The automatic opening device of a military cargo parachute has been designed to open when the
parachute is 200 m above the ground. Suppose opening altitude actually has a normal distribution
with mean value 200 m and standard deviation 30 m. Equipment damage will occur if the
parachute opens at an altitude of less than 100 m. What is the probability that there is equipment
damage to the payload of at least one of ﬁve independently dropped parachutes?
53. The temperature reading from a thermocouple placed in a constant-temperature medium is
normally distributed with mean μ, the actual temperature of the medium, and standard deviation
σ. What would the value of σhave to be to ensure that 95% of all readings are within .1/C14ofμ?
54. Vehicle speed on a particular bridge in China can be modeled as normally distributed (“Fatigue
Reliability Assessment for Long-Span Bridges under Combined Dynamic Loads from Winds and
Vehicles,” J. of Bridge Engr. , 2013: 735–747).
(a) If 5% of all vehicles travel less than 39.12 mph and 10% travel more than 73.24 mph, what
are the mean and standard deviation of vehicle speed? [ Note: The resulting values should
agree with those given in the cited article.]
(b) What is the probability that a randomly selected vehicle’s speed is between 50 and 65 mph?
(c) What is the probability that a randomly selected vehicle’s speed exceeds the speed limit of
70 mph?
55. If adult female heights are normally distributed, what is the probability that the height of a
randomly selected woman is
(a) Within 1.5 SDs of its mean value?
(b) Farther than 2.5 SDs from its mean value?
(c) Between 1 and 2 SDs from its mean value?
56. A machine that produces ball bearings has initially been set so that the true average diameter of
the bearings it produces is .500 in. A bearing is acceptable if its diameter is within .004 in. of this
target value. Suppose, however, that the setting has changed during the course of production, so
that the bearings have normally distributed diameters with mean value .499 in. and standard
deviation .002 in. What percentage of the bearings produced will not be acceptable?
57. The Rockwell hardness of a metal is determined by impressing a hardened point into the surface
of the metal and then measuring the depth of penetration of the point. Suppose the Rockwell
hardness of an alloy is normally distributed with mean 70 and standard deviation 3. (Rockwell
hardness is measured on a continuous scale.)
(a) If a specimen is acceptable only if its hardness is between 67 and 75, what is the probability
that a randomly chosen specimen has an acceptable hardness?
(b) If the acceptable range of hardness is (70 /C0c,7 0+ c), for what value of cwould 95% of all
specimens have acceptable hardness?
(c) If the acceptable range is as in part (a) and the hardness of each of ten randomly selected
specimens is independently determined, what is the expected number of acceptable
specimens among the ten?
(d) What is the probability that at most eight of ten independently selected specimens have a
hardness of less than 73.84? [ Hint:Y¼the number among the ten specimens with hardness
less than 73.84 is a binomial variable; what is p?]
58. The weight distribution of parcels sent in a certain manner is normal with mean value 12 lb and
standard deviation 3.5 lb. The parcel service wishes to establish a weight value cbeyond which
there will be a surcharge. What value of cis such that 99% of all parcels are at least 1 lb under the
surcharge weight?3.3 The Normal (Gaussian) Distribution 185
59. Suppose Appendix Table A.3 contained Φ(z) only for z/C210. Explain how you could still compute
(a) P(/C01.72/C20Z/C20/C0.55)
(b) P(/C01.72/C20Z/C20.55)
Is it necessary to tabulate Φ(z) for znegative? What property of the standard normal curve
justiﬁes your answer?
60. Chebyshev’s inequality (Sect. 3.2) states that for any number ksatisfying k/C211,P(|X/C0μ|/C21kσ)i s
no more than 1/ k2. Obtain this probability in the case of a normal distribution for k¼1, 2, and
3, and compare to Chebyshev’s upper bound.
61. Let Xdenote the number of ﬂaws along a 100-m reel of magnetic tape (an integer-valued
variable). Suppose Xhas approximately a normal distribution with μ¼25 and σ¼5. Use the
continuity correction to calculate the probability that the number of ﬂaws is
(a) Between 20 and 30, inclusive.
(b) At most 30. Less than 30.
62. Let Xhave a binomial distribution with parameters n¼25 and p. Calculate each of the following
probabilities using the normal approximation (with the continuity correction) for the cases p¼.5,
.6, and .8 and compare to the exact probabilities calculated from Appendix Table A.1.
(a) P(15/C20X/C2020)
(b) P(X/C2015)
(c) P(20/C20X)
63. Suppose that 10% of all steel shafts produced by a process are nonconforming but can be
reworked (rather than having to be scrapped). Consider a random sample of 200 shafts, and let
Xdenote the number among these that are nonconforming and can be reworked. What is the
(approximate) probability that Xis
(a) At most 30?
(b) Less than 30?
(c) Between 15 and 25 (inclusive)?
64. Suppose only 70% of all drivers in a state regularly wear a seat belt. A random sample of
500 drivers is selected. What is the probability that
(a) Between 320 and 370 (inclusive) of the drivers in the sample regularly wear a seat belt?
(b) Fewer than 325 of those in the sample regularly wear a seat belt? Fewer than 315?
65. In response to concerns about nutritional contents of fast foods, McDonald’s announced that it
would use a new cooking oil for its french fries that would decrease substantially trans fatty acid
levels and increase the amount of more beneﬁcial polyunsaturated fat. The company claimed that
97 out of 100 people cannot detect a difference in taste between the new and old oils. Assuming
that this ﬁgure is correct (as a long-run proportion), what is the approximate probability that in a
random sample of 1000 individuals who have purchased fries at McDonald’s,
(a) At least 40 can taste the difference between the two oils?
(b) At most 5% can taste the difference between the two oils?
66. The following proof that the normal pdf integrates to 1 comes courtesy of Professor Robert
Young, Oberlin College. Let f(z) denote the standard normal pdf, and consider the function of two
variables
gx,yðÞ ¼ fðxÞ/C1fðyÞ¼1ﬃﬃﬃﬃﬃ
2πp e/C0x2=21ﬃﬃﬃﬃﬃ
2πp e/C0y2=2¼1
2πe/C0x2þy2ðÞ =2
LetVdenote the volume under g(x, y) above the xy-plane.
(a) Let Adenote the area under the standard normal curve. By setting up the double integral for
the volume underneath g(x, y), show that V¼A2.186 3 Continuous Random Variables and Probability Distributions
(b) Using the rotational symmetry of g(x, y),Vcan be determined by adding up the volumes of
shells from rotation about the y-axis:
V¼ð1
02πr/C11
2πe/C0r2=2dr
Show this integral equals 1, then use (a) to establish that the area under the standard normal
curve is 1.
(c) Show thatÐ1
/C01f(x;μ,σ)dx¼1. [Hint: Write out the integral, and then make a substitution to
reduce it to the standard normal case. Then invoke (b).]
67. Suppose X/C24N(μ,σ).
(a) Show via integration that E(X)¼μ.[Hint: Make the substitution u¼(x/C0μ)/σ, which will
create two integrals. For one, use the symmetry of the pdf; for the other, use the fact that the
standard normal pdf integrates to 1.]
(b) Show via integration that Var( X)¼σ2.[Hint: Evaluate the integral for E[(X/C0μ)2] rather
than using the variance shortcut formula. Use the same substitution as in part (a).]
68. The moment generating function can be used to ﬁnd the mean and variance of the normal
distribution.
(a) Use derivatives of MX(t) to verify that E(X)¼μand Var( X)¼σ2.
(b) Repeat (a) using LX(t)¼ln[MX(t)], and compare with part (a) in terms of effort. (Refer back
to Exercise 36 for properties of the function LX(t).)
69. There is no nice formula for the standard normal cdf Φ(z), but several good approximations have
been published in articles. The following is from “Approximations for Hand Calculators Using
Small Integer Coefﬁcients” ( Math. Comput. , 1977: 214–222). For 0 <z/C205.5,
PZ/C21zðÞ ¼ 1/C0ΦðzÞ/C25:5exp /C083zþ351 ðÞ zþ562
703=z ðÞ þ 165/C20/C21/C26/C27
The relative error of this approximation is less than .042%. Use this to calculate approximations
to the following probabilities, and compare whenever possible to the probabilities obtained from
Appendix Table A.3.
(a) P(Z/C211)
(b) P(Z</C03)
(c) P(/C04<Z<4)
(d) P(Z>5)
70. (a) Use mgfs to show that if Xhas a normal distribution with parameters μandσ, then Y¼aX+
b(a linear function of X) also has a normal distribution. What are the parameters of the
distribution of Y[i.e., E(Y) and SD( Y)]?
(b) If when measured in/C14C, temperature is normally distributed with mean 115 and standard
deviation 2, what can be said about the distribution of temperature measured in/C14F?
3.4 The Exponential and Gamma Distributions
The graph of any normal pdf is bell-shaped and thus symmetric. In many practical situations, the
variable of interest to the experimenter might have a skewed distribution. A family of pdfs that yields
a wide variety of skewed distributional shapes is the gamma family. We ﬁrst consider a special case,
the exponential distribution, and then generalize later in the section.3.4 The Exponential and Gamma Distributions 187
3.4.1 The Exponential Distribution
The family of exponential distributions provides probability models that are widely used in engineer-
ing and science disciplines.
DEFINITION
Xis said to have an exponential distribution with parameter λ(λ>0) if the pdf of Xis
fx ;λðÞ ¼λe/C0λxx>0
0 otherwise(
Some sources write the exponential pdf in the form (1/ β)e/C0x/β, so that β¼1/λ. Graphs of several
exponential pdfs appear in Fig. 3.24.
The expected value of an exponentially distributed random variable Xis
EðXÞ¼ð1
0x/C1λe/C0λxdx
Obtaining this expected value requires integration by parts. The variance of Xcan be computed
using the shortcut formula Var( X)¼E(X2)/C0[E(X)]2; evaluating E(X2) uses integration by parts
twice in succession. In contrast, the exponential cdf is easily obtained by integrating the pdf. The
results of these integrations are as follows.
PROPOSITION
LetXbe an exponential variable with parameter λ. Then the cdf of Xis012345678f(x;λ)
2
1.5
1
.5
0 xλ = 2
λ = 1λ = .5Fig. 3.24 Exponential
density curves188 3 Continuous Random Variables and Probability Distributions
Fx ;λðÞ ¼0 x/C200
1/C0e/C0λxx>0(
The mean and standard deviation of Xare both equal to 1/ λ.
Under the alternative parameterization, the exponential cdf becomes 1 /C0e/C0x/βforx>0, and the
mean and standard deviation are both equal to β.
Example 3.23 The response time Xat an on-line computer terminal (the elapsed time between the
end of a user’s inquiry and the beginning of the system’s response to that inquiry) has an exponential
distribution with expected response time equal to 5 s. Then E(X)¼1/λ¼5, so λ¼.2. The probability
that the response time is at most 10 s is
PX/C2010ðÞ ¼ F10;2ðÞ ¼ 1/C0e/C0ð:2Þð10Þ¼1/C0e/C02¼1/C0:135¼:865
The probability that response time is between 5 and 10 s is
P5/C20X/C2010 ðÞ ¼ F10;2ðÞ /C0 F5;2ðÞ ¼ 1/C0e/C02/C0/C1
/C01/C0e/C01/C0/C1
¼:233 ■
The exponential distribution is frequently used as a model for the distribution of times between the
occurrence of successive events, such as customers arriving at a service facility or calls coming in to a
call center. The reason for this is that the exponential distribution is closely related to the Poisson
distribution introduced in Chap. 2. We will explore this relationship fully in Sect. 7.5(Poisson
Processes).
Another important application of the exponential distribution is to model the distribution of
component lifetimes. A partial reason for the popularity of such applications is the “ memoryless ”
property of the exponential distribution. Suppose component lifetime is exponentially distributed
with parameter λ. After putting the component into service, we leave for a period of t0hours and then
return to ﬁnd the component still working; what now is the probability that it lasts at least an
additional thours? In symbols, we wish P(X/C21t+t0|X/C21t0). By the deﬁnition of conditional
probability,
PX/C21tþt0jX/C21t0 ðÞ ¼PX/C21tþt0 ðÞ \ X/C21t0 ðÞ ½/C138
PX/C21t0 ðÞ
But the event X/C21t0in the numerator is redundant, since both events can occur if and only if
X/C21t+t0. Therefore,
PX/C21tþt0jX/C21t0 ðÞ ¼PX/C21tþt0 ðÞ
PX/C21t0 ðÞ¼1/C0Ftþt0;λ ðÞ
1/C0Ft0;λðÞ¼e/C0λtþt0ðÞ
e/C0λt0¼e/C0λt
This conditional probability is identical to the original probability P(X/C21t) that the component
lasted thours. Thus the distribution of additional lifetime is exactly the same as the original
distribution of lifetime , so at each point in time the component shows no effect of wear. In other
words, the distribution of remaining lifetime is independent of current age (we wish that were true
of us!).
Although the memoryless property can be justiﬁed at least approximately in many applied
problems, in other situations components deteriorate with age or occasionally improve with age
(at least up to a certain point). More general lifetime models are then furnished by the gamma,3.4 The Exponential and Gamma Distributions 189
Weibull, and lognormal distributions (the latter two are discussed in the next section). Lifetime
distributions are at the heart of reliability models, which we’ll consider in depth in Sect. 4.8.
3.4.2 The Gamma Distribution
To deﬁne the family of gamma distributions, which generalizes the exponential distribution, we ﬁrst
need to introduce a function that plays an important role in many branches of mathematics.
DEFINITION
Forα>0, the gamma function Γ(α) is deﬁned by
ΓαðÞ ¼ð1
0xα/C01e/C0xdx
The most important properties of the gamma function are the following:
1. For any α>1,Γ(α)¼(α/C01)/C1Γ(α/C01) (via integration by parts)
2. For any positive integer n,Γ(n)¼(n/C01)!
3.Γ1
2/C0/C1
¼ﬃﬃﬃπp
The following proposition will prove useful for several computations that follow.
PROPOSITION
For any α,β>0,
ð1
0xα/C01e/C0x=βdx¼βαΓαðÞ ð 3:5Þ
Proof Make the substitution u¼x/β, so that x¼βuanddx¼βdu:
ð1
0xα/C01e/C0x=βdx¼ð1
0βuðÞα/C01e/C0uβdu¼βαð1
0uα/C01e/C0udu¼βαΓαðÞ
The last equality comes from the deﬁnition of the gamma function. ■
With the preceding proposition in mind, we make the following deﬁnition.
DEFINITION
A continuous random variable Xis said to have a gamma distribution if the pdf of Xis
fx ;α,βðÞ ¼1
βαΓαðÞxα/C01e/C0x=βx>0
0 otherwise8
<
:ð3:6Þ
where the parameters αandβsatisfy α>0,β>0. When β¼1,Xis said to have a standard
gamma distribution , and its pdf may be denoted f(x;α).190 3 Continuous Random Variables and Probability Distributions
The exponential distribution results from taking α¼1 and β¼1/λ.
It’s clear that f(x;α,β)/C210 for all x; the previous proposition guarantees that this function
integrates to 1, as required. Figure 3.25a illustrates the graphs of the gamma pdf for several ( α,β)
pairs, whereas Fig. 3.25b presents graphs of the standard gamma pdf. For the standard pdf, when
α/C201,f(x;α) is strictly decreasing as xincreases; when α>1,f(x;α) rises to a maximum and then
decreases. Because of this difference, αis referred to as a shape parameter . The parameter βin
Eq. ( 3.6) is called the scale parameter because values other than 1 either stretch or compress the pdf
in the xdirection.
The mean and variance of a gamma random variable are
EðXÞ¼μ¼αβ VarðXÞ¼σ2¼αβ2
These can be calculated directly from the gamma pdf using integration by parts, or by employing
properties of the gamma function along with Expression ( 3.5); see Exercise 83. Notice these are
consistent with the aforementioned mean and variance of the exponential distribution: with α¼1 and
β¼1/λwe obtain E(X)¼1(1/λ)¼1/λand Var( X)¼1(1/λ)2¼1/λ2.
In the special case where the shape parameter αis a positive integer, n, the gamma distribution is
sometimes rewritten with the substitution λ¼1/β, and the resulting pdf is
fx ;n,1=λ ðÞ ¼λn
n/C01ðÞ !xn/C01e/C0λx, x>0
This is often called an Erlang distribution , and it plays a central role in the study of Poisson
processes (again, see Sect. 7.5; notice that the n¼1 case of the Erlang distribution is actually the
exponential pdf). In Chap. 4, it will be shown that the sum of nindependent exponential rvs follows
this Erlang distribution.
When Xis a standard gamma rv, the cdf of X, which for x>0i s
Gx ;αðÞ ¼ PX/C20xðÞ ¼ðx
01
ΓαðÞyα/C01e/C0ydy ð3:7Þ
is called the incomplete gamma function. (In mathematics literature, the incomplete gamma
function sometimes refers to Eq. ( 3.7) without the denominator Γ(α) in the integrand.) In Appendix
Table A.4, we present a small tabulation of G(x;α) for α¼1, 2, ..., 10 and x¼1, 2, ..., 15.12345 123456700.51.0 1
3b a
x00.51.0
xf(x; a, b) f(x; a)
a = 1a = 2,b =
a = 1, b = 1
a = 2, b = 2
a = 2, b = 1a = .6
a = 2a = 5 
Fig. 3.25 (a) Gamma density curves; ( b) standard gamma density curves3.4 The Exponential and Gamma Distributions 191
Table 3.2at the end of this section provides the Matlab and R commands related to the gamma cdf,
which are illustrated in the following examples.
Example 3.27 Suppose the reaction time X(in seconds) of a randomly selected individual to a
certain stimulus has a standard gamma distribution with α¼2. Since Xis continuous,
P3/C20X/C205 ðÞ ¼ PX/C205ðÞ /C0 PX/C203ðÞ ¼ G5;2ðÞ /C0 G3;2ðÞ
¼:960/C0:801¼:159
This probability can be obtained in Matlab with gamcdf(5,2,1)-gamcdf(3,2,1) and in R
withpgamma(5,2)-pgamma(3,2) .
The probability that the reaction time is more than 4 s is
PX>4ðÞ ¼ 1/C0PX/C204ðÞ ¼ 1/C0G4;2ðÞ ¼ 1/C0:908¼:092 ■
The incomplete gamma function can also be used to compute probabilities involving gamma
distributions for any β>0.
PROPOSITION
LetXhave a gamma distribution with parameters αandβ. Then for any x>0, the cdf of Xis
given by
PX/C20xðÞ ¼ Gx
β;α/C18/C19
,
the incomplete gamma function evaluated at x/β.
The proof is similar to that of Eq. ( 3.5).
Example 3.28 Suppose the survival time Xin weeks of a randomly selected male mouse exposed to
240 rads of gamma radiation has, rather ﬁttingly, a gamma distribution with α¼8 and β¼15. (Data
inSurvival Distributions: Reliability Applications in the Biomedical Services , by A. J. Gross and
V. Clark, suggest α/C258.5 and β/C2513.3.) The expected survival time is E(X)¼(8)(15) ¼120 weeks,
whereas SD ðXÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð8Þð15Þ2q
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1800p
¼42:43 weeks. The probability that a mouse survives
between 60 and 120 weeks is
P60/C20X/C20120 ðÞ ¼ PX/C20120ðÞ /C0 PX/C2060ðÞ
¼G120=15;8 ðÞ /C0 G60=15;8 ðÞ
¼G8;8ðÞ /C0 G4;8ðÞ ¼ :547/C0:051¼:496
In Matlab, the command gamcdf(120,8,15)-gamcdf(60,8,15) yields the desired prob-
ability; the corresponding R code is pgamma(120,8,1/15)-pgamma(60,8,1/15) .
The probability that a mouse survives at least 30 weeks is
PX/C2130ðÞ ¼ 1/C0PX<30ðÞ ¼ 1/C0PX/C2030ðÞ ¼ 1/C0G30=15;8 ðÞ ¼ :999 ■192 3 Continuous Random Variables and Probability Distributions
3.4.3 The Gamma MGF
The integral proposition earlier in this section makes it easy to determine the mean and variance of a
gamma rv. However, the moment generating function of the gamma distribution — and, as a special
case, of the exponential model — will prove critical in establishing some of the more advanced
properties of these distributions in Chap. 4.
Proposition
The moment generating function of a gamma random variable is
MXðtÞ¼1
1/C0βt ðÞα t<1=β
Proof By deﬁnition, the mgf is
MXðtÞ¼EetX/C0/C1
¼ð1
0etxxα/C01
ΓαðÞβαe/C0x=βdx¼1
ΓαðÞβαð1
0xα/C01e/C0/C0tþ1=β ðÞ xdx
Now use Expression ( 3.5): provided /C0t+1 /β>0, i.e., t<1/β,
1
ΓαðÞβαð1
0xα/C01e/C0/C0tþ1=β ðÞ xdx¼1
ΓαðÞβα/C1ΓαðÞ1
/C0tþ1=β/C18/C19α
¼1
1/C0βt ðÞα■
The exponential mgf can then be determined with the substitution α¼1,β¼1/λ:
MXðtÞ¼1
1/C01=λðÞ t ðÞ1¼λ
λ/C0tt<λ
3.4.4 Gamma and Exponential Calculations with Software
Table 3.2summarizes the syntax for gamma and exponential probability calculations in Matlab
and R, which follows the pattern of the other distributions. In a sense, the exponential commands are
redundant, since they are just a special case ( α¼1) of the gamma distribution.
Notice that Matlab and R parameterize the distributions differently: in Matlab, both the gamma
and exponential functions require β(that is, 1/ λ) as the last input, whereas the R functions take as their
last input the “rate” parameter λ¼1/β. So, for the gamma rv with parameters α¼8 and β¼15 from
Example 3.28, the probability P(X/C2030) would be evaluated as gamcdf(30,8,15) in Matlab but
pgamma(30,8,1/15) in R. This inconsistency of gamma inputs can be remedied by using a name
assignment in the last argument in R; speciﬁcally, pgamma(30,8,scale ¼15) will instruct R to
Table 3.2 Matlab and R code for gamma and exponential calculations
Gamma Exponential
Function: cdf cdf
Notation: G(x/β;α) F(x;λ)¼1/C0e/C0λx
Matlab: gamcdf (x,α,β) expcdf (x,1 /λ)
R: pgamma (x,α,1 /β) pexp (x,λ)3.4 The Exponential and Gamma Distributions 193
useβ¼15 in its gamma probability calculation and produce the same answer as the previous
expressions. Interestingly, as of this writing the same option does not exist in the pexp function.
To graph gamma or exponential distributions, one can request their pdfs by replacing cdf with
pdf (in Matlab) or the leading letter pwithd(in R). To ﬁnd quantiles of either of these distributions,
the appropriate replacements are inv andq, respectively. For example, the 75th percentile of the
gamma distribution from Example 3.28 can be determined with gaminv(.75,8,15) in Matlab or
qgamma(.75,8,scale ¼15) in R (both give 145.2665 weeks).
3.4.5 Exercises: Section 3.4(71–83)
71. Let X¼the time between two successive arrivals at the drive-up window of a local bank. If
Xhas an exponential distribution with λ¼1, compute the following:
(a) The expected time between two successive arrivals
(b) The standard deviation of the time between successive arrivals
(c) P(X/C204)
(d) P(2/C20X/C205)
72. Let Xdenote the distance (m) that an animal moves from its birth site to the ﬁrst territorial
vacancy it encounters. Suppose that for banner-tailed kangaroo rats, Xhas an exponential
distribution with parameter λ¼.01386 (as suggested in the article “Competition and Dispersal
from Multiple Nests,” Ecology , 1997: 873–883).
(a) What is the probability that the distance is at most 100 m? At most 200 m? Between
100 and 200 m?
(b) What is the probability that distance exceeds the mean distance by more than 2 standard
deviations?
(c) What is the value of the median distance?
73. In studies of anticancer drugs it was found that if mice are injected with cancer cells, the survival
time can be modeled with the exponential distribution. Without treatment the expected survival
time was 10 h. What is the probability that
(a) A randomly selected mouse will survive at least 8 h? At most 12 h? Between 8 and 12 h?
(b) The survival time of a mouse exceeds the mean value by more than 2 standard deviations?
More than 3 standard deviations?
74. Data collected at Toronto Pearson International Airport suggests that an exponential distribution
with mean value 2.725 h is a good model for rainfall duration ( Urban Stormwater Management
Planning with Analytical Probabilistic Models , 2000, p.69).
(a) What is the probability that the duration of a particular rainfall event at this location is at
least 2 h? At most 3 h? Between 2 and 3 h?
(b) What is the probability that rainfall duration exceeds the mean value by more than
2 standard deviations? What is the probability that it is less than the mean value by more
than one standard deviation?
75. Evaluate the following:
(a)Γ(6)
(b)Γ(5/2)
(c) G(4; 5) (the incomplete gamma function)
(d) G(5; 4)
(e) G(0; 4)194 3 Continuous Random Variables and Probability Distributions
76. Let Xhave a standard gamma distribution with α¼7. Evaluate the following:
(a) P(X/C205)
(b) P(X<5)
(c) P(X>8)
(d) P(3/C20X/C208)
(e) P(3<X<8)
(f)P(X<4o rX>6)
77. Suppose that when a type of transistor is subjected to an accelerated life test, the lifetime
X(in weeks) has a gamma distribution with mean 24 weeks and standard deviation 12 weeks.
(a) What is the probability that a transistor will last between 12 and 24 weeks?
(b) What is the probability that a transistor will last at most 24 weeks? Is the median of the
lifetime distribution less than 24? Why or why not?
(c) What is the 99th percentile of the lifetime distribution?
(d) Suppose the test will actually be terminated after tweeks. What value of tis such that only
.5% of all transistors would still be operating at termination?
78. The two-parameter gamma distribution can be generalized by introducing a third parameter γ,
called a threshold orlocation parameter: replace xin Eq. ( 3.6)b yx/C0γandx/C210b y x/C21γ.T h i s
amounts to shifting the density curves in Fig. 3.25 so that they begin their ascent or descent at γ
rather than 0. The article “Bivariate Flood Frequency Analysis with Historical Information
Based on Copulas” ( J. of Hydrologic Engr. , 2013: 1018–1030) employs this distribution to
model X¼3-day ﬂood volume (108m3). Suppose that values of the parameters are α¼12,β¼
7,γ¼40 (very close to estimates in the cited article based on past data).
(a) What are the mean value and standard deviation of X?
(b) What is the probability that ﬂood volume is between 100 and 150?
(c) What is the probability that ﬂood volume exceeds its mean value by more than one
standard deviation?
(d) What is the 95th percentile of the ﬂood volume distribution?
79. If Xhas an exponential distribution with parameter λ, derive an expression for the (100 p)th
percentile of the distribution. Then specialize to obtain the median.
80. A system consists of ﬁve identical components connected in series as shown:
12345
As soon as one component fails, the entire system will fail. Suppose each component has a
lifetime that is exponentially distributed with λ¼.01 and that components fail independently of
one another. Deﬁne events Ai¼{ith component lasts at least thours}, i¼1,..., 5, so that the Ais
are independent events. Let X¼the time at which the system fails—that is, the shortest
(minimum) lifetime among the ﬁve components.
(a) The event { X/C21t} is equivalent to what event involving A1,...,A5?
(b) Using the independence of the ﬁve Ais, compute P(X/C21t). Then obtain F(t)¼P(X/C20t) and
the pdf of X. What type of distribution does Xhave?
(c) Suppose there are ncomponents, each having exponential lifetime with parameter λ. What
type of distribution does Xhave?
81. Based on an analysis of sample data, the article “Pedestrians’ Crossing Behaviors and Safety at
Unmarked Roadways in China” ( Accident Analysis and Prevention , 2011: 1927–1936) pro-
posed the pdf f(x)¼.15e/C0.15(x/C01)when x/C211 as a model for the distribution of X¼time (sec)
spent at the median line. This is an example of a shifted exponential distribution, i.e., an
exponential model beginning at an x-value other than 0.3.4 The Exponential and Gamma Distributions 195
(a) What is the probability that waiting time is at most 5 s? More than 5 s?
(b) What is the probability that waiting time is between 2 and 5 s?
(c) What is the mean waiting time?
(d) What is the standard deviation of waiting times?
[Hint: For (c) and (d), you can either use integration or write X¼Y+ 1, where Yhas an
exponential distribution with parameter λ¼.15. Then, apply rescaling properties of mean
and standard deviation.]
82. The double exponential distribution has pdf
fðxÞ¼:5λe/C0λxjjfor/C01 <x<1
The article “Microwave Observations of Daily Antarctic Sea-Ice Edge Expansion and Contri-
bution Rates” ( IEEE Geosci. and Remote Sensing Letters , 2006: 54-58) states that “the
distribution of the daily sea-ice advance/retreat from each sensor is similar and is approximately
double exponential.” The standard deviation is given as 40.9 km.
(a) What is the mean of a random variable with pdf f(x)? [Hint: Draw a picture of the
density curve.]
(b) What is the value of the parameter λwhen σX¼40.9?
(c) What is the probability that the extent of daily sea-ice change is within 1 standard deviation
of the mean value?
83. (a) Find the mean and variance of the gamma di stribution using integ ration and Expression
(3.5)t oo b t a i n E(X)a n d E(X2).
(b) Use the gamma mgf to ﬁnd the mean and variance.
3.5 Other Continuous Distributions
The normal, gamma (including exponential), and uniform families of distributions provide a wide
variety of probability models for continuous variables, but there are many practical situations in
which no member of these families ﬁts a set of observed data very well. Statisticians and other
investigators have developed other families of distributions that are often appropriate in practice.
3.5.1 The Weibull Distribution
The family of Weibull distributions was introduced by the Swedish physicist Waloddi Weibull in
1939; his 1951 article “A Statistical Distribution Function of Wide Applicability” ( J. Appl. Mech ., 18:
293–297) discusses a number of applications.
DEFINITION
A random variable Xis said to have a Weibull distribution with parameters αandβ(α>0,β>0)
if the pdf of Xis
fx ;α,βðÞ ¼α
βαxα/C01e/C0x=βðÞαx/C210
0 x<08
<
:ð3:8Þ196 3 Continuous Random Variables and Probability Distributions
In some situations there are theoretical justiﬁcations for the appropriateness of the Weibull
distribution, but in many applications f(x;α,β) simply provides a good ﬁt to observed data for
particular values of αandβ. When α¼1, the pdf reduces to the exponential distribution (with λ¼
1/β), so the exponential distribution is a special case of both the gamma and Weibull distributions.
However, there are gamma distributions that are not Weibull distributions and vice versa, so one
family is not a subset of the other. Both αand βcan be varied to obtain a number of different
distributional shapes, as illustrated in Fig. 3.26. Note that βis a scale parameter, so different values
stretch or compress the graph in the x-direction; αis referred to as a shape parameter. Integrating to
obtain E(X) and E(X2) yields
μ¼βΓ1þ1
α/C18/C19
σ2¼β2Γ1þ2
α/C18/C19
/C0Γ1þ1
α/C18/C19/C20/C212()
The computation of μand σ2thus necessitate using the gamma function from Sect. 3.4. (The
moment generating function of the Weibull distribution is very complicated, and so we do not include
it here.) On the other hand, the integrationÐx
0f(y;α,β)dyis easily carried out to obtain the cdf of X:.51
0 5 10
2
0468
.5 0 1.0 1.5 2.0 2.5x
xa = 10, b = .5a = 1, b = 1 (exponential)
a = 2, b = 1
a = 2, b = .5
a = 10, b = 1
a = 10, b = 2f(x; a, b)
f(x; a, b)Fig. 3.26 Weibull density
curves3.5 Other Continuous Distributions 197
Fx ;α,βðÞ ¼0 x<0
1/C0e/C0x=βðÞαx/C210/C26
ð3:9Þ
Example 3.29 In recent years the Weibull distribution has been used to model engine emissions of
various pollutants. Let Xdenote the amount of NO xemission (g/gal) from a randomly selected four-
stroke engine of a certain type, and suppose that Xhas a Weibull distribution with α¼2 and
β¼10 (suggested by information in the article “Quantiﬁcation of Variability and Uncertainty in
Lawn and Garden Equipment NO xand Total Hydrocarbon Emission Factors,” J. Air Waste Manag.
Assoc ., 2002: 435–448). The corresponding density curve looks exactly like the one in Fig. 3.26 for
α¼2,β¼1 except that now the values 50 and 100 replace 5 and 10 on the horizontal axis (because β
is a “scale parameter”). Then
PX/C2010ðÞ ¼ F10;2, 10ðÞ ¼ 1/C0e/C010=10ðÞ2¼1/C0e/C01¼:632
Similarly, P(X/C2025)¼.998, so the distribution is almost entirely concentrated on values between
0 g/gal and 25 g/gal. The value cwhich separates the 5% of all engines having the largest amounts of
NO xemissions from the remaining 95%, satisﬁes
:95¼Fc ;2, 10ðÞ ¼ 1/C0e/C0c=10ðÞ2
Isolating the exponential term on one side, taking logarithms, and solving the resulting equation
gives c/C2517.3 g/gal as the 95th percentile of the emission distribution. ■
Frequently, in practical situations, a Weibull model may be reasonable except that the smallest
possible Xvalue may be some value γother than zero (Exercise 78 considered this for a gamma
model). The quantity γcan then be regarded as a third parameter of the distribution, which is what
Weibull did in his original work. For, say, γ¼3, all curves in Fig. 3.26 would be shifted 3 units to the
right. This is equivalent to saying that X/C0γhas the pdf Eq. ( 3.8), so that the cdf of Xis obtained by
replacing xin Eq. ( 3.9)b yx/C0γ.
Example 3.30 An understanding of the volumetric properties of asphalt is important in designing
mixtures that will result in high-durability pavement. The article “Is a Normal Distribution the Most
Appropriate Statistical Distribution for Volumetric Properties in Asphalt Mixtures” ( J. of Testing and
Evaluation , Sept. 2009: 1–11) used the analysis of some sample data to recommend that for a
particular mixture, X¼air void volume (%) be modeled with a three-parameter Weibull distribution.
Suppose the values of the parameters are γ¼4,α¼1.3, and β¼.8, which are quite close to estimates
given in the article.
Forx/C214, the cumulative distribution function is
Fx ;α,β,γ ðÞ ¼ Fx ;1:3,:8, 4 ðÞ ¼ 1/C0e/C0x/C04ðÞ =:8 ½/C1381:3
The probability that the air void volume of a specimen is between 5% and 6% is
P5/C20X/C206 ðÞ ¼ F6;1:3,:8, 4 ðÞ /C0 F5;1:3,:8, 4 ðÞ ¼ e/C05/C04ðÞ =:8 ½/C1381:3/C0e/C06/C04ðÞ =:8 ½/C1381:3
¼:263/C0:037¼:226
Figure 3.27 shows a graph of the corresponding Weibull density function, in which the shaded area
corresponds to the probability just calculated.198 3 Continuous Random Variables and Probability Distributions
3.5.2 The Lognormal Distribution
Lognormal distributions have been used extensively in engineering, medicine, and more recently,
ﬁnance.
DEFINITION
A nonnegative rv Xis said to have a lognormal distribution if the rv Y¼ln(X) has a normal
distribution. The resulting pdf of a lognormal rv when ln( X) is normally distributed with
parameters μandσis
fx ;μ;σðÞ ¼1ﬃﬃﬃﬃﬃ
2πp
σxe/C0lnxðÞ /C0 μ ½/C1382=2σ2ðÞx/C210
0 x<08
<
:
Be careful here: the parameters μandσare not the mean and standard deviation of Xbut of ln( X).
The mean and variance of a lognormal random variable can be shown to be
EðXÞ¼eμþσ2=2VarðXÞ¼e2μþσ2/C1eσ2/C01/C16/C17
In Chap. 4, we will present a theoretical justiﬁcation for this distribution in connection with the
Central Limit Theorem, but as with other distributions, the lognormal can be used as a model even in
the absence of such justiﬁcation. Figure 3.28 illustrates graphs of the lognormal pdf; although a
normal curve is symmetric, a lognormal curve has a positive skew.
Because ln( X) has a normal distribution, the cdf of Xcan be expressed in terms of the cdf Φ(z)o fa
standard normal rv Z.F o r x/C210,
Fx ;μ,σðÞ ¼ PX/C20xðÞ ¼ PlnXðÞ /C20 lnxðÞ ½/C138 ¼ PlnðXÞ/C0μ
σ/C20lnðxÞ/C0μ
σ/C20/C21
¼PZ/C20lnðxÞ/C0μ
σ/C20/C21
¼ΦlnðxÞ/C0μ
σ/C20/C21 ð3:10Þ
Differentiating F(x;μ,σ) with respect to xgives the pdf f(x;μ,σ) above.56 4Shaded area = .226
0.00.10.20.30.40.50.60.70.80.9f(x)
x
Fig. 3.27 Weibull density curve with threshold ¼4, shape ¼1.3, scale ¼.8 ■3.5 Other Continuous Distributions 199
Example 3.31 According to the article “Predictive Model for Pitting Corrosion in Buried Oil and
Gas Pipelines” ( Corrosion , 2009: 332–342), the lognormal distribution has been reported as the best
option for describing the distribution of maximum pit depth data from cast iron pipes in soil. The
authors suggest that a lognormal distribution with μ¼.353 and σ¼.754 is appropriate for maximum
pit depth (mm) of buried pipelines. For this distribution, the mean value and variance of pit depth are
EðXÞ¼e:353þð:754Þ2=2¼e:6383¼1:893
VarðXÞ¼e2ð:353Þþð:754Þ2/C1eð:754Þ2/C01/C16/C17
¼ð3:57697 Þð:765645 Þ¼2:7387
The probability that maximum pit depth is between 1 and 2 mm is
P1/C20X/C202 ðÞ ¼ Plnð1Þ/C20lnXðÞ /C20 ln 2ðÞ ðÞ ¼ P0/C20lnXðÞ /C20 :693 ðÞ
¼P0/C0:353
:754/C18/C19
/C20Z/C20:693/C0:353
:754/C18/C19
¼Φð:45Þ/C0Φ/C0:47ðÞ ¼ :354
This probability is illustrated in Fig. 3.29.
What value cis such that only 1% of all specimens have a maximum pit depth exceeding c? The
desired value satisﬁes.05
0.10.15.20.25
0 5 10 15 20 25x√3m = 1, s = 1
m = 3, s = 1m = 3, s =f(x; m, s) Fig. 3.28 Lognormal
density curves
012Shaded area = .354
0.00.10.20.30.40.5f(x)
xFig. 3.29 Lognormal
density curve with μ¼.353
andσ¼.754200 3 Continuous Random Variables and Probability Distributions
:99¼PX/C20cðÞ ¼ ΦlnðcÞ/C0:353
:754/C18/C19
Appendix Table A.3 indicates that z¼2.33 is the 99th percentile of the standard normal
distribution, which implies that
lnðcÞ/C0:353
:754¼2:33
Solving for cgives ln( c)¼2.1098 and c¼8.247. Thus 8.247 mm is the 99th percentile of the
maximum pit depth distribution. ■
As with the Weibull distribution, a third parameter γcan be introduced so that the distribution has
positive density for x>γrather than for x>0.
3.5.3 The Beta Distribution
All families of continuous distributions discussed so far except for the uniform distribution have
positive density over an inﬁnite interval (although typically the density function decreases rapidly to
zero beyond a few standard deviations from the mean). The beta distribution provides positive density
only for Xin an interval of ﬁnite length.
DEFINITION
A random variable Xis said to have a beta distribution with parameters α,β(both positive), A,
andBif the pdf of Xis
fx ;α,β,A,B ðÞ ¼1
B/C0A/C1ΓαþβðÞ
ΓαðÞ /C1ΓβðÞx/C0A
B/C0A/C18/C19α/C01B/C0x
B/C0A/C18/C19β/C01
A/C20x/C20B
0 otherwise8
<
:
The case A¼0,B¼1 gives the standard beta distribution .
Figure 3.30 illustrates several standard beta pdfs. Graphs of the general pdf are similar, except they
are shifted and then stretched or compressed to ﬁt over [ A,B]. Unless αandβare integers, integration
.8 .6 .4 1 .212345
0a = b = .5a = 2
b = .5
a = 5
b = 2
xf(x; a, b) Fig. 3.30 Standard beta
density curves3.5 Other Continuous Distributions 201
of the pdf to calculate probabilities is difﬁcult, so either a table of the incomplete beta function or
software is generally used.
The standard beta distribution is commonly used to model variation in the proportion or percent-
age of a quantity occurring in different samples, such as the proportion of a 24-h day that an
individual is asleep or the proportion of a certain element in a chemical compound.
The mean and variance of Xare
μ¼AþB/C0AðÞ /C1α
αþβσ2¼B/C0AðÞ2αβ
αþβðÞ2αþβþ1 ðÞ
The moment generating function of the beta distribution is too complicated to be useful.
Example 3.32 Project managers often use a method labeled PERT—for program evaluation and
review technique—to coordinate the various activities making up a large project. (One successful
application was in the construction of the Apollo spacecraft.) A standard assumption in PERT analysis
is that the time necessary to complete any particular activity once it has been started has a beta
distribution with A¼the optimistic time (if everything goes well) and B¼the pessimistic time
(if everything goes badly). Suppose that in constructing a single-family house, the time X(in days)
necessary for laying the foundation has a beta distribution with A¼2,B¼5,α¼2, and β¼3. Then
α/(α+β)¼.4, so E(X)¼2 + (3)(.4) ¼3.2. For these values of αandβ, the pdf of Xis a simple
polynomial function. The probability that it takes at most 3 days to lay the foundation is
PX/C203ðÞ ¼ð3
21
3/C14!
1!/C12!x/C02
3/C18/C195/C0x
3/C18/C192
dx
¼4
27ð3
2x/C02ðÞ 5/C0xðÞ2dx¼4
27/C111
4¼11
27¼:407■
Software, including Matlab and R, can be used to perform probability calculations for the Weibull,
lognormal, and beta distributions. Interested readers should consult the help menus in those packages.
3.5.4 Exercises: Section 3.5(84–100)
84. The lifetime X(in hundreds of hours) of a type of transistor has a Weibull distribution with
parameters α¼2 and β¼3. Compute the following:
(a) E(X) and Var( X)
(b) P(X/C206)
(c) P(1.5/C20X/C206)
(This Weibull distribution is suggested as a model for time in service in “On the Assess-
ment of Equipment Reliability: Trading Data Collection Costs for Precision,” J. Engrg.
Manuf. , 1991: 105–109.)
85. The authors of the article “A Probabilistic Insulation Life Model for Combined Thermal-
Electrical Stresses” ( IEEE Trans. Electr. Insul ., 1985: 519–522) state that “the Weibull distri-
bution is widely used in statistical problems relating to aging of solid insulating materials
subjected to aging and stress.” They propose the use of the distribution as a model for time
(in hours) to failure of solid insulating specimens subjected to ac voltage. The values of the
parameters depend on the voltage and temperature; suppose α¼2.5 and β¼200 (values
suggested by data in the article).202 3 Continuous Random Variables and Probability Distributions
(a) What is the probability that a specimen’s lifetime is at most 250? Less than 250? More
than 300?
(b) What is the probability that a specimen’s lifetime is between 100 and 250?
(c) What value is such that exactly 50% of all specimens have lifetimes exceeding that value?
86. Let X¼the time (in 10/C01weeks) from shipment of a defective product until the customer
returns the product. Suppose that the minimum return time is γ¼3.5 and that the excess X/C03.5
over the minimum has a Weibull distribution with parameters α¼2 and β¼1.5 (see the article
“Practical Applications of the Weibull Distribution,” Indust. Qual. Control , 1964: 71–78).
(a) What is the cdf of X?
(b) What are the expected return time and variance of return time? [ Hint: First obtain both
E(X/C03.5) and Var( X/C03.5).]
(c) Compute P(X>5).
(d) Compute P(5/C20X/C208).
87. Let Xhave a Weibull distribution. Verify that μ¼βΓ(1 + 1/ α). [Hint: In the integral for E(X),
make the change of variable y¼(x/β)α, so that x¼βy1/α.]
88. (a) In Exercise 84, what is the median lifetime of such tubes? [ Hint: Use Expression ( 3.9).]
(b) In Exercise 86, what is the median return time?
(c) If Xhas a Weibull distribution with the cdf from Expression ( 3.9), obtain a general
expression for the (100 p)th percentile of the distribution.
(d) In Exercise 86, the company wants to refuse to accept returns after tweeks. For what value
oftwill only 10% of all returns be refused?
89. Let Xdenote the ultimate tensile strength (ksi) at /C0200/C14of a randomly selected steel specimen
of a certain type that exhibits “cold brittleness” at low temperatures. Suppose that Xhas a
Weibull distribution with α¼20 and β¼100.
(a) What is the probability that Xis at most 105 ksi?
(b) If specimen after specimen is selected, what is the long-run proportion having strength
values between 100 and 105 ksi?
(c) What is the median of the strength distribution?
90. The article “On Assessing the Accuracy of Offshore Wind Turbine Reliability-Based Design
Loads from the Environmental Contour Method” ( Intl. J. of Offshore and Polar Engr. , 2005:
132–140) proposes the Weibull distribution with α¼1.817 and β¼.863 as a model for 1-h
signiﬁcant wave height (m) at a certain site.
(a) What is the probability that wave height is at most .5 m?
(b) What is the probability that wave height exceeds its mean value by more than one standard
deviation?
(c) What is the median of the wave-height distribution?
(d) For 0 <p<1, give a general expression for the 100 pth percentile of the wave-height
distribution.
91. Nonpoint source loads are chemical masses that travel to the main stem of a river and its
tributaries in ﬂows that are distributed over relatively long stream reaches, in contrast to those
that enter at well-deﬁned and regulated points. The article “Assessing Uncertainty in Mass
Balance Calculation of River Nonpoint Source Loads” ( J. of Envir. Engr. , 2008: 247–258)
suggested that for a certain time period and location, nonpoint source load of total dissolved
solids could be modeled with a lognormal distribution having mean value 10,281 kg/day/km and
a coefﬁcient of variation CV ¼.40 (CV ¼σX/μX).
(a) What are the mean value and standard deviation of ln( X)?
(b) What is the probability that Xis at most 15,000 kg/day/km?3.5 Other Continuous Distributions 203
(c) What is the probability that Xexceeds its mean value, and why is this probability not .5?
(d) Is 17,000 the 95th percentile of the distribution?
92. The authors of the article “Study on the Life Distribution of Microdrills” ( J. of Engr. Manufac-
ture, 2002: 301-305) suggested that a reasonable probability model for drill lifetime was a
lognormal distribution with μ¼4.5 and σ¼.8.
(a) What are the mean value and standard deviation of lifetime?
(b) What is the probability that lifetime is at most 100?
(c) What is the probability that lifetime is at least 200? Greater than 200?
93. Use Equation ( 3.10) to write a formula for the median ηof the lognormal distribution. What is the
median for the load distribution of Exercise 91?
94. As in the case of the Weibull distribution, the lognormal distribution can be modiﬁed by the
introduction of a third parameter γsuch that the pdf is shifted to be positive only for x>γ. The
article cited in Exercise 46 suggested that a shifted lognormal distribution with shift ¼1.0, mean
value ¼2.16, and standard deviation ¼1.03 would be an appropriate model for the rv X¼
maximum-to-average depth ratio of a corrosion defect in pressurized steel.
(a) What are the values of μandσfor the proposed distribution?
(b) What is the probability that depth ratio exceeds 2?
(c) What is the median of the depth ratio distribution?
(d) What is the 99th percentile of the depth ratio distribution?
95. Sales delay is the elapsed time between the manufacture of a product and its sale. According to
the article “Warranty Claims Data Analysis Considering Sales Delay” ( Quality and Reliability
Engr. Intl. , 2013: 113–123), it is quite common for investigators to model sales delay using a
lognormal distribution. For a particular product, the cited article proposes this distribution with
parameter values μ¼2.05 and σ2¼.06 (here the unit for delay is months).
(a) What are the variance and standard deviation of delay time?
(b) What is the probability that delay time exceeds 12 months?
(c) What is the probability that delay time is within one standard deviation of its mean value?
(d) What is the median of the delay time distribution?
(e) What is the 99th percentile of the delay time distribution?
(f) Among 10 randomly selected such items, how many would you expect to have a delay time
exceeding 8 months?
96. The article “The Statistics of Phytotoxic Air Pollutants” ( J. Roy. Statist Soc ., 1989: 183–198)
suggests the lognormal distribution as a model for SO 2concentration above a forest. Suppose the
parameter values are μ¼1.9 and σ¼.9.
(a) What are the mean value and standard deviation of concentration?
(b) What is the probability that concentration is at most 10? Between 5 and 10?
97. What condition on αandβis necessary for the standard beta pdf to be symmetric?
98. Suppose the proportion Xof surface area in a randomly selected quadrat that is covered by a
certain plant has a standard beta distribution with α¼5 and β¼2.
(a) Compute E(X) and Var( X).
(b) Compute P(X/C20.2).
(c) Compute P(.2/C20X/C20.4).
(d) What is the expected proportion of the sampling region not covered by the plant?
99. Let Xhave a standard beta density with parameters αandβ.
(a) Verify the formula for E(X) given in the section.
(b) Compute E[(1/C0X)m]. IfXrepresents the proportion of a substance consisting of a particular
ingredient, what is the expected proportion that does not consist of this ingredient?204 3 Continuous Random Variables and Probability Distributions
100. Stress is applied to a 20-in. steel bar that is clamped in a ﬁxed position at each end. Let Y¼the
distance from the left end at which the bar snaps. Suppose Y/20 has a standard beta distribution
with E(Y)¼10 and Var ðYÞ¼100=7.
(a) What are the parameters of the relevant standard beta distribution?
(b) Compute P(8/C20Y/C2012).
(c) Compute the probability that the bar snaps more than 2 in. from where you expect it
to snap.
3.6 Probability Plots
An investigator will often have obtained a numerical sample consisting of nobservations and wish to
know whether it is plausible that this sample came from a population distribution of some particular
type (e.g., from a normal distribution). For one thing, many formal procedures from statistical
inference (Chap. 5) are based on the assumption that the population distribution is of a speciﬁed
type. The use of such a procedure is inappropriate if the actual underlying probability distribution
differs greatly from the assumed type. Additionally, understanding the underlying distribution can
sometimes give insight into the physical mechanisms involved in generating the data. An effective
way to check a distributional assumption is to construct what is called a probability plot . The basis
for our construction is a comparison between percentiles of the sample data and the corresponding
percentiles of the assumed underlying distribution.
3.6.1 Sample Percentiles
The details involved in constructing probability plots differ a bit from source to source. Roughly
speaking, sample percentiles are deﬁned in the same way that percentiles of a population distribution
are deﬁned. The sample 50th percentile (i.e., the sample median) should separate the smallest 50% of
the sample from the largest 50%, the sample 90th percentile should be such that 90% of the sample
lies below that value and 10% lies above, and so on. Unfortunately, we run into problems when we
actually try to compute the sample percentiles for a particular sample of nobservations. If, for
example, n¼10, then we can split off 20% or 30% of the data, but there is no value that will split off
exactly 23% of these ten observations. To proceed further, we need an operational deﬁnition of
sample percentiles (this is one place where different people and different software packages do
slightly different things).
Statistical convention states that when nis odd, the sample median is the middle value in the
ordered list of sample observations, for example, the sixth-largest value when n¼11. This amounts to
regarding the middle observation as being half in the lower half of the data and half in the upper half.
Similarly, suppose n¼10. Then if we call the third-smallest value the 25th percentile, we are
regarding that value as being half in the lower group (consisting of the two smallest observations) and
half in the upper group (the seven largest observations). This leads to the following general deﬁnition
of sample percentiles.3.6 Probability Plots 205
DEFINITION
Order the nsample observations from smallest to largest. Then the ith-smallest observation in
the list is taken to be the sample [100( i/C0.5)/n]th percentile .
For example, if n¼10, the percentages corresponding to the ordered sample observations are 100
(1/C0.5)/10 ¼5%, 100(2 /C0.5)/10 ¼15%, 25%, ..., and 100(10 /C0.5)/10 ¼95%. That is, the smallest
observation is designated the sample 5th percentile, the next-smallest value the sample 15th percen-
tile, and so on. All other percentiles could then be determined by interpolation, e.g., the sample 10th
percentile would then be halfway between the 5th percentile (smallest sample observation) and the
15th percentile (second smallest observation) of the n¼10 values. For the purposes of a probability
plot, such interpolation will not be necessary, because a probability plot will be based only on the
percentages 100( i/C0.5)/ncorresponding to the nsample observations.
3.6.2 A Probability Plot
We now wish to determine whether our sample data could plausibly have come from some particular
population distribution (e.g., a normal distribution with μ¼10 and σ¼3). If the sample was actually
selected from the speciﬁed distribution, the sample percentiles (ordered sample observations) should
be reasonably close to the corresponding population distribution percentiles. That is, for i¼1, 2, ...,
nthere should be reasonable agreement between the ith-smallest sample observation and the
theoretical [100( i/C0.5)/n]th percentile for the speciﬁed distribution. Consider the (sample percentile,
population percentile) pairs—that is, the pairs
ith smallest sample
observation,/C2
100i/C0:5ðÞ =n/C3
th percentile
of the population distribution/C18/C19
fori¼1, ...,n. Each such pair can be plotted as a point on a two-dimensional coordinate system. If
the sample percentiles are close to the corresponding population distribution percentiles, the ﬁrst
number in each pair will be roughly equal to the second number, and the plotted points will then fall
close to a 45/C14line. Substantial deviations of the plotted points from a 45/C14line suggest that the
assumed distribution might be wrong.
Example 3.33 The value of a physical constant is known to an experimenter. The experimenter
makes n= 10 independent measurements of this value using a measurement device and records the
resulting measurement errors (error = observed value /C0true value). These observations appear in the
accompanying table.
Percentage 5 15 25 35 45
Sample observation /C01.91 /C01.25 /C0.75 /C0.53 .20
zpercentile /C01.645 /C01.037 /C0.675 /C0.385 /C0.126
Percentage 55 65 75 85 95
Sample observation .35 .72 .87 1.40 1.56
zpercentile .126 .385 .675 1.037 1.645
Is it plausible that the random variable measurement error has a standard normal distribution? The
needed standard normal ( z) percentiles are also displayed in the table and were determined as follows:206 3 Continuous Random Variables and Probability Distributions
the 5th percentile of the distribution under consideration, N(0,1), is given by Φ(z) = .05. From
software or Appendix Table A.3, the solution is roughly z=/C01.645. The other nine population ( z)
percentiles were found in a similar fashion.
Thus the points in the probability plot are ( /C01.91,/C01.645), ( /C01.25,/C01.037), ..., and (1.56,1.645).
Figure 3.31 shows the resulting plot. Although the points deviate a bit from the 45/C14line, the
predominant impression is that this line ﬁts the points reasonably well. The plot suggests that the
standard normal distribution is a realistic probability model for measurement error.
An investigator is typically not interested in knowing whether a completely speciﬁed probability
distribution, such as the normal distribution with μ¼0 and σ¼1 or the exponential distribution with
λ¼.1, is a plausible model for the population distribution from which the sample was selected.
Instead, the investigator will want to know whether some member of a family of probability
distributions speciﬁes a plausible model—the family of normal distributions, the family of exponen-
tial distributions, the family of Weibull distributions, and so on. The values of the parameters of a
distribution are usually not speciﬁed at the outset. If the family of Weibull distributions is under
consideration as a model for lifetime data, the issue is whether there are anyvalues of the parameters
αandβfor which the corresponding Weibull distribution gives a good ﬁt to the data. Fortunately, it is
almost always the case that just one probability plot will sufﬁce for assessing the plausibility of an
entire family. If the plot deviates substantially from a straight line, but not necessarily the 45/C14line, no
member of the family is plausible.
To see why, let’s focus on a plot for checking normality. As mentioned earlier, such a plot can be
very useful in applied work because many formal statistical procedures are appropriate (i.e., give
accurate inferences) only when the population distribution is at least approximately normal. These
procedures should generally not be used if a normal probability plot shows a very pronounced
departure from linearity. The key to constructing an omnibus normal probability plot is the relation-
ship between standard normal ( z) percentiles and those for any other normal distribution, which was
presented in Sect. 3.3:
percentile for a
Nμ;σðÞ distribution¼ μþσ/C1corresponding zpercentile ðÞ−2
−2−1.5
−1.5−1
−1−0.5
−0.50
00.5
0.5Observed
value1
11.5
1.52
2z percentile
45° line
Fig. 3.31 Plots of pairs (observed value, zpercentile) for the data of Example 3.33 ■3.6 Probability Plots 207
If each sample observation were exactly equal to the corresponding N(μ,σ) percentile, then the
pairs (observation, μ+σ/C1[zpercentile]) would fall on the 45/C14line, y=x. But since μ+σzis itself a
linear function, the pairs (observation, zpercentile) would also fall on a straight line, just not the line
with slope 1 and y-intercept 0. (The latter pairs would pass through the line z=x/σ/C0μ/σ, but the
equation itself isn’t important.)
DEFINITION
A plot of the npairs
(ith-smallest observation, [100( i/C0.5)/n]thzpercentile)
on a two-dimensional coordinate system is called a normal probability plot . If the sample
observations are in fact drawn from a normal distribution then the points should fall close to a
straight line (although not necessarily a 45/C14line). Thus a plot for which the points fall close to
some straight line suggests that the assumption of a normal population distribution is plausible.
Example 3.34 The accompanying sample consisting of n¼20 observations on dielectric breakdown
voltage of a piece of epoxy resin appeared in the article “Maximum Likelihood Estimation in the
3-Parameter Weibull Distribution” ( IEEE Trans. Dielectrics Electr. Insul ., 1996: 43–55). Values of
(i/C0.5)/nfor which zpercentiles are needed are (1 /C0.5)/20 ¼.025, (2 /C0.5)/20 ¼.075, ..., and .975.
Observation 24.46 25.61 26.25 26.42 26.66 27.15 27.31 27.54 27.74 27.94
zpercentile /C01.96 /C01.44 /C01.15 /C0.93 /C0.76 /C0.60 /C0.45 /C0.32 /C0.19 /C0.06
Observation 27.98 28.04 28.28 28.49 28.50 28.87 29.11 29.13 29.50 30.88
zpercentile .06 .19 .32 .45 .60 .76 .93 1.15 1.44 1.96
Figure 3.32 shows the resulting normal probability plot. The pattern in the plot is quite straight,
indicating it is plausible that the population distribution of dielectric breakdown voltage is normal.
27 26 25 24 28 29 30 31−2−112
0
Voltagez percentile
Fig. 3.32 Normal probability plot for the dielectric breakdown voltage sample ■208 3 Continuous Random Variables and Probability Distributions
There is an alternative version of a normal probability plot in which the zpercentile axis is
replaced by a nonlinear probability axis. The scaling on this axis is constructed so that plotted points
should again fall close to a line when the sampled distribution is normal. Figure 3.33 shows such a
plot from Matlab, obtained using the normplot command, for the breakdown voltage data of
Example 3.34. The plot remains essentially the same, and it is just the labeling of the axis that
changes.
3.6.3 Departures from Normality
A nonnormal population distribution can often be placed in one of the following three categories:
1. It is symmetric and has “lighter tails” than does a normal distribution; that is, the density curve
declines more rapidly out in the tails than does a normal curve.
2. It is symmetric and heavy-tailed compared to a normal distribution.
3. It is skewed; that is, the distribution is not symmetric, but rather tapers off more in one direction
than the other.
A uniform distribution is light-tailed, since its density function drops to zero outside a ﬁnite
interval. The density function f(x)¼1/[π(1 +x2)], for /C01 <x<1, is one example of a heavy-tailed
distribution, since 1/(1 + x2) declines much less rapidly than does e/C0x2=2. Lognormal and Weibull
distributions are among those that are skewed. When the points in a normal probability plot do not
adhere to a straight line, the pattern will frequently suggest that the population distribution is in a
particular one of these three categories.
Figure 3.34 illustrates typical normal probability plots corresponding to three situations above.
If the sample was selected from a light-tailed distribution, the largest and smallest observations are25 26 27 28 29 30 310.020.050.100.250.500.750.900.950.98
DataProbabilityNormal Probability Plot Fig. 3.33 Normal
probability plot of the
breakdown voltage data
from Matlab3.6 Probability Plots 209
usually not as extreme as would be expected from a normal random sample. Visualize a straight line
drawn through the middle part of the plot; points on the far right tend to be above the line ( zpercentile
>observed value), whereas points on the left end of the plot tend to fall below the straight line
(zpercentile <observed value). The result is an S-shaped pattern of the type pictured in Fig. 3.34a. For
sample observations from a heavy-tailed distribution, the opposite effect will occur, and a normal
probability plot will have an S shape with the opposite orientation, as in Fig. 3.34b. If the underlying
distribution is positively skewed (a short left tail and a long right tail), the smallest sample
observations will be larger than expected from a normal sample and so will the largest observations.
In this case, points on both ends of the plot will fall below a straight line through the middle part,
yielding a curved pattern, as illustrated in Fig. 3.34c. For example, a sample from a lognormal
distribution will usually produce such a pattern; a plot of (ln(observation), zpercentile) pairs should
then resemble a straight line.
Even when the population distribution is normal, the sample percentiles will not coincide exactly
with the theoretical percentiles because of sampling variability. How much can the points in the
probability plot deviate from a straight-line pattern before the assumption of population normality is
no longer plausible? This is not an easy question to answer. Generally speaking, a small sample from
a normal distribution is more likely to yield a plot with a nonlinear pattern than is a large sample.Observation Observationz percentile
z percentile
Observationa
cbz percentile
Fig. 3.34 Probability plots that suggest a non-normal distribution: ( a) a plot consistent with a light-tailed distribution;
(b) a plot consistent with a heavy-tailed distribution; ( c) a plot consistent with a (positively) skewed distribution210 3 Continuous Random Variables and Probability Distributions
The book Fitting Equations to Data by Daniel Cuthbert and Fred Wood presents the results of a
simulation study in which numerous samples of different sizes were selected from normal
distributions. The authors concluded that there is typically greater variation in the appearance of
the probability plot for sample sizes smaller than 30, and only for much larger sample sizes does a
linear pattern generally predominate. When a plot is based on a small sample size, only a very
substantial departure from linearity should be taken as conclusive evidence of nonnormality. A
similar comment applies to probability plots for checking the plausibility of other types of
distributions.
3.6.4 Beyond Normality
Consider a generic family of probability distributions involving two parameters, θ1andθ2, and let
F(x;θ1,θ2) denote the corresponding cdf. The family of normal distributions is one such family, with
θ1¼μ,θ2¼σ, and F(x;μ,σ)¼Φ[(x/C0μ)/σ]. Another example is the Weibull family, with θ1¼α,
θ2¼β, and
Fx ;α,βðÞ ¼ 1/C0e/C0x=βðÞα
Still another family of this type is the gamma family, for which the cdf is an integral involving the
incomplete gamma function that cannot be expressed in any simpler form.
The parameters θ1andθ2are said to be location andscale parameters , respectively, if F(x;θ1,θ2)
is a function of ( x/C0θ1)/θ2. The parameters μand σof the normal family are location and scale
parameters, respectively. Changing μshifts the location of the bell-shaped density curve to the right
or left, and changing σamounts to stretching or compressing the measurement scale (the scale on the
horizontal axis when the density function is graphed). Another example is given by the cdf
Fx ;θ1,θ2 ðÞ ¼ 1/C0e/C0ex/C0θ1ðÞ =θ2/C01 <x<1
A random variable with this cdf is said to have an extreme value distribution . It is used in
applications involving component lifetime and material strength.
The parameter βof the Weibull distribution is a scale parameter. However, αis not a location
parameter but instead is called a shape parameter . The same is true for the parameters αandβof the
gamma distribution. In the usual form, the density function for any member of either the gamma or
Weibull distribution is positive for x>0 and zero otherwise. A location (or shift) parameter can be
introduced as a third parameter γ(we did this for the Weibull distribution in Sect. 3.5) to shift the
density function so that it is positive if x>γand zero otherwise.
When the family under consideration has only location and scale parameters, the issue of whether
any member of the family is a plausible population distribution can be addressed by a single
probability plot. This is exactly what we did to obtain an omnibus normal probability plot. One
ﬁrst obtains the percentiles of the standardized distribution , i.e. the one with θ1¼0 and θ2¼1, for
percentages 100( i/C0.5)/n(i¼1, ...,n). The n(observation, standardized percentile) pairs give the
points in the plot.
Somewhat surprisingly, this methodology can be applied to yield an omnibus Weibull probability
plot. The key result is that if Xhas a Weibull distribution with shape parameter αand scale parameter
β, then the transformed variable ln( X) has an extreme value distribution with location parameter θ1¼
ln(β) and scale parameter θ2¼1/α(see Exercise 169). Thus a plot of the3.6 Probability Plots 211
ln observationðÞ , extreme value standardized percentile ðÞ
pairs that shows a strong linear pattern provides support for choosing the Weibull distribution as a
population model.
Example 3.35 The accompanying observations are on lifetime (in hours) of power apparatus
insulation when thermal and electrical stress acceleration were ﬁxed at particular values (“On the
Estimation of Life of Power Apparatus Insulation Under Combined Electrical and Thermal Stress,”
IEEE Trans. Electr. Insul ., 1985: 70–78). A Weibull probability plot necessitates ﬁrst computing the
5th, 15th, ..., and 95th percentiles of the standard extreme value distribution. The (100 p)th percentile
ηpsatisﬁes
p¼Fηp;0, 1/C0/C1
¼1/C0e/C0eηp
from which ηp¼ln(/C0ln(1/C0p)).
Observation 282 501 741 851 1072 1122 1202 1585 1905 2138
ln(Obs.) 5.64 6.22 6.61 6.75 6.98 7.02 7.09 7.37 7.55 7.67
Percentile /C02.97 /C01.82 /C01.25 /C0.84 /C0.51 /C0.23 .05 .33 .64 1.10
The pairs (5.64, /C02.97), (6.22, /C01.82), ..., (7.67, 1.10) are plotted as points in Fig. 3.35. The
straightness of the plot argues strongly for using the Weibull distribution as a model for insulation
life, a conclusion also reached by the author of the cited article.
The gamma distribution is an example of a family involving a shape parameter for which there is
no transformation into a distribution that depends only on location and scale parameters. Construction
of a probability plot necessitates ﬁrst estimating the shape parameter from sample data (some general
methods for doing this are described in Chap. 5).
Sometimes an investigator wishes to know whether the transformed variable Xθhas a normal
distribution for some value of θ(by convention, θ¼0 is identiﬁed with the logarithmic transforma-
tion, in which case Xhas a lognormal distribution). The book Graphical Methods for Data Analysis
by John Chambers et al. discusses this type of problem as well as other reﬁnements of probability
plotting.7.5 7.0 6.5 6.0 5.51
0
−1
−2
−3ln(x)Percentile
Fig. 3.35 A Weibull probability plot of the insulation lifetime data ■212 3 Continuous Random Variables and Probability Distributions
3.6.5 Probability Plots in Matlab and R
Matlab, along with many statistical software packages (including R), have built-in probability
plotting commands that vitiate the need for manual calculation of percentiles from the assumed
population distribution. In Matlab, the normplot(x) command will produce a graph like the one
seen in Fig. 3.33, assuming the vector xcontains the observed data. The R command qqnorm(x)
creates a similar graph, except that the axes are transposed (ordered observations on the vertical axis,
theoretical quantiles on the horizontal). Both Matlab and R have a package called probplot that,
with appropriate speciﬁcations of the inputs, can create probability plots for distributions besides
normal (e.g., Weibull, exponential, extreme value). Refer to the help documentation in those
languages for more information.
3.6.6 Exercises: Section 3.6(101–111)
101. The accompanying normal probability plot was constructed from a sample of 30 readings on
tension for mesh screens behind the surface of video display tubes. Does it appear plausible that
the tension distribution is normal?
0
200 250 300 3501
−1
−22
Tensionz percentile
102. A sample of 15 female collegiate golfers was selected and the clubhead velocity (km/h) while
swinging a driver was determined for each one, resulting in the following data (“Hip Rotational
Velocities during the Full Golf Swing,” J. of Sports Science and Medicine , 2009: 296-299):
69.0 69.7 72.7 80.3 81.0
85.0 86.0 86.3 86.7 87.7
89.3 90.7 91.0 92.5 93.0
The corresponding zpercentiles are
/C01.83 /C01.28 /C00.97 /C00.73 /C00.52
/C00.34 /C00.17 0.0 0.17 0.34
0.52 0.73 0.97 1.28 1.83
Construct a normal probability plot. Is it plausible that the population distribution is normal?
103. Construct a normal probability plot for the following sample of observations on coating
thickness for low-viscosity paint (“Achieving a Target Value for a Manufacturing Process:
A Case Study,” J. Qual. Tech. , 1992: 22–26). Would you feel comfortable estimating population
mean thickness using a method that assumed a normal population distribution?3.6 Probability Plots 213
.83 .88 .88 1.04 1.09 1.12 1.29 1.31
1.48 1.49 1.59 1.62 1.65 1.71 1.76 1.83
104. The article “A Probabilistic Model of Fracture in Concrete and Size Effects on Fracture
Toughness” ( Mag. Concrete Res. , 1996: 311–320) gives arguments for why fracture toughness
in concrete specimens should have a Weibull distribution and presents several histograms of
data that appear well ﬁt by superimposed Weibull curves. Consider the following sample of size
n¼18 observations on toughness for high-strength concrete (consistent with one of the
histograms); values of pi¼(i/C0.5)/18 are also given.
Observation .47 .58 .65 .69 .72 .74
pi .0278 .0833 .1389 .1944 .2500 .3056
Observation .77 .79 .80 .81 .82 .84
pi .3611 .4167 .4722 .5278 .5833 .6389
Observation .86 .89 .91 .95 1.01 1.04
pi .6944 .7500 .8056 .8611 .9167 .9722
Construct a Weibull probability plot and comment.
105. The propagation of fatigue cracks in various aircraft parts has been the subject of extensive
study. The accompanying data consists of propagation lives (ﬂight hours/104) to reach a given
crack size in fastener holes for use in military aircraft (“Statistical Crack Propagation in Fastener
Holes Under Spectrum Loading,” J. Aircraft , 1983: 1028-1032):
.736 .863 .865 .913 .915 .937 .983 1.007
1.011 1.064 1.109 1.132 1.140 1.153 1.253 1.394
Construct a normal probability plot for this data. Does it appear plausible that propagation life
has a normal distribution? Explain.
106. The article “The Load-Life Relationship for M50 Bearings with Silicon Nitride Ceramic Balls”
(Lubricat. Engrg ., 1984: 153–159) reports the accompanying data on bearing load life (million
revs.) for bearings tested at a 6.45 kN load.
47.1 68.1 68.1 90.8 103.6 106.0 115.0
126.0 146.6 229.0 240.0 240.0 278.0 278.0
289.0 289.0 367.0 385.9 392.0 505.0
(a) Construct a normal probability plot. Is normality plausible?
(b) Construct a Weibull probability plot. Is the Weibull distribution family plausible?
107. The accompanying data on rainfall (acre-feet) from 26 seed clouds is taken from the article “A
Bayesian Analysis of a Multiplicative Treatment Effect in Weather Modiﬁcation”
(Technometrics , 1975: 161-166). Construct a probability plot that will allow you to assess the
plausibility of the lognormal distribution as a model for the rainfall data, and comment on what
you ﬁnd.
4.1 7.7 17.5 31.4 32.7 40.6 92.4
115.3 118.3 119.0 129.6 198.6 200.7 242.5
255.0 274.7 274.7 302.8 334.1 430.0 489.1
703.4 978.0 1656.0 1697.8 2745.6214 3 Continuous Random Variables and Probability Distributions
108. The accompanying observations are precipitation values during March over a 30-year period in
Minneapolis–St. Paul.
.77 1.20 3.00 1.62 2.81 2.48
1.74 .47 3.09 1.31 1.87 .96
.81 1.43 1.51 .32 1.18 1.89
1.20 3.37 2.10 .59 1.35 .90
1.95 2.20 .52 .81 4.75 2.05
(a) Construct and interpret a normal probability plot for this data set.
(b) Calculate the square root of each value and then construct a normal probability plot based
on this transformed data. Does it seem plausible that the square root of precipitation is
normally distributed?
(c) Repeat part (b) after transforming by cube roots.
109. Allowable mechanical properties for structural design of metallic aerospace vehicles requires an
approval method for statistically analyzing empirical test data. The article “Establishing
Mechanical Property Allowables for Metals” ( J. of Testing and Evaluation , 1998: 293-299)
used the accompanying data on tensile ultimate strength (ksi) as a basis for addressing the
difﬁculties in developing such a method.
122.2 124.2 124.3 125.6 126.3 126.5 126.5 127.2 127.3
127.5 127.9 128.6 128.8 129.0 129.2 129.4 129.6 130.2
130.4 130.8 131.3 131.4 131.4 131.5 131.6 131.6 131.8
131.8 132.3 132.4 132.4 132.5 132.5 132.5 132.5 132.6
132.7 132.9 133.0 133.1 133.1 133.1 133.1 133.2 133.2
133.2 133.3 133.3 133.5 133.5 133.5 133.8 133.9 134.0
134.0 134.0 134.0 134.1 134.2 134.3 134.4 134.4 134.6
134.7 134.7 134.7 134.8 134.8 134.8 134.9 134.9 135.2
135.2 135.2 135.3 135.3 135.4 135.5 135.5 135.6 135.6
135.7 135.8 135.8 135.8 135.8 135.8 135.9 135.9 135.9
135.9 136.0 136.0 136.1 136.2 136.2 136.3 136.4 136.4
136.6 136.8 136.9 136.9 137.0 137.1 137.2 137.6 137.6
137.8 137.8 137.8 137.9 137.9 138.2 138.2 138.3 138.3
138.4 138.4 138.4 138.5 138.5 138.6 138.7 138.7 139.0
139.1 139.5 139.6 139.8 139.8 140.0 140.0 140.7 140.7
140.9 140.9 141.2 141.4 141.5 141.6 142.9 143.4 143.5
143.6 143.8 143.8 143.9 144.1 144.5 144.5 147.7 147.7
Use software to construct a normal probability plot of this data, and comment.
110. Let the ordered sample observations be denoted by y1,y2,...,yn(y1being the smallest and ynthe
largest). Our suggested check for normality is to plot the ( yi,Φ/C01[(i/C0.5)/n]) pairs. Suppose we
believe that the observations come from a distribution with mean 0, and let w1,...,wnbe the
ordered absolute values of the observed data. A half-normal plot is a probability plot of the
wis. More speciﬁcally, since P(|Z|/C20w)¼P(/C0w/C20Z/C20w)¼2Φ(w)/C01, a half-normal plot is a
plot of the ( wi,Φ/C01[(pi+ 1)/2]) pairs, where pi¼(i/C0.5)/n. The virtue of this plot is that small or
large outliers in the original sample will now appear only at the upper end of the plot rather than
at both ends. Construct a half-normal plot for the following sample of measurement errors, and
comment:
/C03.78,/C01.27, 1.44, /C0.39, 12.38, /C043.40, 1.15, /C03.96,/C02.34, 30.84.3.6 Probability Plots 215
111. The following failure time observations (1000s of hours) resulted from accelerated life testing
of 16 integrated circuit chips of a certain type:
82.8 11.6 359.5 502.5 307.8 179.7
242.0 26.5 244.8 304.3 379.1 212.6
229.9 558.9 366.7 203.6
Use the corresponding percentiles of the exponential distribution with λ¼1 to construct a
probability plot. Then explain why the plot assesses the plausibility of the sample having been
generated from anyexponential distribution.
3.7 Transformations of a Random Variable
Often we need to deal with a transformation Y¼g(X) of the random variable X. Here g(X) could be a
simple change of time scale. If Xis the time to complete a task in minutes, then Y¼60Xis the
completion time expressed in seconds. How can we get the pdf of Yfrom the pdf of X? Consider ﬁrst a
simple example.
Example 3.36 The interval Xin minutes between calls to a 911 center is exponentially distributed
with mean 2 min, so its pdf is fX(x)¼.5e/C0.5xforx>0. In order to get the pdf of Y¼60X, we ﬁrst
obtain its cdf:
FYðyÞ¼PY/C20yðÞ ¼ P60X/C20y ðÞ ¼ PX/C20y=60 ðÞ ¼ FXy=60ðÞ
¼ðy=60
0:5e/C0:5xdx¼1/C0e/C0y=120
Differentiating this with respect to ygives fY(y)¼(1/120) e/C0y/120fory>0. We see that the
distribution of Yis exponential with mean 120 s (2 min).
There is nothing special here about the mean 2 and the multiplier 60. It should be clear that if we
multiply an exponential random variable with mean μby a positive constant cwe get another
exponential random variable with mean cμ. ■
Sometimes it isn’t possible to evaluate the cdf in closed form. Could we still ﬁnd the pdf of
Ywithout evaluating the integral? Yes, thanks to the following theorem.
TRANSFORMATION THEOREM
LetXhave pdf fX(x) and let Y¼g(X), where gis monotonic (either strictly increasing or strictly
decreasing) on the set of all possible values of X, so it has an inverse function X¼h(Y). Assume
thathhas a derivative h0(y). Then
fYðyÞ¼fXhðyÞðÞ /C1 h0ðyÞ/C12/C12/C12/C12 ð3:11Þ
Proof Here is the proof assuming that gis monotonically increasing. The proof for gmonotonically
decreasing is similar. First ﬁnd the cdf of Y:216 3 Continuous Random Variables and Probability Distributions
FYðyÞ¼PY/C20yðÞ ¼ PgðXÞ/C20y ðÞ ¼ PX/C20hðyÞ ðÞ ¼ FXhðyÞðÞ
The third equality above, wherein g(X)/C20yis true iff X/C20g/C01(y)¼h(y), relies on gbeing a
monotonically increasing function. Now differentiate the cdf with respect to y, using the Chain Rule:
fYðyÞ¼d
dyFYðyÞ¼d
dyFXhðyÞðÞ ¼ F0
XhðyÞðÞ /C1 h0ðyÞ¼fXhðyÞðÞ /C1 h0ðyÞ
The absolute value on the derivative in Eq. ( 3.11) is needed only in the other case where gis
decreasing. The set of possible values for Yis obtained by applying gto the set of possible values
forX. ■
Example 3.37 Let’s apply the Transformation Theorem to the situation introduced in Example 3.36.
There Y¼g(X)¼60XandX¼h(Y)¼Y/60.
fYðyÞ¼fXhðyÞðÞ h0ðyÞ/C12/C12/C12/C12¼:5e/C0:5x1
60/C12/C12/C12/C12/C12/C12/C12/C12¼1
120e/C0y=120y>0
This matches the pdf of Yderived through the cdf in Example 3.36. ■
Example 3.38 LetX/C24Unif[0, 1], so fX(x)¼1 for 0 /C20x/C201, and deﬁne a new variable Y¼2ﬃﬃﬃﬃ
Xp
.
The function g(x)¼2ﬃﬃﬃxpis monotone on [0, 1], with inverse x¼h(y)¼y2/4. Apply the
Transformation Theorem:
fYðyÞ¼fXhðyÞðÞ h0ðyÞ/C12/C12/C12/C12¼ð1Þ2y
4/C12/C12/C12/C12/C12/C12/C12/C12¼y
20/C20y/C202
The range 0 /C20y/C202 comes from the fact that y¼2ﬃﬃﬃxpmaps [0, 1] to [0, 2]. A graphical
representation may help in understanding why the transformation Y¼2ﬃﬃﬃﬃ
Xp
yields fY(y)¼y/2 if
X/C24Unif[0, 1]. Figure 3.36a shows the uniform distribution with [0, 1] partitioned into ten
subintervals. In Fig. 3.36b the endpoints of these intervals are shown after transforming according
toy¼2ﬃﬃﬃxp. The heights of the rectangles are arranged so each rectangle still has area .1, and therefore
the probability in each interval is preserved. Notice the close ﬁt of the dashed line, which has the
equation fY(y)¼y/2.
01.0
.8
.6
.4
.2
01.0
.8
.6
.4
.2
0b a
0 .5 1.0 1.5 2.0 .5 1.0 1.5 2.0fX(x) fY(y)
xy
Fig. 3.36 The effect on the pdf if Xis uniform on [0, 1] and Y¼2ﬃﬃﬃﬃ
Xp
■3.7 Transformations of a Random Variable 217
Example 3.39 The variation in a certain electrical current source X(in milliamps) can be modeled by
the pdf
fXðxÞ¼1:25/C0:25x2/C20x/C204
0 otherwise(
If this current passes through a 220- Ωresistor, the resulting power Y(in microwatts) is given by the
expression Y¼220X2. The function y¼g(x)¼220x2is monotonically increasing on the range of X,
the interval [2, 4], and has inverse function x¼hðyÞ¼g/C01ðyÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
y=220p
. (Notice that g(x)i sa
parabola and thus not monotone on the entire real number line, but for the purposes of the theorem
g(x) only needs to be monotone on the range of the rv X.) Apply Eq. ( 3.11):
fYðyÞ¼fXhðyÞðÞ /C1 h0ðyÞ/C12/C12/C12/C12
¼fXﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
y=220p/C16/C17
/C1d
dyﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
y=220p/C12/C12/C12/C12/C12/C12/C12/C12
¼1:25/C0:25ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
y=220p/C16/C17
/C11
2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ220yp ¼5
8ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ220yp /C01
1760
The set of possible Y-values is determined by substituting x¼2 and x¼4 into g(x)¼220x2; the
resulting range for Yis [880, 3520]. Therefore, the pdf of Y¼220X2is
fYðyÞ¼5
8ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ220yp /C01
1760880/C20y/C203520
0 otherwise8
<
:
The pdfs of XandYappear in Fig. 3.37.
The Transformation Theorem requires a monotonic transformation, but there are important
applications in which the transformation is not monotone. Nevertheless, it may be possible to use
the theorem anyway with a little trickery.0.8
0.6
0.4
0.2
0
2 3 40.0008
0.0006
0.0004
0.0002
0
0 880 1760 2640 3520ab
fX(x) fY(y)
x y
Fig. 3.37 pdfs from Example 3.39: ( a) pdf of X;(b) pdf of Y ■218 3 Continuous Random Variables and Probability Distributions
Example 3.40 In this example, we start with a standard normal random variable Z, and we transform
toY¼Z2. Of course, this is not monotonic over the interval for Z,(/C01,1). However, consider the
transformation U¼|Z|. Because Zhas a symmetric distribution, the pdf of UisfU(u)¼fZ(u)+fZ(/C0u)
¼2fZ(u). (Don’t despair if this is not intuitively clear, because we’ll verify it shortly. For the time
being, assume it to be true.) Then Y¼Z2¼|Z|2¼U2, and the transformation in terms of Uis
monotonic because its set of possible values is [0, 1). Thus we can use the Transformation Theorem
with h(y)¼y1/2:
fYðyÞ¼fUhðyÞ½/C138 h0ðyÞ/C12/C12/C12/C12¼2fXhðyÞ½/C138 h0ðyÞ/C12/C12/C12/C12
¼2ﬃﬃﬃﬃﬃ
2πp e/C0:5y1=2ðÞ21
2y/C01=2/C12/C12/C12/C12/C12/C12/C12/C12¼1ﬃﬃﬃﬃﬃﬃﬃ2πyp e/C0y=2y>0
This distribution is known as the chi-squared distribution with one degree of freedom . Chi-squared
distributions arise frequently in statistical inference procedures, such as those in Chap. 5.
You were asked to believe intuitively that fU(u)¼2fZ(u). Here is a little derivation that works as
long as the distribution of Zis symmetric about 0. If u>0,
FUðuÞ¼PU/C20u ðÞ ¼ PZjj/C20u ðÞ ¼ P/C0u/C20Z/C20u ðÞ ¼ 2P0/C20Z/C20u ðÞ
¼2FZðuÞ/C0FZð0Þ ½/C138 :
Differentiating this with respect to ugives fU(u)¼2fZ(u). ■
Example 3.41 Sometimes the Transformation Theorem cannot be used at all, and you need to use the
cdf. Let fX(x)¼(x+ 1)/8, /C01/C20x/C203, and Y¼X2. The transformation is not monotonic on [ /C01, 3];
and, since fX(x) is not an even function, we can’t employ the symmetry trick of the previous example.
Possible values of Yare { y:0/C20y/C209}. Considering ﬁrst 0 /C20y/C201,
FYðyÞ¼PY/C20yðÞ ¼ PX2/C20y/C0/C1
¼P/C0ﬃﬃﬃyp/C20X/C20ﬃﬃﬃyp/C0/C1
¼ðﬃﬃyp
/C0ﬃﬃypuþ1
8du¼ﬃﬃﬃyp
4
Then, on the other subinterval, 1 <y/C209,
FYðyÞ¼PY/C20yðÞ ¼ PX2/C20y/C0/C1
¼P/C0ﬃﬃﬃyp/C20X/C20ﬃﬃﬃyp /C0/C1
¼P/C01/C20X/C20ﬃﬃﬃyp/C0/C1
¼ðﬃﬃyp
/C01uþ1
8du¼1þyþ2ﬃﬃﬃyp/C0/C1
=16
Differentiating, we get
fYðyÞ¼1
8ﬃﬃﬃyp 0<y/C201
yþﬃﬃﬃyp
16y1<y/C209
0 otherwise8
>>>>><
>>>>>:
Figure 3.38 shows the pdfs of both XandY.3.7 Transformations of a Random Variable 219
3.7.1 Exercises: Section 3.7(112–128)
112. Relative to the winning time, the time Xof another runner in a ten kilometer race has pdf fX(x)¼
2/x3,x>1. The reciprocal Y¼1/Xrepresents the ratio of the time for the winner divided by the
time of the other runner. Find the pdf of Y. Explain why Yalso represents the speed of the other
runner relative to the winner.
113. Let Xbe the fuel efﬁciency in miles per gallon of an extremely inefﬁcient vehicle (a military
tank, perhaps?), and suppose Xhas the pdf fX(x)¼2x,0<x<1. Determine the pdf of Y¼1/X,
which is fuel efﬁciency in gallons per mile. [ Note: The distribution of Yis a special case of the
Pareto distribution (see Exercise 10).]
114. Let Xhave the pdf fX(x)¼2/x3,x>1. Find the pdf of Y¼ﬃﬃﬃﬃ
Xp
.
115. Let Xhave an exponential distribution with mean 2, so fXðxÞ¼1
2e/C0x=2,x>0. Find the pdf of
Y¼ﬃﬃﬃﬃ
Xp
.[Note: Suppose you choose a point in two dimensions randomly, with the horizontal
and vertical coordinates chosen independently from the standard normal distribution. Then
Xhas the distribution of the squared distance from the origin and Yhas the distribution of the
distance from the origin. Yhas a Rayleigh distribution (see Exercise 4).]
116. If Xis distributed as N(μ,σ), ﬁnd the pdf of Y¼eX. Verify that the distribution of Ymatches the
lognormal pdf provided in Sect. 3.5.
117. If the side of a square Xis random with the pdf fX(x)¼x/8, 0 <x<4, and Yis the area of the
square, ﬁnd the pdf of Y.
118. Let X/C24Unif[0, 1]. Find the pdf of Y¼/C0ln(X).
119. Let X/C24Unif[0, 1]. Find the pdf of Y¼tan[π(X/C0.5)]. [ Note: The random variable Yhas the
Cauchy distribution , named after the famous mathematician.]
120. If X/C24Unif[0, 1], ﬁnd a linear transformation Y¼cX+dsuch that Yis uniformly distributed on
[A,B], where AandBare any two numbers such that A<B. Is there any other solution? Explain.
121. If Xhas the pdf fX(x)¼x/8, 0 <x<4, ﬁnd a transformation Y¼g(X) such that Y/C24Unif[0, 1].
[Hint: The target is to achieve fY(y)¼1 for 0 /C20y/C201. The Transformation Theorem will allow
you to ﬁnd h(y), from which g(x) can be obtained.]
122. If a measurement error Xis uniformly distributed on [ /C01, 1], ﬁnd the pdf of Y¼|X|, which is the
magnitude of the measurement error.
123. If X/C24Unif[/C01, 1], ﬁnd the pdf of Y¼X2.123b a
159 0fX(x) fY(y)
y x
−11/83/8
1/41/2
0.05
00.15
0.10.2
Fig. 3.38 pdfs from Example 3.41: ( a) pdf of X;(b) pdf of Y ■220 3 Continuous Random Variables and Probability Distributions
124. Ann is expected at 7:00 pm after an all-day drive. She may be as much as 1 h early or as much as
3 h late. Assuming that her arrival time Xis uniformly distributed over that interval, ﬁnd the pdf
of |X/C07|, the unsigned difference between her actual and predicted arrival times.
125. If X/C24Unif[/C01, 3], ﬁnd the pdf of Y¼X2.
126. If a measurement error Xis distributed as N(0, 1), ﬁnd the pdf of | X|, which is the magnitude of
the measurement error.
127. A circular target has radius 1 foot. Assume that you hit the target (we shall ignore misses) and
that the probability of hitting any region of the target is proportional to the region’s area. If you
hit the target at a distance Yfrom the center, then let X¼πY2be the corresponding circular area.
Show that
(a) Xis uniformly distributed on [0, π]. [Hint: Show that FX(x)¼P(X/C20x)¼x/π.]
(b) Yhas pdf fY(y)¼2y,0<y<1.
128. In Exercise 127 suppose instead that Yis uniformly distributed on [0, 1]. Find the pdf of
X¼πY2. Geometrically speaking, why should Xhave a pdf that is unbounded near 0?
3.8 Simulation of Continuous Random Variables
In Sects. 1.6and2.8, we discussed the need for simulation of random events and discrete random
variables in situations where an “analytic” solution is very difﬁcult or simply not possible. This
section presents methods for simulating continuous random variables, including some of the built-in
simulation tools of Matlab and R.
3.8.1 The Inverse CDF Method
Section 2.8introduced the inverse cdf method for simulating discrete random variables. The basic
idea was this: generate a Unif[0, 1) random number and align it with the cdf of the random variable
Xwe want to simulate. Then, determine which Xvalue corresponds to that cdf value. We now extend
this methodology to the simulation of values from a continuous distribution; the heart of the algorithm
relies on the following theorem, often called the probability integral transform .
THEOREM
Consider any continuous distribution with pdf fand cdf F. Let U/C24Unif[0, 1), and deﬁne a
random variable Xby
X¼F/C01ðUÞð 3:12Þ
Then the pdf of Xisf.
Before proving this theorem, let’s consider its practical usage: Suppose we want to simulate a
continuous rv whose pdf is f(x), i.e., obtain successive values of Xhaving pdf f(x). If we can compute
the corresponding cdf F(x) and apply its inverse F/C01to standard uniform variates u1,...,un, the
theorem states that the resulting values x1¼F/C01(u1),...,xn¼F/C01(un) will follow the desired
distribution f. (We’ll discuss the practical difﬁculties of implementing this method a little later.)
A graphical description of the algorithm appears in Fig. 3.39.3.8 Simulation of Continuous Random Variables 221
Proof Apply the Transformation Theorem (Sect. 3.7)w i t h fU(u)¼1 for 0 /C20u<1,X¼g(U)¼
F/C01(U), and thus U¼h(X)¼g/C01(X)¼F(X). The pdf of the transformed variable Xis
fXðxÞ¼fUhðxÞðÞ /C1 h0ðxÞ/C12/C12/C12/C12¼fUFðxÞðÞ /C1 F0ðxÞ/C12/C12/C12/C12¼1/C1fðxÞjj ¼fðxÞ
In the last step, the absolute values may be removed because a pdf is always nonnegative. ■
The following box explains the implementation of the inverse cdf method justiﬁed by the
preceding theorem.
INVERSE CDF METHOD
It is desired to simulate nvalues from a distribution with pdf f(x). Let F(x) be the corresponding
cdf. Repeat ntimes:
1. Use a random-number generator (RNG) to produce a value, u, from [0, 1).
2. Assign x¼F/C01(u).
The resulting values x1,...,xnform a simulation of a random variable with the original pdf, f(x).
Example 3.42 Consider the electrical current distribution model of Example 3.11, where the pdf of
Xis given by f(x)¼1.25/C0.25xfor 2/C20x/C204. Suppose a simulation of Xis required as part of some
larger system analysis. To implement the above method, the inverse of the cdf of Xis required. First,
compute the cdf:
FðxÞ¼PX/C20xðÞ ¼ðx
2fðyÞdy
¼ðx
21:25/C0:25y ðÞ dy¼/C00:125x2þ1:25x/C02, 2 /C20x/C204
To ﬁnd the probability integral transform Eq. ( 3.12), set u¼F(x) and solve for x:
u¼FxðÞ ¼ /C0 0:125x2þ1:25x/C02)x¼F/C01uðÞ ¼ 5/C0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
9/C08up
The equation above has been solved using the quadratic formula; care must be taken to select the
solution whose values lie in the interval [2, 4] (the other solution, x¼5þﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
9/C08up
, does not have
that feature). Beginning with the usual Unif[0, 1) RNG, the algorithm for simulating Xis the
following: given a value ufrom the RNG, assign x¼5/C0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
9/C08up
. Repeating this algorithm
ntimes gives nsimulated values of X. Programs in Matlab and R that implement this algorithm
appear in Fig. 3.40; both return a vector, x, containing n¼10,000 simulated values of the speciﬁed
distribution.1
u1
u2
0F(x)
F−1(u2) F−1(u1)xFig. 3.39 The inverse cdf
method, illustrated222 3 Continuous Random Variables and Probability Distributions
As discussed in Chap. 1, both of these programs can be accelerated by “vectorizing” the operations
rather than using a for loop. In fact, a single line of code in either language can produce the desired result:
in Matlab: x¼5-sqrt(9-8*rand(10000,1))
in R: x<-5-sqrt(9-8*runif(10000))
The pdf of the rv Xand a histogram of simulation results from R appear in Fig. 3.41.
Example 3.43 The lifetime of a certain type of drill bit has an exponential distribution with mean
100 h. An analysis of a large manufacturing process that uses these drill bits requires the simulation of
this lifetime distribution, which can be achieved through the inverse cdf method. From Sect. 3.4, the
cdf of this distribution is F(x)¼1/C0e/C0.01x, and so the inverse cdf is x¼F/C01(u)¼/C0100ln(1 /C0u).
Applying this function to Unif[0, 1) random numbers will generate the desired simulation. (Don’t let
the negative sign at the front worry you: since 0 /C20u<1, 1/C0ulies between 0 and 1, and so its
logarithm is negative and the resulting value of xis actually positive.)
As a check, the code x¼-100*log(1-rand(10000,1)) was submitted to Matlab and the
resulting sample mean and sd were obtained using mean(x) andstd(x) . Exponentially distributed
rvs have standard deviation equal to the mean, so the theoretical answers are μ¼100 and σ¼100.
The Matlab simulation yielded x¼99:3724 and s¼100.8908, both of which are reasonably close to
100 and validate the inverse cdf formula.
In general, an exponential distribution with mean μ(equivalently, parameter λ¼1/μ) can be
simulated using the transform x¼/C0 μln(1/C0u). ■
The preceding two examples illustrated the inverse cdf method for fairly simple density functions:
a linear polynomial and an exponential function. In practice, the algebraic complexity of f(x) canx=zeros(10000,1);
for i=1:10000
u=rand; 
x(i)=5-sqrt(9-8*u);
endx <- NULL
for (i in 1:10000){
u<-runif(1)
x[i]<-5-sqrt(9-8*u)
}ab
Fig. 3.40 Simulation code for Example 3.42: ( a) Matlab; ( b)R
Histogram of x b
xFrequency
2.0 2.5 3.0 3.5 4.00200400600 0.8
0.6
0.4
0.2
0
2 3 4a
fX(x)
x
Fig. 3.41 (a) Theoretical pdf and ( b) R simulation results for Example 3.42 ■3.8 Simulation of Continuous Random Variables 223
often be a barrier to implementing this simulation technique. After all, the algorithm requires that we
can (1) obtain the cdf F(x) in closed form and (2) ﬁnd the inverse function of Fin closed form.
Consider, for example, attempting to simulate values from the N(0, 1) distribution: its cdf is the
function denoted Φ(z) and given by the integral expression 1 =ﬃﬃﬃﬃﬃ
2πp/C0/C1Ðz
/C01e/C0u2=2du. There is no
closed-form expression for this integral, let alone a method to solve u¼Φ(z) for zand implement
Eq. ( 3.12). (As a reminder, the lack of a closed-form expression for Φ(z) is the reason that software or
tables are always required for calculations involving normal probabilities.) Thankfully, most software
packages, including Matlab and R, have built-in tools to simulate normally distributed variates (using
a very clever algorithm called the Box-Muller method ; see Sect. 4.6). We’ll discuss built-in simula-
tion tools at the end of this section.
As the next example illustrates, even when F(x) can be determined in closed form we cannot
necessarily implement the inverse cdf method, because F(x) cannot always be inverted. This
difﬁculty surfaces in practice when attempting to simulate values from a gamma distribution, for
instance.
Example 3.44 The measurement error X(in mV) of a particular volt-meter has the following
distribution: f(x)¼(4/C0x2)/9 for /C01/C20x/C202 (and f(x)¼0 otherwise). To use the inverse cdf
method to simulate X, begin by calculating its cdf:
FxðÞ ¼ðx
/C014/C0y2
9dy¼/C0x3þ12xþ11
27
To implement step 2 of the inverse cdf method requires solving F(x)¼uforx; since F(x) is a cubic
polynomial, this is not a simple task. Advanced computer algebra systems can solve this equation,
though the general solution is unwieldy (and such a solution doesn’t exist at all for 5th-degree and
higher polynomials). Readers familiar with numerical analysis methods may recognize that, for any
speciﬁed numerical value of u, a root-ﬁnding algorithm (such as Newton–Raphson) can be
implemented to approximate the solution x. This latter method, however, is computationally inten-
sive, especially if it’s desirable to generate 10,000 or more simulated values of x. ■
The preceding example suggests that the inverse cdf method is insufﬁcient for simulating all
continuous distributions in practice. We next consider an alternative algorithm that, while less
efﬁcient, has a broader scope.
3.8.2 The Accept–Reject Method
When the inverse cdf method of simulation cannot be implemented, the accept–reject method
provides an alternative. The downside of the accept–reject method, as will be explained below, is
that only some of the random numbers generated by software will be used (“accepted”), while others
will be “rejected.” As a result, one needs to create more—sometimes, many more—random variates
than the desired number of simulated values.
Suppose we wish to simulate a random variable X, whose pdf is f(x). The key to the accept–reject
method is to begin with a different pdf, call it g(x), that satisﬁes two properties: (1) we can already
simulate values from g(x), so gis either algebraically simple or else built into our software package;
(2) the set of possible x-values for the distribution speciﬁed by g(x) equals (or exceeds) that of f(x).
For example, to simulate the distribution in Example 3.44, whose range of x-values is [ /C01, 2], one
might select for g(x) the uniform distribution on [ /C01, 2], i.e., g(x)¼1/3 for /C01/C20x/C202. IfXtakes on
values across [0, 1), then an exponential pdf would be a logical choice for g(x).224 3 Continuous Random Variables and Probability Distributions
ACCEPT–REJECT METHOD
It is desired to simulate nvalues from a distribution with pdf f(x). Let g(x) be some other pdf
such that the ratio f/gis bounded, i.e., there exists a constant csuch that f(x)/g(x)/C20cfor all x.
(The constant cis sometimes called the majorization constant .) Proceed as follows:
1. Generate a variate, y, from the distribution g. This value yis called a candidate .
2. Generate a standard uniform variate, u.
3. If u/C1c/C1g(y)/C20f(y), then assign x¼y(i.e., “accept” the candidate). Otherwise, discard
(“reject”) yand return to step 1.
These steps are repeated until ncandidate values have been accepted. The resulting accepted
values x1,...,xnform a simulation of a random variable with the original pdf, f(x).
A proof that the method works—i.e., that the resulting values really do simulate the target
distribution f(x)— requires material from Chap. 4(see Exercise 22 at the end of Sect. 4.1).
Figure 3.42 illustrates the key step in this algorithm. A candidate yhas been generated on the
common interval of the pdfs fandg. Given y, the left-hand side of the inequality in step 3, U/C1c/C1g(y),
is uniformly distributed on the interval from 0 to c/C1g(y) (since Uitself is standard uniform). If it
happens that u/C1c/C1g(y) falls between 0 and f(y), i.e., lies underneath the target pdf f, then that y-value
is accepted as coming from f; otherwise, yis rejected.
As a corollary to proving the validity of the accept–reject method, it can also be shown that the
probability any particular candidate yis accepted equals 1/ c. (The value of cmust always exceed 1;
can you see why?) Since successive candidates are independent, it follows that the number of
candidates required to generate a single acceptable value has a geometric distribution, and the
expected number of candidates to generate one xfrom f(x) is 1/(1/ c)¼c. By extension, the expected
number of candidates required to generate our simulation sample of size niscn. Consequently, the
majorization constant cshould always be made as small as possible, i.e., we should ﬁnd the smallest
value csuch that f(x)/g(x)/C20cfor all xunder consideration.
Example 3.45 (Example 3.44 continued) In order to simulate 10,000 values from f(x)¼(4/C0x2)/9,
/C01/C20x/C202, we will rely on our ability to generate variates from g(x)¼1/3 on /C01/C20x/C202,
the uniform pdf. To implement the accept–reject method, we must determine the majorization
constant, c, by looking at the ratio f/g:f(x)c.g(x)
c.g(y)
f(y)Reject
Accept
0
y (candidate)Fig. 3.42 The accept–
reject method3.8 Simulation of Continuous Random Variables 225
fðxÞ
gðxÞ¼4/C0x2ðÞ =9
1=3¼4/C0x2
3/C204/C002
3¼4
3for/C01/C20x/C202
The expression 4 /C0x2represents a downward-facing parabola with vertex at x¼0, so it is clearly
maximized at 0. We conclude that c¼4/3 is the smallest possible majorization constant, and that is
what we shall use. To create the desired simulation, the following steps are repeated until 10,000
values are accepted in step 3.
1. Generate yfrom the uniform distribution on [ /C01, 2].
2. Generate ufrom the standard uniform RNG.
3. If u/C14
3/C11
3/C204/C0y2
9, assign x¼y; otherwise, discard yand return to step 1.
Figure 3.43 shows the preceding algorithm implemented in Matlab and R. Both programs result in
a vector of 10,000 simulated values from the pdf f(x). Figure 3.44 shows f(x) alongside the simulated
values from Matlab. Since c¼4/3, it’s expected to require 4/3(10,000) ¼13,333 iterations of the
while loop to create the desired simulation size; by adding a counter to the program, one run of the
Matlab code was found to use 13,303 candidates.
ab
x=zeros(10000,1);
i=0;
while i<10000
y=-1+3*rand;
u=rand; 
if u*4/3*1/3<=(4-y^2)/9
i=i+1;
x(i)=y;
end
endx <- NULL
i <- 0
while (i<10000){
y <- -1+3*runif(1)
u <- runif(1)
if (u*4/3*1/3<=(4-y^2)/9){
i <- i+1
x[i] <- y
}
}
Fig. 3.43 Simulation code for Example 3.45: ( a) Matlab; ( b)R
−1 −0.5 0 0.5 1 1.5 2050100150200250300350400450500
−1 00.10.20.30.4
12f(x)
xab
Fig. 3.44 pdf and histogram of simulated values for Example 3.45226 3 Continuous Random Variables and Probability Distributions
You may have noticed that step 3 may be simpliﬁed: the inequality u/C20(4/C0y2)/4 would be
equivalent to the one presented. In fact, it is very common to see this ﬁnal step of the accept–reject
algorithm written as “accept yiffu/C20f(y)/[c/C1g(y)].” ■
For more information on the accept–reject method and selection of a sensible “candidate”
distribution g(x) consult the text Simulation by Ross listed in the references.
3.8.3 Built-In Simulation Packages for Matlab and R
As was true for the most common discrete distributions, many software packages have built-in tools
for simulating values from the continuous models named in this chapter. Table 3.3summarizes the
relevant functions in Matlab and R for the uniform, normal, gamma, and exponential distributions;
the variable nrefers to the desired number of simulated values of the distribution. Both packages
include similar commands for the Weibull, lognormal, and beta distributions.
As was the case with the cdf commands discussed in Sect. 3.4, Matlab and R parameterize the
gamma and exponential distributions differently: Matlab always requires the “scale” parameter β¼
1/λ, while R takes in the “rate” parameter λ¼1/β. (In the gamma simulation command, this can be
overridden by naming the ﬁnal argument scale ,a si n rgamma( n,α,scale ¼β).) In R, the
command rnorm( n)will generate standard normal variates (i.e., with μ¼0 and σ¼1), but the
μand σarguments are required in Matlab. Similarly, R will generate standard uniform variates
(A¼0 and B¼1), the basis for many of our simulation methods, with the command runif( n).
Matlab’s corresponding syntax is rand( n,1); if you type rand(100) instead of rand(100,1) ,
you will receive a 100-by-100 matrix of Unif[0, 1) values.
3.8.4 Precision of Simulation Results
Sect. 2.8discusses in detail the precision of estimates associated with simulating discrete random
variables. The same results apply in the continuous case. In particular, the estimated standard error in
using a sample proportion bpto estimate the true probability of an event is stillﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
bp1/C0bpðÞ =np
, where
nis the simulation size. Also, the estimated standard error in using a sample mean, x, to estimate the
true expected value μof a (continuous) rv Xiss=ﬃﬃﬃnp, where sis the sample standard deviation of the
simulated values of X. Refer back to Sect. 2.8for more details.Table 3.3 Functions to simulate major continuous distributions in Matlab and R
Distribution Matlab code R code
Unif[ A,B] unifrnd( A,B,n,1) runif( n,A,B)
N(μ,σ) normrnd( μ,σ,n,1) rnorm( n,μ,σ)
Gamma( α,β) gamrnd( α,β,n,1) rgamma( n,α,1 /β)
Exponential( λ) exprnd( 1/λ,n,1) rexp( n,λ)3.8 Simulation of Continuous Random Variables 227
3.8.5 Exercises: Section 3.8(129–139)
129. The amount of time (hours) required to complete an unusually short statistics homework
assignment is modeled by the pdf f(x)¼x/2 for 0 <x<2 (and ¼0 otherwise).
(a) Obtain the cdf and then its inverse.
(b) Write a program to simulate 10,000 values from this distribution.
(c) Compare the sample mean and standard deviation of your 10,000 simulated values to the
theoretical mean and sd of this distribution (which you can determine by calculating the
appropriate integrals).
130. The Weibull distribution was introduced in Sect. 3.5.
(a) Find the inverse cdf for the Weibull distribution.
(b) Write a program to simulate nvalues from a Weibull distribution. Your program should
have three inputs: the desired number of simulated values nand the two parameters αandβ.
It should have a single output: an n/C21 vector of simulated values.
(c) Use your program from part (b) to simulate 10,000 values from a Weibull(4, 6) distribution
and estimate the mean of this distribution. The correct value of the mean is 6 Γ(5/4)/C25
5.438; how close is your sample mean?
131. Consider the pdf for the rv X¼magnitude (in newtons) of a dynamic load on a bridge, given in
Example 3.7:
fxðÞ ¼1
8þ3
8x0/C20x/C202
0 otherwise8
<
:
Write a program to simulate values from this distribution using the inverse cdf method.
132. In distributed computing, any given task is split into smaller sub-tasks which are handled by
separate processors (which are then recombined by a multiplexer). Consider a distributed
computing system with 4 processors, and suppose for one particular purpose that pdf of
completion time for a particular sub-task (microseconds) on any one of the processors is
given by
fðxÞ¼20
3x24/C20x/C2010
0 otherwise8
<
:
That is, the sub-task completion times X1,X2,X3,X4of the four processors each have the above
pdf.
(a) Write a program to simulate the above pdf using the inverse cdf method.
(b) The overall time to complete any task is the largest of the four sub-task completion times: if
we call this variable Y, then Y¼max( X1,X2,X3,X4). (We assume that the multiplexing
time is negligible). Use your program in part (a) to simulate 10,000 values of the rv Y.
Create a histogram of the simulated values of Y, and also use your simulation to estimate
both E(Y) and SD( Y).
133. Exercise 16 in Sect. 3.1introduced the following model for wait times at street crossings:
fx ;θ,τðÞ ¼θ
τ1/C0x=τ ðÞθ/C010/C20x<τ
0 otherwise8
<
:
where θ>0 and τ>0 are the parameters of the model.228 3 Continuous Random Variables and Probability Distributions
(a) Write a function to simulate values from this distribution, implementing the inverse cdf
method. Your function should have three inputs: the desired number of simulated values
nand values for the two parameters for θandτ.
(b) Use your function in part (a) to simulate 10,000 values from this wait time distribution with
θ¼4 and τ¼80. Estimate E(X) under these parameter settings. How close is your estimate
to the correct value of 16?
134. Explain why the transformation x¼/C0 μln(u) may be used to simulate values from an exponen-
tial distribution with mean μ. (This expression is slightly simpler than the one established in this
section.)
135. Recall the rv X¼amount of gravel (in tons) sold by a construction supply company in a given
week from Example 3.9, whose pdf is
fxðÞ ¼3
21/C0x2/C0/C1
0/C20x/C201
0 otherwise8
<
:
Consider simulating values from this distribution using the accept–reject method with a Unif[0, 1]
“candidate” distribution, i.e., g(x)¼1 for 0 /C20x/C201.
(a) Find the smallest majorization constant cso that f(x)/g(x)/C20cfor all xin [0, 1].
(b) Write a program to simulate values from this distribution.
(c) On the average, how many candidate values must your program generate in order to create
10,000 “accepted” values?
(d) Simulate 10,000 values from this distribution, and use these to estimate the mean μof this
distribution. How close is your sample mean to the true value of μ(which you can
determine using the appropriate integral)?
(e) The supply company’s management looks at quarterly data for X, i.e., values X1,...,X13for
13 weeks (one quarter of a year). Of particular interest is the variable M¼min(X1,...,X13),
the least amount of gravel sold in one week during a quarter. Use your program in (b) to
simulate the rv M, and use the results of at least 10,000 simulated values of Mto estimate
P(M<.1), the chance that the worst sales week in a quarter saw less than .1 tons of gravel
sold. [ Hint: Simulate each Xi10,000 times for i¼1, ..., 13, and then compute the
minimum of each set of 13 values to create a value for M.]
136. The time required to complete a 3-h ﬁnal exam is modeled by the following pdf:
fðxÞ¼4
27x23/C0xðÞ 0/C20x/C203
0 otherwise8
<
:
Consider simulating values from this distribution using the accept–reject method with a
uniform “candidate” distribution on the interval [0, 3].
(a) Find the smallest majorization constant cso that f(x)/g(x)/C20cfor all xin [0, 3]. [ Hint:
What is the pdf of the uniform distribution on [0, 3]?]
(b) Write a program to simulate values from this distribution.
(c) On the average, how many candidate values must your program generate in order to
create 10,000 “accepted” values?
(d) A professor has 20 students taking her class (lucky professor!). Assume her 20 students’
completion times on the ﬁnal exam can be modeled as 20 independent observations from
the above pdf. The professor must stay at the ﬁnal exam until all 20 students are ﬁnished
(i.e., until the last student leaves). Use your program in (b) to simulate the rv L¼time, in3.8 Simulation of Continuous Random Variables 229
hours, that the professor sits proctoring her ﬁnal exam to 20 students. Use your simulation
to estimate P(L/C2135/12), the probability she will have to stay into the last 5 min of the
ﬁnal exam period.
137. The half-normal distribution has the following pdf:
fðxÞ¼ﬃﬃﬃ
2
πr
/C1e/C0x2=2x/C210
0 otherwise8
><
>:
This is the distribution of | Z|, where Z/C24N(0, 1); equivalently, it’s the pdf that arises by
“folding” the standard normal distribution in half along its line of symmetry. Consider
simulating values from this distribution using the accept–reject method with a candidate
distribution g(x)¼e/C0xforx/C210 (i.e., an exponential pdf with λ¼1).
(a) Find the inverse cdf corresponding to g(x). (This will allow us to simulate values from the
candidate distribution.)
(b) Find the smallest majorization constant cso that f(x)/g(x)/C20cfor all x/C210. [Hint: Use
calculus to determine where the ratio f(x)/g(x) is maximized.]
(c) On the average, how many candidate values will be required to generate 10,000
“accepted” values?
(d) Write a program to construct 10,000 values from a half-normal distribution.
138. As discussed previously, the normal distribution cannot be simulated using the inverse cdf
method. One possibility for simulating from a standard normal distribution is to employ the
accept–reject method with candidate distribution
gðxÞ¼1
π1þx2 ðÞ/C01 <x<1
(This is the Cauchy distribution.)
(a) Find the cdf and inverse cdf corresponding to g(x). (This will allow us to simulate values
from the candidate distribution.)
(b) Find the smallest majorization constant cso that f(x)/g(x)/C20cfor all x, where f(x) is the
standard normal pdf. [ Hint: Use calculus to determine where the ratio f(x)/g(x)i s
maximized.]
(c) On the average, how many candidate values will be required to generate 10,000 “accepted”
values?
(d) Write a program to construct 10,000 values from a standard normal distribution.
(e) Suppose that you now wish to simulate from a N(μ,σ) distribution. How would you modify
your program in part (d)?
139. Explain why the majorization constant cin the accept–reject algorithm must be /C211. [Hint:I f
c<1, then f(x)<g(x) for all x. Why is this bad?]
3.9 Supplementary Exercises (140–172)
140. An insurance company issues a policy covering losses up to 5 (in thousands of dollars). The loss,
X, follows a distribution with density function:230 3 Continuous Random Variables and Probability Distributions
fðxÞ¼3
x4x/C211
0x<18
<
:
What is the expected value of the amount paid under the policy?
141. Let X¼the time it takes a read/write head to locate a desired record on a computer disk memory
device once the head has been positioned over the correct track. If the disks rotate once every
25 msec, a reasonable assumption is that Xis uniformly distributed on the interval [0, 25].
(a) Compute P(10/C20X/C2020).
(b) Compute P(X/C2110).
(c) Obtain the cdf F(x).
(d) Compute E(X) and SD( X).
142. A 12-in. bar clamped at both ends is subjected to an increasing amount of stress until it snaps.
LetY¼the distance from the left end at which the break occurs. Suppose Yhas pdf
fðyÞ¼y
241/C0y
12/C16/C17
0/C20y/C2012
0 otherwise8
<
:
Compute the following:
(a) The cdf of Y, and graph it.
(b) P(Y/C204),P(Y>6), and P(4/C20Y/C206).
(c) E(Y),E(Y2), and SD( Y).
(d) The probability that the break point occurs more than 2 in. from the expected break point.
(e) The expected length of the shorter segment when the break occurs.
143. Let Xdenote the time to failure (in years) of a hydraulic component. Suppose the pdf of Xisf(x)
¼32/(x+4 )3forx>0.
(a) Verify that f(x) is a legitimate pdf.
(b) Determine the cdf.
(c) Use the result of part (b) to calculate the probability that time to failure is between 2 and
5 years.
(d) What is the expected time to failure?
(e) If the component has a salvage value equal to 100/(4 + x) when its time to failure is x, what
is the expected salvage value?
144. The completion time Xfor a task has cdf F(x) given by
0 x<0
x3
30/C20x<1
1/C01
27
3/C0x/C18/C197
4/C03
4x/C18/C19
1/C20x/C207
3
1 x/C217
38
>>>>>>>>><
>>>>>>>>>:
(a) Obtain the pdf f(x) and sketch its graph.
(b) Compute P(.5/C20X/C202).
(c) Compute E(X).
145. The breakdown voltage of a randomly chosen diode of a certain type is known to be normally
distributed with mean value 40 V and standard deviation 1.5 V.3.9 Supplementary Exercises (140–172) 231
(a) What is the probability that the voltage of a single diode is between 39 and 42?
(b) What value is such that only 15% of all diodes have voltages exceeding that value?
(c) If four diodes are independently selected, what is the probability that at least one has a
voltage exceeding 42?
146. The article “Computer Assisted Net Weight Control” ( Qual. Prog. , 1983: 22–25) suggests a
normal distribution with mean 137.2 oz and standard deviation 1.6 oz, for the actual contents
of jars of a certain type. The stated contents was 135 oz.
(a) What is the probability that a single jar contains more than the stated contents?
(b) Among ten randomly selected jars, what is the probability that at least eight contain more
than the stated contents?
(c) Assuming that the mean remains at 137.2, to what value would the standard deviation
have to be changed so that 95% of all jars contain more than the stated contents?
147. When circuit boards used in the manufacture of compact disk players are tested, the long-run
percentage of defectives is 5%. Suppose that a batch of 250 boards has been received and that
the condition of any particular board is independent of that of any other board.
(a) What is the approximate probability that at least 10% of the boards in the batch are
defective?
(b) What is the approximate probability that there are exactly 10 defectives in the batch?
148. The article “Reliability of Domestic-Waste Bioﬁlm Reactors” ( J. Envir. Engr. , 1995:
785–790) suggests that substrate concentration (mg/cm3) of inﬂuent to a reactor is normally
distributed with μ¼.30 and σ¼.06.
(a) What is the probability that the concentration exceeds .25?
(b) What is the probability that the concentration is at most .10?
(c) How would you characterize the largest 5% of all concentration values?
149. Let X¼the hourly median power (in decibels) of received radio signals transmitted between
two cities. The authors of the article “Families of Distributions for Hourly Median Power and
Instantaneous Power of Received Radio Signals” ( J. Res. Nat. Bureau Standards , vol. 67D,
1963: 753–762) argue that the lognormal distribution provides a reasonable probability model
forX. If the parameter values are μ¼3.5 and σ¼1.2, calculate the following:
(a) The mean value and standard deviation of received power.
(b) The probability that received power is between 50 and 250 dB.
(c) The probability that Xis less than its mean value. Why is this probability not .5?
150. Let Xbe a nonnegative continuous random variable with cdf F(x) and mean E(X).
(a) The deﬁnition of expected value is E(X)¼Ð1
0xf(x)dx. Replace the ﬁrst xinside the integral
withÐx
01dyto create a double integral expression for E(X). [The “order of integration”
should be dy dx .]
(b) Rearrange the order of integration, keeping track of the revised limits of integration, to
show that
EðXÞ¼ð1
0ð1
yfðxÞdxdy
(c) Evaluate the dxintegral in (b) to show that E(X)¼Ð1
0[1/C0F(y)]dy. (This provides an
alternate derivation of the formula established in Exercise 38.)
(d) Use the result of (c) to verify that the expected value of an exponentially distributed rv with
parameter λis 1/λ.232 3 Continuous Random Variables and Probability Distributions
151. The reaction time (in seconds) to a stimulus is a continuous random variable with pdf
fðxÞ¼3
2x21/C20x/C203
0 otherwise8
<
:
(a) Obtain the cdf.
(b) What is the probability that reaction time is at most 2.5 s? Between 1.5 and 2.5 s?
(c) Compute the expected reaction time.
(d) Compute the standard deviation of reaction time.
(e) If an individual takes more than 1.5 s to react, a light comes on and stays on either until one
further second has elapsed or until the person reacts (whichever happens ﬁrst). Determine
the expected amount of time that the light remains lit. [ Hint: Let h(X)¼the time that the
light is on as a function of reaction time X.]
152. The article “Characterization of Room Temperature Damping in Aluminum-Indium Alloys”
(Metallurgical Trans. , 1993: 1611-1619) suggests that aluminum matrix grain size ( μm) for an
alloy consisting of 2% indium could be modeled with a normal distribution with mean 96 and
standard deviation 14.
(a) What is the probability that grain size exceeds 100 μm?
(b) What is the probability that grain size is between 50 and 80 μm?
(c) What interval ( a, b) includes the central 90% of all grain sizes (so that 5% are below aand
5% are above b)?
153. The article “Determination of the MTF of Positive Photoresists Using the Monte Carlo Method”
(Photographic Sci. Engrg. , 1983: 254–260) proposes the exponential distribution with parame-
terλ¼.93 as a model for the distribution of a photon’s free path length (mm) under certain
circumstances. Suppose this is the correct model.
(a) What is the expected path length, and what is the standard deviation of path length?
(b) What is the probability that path length exceeds 3.0? What is the probability that path
length is between 1.0 and 3.0?
(c) What value is exceeded by only 10% of all path lengths?
154. The article “The Prediction of Corrosion by Statistical Analysis of Corrosion Proﬁles” ( Corro-
sion Sci. , 1985: 305–315) suggests the following cdf for the depth Xof the deepest pit in an
experiment involving the exposure of carbon manganese steel to acidiﬁed seawater:
Fx ;θ1,θ2 ðÞ ¼ e/C0e/C0x/C0θ1ðÞ =θ2/C01 <x<1
(This is called the largest extreme value distribution orGumbel distribution .) The investigators
proposed the values θ1¼150 and θ2¼90. Assume this to be the correct model.
(a) What is the probability that the depth of the deepest pit is at most 150? At most 300?
Between 150 and 300?
(b) Below what value will the depth of the maximum pit be observed in 90% of all such
experiments?
(c) What is the density function of X?
(d) The density function can be shown to be unimodal (a single peak). Above what value on
the measurement axis does this peak occur? (This value is the mode.)
(e) It can be shown that E(X)/C25.5772 θ2+θ1. What is the mean for the given values of θ1and
θ2, and how does it compare to the median and mode? Sketch the graph of the density
function.3.9 Supplementary Exercises (140–172) 233
155. Let t¼the amount of sales tax a retailer owes the government for a certain period. The article
“Statistical Sampling in Tax Audits” ( Statistics and the Law , 2008: 320–343) proposes
modeling the uncertainty in tby regarding it as a normally distributed random variable with
mean value μand standard deviation σ(in the article, these two parameters are estimated from
the results of a tax audit involving nsampled transactions). If arepresents the amount the
retailer is assessed, then an underassessment results if t>aand an overassessment if a>t.W e
can express this in terms of a loss function , a function that shows zero loss if t¼abut increases
as the gap between tandaincreases. The proposed loss function is L(a,t)¼t/C0aift>aand¼
k(a/C0t)i ft/C20a(k>1 is suggested to incorporate the idea that over-assessment is more serious
than under-assessment).
(a) Show that a*¼μ+σΦ/C01(1/(k+ 1)) is the value of athat minimizes the expected loss,
where Φ/C01is the inverse function of the standard normal cdf.
(b) If k¼2 (suggested in the article), μ¼$100,000, and σ¼$10,000, what is the optimal
value of a, and what is the resulting probability of over-assessment?
156. A mode of a continuous distribution is a value x* that maximizes f(x).
(a) What is the mode of a normal distribution with parameters μandσ?
(b) Does the uniform distribution with parameters AandBhave a single mode? Why or why
not?
(c) What is the mode of an exponential distribution with parameter λ? (Draw a picture.)
(d) If Xhas a gamma distribution with parameters αandβ, and α>1, determine the mode.
[Hint: ln[f(x)] will be maximized if and only if f(x) is, and it may be simpler to take the
derivative of ln[ f(x)].]
157. The article “Error Distribution in Navigation” ( J. Institut. Navigation , 1971: 429–442) suggests
that the frequency distribution of positive errors (magnitudes of errors) is well approximated by
an exponential distribution. Let X¼the lateral position error (nautical miles), which can be
either negative or positive. Suppose the pdf of Xis
fðxÞ¼:1e/C0:2xjj/C01 <x<1
(a) Sketch a graph of f(x) and verify that f(x) is a legitimate pdf (show that it integrates to 1).
(b) Obtain the cdf of Xand sketch it.
(c) Compute P(X/C200),P(X/C202),P(/C01/C20X/C202), and the probability that an error of more than
2 miles is made.
158. The article “Statistical Behavior Modeling for Driver-Adaptive Precrash Systems” ( IEEE
Trans. on Intelligent Transp. Systems , 2013: 1-9) proposed the following distribution for
modeling the behavior of what the authors called “the criticality level of a situation” X.
fx ;λ1,λ2,p ðÞ ¼pλ1e/C0λ1xþ1/C0pðÞ λ2e/C0λ2xx/C210
0 otherwise(
This is often called the hyperexponential ormixed exponential distribution .
(a) What is the cdf F(x;λ1,λ2,p)?
(b) If p¼.5,λ1¼40,λ2¼200 (values of the λs suggested in the cited article), calculate P(X>.01).
(c) If Xhasf(x;λ1,λ2,p) as its pdf, what is E(X)?
(d) Using the fact that E(X2)¼2/λ2when Xhas an exponential distribution with parameter λ,
compute E(X2) when Xhas pdf f(x;λ1,λ2,p). Then compute Var( X).234 3 Continuous Random Variables and Probability Distributions
(e) The coefﬁcient of variation of a random variable (or distribution) is CV ¼σ/μ. What is the
CV for an exponential rv? What can you say about the value of CV when Xhas a
hyperexponential distribution?
(f) What is the CV for an Erlang distribution with parameters λandnas deﬁned in Sect. 3.4?
[Note: In applied work, the sample CV is used to decide which of the three distributions
might be appropriate.]
(g) For the parameter values given in (b), calculate the probability that Xis within one standard
deviation of its mean value. Does this probability depend upon the values of the λs (it does
not depend on λwhen Xhas an exponential distribution)?
159. Suppose a state allows individuals ﬁling tax returns to itemize deductions only if the total of all
itemized deductions is at least $5,000. Let X(in 1000s of dollars) be the total of itemized
deductions on a randomly chosen form. Assume that Xhas the pdf
fx ;αðÞ ¼k=xα
0/C26
x/C215
otherwise
(a) Find the value of k. What restriction on αis necessary?
(b) What is the cdf of X?
(c) What is the expected total deduction on a randomly chosen form? What restriction on αis
necessary for E(X) to be ﬁnite?
(d) Show that ln( X/5) has an exponential distribution with parameter α/C01.
160. Let Iibe the input current to a transistor and Iobe the output current. Then the current gain is
proportional to ln( Io/Ii). Suppose the constant of proportionality is 1 (which amounts to choosing
a particular unit of measurement), so that current gain ¼X¼ln(Io/Ii). Assume Xis normally
distributed with μ¼1 and σ¼.05.
(a) What type of distribution does the ratio Io/Iihave?
(b) What is the probability that the output current is more than twice the input current?
(c) What are the expected value and variance of the ratio of output to input current?
161. The article “Response of SiC f/Si3N4Composites Under Static and Cyclic Loading—An Exper-
imental and Statistical Analysis” ( J. Engr. Materials Tech. , 1997: 186–193) suggests that tensile
strength (MPa) of composites under speciﬁed conditions can be modeled by a Weibull distribu-
tion with α¼9 and β¼180.
(a) Sketch a graph of the density function.
(b) What is the probability that the strength of a randomly selected specimen will exceed 175?
Will be between 150 and 175?
(c) If two randomly selected specimens are chosen and their strengths are independent of each
other, what is the probability that at least one has strength between 150 and 175?
(d) What strength value separates the weakest 10% of all specimens from the remaining 90%?
162. (a) Suppose the lifetime Xof a component, when measured in hours, has a gamma distribution
with parameters αandβ. Let Y¼lifetime measured in minutes. Derive the pdf of Y.
(b) If Xhas a gamma distribution with parameters αandβ, what is the probability distribution
ofY¼cX?
163. Based on data from a dart-throwing experiment, the article “Shooting Darts” ( Chance , Summer
1997: 16–19) proposed that the horizontal and vertical errors from aiming at a point target
should be independent of each other, each with a normal distribution having mean 0 and
standard deviation σ. It can then be shown that the pdf of the distance Vfrom the target to the
landing point is3.9 Supplementary Exercises (140–172) 235
fðvÞ¼v
σ2/C1e/C0v2=2σ2ðÞv>0
(a) This pdf is a member of what family introduced in this chapter?
(b) If σ¼20 mm (close to the value suggested in the paper), what is the probability that a dart
will land within 25 mm (roughly 1 in.) of the target?
164. The article “Three Sisters Give Birth on the Same Day” ( Chance , Spring 2001: 23–25) used the
fact that three Utah sisters had all given birth on March 11, 1998, as a basis for posing some
interesting questions regarding birth coincidences.
(a) Disregarding leap year and assuming that the other 365 days are equally likely, what is the
probability that three randomly selected births all occur on March 11? Be sure to indicate
what, if any, extra assumptions you are making.
(b) With the assumptions used in part (a), what is the probability that three randomly selected
births all occur on the same day?
(c) The author suggested that, based on extensive data, the length of gestation (time between
conception and birth) could be modeled as having a normal distribution with mean value
280 days and standard deviation 19.88 days. The due dates for the three Utah sisters were
March 15, April 1, and April 4, respectively. Assuming that all three due dates are at the
mean of the distribution, what is the probability that all births occurred on March 11?
[Hint: The deviation of birth date from due date is normally distributed with mean 0.]
(d) Explain how you would use the information in part (c) to calculate the probability of a
common birth date.
165. Exercise 49 introduced two machines that produce wine corks, the ﬁrst one having a normal
diameter distribution with mean value 3 cm and standard deviation .1 cm and the second having
a normal diameter distribution with mean value 3.04 cm and standard deviation .02
cm. Acceptable corks have diameters between 2.9 and 3.1 cm. If 60% of all corks used come
from the ﬁrst machine and a randomly selected cork is found to be acceptable, what is the
probability that it was produced by the ﬁrst machine?
166. A function g(x)i sconvex if the chord connecting any two points on the function’s graph lies
above the graph. When g(x) is differentiable, an equivalent condition is that for every x, the
tangent line at xlies entirely on or below the graph. (See the accompanying ﬁgure.) How does
g(μ)¼g[E(X)] compare to the expected value E[g(X)]? [Hint: The equation of the tangent line
atx¼μisy¼g(μ)+g0(μ)/C1(x/C0μ). Use the condition of convexity, substitute Xforx, and take
expected values. Note: Unless g(x) is linear, the resulting inequality (usually called Jensen’s
inequality) is strict ( <rather than /C20); it is valid for both continuous and discrete rvs.]
Tangent
lineChord
x
167. Let Xhave a Weibull distribution with parameters α¼2 and β. Show that Y¼2X2/β2has an
exponential distribution with λ¼1/2.236 3 Continuous Random Variables and Probability Distributions
168. Let Xhave the pdf f(x)¼1/[π(1 + x2)] for /C01 <x<1(a central Cauchy distribution), and
show that Y¼1/Xhas the same distribution. [ Hint: Consider P(|Y|/C20y), the cdf of | Y|, then obtain
its pdf and show it is identical to the pdf of | X|.]
169. Let Xhave a Weibull distribution with shape parameter αand scale parameter β. Show that the
transformed variable Y¼ln(X) has an extreme value distribution as deﬁned in Section 3.6,w i t h
θ1¼ln(β) and θ2¼1/α.
170. A store will order qgallons of a liquid product to meet demand during a particular time period.
This product can be dispensed to customers in any amount desired, so demand during the period
is a continuous random variable Xwith cdf F(x). There is a ﬁxed cost c0for ordering the product
plus a cost of c1per gallon purchased. The per-gallon sale price of the product is d. Liquid left
unsold at the end of the time period has a salvage value of eper gallon. Finally, if demand
exceeds q, there will be a shortage cost for loss of goodwill and future business; this cost is fper
gallon of unfulﬁlled demand. Show that the value of qthat maximizes expected proﬁt, denoted
byq*, satisﬁes
Psatisfying demandðÞ ¼ Fq *ðÞ ¼d/C0c1þf
d/C0eþf
Then determine the value of F(q*) if d¼$35, c0¼$25, c1¼$15, e¼$5, and f¼$25.
[Hint: Let xdenote a particular value of X. Develop an expression for proﬁt when x/C20qand
another expression for proﬁt when x>q. Now write an integral expression for expected proﬁt
(as a function of q) and differentiate.]
171. An individual’s credit score is a number calculated based on that person’s credit history that
helps a lender determine how much s/he should be loaned or what credit limit should be
established for a credit card. An article in the Los Angeles Times gave data which suggested
that a beta distribution with parameters A¼150, B¼850, α¼8,β¼2 would provide a
reasonable approximation to the distribution of American credit scores. [ Note: credit scores are
integer-valued.]
(a) Let Xrepresent a randomly selected American credit score. What are the mean and
standard deviation of this random variable? What is the probability that Xis within
1 standard deviation of its mean?
(b) What is the approximate probability that a randomly selected score will exceed 750 (which
lenders consider a very good score)?
172. Let Vdenote rainfall volume and Wdenote runoff volume (both in mm). According to the article
“Runoff Quality Analysis of Urban Catchments with Analytical Probability Models”
(J. of Water Resource Planning and Management , 2006: 4–14), the runoff volume will be 0 if
V/C20vdand will be k(V/C0vd)i fV>vd. Here vdis the volume of depression storage (a constant),
andk(also a constant) is the runoff coefﬁcient. The cited article proposes an exponential
distribution with parameter λforV.
(a) Obtain an expression for the cdf of W.[Note:Wis neither purely continuous nor purely
discrete; instead it has a “mixed” distribution with a discrete component at 0 and is
continuous for values w>0.]
(b) What is the pdf of Wforw>0? Use this to obtain an expression for the expected value of
runoff volume.3.9 Supplementary Exercises (140–172) 237
Joint Probability Distributions and Their
Applications 4
In Chaps. 2and3, we studied probability models for a single random variable. Many problems in
probability and statistics lead to models involving several random variables simultaneously. For
example, we might consider randomly selecting a college student and deﬁning X¼the student’s high
school GPA and Y¼the student’s college GPA. In this chapter, we ﬁrst discuss probability models
for the joint behavior of several random variables, putting special emphasis on the case in which the
variables are independent of each other. We then study expected values of functions of several
random variables, including covariance andcorrelation as measures of the degree of association
between two variables.
Many problem scenarios involve linear combinations of random variables. For example, suppose
an investor owns 100 shares of one stock and 200 shares of another. If X1andX2are the prices per
share of the two stocks, then the value of investor’s portfolio is 100 X1+ 200 X2. Sections 4.3and4.5
enumerate the properties of linear combinations of random variables, including the celebrated
Central Limit Theorem (CLT), which characterizes the behavior of a sum X1+X2+...+Xn
asnincreases.
The ﬁfth section considers conditional distributions, the distributions of some random variables
given the values of other random variables, e.g., the distribution of fuel efﬁciency conditional on the
weight of a vehicle.
In Sect. 3.7, we developed methods for obtaining the distribution of some function g(X)o fa
random variable. Section 4.6extends these ideas to transformations of two or more rvs. For example,
ifXandYare the scores on a two-part exam, we might be interested in the total score X+Yand also
X/(X+Y), the proportion of total points achieved on the ﬁrst part.
The chapter ends with sections on the bivariate normal distribution (Sect. 4.7), the reliability of
devices and systems (Sect. 4.8), “order statistics” such as the median and range obtained by ordering
sample observations from smallest to largest (Sect. 4.9), and simulation techniques for jointly
distributed random variables (Sect. 4.10).
4.1 Jointly Distributed Random Variables
There are many experimental situations in which more than one random variable (rv) will be of
interest to an investigator. For example Xmight be the number of books checked out from a public
library on a particular day and Ythe number of videos checked out on the same day. Or XandYmight
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_4239
be the height and weight, respectively, of a randomly selected adult. In general, the two rvs of interest
could both be discrete, both be continuous, or one could be discrete and the other continuous. In
practice, the two “pure” cases—both of the same type—predominate. We shall ﬁrst consider joint
probability distributions for two discrete rvs, then for two continuous variables, and ﬁnally for more
than two variables.
4.1.1 The Joint Probability Mass Function for Two Discrete Random Variables
The probability mass function (pmf) of a single discrete rv Xspeciﬁes how much probability mass is
placed on each possible Xvalue. The joint pmf of two discrete rvs XandYdescribes how much
probability mass is placed on each possible pair of values ( x,y).
DEFINITION
LetXandYbe two discrete rvs deﬁned on the sample space Sof an experiment. The joint
probability mass function p(x,y) is deﬁned for each pair of numbers ( x,y)b y
px;yðÞ ¼ PX¼xandY¼y ðÞ
A function p(x,y) can be used as a joint pmf provided that p(x,y)/C210 for all xandyand
∑x∑yp(x,y)¼1. Let Abe any set consisting of pairs of ( x,y) values, such as {( x, y):x+y<10}.
Then the probability that the random pair ( X,Y) lies in Ais obtained by summing the joint pmf over
pairs in A:
PX ;YðÞ 2 A ðÞ ¼XX
x;yðÞ 2 Apx;yðÞ
Example 4.1 A large insurance agency services a number of customers who have purchased both a
homeowner’s policy and an automobile policy from the agency. For each type of policy, a deductible
amount must be speciﬁed. For an automobile policy, the choices are $100 and $250, whereas for a
homeowner’s policy, the choices are 0, $100, and $200. Suppose an individual with both types of
policy is selected at random from the agency’s ﬁles. Let X¼the deductible amount on the auto
policy and Y¼the deductible amount on the homeowner’s policy. Possible ( X, Y) pairs are then
(100, 0), (100, 100), (100, 200), (250, 0), (250, 100), and (250, 200); the joint pmf speciﬁes the
probability associated with each one of these pairs, with any other pair having probability zero.
Suppose the joint pmf is as given in the accompanying joint probability table :
y
p(x,y) 0 100 200
x100 .20 .10 .20
250 .05 .15 .30
Then p(100, 100) ¼P(X¼100 and Y¼100)¼P($100 deductible on both policies) ¼.10. The
probability P(Y/C21100) is computed by summing probabilities of all ( x,y) pairs for which y/C21100:
PY/C21100ðÞ ¼ p100;100ðÞ þ p250;100ðÞ þ p100;200ðÞ þ p250;200ðÞ ¼ :75
■240 4 Joint Probability Distributions and Their Applications
Looking at the joint probability table in Example 4.1, we see that P(X¼100), i.e. pX(100), equals
.20þ.10þ.20¼.50, and similarly pX(250)¼.05þ.15þ.30¼.50 as well. That is, the pmf of Xat a
speciﬁed number is calculated by ﬁxing an xvalue (say, 100 or 250) and summing across all possible
yvalues; e.g., pX(250) ¼p(250,0) þp(250,100) þp(250,200). The pmf of Ycan be obtained by
analogous summation (adding “down” the table instead of “across”). In fact, by adding across rows
and down columns, we could imagine writing these probabilities in the margins of the joint
probability table; for this reason, pXandpYare called the marginal distributions of XandY.
DEFINITION
The marginal probability mass functions ofXand of Y, denoted by pX(x) and pY(y),
respectively, are given by
pXxðÞ ¼X
ypx;yðÞ pYyðÞ ¼X
xpx;yðÞ
Thus to obtain the marginal pmf of Xevaluated at, say, x¼100, the probabilities p(100, y) are
added over all possible yvalues. Doing this for each possible Xvalue gives the marginal pmf of
Xalone (i.e., without reference to Y). From the marginal pmfs, probabilities of events involving only
Xor only Ycan be computed.
Example 4.2 (Example 4.1 continued) The possible Xvalues are x¼100 and x¼250, so comput-
ing row totals in the joint probability table yields
pX100ðÞ ¼ p100;0ðÞ þ p100;100ðÞ þ p100;200ðÞ ¼ :50
and
pX250ðÞ ¼ p250;0ðÞ þ p250;100ðÞ þ p250;200ðÞ ¼ :50
The marginal pmf of Xis then
pXxðÞ ¼:5
0/C26
x¼100, 250
otherwise
Similarly, the marginal pmf of Yis obtained from column totals as
pYyðÞ ¼:25
:50
08
<
:y¼0, 100
y¼200
otherwise
soP(Y/C21100)¼pY(100) +p Y(200) ¼.75 as before. ■
4.1.2 The Joint Probability Density Function for Two Continuous
Random Variables
The probability that the observed value of a continuous rv Xlies in a one-dimensional set A(such as
an interval) is obtained by integrating the pdf f(x) over the set A. Similarly, the probability that the pair
(X,Y) of continuous rvs falls in a two-dimensional set A(such as a rectangle) is obtained by
integrating a function called the joint density function .4.1 Jointly Distributed Random Variables 241
DEFINITION
LetXandYbe continuous rvs. Then f(x,y) is the joint probability density function forXand
Yif for any two-dimensional set A,
PX ;YðÞ 2 A ðÞ ¼ðð
Afx;yðÞ dxdy
In particular, if Ais the two-dimensional rectangle {( x,y):a/C20x/C20b,c/C20y/C20d}, then
PX ;YðÞ 2 A ðÞ ¼ Pa/C20X/C20b,c/C20Y/C20d ðÞ ¼ðb
aðd
cfx;yðÞ dydx
Forf(x,y) to be a joint pdf, it must satisfy f(x,y)/C210 andÐ1
/C01Ð1
/C01f(x,y)dxdy¼1. We can
think of f(x,y) as specifying a surface at height f(x,y) above the point ( x,y) in a three-dimensional
coordinate system. Then P((X,Y)2A) is the volume underneath this surface and above the region A,
analogous to the area under a curve in the one-dimensional case. This is illustrated in Fig. 4.1.
Example 4.3 A bank operates both a drive-up facility and a walk-up window. On a randomly
selected day, let X¼the proportion of time that the drive-up facility is in use (at least one customer
is being served or waiting to be served) and Y¼the proportion of time that the walk-up window is in
use. Then the set of possible values for ( X,Y) is the rectangle {( x,y): 0/C20x/C201, 0/C20y/C201}.
Suppose the joint pdf of ( X,Y) is given by
fx;yðÞ ¼6
5xþy2/C0/C1
0/C20x/C201, 0/C20y/C201
0 otherwise8
<
:
To verify that this is a legitimate pdf, note that f(x, y)/C210 and
ð1
/C01ð1
/C01fx;yðÞ dxdy¼ð1
0ð1
06
5xþy2/C0/C1
dxdy¼ð1
0ð1
06
5xdxdyþð1
0ð1
06
5y2dxdy
¼ð1
06
5xdxþð1
06
5y2dy¼6
10þ6
15¼1
The probability that neither facility is busy more than one-quarter of the time isy
xf(x, y)
Surface f(x, y)
A = Shaded
      rectangle  Fig. 4.1 P((X,Y)2A)¼
volume under density
surface above A242 4 Joint Probability Distributions and Their Applications
P0/C20X/C201
4,0/C20Y/C201
4/C18/C19
¼ð1=4
0ð1=4
06
5xþy2/C0/C1
dxdy¼6
5ð1=4
0ð1=4
0xdxdy þ6
5ð1=4
0ð1=4
0y2dxdy
¼6
20/C1x2
2/C12/C12/C12/C12x¼1=4
x¼0þ6
20/C1y3
3/C12/C12/C12/C12y¼1=4
y¼0¼7
640¼:0109
■
The marginal pmf of one discrete variable results from summing the joint pmf over all values of
theother variable. Similarly, the marginal pdf of one continuous variable is obtained by integrating
the joint pdf over all values of the other variable.
DEFINITION
The marginal probability density functions ofXandY, denoted by fX(x) and fY(y), respec-
tively, are given by
fXxðÞ ¼ð1
/C01fx;yðÞ dy for/C01 <x<1
fYyðÞ ¼ð1
/C01fx;yðÞ dx for/C01 <y<1
Example 4.4 (Example 4.3 continued) The marginal pdf of X, which gives the probability distribu-
tion of busy time for the drive-up facility without reference to the walk-up window, is
fXxðÞ ¼ð1
/C01fx;yðÞ dy¼ð1
06
5xþy2/C0/C1
dy¼6
5xþ2
5
for 0/C20x/C201 and 0 otherwise. Similarly, the marginal pdf of Yis
fYyðÞ ¼6
5y2þ3
50/C20y/C201
0 otherwise8
<
:
Then, for example,
P1
4/C20Y/C203
4/C18/C19
¼ð3=4
1=46
5y2þ3
5/C18/C19
dy¼37
80¼:4625 ■
In Example 4.3, the region of positive joint density was a rectangle, which made computation of
the marginal pdfs relatively easy. Consider now an example in which the region of positive density is
a more complicated ﬁgure.
Example 4.5 A nut company markets cans of deluxe mixed nuts containing almonds, cashews, and
peanuts. Suppose the net weight of each can is exactly 1 lb, but the weight contribution of each type of
nut is random. Because the three weights sum to 1, a joint probability model for any two gives all
necessary information about the weight of the third type. Let X¼the weight of almonds in a selected
can and Y¼the weight of cashews. Then the region of positive density is D¼{(x,y): 0/C20x/C201,
0/C20y/C201,x+y /C201}, the shaded region pictured in Fig. 4.2.4.1 Jointly Distributed Random Variables 243
Now let the joint pdf for ( X,Y)b e
fx;yðÞ ¼24xy
0/C26
0/C20x/C201, 0 /C20y/C201, xþy/C201
otherwise
For any ﬁxed x,f(x,y) increases with y; for ﬁxed y,f(x,y) increases with x. This is appropriate
because the word deluxe implies that most of the can should consist of almonds and cashews rather
than peanuts, so that the density function should be large near the upper boundary and small near the
origin. The surface determined by f(x,y) slopes upward from zero as ( x,y) moves away from
either axis.
Clearly, f(x,y)/C210. To verify the second condition on a joint pdf, recall that a double integral is
computed as an iterated integral by holding one variable ﬁxed (such as xas in Fig. 4.2), integrating
over values of the other variable lying along the straight line passing through the value of the ﬁxed
variable, and ﬁnally integrating over all possible values of the ﬁxed variable. Thus
ð1
/C01ð1
/C01fx;yðÞ dydx¼ðð
Dfx;yðÞ dydx¼ð1
0ð1/C0x
024xydy/C26/C27
dx
¼ð1
024xy2
2/C12/C12/C12/C12y¼1/C0x
y¼0()
dx¼ð1
012x1/C0xðÞ2dx¼1
To compute the probability that the two types of nuts together make up at most 50% of the can, let
A¼{(x,y): 0/C20x/C201, 0/C20y/C201, and x+y /C20.5}, as shown in Fig. 4.3.T h e n
PX ;YðÞ 2 A ðÞ ¼ðð
Afx;yðÞ dxdy¼ð:5
0ð:5/C0x
024xydydx ¼:0625x(0, 1)
x (1, 0)y
x+y 
= 1(x, 1−x)Fig. 4.2 Region of
positive density for
Example 4.5
x 1 .5 01
.5
0y = .5 − x A = Shaded region  
x+y = 1x+y = .5
Fig. 4.3 Computing P((X,Y)2A) for Example 4.5244 4 Joint Probability Distributions and Their Applications
The marginal pdf for almonds is obtained by holding Xﬁxed at x(again, as in Fig. 4.2) and
integrating f(x, y) along the vertical line through x:
fXxðÞ ¼ð1
/C01fx;yðÞ dy¼ð1/C0x
024xydy¼12x1/C0xðÞ20/C20x/C201
0 otherwise8
<
:
By symmetry of f(x,y) and the region D, the marginal pdf of Yis obtained by replacing xandXin
fX(x)b yyandY, respectively. ■
4.1.3 Independent Random Variables
In many situations, information about the observed value of one of the two variables XandYgives
information about the value of the other variable. In Example 4.1, the marginal probability of Xat
x¼250 was .5, as was the probability that X¼100. If, however, we are told that the selected
individual had Y¼0, then X¼100 is four times as likely as X¼250. Thus there is a dependence
between the two variables.
In Chap. 1we pointed out that one way of deﬁning independence of two events is to say that Aand
Bare independent if P(A\B)¼P(A)/C1P(B). Here is an analogous deﬁnition for the independence
of two rvs.
DEFINITION
Two random variables XandYare said to be independent if for every pair of xandyvalues,
px;yðÞ ¼ pXxðÞ /C1 pYyðÞwhen XandYare discrete
or
fx;yðÞ ¼ fXxðÞ /C1 fYyðÞwhen XandYare continuousð4:1Þ
If Eq. ( 4.1) is not satisﬁed for all ( x,y), then XandYare said to be dependent .
The deﬁnition says that two variables are independent if their joint pmf or pdf is the product of the
two marginal pmfs or pdfs.
Example 4.6 In the insurance situation of Examples 4.1 and 4.2,
p100;100ðÞ ¼ :106¼:5ðÞ:25ðÞ ¼ pX100ðÞ /C1 pY100ðÞ
soXandYare not independent. Independence of XandYrequires that every entry in the joint
probability table be the product of the corresponding row and column marginal probabilities. ■
Example 4.7 (Example 4.5 continued) Because f(x,y) in the nut scenario has the form of a product,
XandYmight appear to be independent. However, although fX3
4/C0/C1
¼fY3
4/C0/C1
¼9
16,f3
4;3
4/C0/C1
¼06¼
9
16/C19
16, so the variables are not in fact independent. To be independent, f(x,y) must have the form
g(x)/C1h(y)andthe region of positive density must be a rectangle whose sides are parallel to the
coordinate axes. ■
Independence of two random variables most often arises when the description of the experiment
under study tells us that XandYhave no effect on each other. Then once the marginal pmfs or pdfs4.1 Jointly Distributed Random Variables 245
have been speciﬁed, the joint pmf or pdf is simply the product of the two marginal functions. It
follows that
Pa/C20X/C20b,c/C20Y/C20d ðÞ ¼ Pa/C20X/C20b ðÞ /C1 Pc/C20Y/C20d ðÞ
Example 4.8 Suppose that the lifetimes of two components are independent of each other and that
the ﬁrst lifetime, X1, has an exponential distribution with parameter λ1whereas the second, X2, has an
exponential distribution with parameter λ2. Then the joint pdf is
fx1;x2ðÞ ¼ fX1/C0
x1/C1
/C1fX2/C0
x2/C1
¼λ1e/C0λ1x1/C1λ2e/C0λ2x2¼λ1λ2e/C0λ1x1/C0λ2x2 x1>0,x2>0
0 otherwise/C26
Letλ1¼1/1000 and λ2¼1/1200, so that the expected lifetimes are 1000 h and 1200 h, respec-
tively. The probability that both component lifetimes are at least 1500 h is
PX 1/C211500, X2/C211500 ðÞ ¼ P/C0
X1/C211500/C1
/C1P/C0
X2/C211500/C1
¼ð1
1500λ1e/C0λ1x1dx1/C1ð1
1500λ2e/C0λ2x2dx2
¼e/C0λ11500ðÞ/C1e/C0λ21500ðÞ¼:2231ðÞ/C0
:2865/C1
¼:0639
The probability that the sum of their lifetimes, X1+X2, is at most 3000 h requires a double integral
of the joint pdf:
PX 1þX2/C203000 ðÞ ¼ PX 1/C203000/C0X2 ðÞ ¼ð3000
0ð3000/C0x2
0fx1;x2ðÞ dx1dx2
¼ð3000
0ð3000/C0x2
0λ1λ2e/C0λ1x1/C0λ2x2dx1dx2¼ð3000
0λ2e/C0λ2x2/C0e/C0λ1x1/C2/C33000/C0x2
0dx2
¼ð3000
0λ2e/C0λ2x21/C0e/C0λ13000/C0x2 ðÞ/C2/C3
dx2¼λ2ð3000
0e/C0λ2x2/C0e/C03000 λ1eλ1/C0λ2 ðÞ x2/C2/C3
dx2¼:7564 ■
4.1.4 More Than Two Random Variables
To model the joint behavior of more than two random variables, we extend the concept of a joint
distribution of two variables.
DEFINITION
IfX1,X2,...,Xnare all discrete random variables, the joint pmf of the variables is the function
px 1;x2;...;xn ðÞ ¼ PX 1¼x1\X2¼x2\...\Xn¼xn ðÞ
If the variables are continuous, the joint pdf ofX1,X2,...,Xnis the function f(x1,x2,...,xn)
such that for any nintervals [ a1,b1],...,[an,bn],
Pa 1/C20X1/C20b1,...,an/C20Xn/C20bn ðÞ ¼ðb1
a1...ðbn
anfx1;...;xn ðÞ dxn...dx1246 4 Joint Probability Distributions and Their Applications
Example 4.9 A binomial experiment consists of ndichotomous (success-failure), homogenous
(constant success probability) independent trials. Now consider a trinomial experiment in which
each of the ntrials can result in one of three possible outcomes. For example, each successive
customer at a store might pay with cash, a credit card, or a debit card. The trials are assumed
independent. Let p1¼P(trial results in a type 1 outcome) and deﬁne p2andp3analogously for type
2 and type 3 outcomes. The random variables of interest here are Xi¼the number of trials that result
in a type ioutcome for i¼1, 2, 3.
Inn¼10 trials, the probability that the ﬁrst ﬁve are type 1 outcomes, the next three are type 2,
and the last two are type 3—i.e., the probability of the experimental outcome 1111122233—is
p15/C1p23/C1p32. This is also the probability of the outcome 1122311123, and in fact the probability of
any outcome that has exactly ﬁve 1s, three 2s, and two 3s. Now to determine the probability
P(X1¼5,X2¼3, and X3¼2), we have to count the number of outcomes that have exactly ﬁve
1s, three 2s, and two 3s. First, there are10
5/C18/C19
ways to choose ﬁve of the trials to be the type
1 outcomes. Now from the remaining ﬁve trials, we choose three to be the type 2 outcomes, which can
be done in5
3/C18/C19
ways. This determines the remaining two trials which consist of type 3 outcomes. So
the total number of ways of choosing ﬁve 1s, three 2s, and two 3s is
10
5/C18/C19
/C15
3/C18/C19
¼10!
5!5!/C15!
3!2!¼10!
5!3!2!¼2520
Thus we see that P(X1¼5,X2¼3,X3¼2)¼2520 p15/C1p23/C1p32. Generalizing this to ntrials
gives
px 1;x2;x3 ðÞ ¼ PX 1¼x1,x2,X2¼x2,X3¼x3 ðÞ ¼n!
x1!x2!x3!px1
1px2
2px3
3
forx1¼0, 1, 2, ...;x2¼0, 1, 2, ...;x3¼0, 1, 2, ...such that x1+x2+x3¼n. Notice that whereas
there are three random variables here, the third variable X3is actually redundant, because for example
in the case n¼10, having X1¼5 and X2¼3 implies that X3¼2 (just as in a binomial experiment
there are actually two rvs—the number of successes and number of failures—but the latter is
redundant).
As an example, the genotype of a pea section can be either AA, Aa, or aa. A simple genetic model
speciﬁes P(AA) ¼.25,P(Aa)¼.50, and P(aa)¼.25. If the alleles of ten independently obtained
sections are determined, the probability that exactly ﬁve of these are Aa and two are AA is
p2;5;3ðÞ ¼10!
2!5!3!:25ðÞ2:50ðÞ5:25ðÞ3¼:0769 ■
The trinomial scenario of Example 4.9 can be generalized by considering a multinomial experi-
ment consisting of nindependent and identical trials, in which each trial can result in any one of
rpossible outcomes. Let pi¼P(outcome ion any particular trial), and deﬁne random variables by
Xi¼the number of trials resulting in outcome i(i¼1,...,r). The joint pmf of X1,...,Xris called the
multinomial distribution . An argument analogous to what was done in Example 4.9 gives the joint
pmf of X1,...,Xr:4.1 Jointly Distributed Random Variables 247
px1;...;xr ðÞ ¼n!
x1!x2!/C1/C1/C1xr!px1
1/C1/C1/C1/C1/C1 pxr
rforxi¼0, 1, 2, ...with x1þ/C1/C1/C1þ xr¼n
0 otherwise8
<
:
The case r¼2 reduces to the binomial distribution, with X1¼number of successes and X2¼
n/C0X1¼number of failures. Both the multinomial and binomial distributions model discrete rvs
(counts). Next, let’s consider some examples with more than two continuous random variables.
Example 4.10 When a certain method is used to collect a ﬁxed volume of rock samples in a region,
there are four resulting rock types. Let X1,X2, and X3denote the proportion by volume of rock types
1, 2, and 3 in a randomly selected sample (the proportion of rock type 4 is 1 /C0X1/C0X2/C0X3,s oa
variable X4would be redundant). If the joint pdf of X1,X2,X3is
fx1;x2;x3 ðÞ ¼kx1x21/C0x3 ðÞ
0(
0/C20x1/C201, 0/C20x2/C201, 0/C20x3/C201,x1þx2þx3/C201
otherwise
then kis determined by
1¼ð1
/C01ð1
/C01ð1
/C01fx1;x2;x3 ðÞ dx3dx2dx1
¼ð1
0ð1/C0x1
0ð1/C0x1/C0x2
0kx1x21/C0x3 ðÞ dx3/C20/C21
dx2/C26/C27
dx1
This iterated integral has value k/144, so k¼144. The probability that rocks of types 1 and
2 together account for at most 50% of the sample is
PX 1þX2/C20:5 ðÞ ¼ððð
0/C20xi/C201 for i¼1, 2, 3
x1þx2þx3/C201,x1þx2/C20:5/C26/C27fx1;x2;x3 ðÞ dx3dx2dx1
¼ð:5
0ð:5/C0x1
0ð1/C0x1/C0x2
0144x1x21/C0x3 ðÞ dx3/C20/C21
dx2/C26/C27
dx1¼:6066 ■
The notion of independence of more than two random variables is similar to the notion of
independence of more than two events. Random variables X1,X2,...,Xnare said to be independent
if for every subset Xi1,Xi2,...,Xikof the variables (each pair, each triple, and so on), the joint pmf or
pdf of the subset is equal to the product of the marginal pmfs or pdfs. Thus if the variables are
independent with n¼4, then the joint pmf or pdf of any two variables is the product of the two
marginals, and similarly for any three variables and all four variables together. Most important, once
we are told that nvariables are independent, then the joint pmf or pdf is the product of the nmarginals.
Example 4.11 IfX1,...,Xnrepresent the lifetimes of ncomponents, the components operate
independently of each other, and each lifetime is exponentially distributed with parameter λ, then
fx1;x2;...;xn ðÞ ¼ λe/C0λx1/C0/C1
/C1λe/C0λx2/C0/C1
/C1/C1/C1/C1/C1 λe/C0λxn/C0/C1
¼λne/C0λΣxix1>0,x2>0,...,xn>0
0 otherwise/C26248 4 Joint Probability Distributions and Their Applications
If these ncomponents are connected in series, so that the system will fail as soon as a single
component fails, then the probability that the system lasts past time tis
PX 1>t,...,Xn>t ðÞ ¼ð1
t...ð1
tfx1;...;xn ðÞ dx1...dxn
¼ð1
tλe/C0λx1dx1/C18/C19
/C1/C1/C1ð1
tλe/C0λxndxn/C18/C19
¼e/C0λt/C0/C1n¼e/C0nλt
Therefore,
Psystem lifetime /C20t ðÞ ¼ 1/C0e/C0nλtfort/C210
which shows that system lifetime has an exponential distribution with parameter nλ; the expected
value of system lifetime is 1/( nλ).
A variation on the foregoing scenario appeared in the article “A Method for Correlating Field Life
Degradation with Reliability Prediction for Electronic Modules” ( Quality and Reliability Engr. Intl. ,
2005: 715–726). The investigators considered a circuit card with nsoldered chip resistors. The failure
time of a card is the minimum of the individual solder connection failure times (mileages here). It was
assumed that the solder connection failure mileages were independent, that failure mileage would exceed
tif and only if the shear strength of a connection exceeded a threshold d, and that each shear strength was
normally distributed with a mean value and standard deviation that depended on the value of mileage t:
μ(t)¼a1/C0a2tandσ(t)¼a3+a4t(a weld’s shear strength typically deteriorates and becomes more
variable as mileage increases). Then the probability that the failure mileage of a card exceeds tis
PT>tðÞ ¼ 1/C0Φd/C0a1/C0a2t ðÞ
a3þa4t/C18/C19/C18/C19n
The cited article suggested values for dand the ais based on data. In contrast to the exponential
scenario, normality of individual lifetimes does not imply normality of system lifetime. ■
Example 4.11 gives you a taste of the sub-ﬁeld of probability called reliability , the study of how
long devices and/or systems operate; see Exercises 16 and 17 as well. We will explore reliability in
great depth in Sect. 4.8.
4.1.5 Exercises: Section 4.1(1–22)
1. A service station has both self-service and full-service islands. On each island, there is a single
regular unleaded pump with two hoses. Let Xdenote the number of hoses being used on the self-
service island at a particular time, and let Ydenote the number of hoses on the full-service island in
use at that time. The joint pmf of XandYappears in the accompanying table.
y
p(x, y) 012
x0 .10 .04 .02
1 .08 .20 .06
2 .06 .14 .30
(a) What is P(X¼1 and Y¼1)?
(b) Compute P(X/C201 and Y/C201).
(c) Give a word description of the event { X6¼0a n d Y6¼0}, and compute the probability of this
event.4.1 Jointly Distributed Random Variables 249
(d) Compute the marginal pmf of Xand of Y. Using pX(x), what is P(X/C201)?
(e) Are XandYindependent rvs? Explain.
2. A large but sparsely populated county has two small hospitals, one at the south end of the county
and the other at the north end. The south hospital’s emergency room has 4 beds, whereas the north
hospital’s emergency room has only 3 beds. Let Xdenote the number of south beds occupied at a
particular time on a given day, and let Ydenote the number of north beds occupied at the same time
on the same day. Suppose that these two rvs are independent, that the pmf of Xputs probability
masses .1, .2, .3, .2, and .2 on the xvalues 0, 1, 2, 3, and 4, respectively, and that the pmf of
Ydistributes probabilities .1, .3, .4, and .2 on the yvalues 0, 1, 2, and 3, respectively.
(a) Display the joint pmf of XandYin a joint probability table.
(b) Compute P(X/C201 and Y/C201) by adding probabilities from the joint pmf, and verify that this
equals the product of P(X/C201) and P(Y/C201).
(c) Express the event that the total number of beds occupied at the two hospitals combined is at
most 1 in terms of XandY, and then calculate this probability.
(d) What is the probability that at least one of the two hospitals has no beds occupied?
3. A market has both an express checkout line and a superexpress checkout line. Let X1denote the
number of customers in line at the express checkout at a particular time of day, and let X2denote
the number of customers in line at the superexpress checkout at the same time. Suppose the joint
pmf of X1andX2is as given in the accompanying table.
x2
0123
0 .08 .07 .04 .00
1 .06 .15 .05 .04
x1 2 .05 .04 .10 .06
3 .00 .03 .04 .07
4 .00 .01 .05 .06
(a) What is P(X1¼1,X2¼1), that is, the probability that there is exactly one customer in each
line?
(b) What is P(X1¼X2), that is, the probability that the numbers of customers in the two lines are
identical?
(c) Let Adenote the event that there are at least two more customers in one line than in the other
line. Express Ain terms of X1andX2, and calculate the probability of this event.
(d) What is the probability that the total number of customers in the two lines is exactly four? At
least four?
(e) Determine the marginal pmf of X1, and then calculate the expected number of customers in
line at the express checkout.
(f) Determine the marginal pmf of X2.
(g) By inspection of the probabilities P(X1¼4),P(X2¼0), and P(X1¼4,X2¼0), are X1and
X2independent random variables? Explain.
4. Suppose 51% of the individuals in a certain population have brown eyes, 32% have blue eyes, and
the remainder have green eyes. Consider a random sample of 10 people from this population.
(a) What is the probability that 5 of the 10 people have brown eyes, 3 of 10 have blue eyes, and
the other 2 have green eyes?
(b) What is the probability that exactly one person in the sample has blue eyes and exactly one
has green eyes?250 4 Joint Probability Distributions and Their Applications
(c) What is the probability that at least 7 of the 10 people have brown eyes? [ Hint: Think of
brown as a success and all other eye colors as failures.]
5. At a certain university, 20% of all students are freshmen, 18% are sophomores, 21% are juniors,
and 41% are seniors. As part of a promotion, the university bookstore is running a rafﬂe for which
all students are eligible. Ten students will be randomly selected to receive prizes (in the form of
textbooks for the term).
(a) What is the probability the winners consist of two freshmen, two sophomores, two juniors,
and four seniors?
(b) What is the probability the winners are split equally among underclassmen (freshmen and
sophomores) and upperclassmen (juniors and seniors)?
(c) The rafﬂe resulted in no freshmen being selected. The freshman class president complained
that something must be amiss for this to occur. Do you agree? Explain.
6. According to the Mars Candy Company, the long-run percentages of various colors of M&M’s
milk chocolate candies are as follows:
Blue: 24% Orange: 20% Green: 16% Yellow: 14% Red: 13% Brown: 13%
(a) In a random sample of 12 candies, what is the probability that there are exactly two of each
color?
(b) In a random sample of 6 candies, what is the probability that at least one color is not included?
(c) In a random sample of 10 candies, what is the probability that there are exactly 3 blue candies
and exactly 2 orange candies?
(d) In a random sample of 10 candies, what is the probability that there are at most 3 orange
candies? [ Hint: Think of an orange candy as a success and any other color as a failure.]
(e) In a random sample of 10 candies, what is the probability that at least 7 are either blue,
orange, or green?
7. The number of customers waiting for gift-wrap service at a department store is an rv Xwith
possible values 0, 1, 2, 3, 4 and corresponding probabilities .1, .2, .3, .25, .15. A randomly selected
customer will have 1, 2, or 3 packages for wrapping with probabilities .6, .3, and .1, respectively.
LetY¼the total number of packages to be wrapped for the customers waiting in line (assume that
the number of packages submitted by one customer is independent of the number submitted by any
other customer).
(a) Determine P(X¼3,Y¼3), that is, p(3, 3).
(b) Determine p(4, 11).
8. Let Xdenote the number of Canon digital cameras sold during a particular week by a certain store.
The pmf of Xis
x 01234
pX(x) .1 .2 .3 .25 .15
Sixty percent of all customers who purchase these cameras also buy an extended warranty. Let
Ydenote the number of purchasers during this week who buy an extended warranty.
(a) What is P(X¼4,Y¼2)? [ Hint: This probability equals P(Y¼2jX¼4)/C1P(X¼4); now
think of the four purchases as four trials of a binomial experiment, with success on a trial
corresponding to buying an extended warranty.]
(b) Calculate P(X¼Y).
(c) Determine the joint pmf of XandYand then the marginal pmf of Y.
9. The joint probability distribution of the number Xof cars and the number Yof buses per signal
cycle at a proposed left-turn lane is displayed in the accompanying joint probability table.4.1 Jointly Distributed Random Variables 251
y
p(x,y) 012
0.025 .015 .010
1.050 .030 .020
2.125 .075 .050
x 3.150 .090 .060
4.100 .060 .040
5.050 .030 .020
(a) What is the probability that there is exactly one car and exactly one bus during a cycle?
(b) What is the probability that there is at most one car and at most one bus during a cycle?
(c) What is the probability that there is exactly one car during a cycle? Exactly one bus?
(d) Suppose the left-turn lane is to have a capacity of ﬁve cars, and one bus is equivalent to three
cars. What is the probability of an overﬂow during a cycle?
(e) Are XandYindependent rvs? Explain.
10. A stockroom currently has 30 components of a certain type, of which 8 were provided by supplier
1, 10 by supplier 2, and 12 by supplier 3. Six of these are to be randomly selected for a particular
assembly. Let X¼the number of supplier 1’s components selected, Y¼the number of supplier
2’s components selected, and p(x,y) denote the joint pmf of XandY.
(a) What is p(3, 2)? [ Hint: Each sample of size 6 is equally likely to be selected. Therefore,
p(3, 2) ¼(number of outcomes with X¼3 and Y¼2)/(total number of outcomes). Now
use the product rule for counting to obtain the numerator and denominator.]
(b) Using the logic of part (a), obtain p(x,y). (This can be thought of as a multivariate
hypergeometric distribution—sampling without replacement from a ﬁnite population
consisting of more than two categories.)
11. Each front tire of a vehicle is supposed to be ﬁlled to a pressure of 26 psi. Suppose the actual air
pressure in each tire is a random variable— Xfor the right tire and Yfor the left tire, with joint pdf
fx;yðÞ ¼kx2þy2ðÞ 20/C20x/C2030, 20 /C20y/C2030
0 otherwise/C26
(a) What is the value of k?
(b) What is the probability that both tires are underﬁlled?
(c) What is the probability that the difference in air pressure between the two tires is at most 2 psi?
(d) Determine the (marginal) distribution of air pressure in the right tire alone.
(e) Are XandYindependent rvs?
12. Annie and Alvie have agreed to meet between 5:00 and 6:00 p.m. for dinner at a local health-food
restaurant. Let X¼Annie’s arrival time and Y¼Alvie’s arrival time. Suppose XandYare
independent with each uniformly distributed on the interval [5, 6].
(a) What is the joint pdf of XandY?
(b) What is the probability that they both arrive between 5:15 and 5:45?
(c) If the ﬁrst one to arrive will wait only 10 min before leaving to eat elsewhere, what is the
probability that they have dinner at the health-food restaurant? [ Hint: The event of interest is
A¼ x;yðÞ :jx/C0yj/C201
6/C8/C9
:]
13. Two different professors have just submitted ﬁnal exams for duplication. Let Xdenote the
number of typographical errors on the ﬁrst professor’s exam and Ydenote the number of such
errors on the second exam. Suppose Xhas a Poisson distribution with parameter μ1,Yhas a
Poisson distribution with parameter μ2, and XandYare independent.252 4 Joint Probability Distributions and Their Applications
(a) What is the joint pmf of XandY?
(b) What is the probability that at most one error is made on both exams combined?
(c) Obtain a general expression for the probability that the total number of errors in the two
exams is m(where mis a nonnegative integer). [ Hint:A¼{(x, y):x+y ¼m}¼{(m, 0),
(m/C01, 1), ..., (1, m/C01), (0, m)}. Now sum the joint pmf over ( x, y)2Aand use the
binomial theorem, which says that
Xm
k¼0m
k/C18/C19
akbm/C0k¼aþbðÞm
for any a,b.]
14. Two components of a computer have the following joint pdf for their useful lifetimes XandY:
fx;yðÞ ¼xe/C0x1þyðÞx/C210 and y/C210
0 otherwise/C26
(a) What is the probability that the lifetime Xof the ﬁrst component exceeds 3?
(b) What are the marginal pdfs of XandY? Are the two lifetimes independent? Explain.
(c) What is the probability that the lifetime of at least one component exceeds 3?
15. You have two lightbulbs for a particular lamp. Let X¼the lifetime of the ﬁrst bulb and Y¼the
lifetime of the second bulb (both in thousands of hours). Suppose that XandYare independent
and that each has an exponential distribution with parameter λ¼1.
(a) What is the joint pdf of XandY?
(b) What is the probability that each bulb lasts at most 1000 h (i.e., X/C201 and Y/C201)?
(c) What is the probability that the total lifetime of the two bulbs is at most 2? [ Hint: Draw a
picture of the region A¼{(x, y):x/C210,y/C210,x+y /C202} before integrating.]
(d) What is the probability that the total lifetime is between 1 and 2?
16. Suppose that you have ten lightbulbs, that the lifetime of each is independent of all the other
lifetimes, and that each lifetime has an exponential distribution with parameter λ.
(a) What is the probability that all ten bulbs fail before time t?
(b) What is the probability that exactly kof the ten bulbs fail before time t?
(c) Suppose that nine of the bulbs have lifetimes that are exponentially distributed with
parameter λand that the remaining bulb has a lifetime that is exponentially distributed
with parameter θ(it is made by another manufacturer). What is the probability that exactly
ﬁve of the ten bulbs fail before time t?
17. Consider a system consisting of three components as pictured. The system will continue to function
as long as the ﬁrst component functions and either component 2 or component 3 functions. Let X1,
X2,a n d X3denote the lifetimes of components 1, 2, and 3, respectively. Suppose the Xisa r e
independent of each other and each Xihas an exponential distribution with parameter λ.
12
3
(a) Let Ydenote the system lifetime. Obtain the cumulative distribution function of Yand
differentiate to obtain the pdf. [ Hint:F(y)¼P(Y/C20y); express the event { Y/C20y} in terms
of unions and/or intersections of the three events { X1/C20y}, {X2/C20y}, and { X3/C20y}.]
(b) Compute the expected system lifetime.4.1 Jointly Distributed Random Variables 253
18. (a) For f(x1,x2,x3) as given in Example 4.10, compute the joint marginal density function of
X1andX3alone (by integrating over x2).
(b) What is the probability that rocks of types 1 and 3 together make up at most 50% of the
sample? [ Hint: Use the result of part (a).]
(c) Compute the marginal pdf of X1alone. [ Hint: Use the result of part (a).]
19. An ecologist selects a point inside a circular sampling region according to a uniform distribution.
LetX¼thexcoordinate of the point selected and Y¼theycoordinate of the point selected. If
the circle is centered at (0, 0) and has radius r, then the joint pdf of XandYis
fx;yðÞ ¼1
πr2x2þy2/C20r2
0 otherwise8
<
:
(a) What is the probability that the selected point is within r/2 of the center of the circular
region? [ Hint: Draw a picture of the region of positive density D. Because f(x,y) is constant
onD, computing a probability reduces to computing an area.]
(b) What is the probability that both XandYdiffer from 0 by at most r/2?
(c) Answer part (b) for r=ﬃﬃﬃ
2p
replacing r/2.
(d) What is the marginal pdf of X?O f Y?A r e XandYindependent?
20. Each customer making a particular Internet purchase must pay with one of three types of
credit cards (think Visa, MasterCard, AmEx). Let Ai(i¼1, 2, 3) be the event that a type
icredit card is used, with P(A1)¼.5,P(A2)¼.3,P(A3)¼.2. Suppose that the number of
customers who make a purchase on a given day, N, is a Poisson rv with parameter μ. Deﬁne
rvsX1,X2,X3byXi¼the number among the Ncustomers who use a type icard ( i¼1, 2, 3).
Show that these three rvs are independent with Poisson distributions having parameters .5 μ,. 3μ,
and .2 μ, respectively. [ Hint: For non-negative integers x1,x2,x3, let n¼x1+x2+x3. Then
P(X1¼x1,X2¼x2,X3¼x3)¼P(X1¼x1,X2¼x2,X3¼x3,N¼n). Now condition on
N¼n, in which case the three Xis have a trinomial distribution (multinomial with 3 categories)
with category probabilities .5, .3, and .2.]
21. Consider randomly selecting two points A and B on the circumference of a circle by selecting
their angles of rotation, in degrees, independently from a uniform distribution on the interval [0,
360]. Connect points A and B with a straight line segment. What is the probability that this
random chord is longer than the side of an equilateral triangle inscribed inside the circle?
(This is called Bertrand’s Chord Problem in the probability literature. There are other ways of
randomly selecting a chord that give different answers from the one appropriate here.) [ Hint:
Place one of the vertices of the inscribed triangle at A. You should then be able to intuit the
answer visually without having to do any integration.]
22. Section 3.8introduced the accept–reject method for simulating continuous rvs. Refer back to that
algorithm in order to answer the questions below.
(a) Show that the probability a candidate value is “accepted” equals 1/ c.[Hint: According to the
algorithm, this occurs iff U/C20f(Y)/cg(Y), where U~ Unif[0, 1) and Y~g. Compute the
relevant double integral.]
(b) Argue that the average number of candidates required to generate a single accepted value is c.
(c) Show that the accept–reject method does result in an observation from the pdf fby
showing that P(accepted value /C20x)¼F(x), where Fis the cdf corresponding to f.
[Hint: Let Xdenote the accepted value. Then P(X/C20x)¼P(Y/C20x|Yaccepted) ¼
P(Y/C20x\Yaccepted)/ P(Yaccepted).]254 4 Joint Probability Distributions and Their Applications
4.2 Expected Values, Covariance, and Correlation
We previously saw that any function h(X) of a single rv Xis itself a random variable. However, to
compute E[h(X)], it was not necessary to obtain the probability distribution of h(X); instead, E[h(X)]
was computed as a weighted average of h(X) values, where the weight function was the pmf p(x)o r
pdff(x)o fX. A similar result holds for a function h(X,Y) of two jointly distributed random variables.
PROPOSITION
LetXandYbe jointly distributed rvs with pmf p(x,y) or pdf f(x,y) according to whether the
variables are discrete or continuous. Then the expected value of a function h(X,Y), denoted by
E[h(X,Y)] or μh(X,Y), is given by
EhX ;YðÞ½/C138 ¼X
xX
yhx;yðÞ /C1 px;yðÞ ifXandYare discrete
ð1
/C01ð1
/C01hx;yðÞ /C1 fx;yðÞ dxdy ifXandYare continuous8
>><
>>:ð4:2Þ
This is sometimes referred to as the Law of the Unconscious Statistician .
The method of computing the expected value of a function h(X1,...,Xn)o fnrandom variables is
similar to Eq. ( 4.2). If the Xis are discrete, E[h(X1,...,Xn)] is an n-dimensional sum; if the Xis are
continuous, it is an n-dimensional integral.
Example 4.12 Five friends have purchased tickets to a concert. If the tickets are for seats 1–5 in a
particular row and the tickets are randomly distributed among the ﬁve, what is the expected number of
seats separating any particular two of the ﬁve friends? Let XandYdenote the seat numbers of the ﬁrst
and second individuals, respectively. Possible ( X,Y) pairs are {(1, 2), (1, 3), ..., (5, 4)}, and the joint
pmf of ( X,Y)i s
px;yðÞ ¼1
20x¼1,...,5;y¼1,...,5; x6¼y
0 otherwise8
<
:
The number of seats separating the two individuals is h(X,Y)¼jX/C0Yj/C01. The accompanying
table gives h(x,y) for each possible ( x,y) pair.
x
h(x,y) 12345
1 –0123
2 0–012
y 3 10–01
4 210–0
5 3210–4.2 Expected Values, Covariance, and Correlation 255
Thus
EhX ;YðÞ½/C138 ¼XX
x;yðÞhx;yðÞ /C1 px;yðÞ ¼X5
x¼1
x6¼yX5
y¼1jx/C0yj/C01 ðÞ /C11
20¼1
■
Example 4.13 In Example 4.5, the joint pdf of the amount Xof almonds and amount Yof cashews in
a 1-lb can of nuts was
fx;yðÞ ¼24xy 0/C20x/C201, 0 /C20y/C201, xþy/C201
0 otherwise/C26
If 1 lb of almonds costs the company $6.00, 1 lb of cashews costs $10.00, and 1 lb of peanuts costs
$3.50, then the total cost of the contents of a can is
hX;YðÞ ¼ 6Xþ10Yþ3:51/C0X/C0Y ðÞ ¼ 3:5þ2:5Xþ6:5Y
(since 1 /C0X/C0Yof the weight consists of peanuts). The expected total cost is
EhX ;YðÞ½/C138 ¼ð1
/C01ð1
/C01hx;yðÞ /C1 fx;yðÞ dxdy
¼ð1
0ð1/C0x
03:5þ2:5xþ6:5y ðÞ /C1 24xy dydx ¼$7:10 ■
4.2.1 Properties of Expected Value
In Chaps. 2and3, we saw that expected values can be distributed across addition, subtraction, and
multiplication by constants. In the language of mathematics, expected value is a linear operator .T h i s
was a simple consequence of expectation being a sum or an integral, both of which are linear. This
obvious but important property, linearity of expectation, extends to more than one variable.
LINEARITY OF EXPECTATION
LetXandYbe random variables. Then, for any functions h1,h2and any constants a1,a2,b ,
Ea 1h1X;YðÞ þ a2h2X;YðÞ þ b ½/C138 ¼ a1Eh 1X;YðÞ½/C138 þ a2Eh 2X;YðÞ½/C138 þ b
In the previous example, E(3.5 + 2.5 X+ 6.5 Y) can be rewritten as 3.5 + 2.5 E(X) + 6.5 E(Y); the
means of XandYcan be computed either by using Eq. ( 4.2) or by ﬁrst ﬁnding the marginal pdfs of
XandYand then computing the appropriate single integrals.
As another illustration, linearity of expectation tells us that for any two rvs XandY,
E5XY2/C04XYþeXþ12/C0/C1
¼5EX Y2/C0/C1
/C04EX YðÞ þ EeX/C0/C1
þ12 ð4:3Þ
In general, we cannot distribute the expected value operation any further. But when h(X,Y)i s
a product of a function of Xand a function of Y, the expected value simpliﬁes in the case of
independence.256 4 Joint Probability Distributions and Their Applications
THEOREM
LetXandYbeindependent random variables. If h(X, Y)¼g1(X)/C1g2(Y), then
EhX ;YðÞ½/C138 ¼ Eg1XðÞ /C1 g2YðÞ ½/C138 ¼ Eg1XðÞ½/C138 /C1 Eg2YðÞ½/C138
Proof We present the proof here for two continuous rvs; the discrete case is similar. Apply Eq. ( 4.2):
E/C2
hX;YðÞ/C3
¼E/C2
g1/C0
X/C1
/C1g2/C0
Y/C1/C3
¼ð1
/C01ð1
/C01g1xðÞ /C1 g2yðÞ /C1 f/C0
x,y/C1
dx dy by/C0
4:2/C1
¼ð1
/C01ð1
/C01g1xðÞ /C1 g2yðÞ /C1 fXxðÞ /C1 fY/C0
y/C1
dx dy because XandYare independent
¼ð1
/C01g1xðÞ /C1 fXxðÞdx/C18/C19 ð1
/C01g2yðÞ /C1 fYyðÞdy/C18/C19
¼E/C2
g1XðÞ/C3
E/C2
g2/C0
Y/C1/C3
■
So, if Xand Yare independent, Eq. ( 4.3) simpliﬁes further, to 5 E(X)E(Y2)/C04E(X)E(Y)+
E(eX) + 12. Not surprisingly, both linearity of expectation and the foregoing corollary can be
extended to more than two random variables.
4.2.2 Covariance
When two random variables XandYare not independent, it is frequently of interest to assess how
strongly they are related to each other.
DEFINITION
The covariance between two rvs XandYis
Cov X;YðÞ ¼ E/C2/C0
X/C0μX/C1/C0
Y/C0μY/C1/C3
¼X
xX
yx/C0μX ðÞ y/C0μY ðÞ px;yðÞ ifXandYare discrete
ð1
/C01ð1
/C01x/C0μX ðÞ y/C0μY ðÞ fx;yðÞ dxdy ifXandYare continuous8
>><
>>:
The rationale for the deﬁnition is as follows. Suppose XandYhave a strong positive relationship to
each other, by which we mean that large values of Xtend to occur with large values of Yand small
values of Xwith small values of Y(e.g., X¼height and Y¼weight). Then most of the probability
mass or density will be associated with ( x/C0μX) and ( y/C0μY) either both positive (both Xand
Yabove their respective means) or both negative ( XandYsimultaneously below average). So, the
product ( x/C0μX)(y/C0μY) will tend to be positive. Thus for a strong positive relationship, Cov( X,Y)
should be quite positive, because it’s the expectation of a generally positive quantity. For a strong
negative relationship, the signs of ( x/C0μX) and ( y/C0μY) will tend to be opposite, resulting in a
negative product. Thus for a strong negative relationship, Cov( X,Y) should be quite negative. If Xand
Yare not strongly related, positive and negative products will tend to cancel each other, yielding a
covariance near 0. Figure 4.4illustrates the different possibilities. The covariance depends on both
the set of possible pairs and the probabilities. In Fig. 4.4, the probabilities could be changed without
altering the set of possible pairs, and this could drastically change the value of Cov( X,Y).4.2 Expected Values, Covariance, and Correlation 257
Example 4.14 The joint and marginal pmfs for X¼automobile policy deductible amount and
Y¼homeowner policy deductible amount in Example 4.1 were
y
p(x,y) 0 100 200
100 .20 .10 .20x250 .05 .15 .30
x 100 250 y 0 100 200
pX(x) .5 .5 pY(y) .25 .25 .50
from which μX¼∑x/C1pX(x)¼175 and μY¼125. Therefore,
Cov X;YðÞ ¼XX
x;yðÞx/C0175ðÞ y/C0125ðÞ px;yðÞ
¼100/C0175 ðÞ/C0
0/C0125/C1/C0
:20/C1
þ/C1/C1/C1þ/C0
250/C0175/C1/C0
200/C0125/C1/C0
:30/C1
¼1875 ■
The following proposition summarizes some important properties of covariance.
PROPOSITION
For any two random variables XandY,
1. Cov( X, Y)¼Cov( Y, X)
2. Cov( X, X)¼Var(X)
3. (Covariance shortcut formula) Cov( X, Y)¼E(XY)/C0μX/C1μY
4. (Distributive property of covariance) For any rv Zand any constants, a, b, c ,
Cov aXþbYþc,Z ðÞ ¼ aCov X;ZðÞ þ bCov Y;ZðÞ
Proof Property 1 is obvious from the deﬁnition of covariance. To establish property 2, replace Ywith
Xin the deﬁnition:
Cov X;XðÞ ¼ EX/C0μX ðÞ X/C0μX ðÞ ½/C138 ¼ EX/C0μX ðÞ2hi
¼VarXðÞyc b a
xy y
x xmY mY mY
mX mX mX
Fig. 4.4 p(x,y)¼.10 for each of ten pairs corresponding to indicated points; ( a) positive covariance; ( b) negative
covariance; ( c) covariance near zero258 4 Joint Probability Distributions and Their Applications
To prove property 3, apply linearity of expectation:
Cov X;YðÞ ¼ E/C2
X/C0μX ðÞ/C0
Y/C0μY/C1/C3
¼EX Y /C0μXY/C0μYXþμXμY ðÞ
¼EX YðÞ /C0 μXE/C0
Y/C1
/C0μYE/C0
X/C1
þμXμY
¼EX YðÞ /C0 μXμY/C0μXμYþμXμY¼E/C0
XY/C1
/C0μXμY
Property 4 also follows from linearity of expectation (Exercise 39). ■
According to property 3, the covariance shortcut, no intermediate subtractions are necessary to
calculate covariance; only at the end of the computation is μX/C1μYsubtracted from E(XY).
Example 4.15 (Example 4.5 continued) The joint and marginal pdfs of X¼amount of almonds and
Y¼amount of cashews were
fx;yðÞ ¼24xy 0/C20x/C201, 0 /C20y/C201, xþy/C201
0 otherwise/C26
fXxðÞ ¼12x1/C0xðÞ20/C20x/C201
0 otherwise/C26
with fY(y) obtained by replacing xbyyinfX(x). It is easily veriﬁed that μX¼μY¼2
5, and
EX YðÞ ¼ð1
/C01ð1
/C01xyf x ;yðÞ dxdy¼ð1
0ð1/C0x
0xy/C124xy dydx ¼8ð1
0x21/C0xðÞ3dx¼2
15
Thus Cov X;YðÞ ¼2
15/C02
5/C18/C192
5/C18/C19
¼2
15/C04
25¼/C02
75. A negative covariance is reasonable here
because more almonds in the can implies fewer cashews. ■
4.2.3 Correlation
It would appear that the relationship in the insurance example is quite strong since Cov( X,Y)¼1875,
whereas in the nut example Cov X;YðÞ ¼ /C0 2=75 would seem to imply quite a weak relationship.
Unfortunately, the covariance has a serious defect that makes it impossible to interpret a computed
value of the covariance. In the insurance example, suppose we had expressed the deductible amount
in cents rather than in dollars. Then 100 Xwould replace X, 100 Ywould replace Y, and the resulting
covariance would be Cov(100 X, 100 Y)¼(100)(100)Cov( X,Y)¼18,750,000. [To see why,
apply properties 1 and 4 of the previous proposition.] If, on the other hand, the deductible amounts
had been expressed in hundreds of dollars, the computed covariance would have changed to
(.01)(.01)(1875) ¼.1875. The defect of covariance is that its computed value depends critically on
the units of measurement. Ideally, the choice of units should have no effect on a measure of strength
of relationship. This is achieved by scaling the covariance.4.2 Expected Values, Covariance, and Correlation 259
DEFINITION
Thecorrelation coefﬁcient ofXandY, denoted by Corr( X,Y), or ρX,Y, or just ρ, is deﬁned by
ρX,Y¼Cov X;YðÞ
σX/C1σY
Example 4.16 It is easily veriﬁed that in the insurance scenario of Example 4.14, E(X2)¼36,250,
σX2¼36,250 /C0(175)2¼5625, σX¼75,E(Y2)¼22,500, σY2¼6875, and σY¼82.92. This gives
ρ¼1875
75ðÞ 82:92ðÞ¼:301 ■
The following proposition shows that ρremedies the defect of Cov( X,Y) and also suggests how to
recognize the existence of a strong (linear) relationship.
PROPOSITION
For any two rvs XandY,
1. Corr( X, Y)¼Corr( Y, X)
2. Corr( X, X)¼1
3. (Scale invariance property) If a, b, c, d are constants and ac>0,
Corr aXþb,cYþd ðÞ ¼ Corr X;YðÞ
4. –1 /C20Corr( X,Y)/C201
Proof Property 1 is clear from the deﬁnition of correlation and the corresponding property of
covariance. For Property 2, write Corr( X,X)¼Cov( X,X)/[σX/C1σX]¼Var(X)/σX2¼1. The second-
to-last step uses Property 2 of covariance. The proofs of Properties 3 and 4 appear as exercises. ■
Property 3 (scale invariance) says precisely that the correlation coefﬁcient is not affected by a
linear change in the units of measurement. If, say, Y¼completion time for a chemical reaction in
seconds and X¼temperature in/C14C, then Y/60¼time in minutes and 1.8 X+3 2¼temperature
in/C14F, but Corr( X, Y) will be exactly the same as Corr(1.8 X+ 32,Y/60).
According to Properties 2 and 4, the strongest possible positive relationship is evidenced by
ρ¼+1, whereas the strongest possible negative relationship corresponds to ρ¼–1. Therefore, the
correlation coefﬁcient provides information about both the nature and strength of the relationship
between XandY: the sign of ρindicates whether XandYare positively or negatively related, and the
magnitude of ρdescribes the strength of that relationship on an absolute 0–1 scale.
While superior to covariance, the correlation coefﬁcient ρis actually not a completely general
measure of the strength of a relationship.260 4 Joint Probability Distributions and Their Applications
PROPOSITION
1. If XandYare independent, then ρ¼0, but ρ¼0 does not imply independence.
2.ρ¼1 or –1 iff Y¼aX+bfor some numbers aandbwith a6¼0.
Exercise 38 and Example 4.17 relate to Statement 1, and Statement 2 is investigated in Exercises
41 and 42(d).
This proposition says that ρis a measure of the degree of linear relationship between XandY, and
only when the two variables are perfectly related in a linear manner will ρbe as positive or negative as
it can be. A ρless than 1 in absolute value indicates only that the relationship is not completely linear,
but there may still be a very strong nonlinear relation. Also, ρ¼0 does not imply that XandYare
independent, but only that there is complete absence of a linear relationship. When ρ¼0,XandYare
said to be uncorrelated . Two variables could be uncorrelated yet highly dependent because of a
strong nonlinear relationship, so be careful not to conclude too much from knowing that ρ¼0.
Example 4.17 LetXandYbe discrete rvs with joint pmf
px;yðÞ ¼:25 x;yðÞ ¼/C0
/C04, 1/C1
,/C0
4,/C01/C1
,/C0
2, 2/C1
,/C0
/C02,/C02/C1
0 otherwise/C26
The points that receive positive probability mass are identiﬁed on the ( x,y) coordinate system in
Fig.4.5. It is evident from the ﬁgure that the value of Xis completely determined by the value of Yand
vice versa, so the two variables are completely dependent. However, by symmetry μX¼μY¼0 and
E(XY)¼(/C04)(.25) + ( /C04)(.25) + (4)(.25) + (4)(.25) ¼0, so Cov( X,Y)¼E(XY)/C0μX/C1μY¼0
and thus ρX,Y¼0. Although there is perfect dependence, there is also complete absence of any linear
relationship!
The next result provides an alternative view of zero correlation.
PROPOSITION
Two rvs XandYare uncorrelated if, and only if, E[XY]¼μX/C1μY.
Proof By its deﬁnition, Corr( X, Y)¼0 iff Cov( X, Y)¼0. Apply the covariance shortcut formula:
ρ¼0,Cov X;YðÞ ¼ 0,EX Y½/C138 /C0 μX/C1μY¼0,EX Y½/C138 ¼ μX/C1μY■2
1
1234 −1
−1−2
−2−3 −4
Fig. 4.5 The population of pairs for Example 4.17 ■4.2 Expected Values, Covariance, and Correlation 261
Contrast this with an earlier proposition from this section: if XandYareindependent rvs, then
E[g1(X)g2(Y)]¼E[g1(X)]/C1E[g2(Y)] for allfunctions g1andg2. Thus, independence is stronger than
zero correlation, the latter being the special case corresponding to g1(X)¼Xandg2(Y)¼Y.
4.2.4 Correlation Versus Causation
A value of ρnear 1 does not necessarily imply that increasing the value of X causes Y to increase.
It implies only that large Xvalues are associated with large Yvalues. For example, in the population
of children, vocabulary size and number of cavities are quite positively correlated, but it is certainly
not true that cavities cause vocabulary to grow. Instead, the values of both these variables tend to
increase as the value of age, a third variable, increases. For children of a ﬁxed age, there is probably a
very low correlation between number of cavities and vocabulary size. In summary, association (a high
correlation) is not the same as causation.
4.2.5 Exercises: Section 4.2(23–42)
23. The two most common types of errors made by programmers are syntax errors and logic errors.
LetXdenote the number of syntax errors and Ythe number of logic errors on the ﬁrst run of a
program. Suppose Xand Yhave the following joint pmf for a particular programming
assignment:
x
p(x,y) 0123
y0 .71 .03 .02 .01
1 .04 .06 .03 .01
2 .03 .03 .02 .01
(a) What is the probability a program has more syntax errors than logic errors on the ﬁrst run?
(b) Find the marginal pmfs of XandY.
(c) Are XandYindependent? How can you tell?
(d) What is the average number of syntax errors in the ﬁrst run of a program? What is the
average number of logic errors?
(e) Suppose an evaluator assigns points to each program with the formula 100 /C04X/C09Y.
What is the expected point score for a randomly selected program?
24. An instructor has given a short quiz consisting of two parts. For a randomly selected student, let
X¼the number of points earned on the ﬁrst part and Y¼the number of points earned on the
second part. Suppose that the joint pmf of XandYis given in the accompanying table.
y
p(x,y)0 5 1 0 1 5
x0 .02 .06 .02 .10
5 .04 .15 .20 .10
10 .01 .15 .14 .01
(a) If the score recorded in the grade book is the total number of points earned on the two parts,
what is the expected recorded score E(X+Y)?
(b) If the maximum of the two scores is recorded, what is the expected recorded score?262 4 Joint Probability Distributions and Their Applications
25. The difference between the number of customers in line at the express checkout and the number
in line at the superexpress checkout in Exercise 3 is X1/C0X2. Calculate the expected difference.
26. Six individuals, including A and B, take seats around a circular table in a completely random
fashion. Suppose the seats are numbered 1, ..., 6. Let X¼A’s seat number and Y¼B’s seat
number. If A sends a written message around the table to B in the direction in which they are
closest, how many individuals (including A and B) would you expect to handle the message?
27. A surveyor wishes to lay out a square region with each side having length L. However, because of
measurement error, he instead lays out a rectangle in which the north–south sides both have
length Xand the east–west sides both have length Y. Suppose that XandYare independent and
that each is uniformly distributed on the interval [ L/C0A,L+A] (where 0 <A<L). What is the
expected area of the resulting rectangle?
28. Consider a small ferry that can accommodate cars and buses. The toll for cars is $3, and the toll
for buses is $10. Let XandYdenote the number of cars and buses, respectively, carried on a
single trip. Suppose the joint distribution of XandYis as given in the table of Exercise 9. Compute
the expected revenue from a single trip.
29. Annie and Alvie have agreed to meet for lunch between noon (0:00 p.m.) and 1:00 p.m. Denote
Annie’s arrival time by X, Alvie’s by Y, and suppose XandYare independent with pdfs
fXxðÞ ¼3x20/C20x/C201
0 otherwise/C26
fYyðÞ ¼2y
0/C26
0/C20y/C201
otherwise
What is the expected amount of time that the one who arrives ﬁrst must wait for the other person?
[Hint:h(X,Y)¼jX/C0Yj.]
30. Suppose that XandYare independent rvs with moment generating functions MX(t) and MY(t),
respectively. If Z¼X+Y, show that MZ(t)¼MX(t)/C1MY(t). [Hint: Use the proposition on the
expected value of a product.]
31. Compute the correlation coefﬁcient ρforXandYof Example 4.15 (the covariance has already
been computed).
32. (a) Compute the covariance for XandYin Exercise 24.
(b) Compute ρforXandYin the same exercise.
33. (a) Compute the covariance between XandYin Exercise 11.
(b) Compute the correlation coefﬁcient ρfor this XandY.
34. Reconsider the computer component lifetimes XandYas described in Exercise 14. Determine E
(XY).What can be said about Cov( X,Y) and ρ?
35. Refer back to Exercise 23.
(a) Calculate the covariance of XandY.
(b) Calculate the correlation coefﬁcient of XandY. Interpret this value.
36. In practice, it is often desired to predict the value of a variable Yfrom the known value of some
other variable, X. For example, a doctor might wish to predict the lifespan Yof someone who
smokes Xcigarettes a day, or an engineer may require predictions of the tensile strength Yof steel
made with concentration Xof a certain additive. A linear predictor ofYis anything of the form
^Y¼aþbX; the “hat” ^ on Yindicates prediction.
A common measure of the quality of a predictor is given by the mean square prediction error :
EY/C0^Y/C0/C1 2hi4.2 Expected Values, Covariance, and Correlation 263
(a) Show that the choices of aandbthat minimize mean square prediction error are
b¼ρ/C1σY
σXa¼μY/C0b/C1μX
where ρ¼Corr( X, Y). The resulting expression for ^Yis often called the best linear predictor
ofY, given X.[Hint: Expand the expression for mean square prediction error, apply linearity
of expectation, and then use calculus.]
(b) Determine the mean square prediction error for the best linear predictor. How does the value
ofρaffect this quantity?
37. (a) Recalling the deﬁnition of σ2for a single rv X,write a formula that would be appropriate for
computing the variance of a function h(X, Y) of two random variables. [ Hint: Remember
that variance is just a special expected value.]
(b) Use this formula to compute the variance of the recorded score h(X, Y)[¼max( X,Y)] in part
(b) of Exercise 24.
38. Show that when XandYare independent, Cov( X,Y)¼Corr( X,Y)¼0.
39. Use linearity of expectation to establish the covariance property
Cov aXþbYþc,Z ðÞ ¼ aCov X;ZðÞ þ bCov Y;ZðÞ
40. (a) Use the properties of covariance to show that Cov( aX + b ,cY + d )¼acCov( X,Y).
(b) Use part (a) along with the rescaling property of standard deviation to show that
Corr( aX + b ,cY + d )¼Corr( X,Y) when ac>0 (this is the scale invariance property of
correlation).
(c) What happens if aandchave opposite signs, so ac<0?
41. Show that if Y¼aX+b(a6¼0), then Corr( X,Y)¼+1 or –1. Under what conditions will
ρ¼+1?
42. Let ZXbe the standardized X,ZX¼(X/C0μX)/σX, letZYbe the standardized Y,ZY¼(Y/C0μY)/σY,
and let ρ¼Corr( X, Y).
(a) Show that Corr( X,Y)¼Cov( ZX,ZY)¼E(ZXZY).
(b) Use the linearity of expectation along with part (a), to show that E[(ZY/C0ρZX)2]¼1/C0ρ2.
[Hint: IfZis a standardized rv, what are its mean and variance, and how can you use those to
determine E(Z2)?]
(c) Use part (b) to show that –1 /C20ρ/C201.
(d) Use part (b) to show that ρ¼1 implies that Y¼aX+bwhere a>0, and ρ¼–1 implies
thatY¼aX+bwhere a<0.
4.3 Properties of Linear Combinations
Alinear combination of random variables refers to anything of the form a1X1+/C1/C1/C1+anXn+b,
where the Xis are random variables and the ais and bare numerical constants. (Some sources do not
include the constant bin the deﬁnition.) For example, suppose your investment portfolio with a
particular ﬁnancial institution includes 100 shares of stock #1, 200 shares of stock #2, and 500 shares
of stock #3. Let X1,X2, and X3denote the share prices of these three stocks at the end of the current
ﬁscal year. Suppose also that the ﬁnancial institution will levy a management fee of $150. Then the
value of your investments with this institution at the end of the year is 100 X1+ 200 X2+ 500 X3/C0150,
which is a particular linear combination. Important special cases include the total X1+/C1/C1/C1+Xn(take264 4 Joint Probability Distributions and Their Applications
a1¼ /C1/C1/C1 ¼ an¼1,b¼0), the difference of two rvs X1/C0X2(n¼2,a1¼1,a2¼–1), and anything
of the form aX+b(take n¼1 or, equivalently, set a2¼...¼an¼0). Another very important
linear combination is the sample mean (X1+/C1/C1/C1+Xn)/n, conventionally denoted /C22X; just take a1¼
/C1/C1/C1 ¼ an¼1/nandb¼0.
Notice that we are not requiring the Xis to be independent or to have the same probability
distribution. All the Xis could have different distributions and therefore different mean values and
standard deviations. In this section, we investigate the general properties of linear combinations.
Section 4.5will explore some special properties of the total and the sample mean under additional
assumptions.
We ﬁrst consider the expected value and variance of a linear combination.
THEOREM
Let the rvs X1,X2,...,Xnhave mean values μ1,...,μnand standard deviations σ1,...,σn,
respectively.
1. Whether or not the Xis are independent,
Ea 1X1þ/C1/C1/C1þ anXnþb ðÞ ¼ a1E/C0
X1/C1
þ/C1/C1/C1þ anE/C0
Xn/C1
þb
¼a1μ1þ/C1/C1/C1þ anμnþbð4:4Þ
and
Vara1X1þ/C1/C1/C1þ anXnþb ðÞ ¼Xn
i¼1Xn
j¼1aiajCov/C0
Xi,Xj/C1
¼Xn
i¼1a2
iσ2
iþ2XX
aiajCov Xi;Xj/C0/C1
i<jð4:5Þ
2. If X1,...,Xnare independent,
Vara1X1þ/C1/C1/C1þ anXnþb ðÞ ¼ a2
1Var/C0
X1/C1
þ/C1/C1/C1þ a2
nVar/C0
Xn/C1
¼a2
1σ2
1þ/C1/C1/C1þ a2
nσ2
nð4:6Þ
and
SDa1X1þ/C1/C1/C1þ anXnþb ðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
a2
1σ2
1þ/C1/C1/C1þ a2
nσ2
nq
A paraphrase of Eq. ( 4.4) is that the expected value of a linear combination is the same linear
combination of the expected values—for example, E(2X1+5X2)¼2μ1+5μ2. Equation ( 4.6)i n
Statement 2 is a special case of Eq. ( 4.5) in Statement 1: when the Xis are independent, Cov( Xi,
Xj)¼0 for i6¼j(this simpliﬁcation actually occurs when the Xis are uncorrelated, a weaker
condition than independence).
Proofs for the Case n¼2To establish Eq. ( 4.4), we could invoke linearity of expectation from
Sect. 4.2, but we present an independent proof here. Suppose that X1andX2are continuous with joint
pdff(x1,x2). Then4.3 Properties of Linear Combinations 265
Ea 1X1þa2X2þb ðÞ ¼ð1
/C01ð1
/C01a1x1þa2x2þb ðÞ fx1;x2ðÞ dx1dx2
¼a1ð1
/C01ð1
/C01x1fx1;x2ðÞ dx2dx1þa2ð1
/C01ð1
/C01x2fx1;x2ðÞ dx1dx2
þbð1
/C01ð1
/C01fx1;x2ðÞ dx1dx2
¼a1ð1
/C01x1ð1
/C01fx1;x2ðÞ dx2/C20/C21
dx1
þa2ð1
/C01x2ð1
/C01fx1;x2ðÞ dx1/C20/C21
dx2þb1ðÞ
¼a1ð1
/C01x1fX1x1ðÞdx1þa2ð1
/C01x2fX2x2ðÞdx2þb
¼a1EX 1ðÞ þ a2E/C0
X2/C1
þb
Summation replaces integration in the discrete case. The argument for Eq. ( 4.5) does not require
specifying whether either variable is discrete or continuous. Recalling that Var( Y)¼E[(Y/C0μY)2],
Vara1X1þa2X2þb ðÞ ¼ E/C2
a1X1þa2X2þb/C0a1μ1þa2μ2þb ðÞ ðÞ2/C3
¼E/C2
a1X1/C0a1μ1þa2X2/C0a2μ2 ðÞ2/C3
¼E/C2
a2
1X1/C0μ1 ðÞ2þa2
2/C0
X2/C0μ2/C12þ2a1a2/C0
X1/C0μ1/C1/C0
X2/C0μ2/C1/C3
¼a2
1E/C2
X1/C0μ1 ðÞ2/C3
þa2
2E/C2/C0
X2/C0μ2/C12/C3
þ2a1a2E/C2/C0
X1/C0μ1/C1/C0
X2/C0μ2/C1/C3
where the last equality comes from linearity of expectation. We recognize the terms in this last
expression as variances and covariance, all together a12Var(X1)+a22Var(X2)+2 a1a2Cov( X1,X2),
as required. ■
Example 4.18 A gas station sells three grades of gasoline: regular, plus, and premium. These are
priced at $3.50, $3.65, and $3.80 per gallon, respectively. Let X1,X2, and X3denote the amounts
of these grades purchased (gallons) on a particular day. Suppose the Xis are independent with
μ1¼1000, μ2¼500, μ3¼300, σ1¼100, σ2¼80, and σ3¼50. The revenue from sales is
Y¼3.5X1+ 3.65 X2+ 3.8 X3, and
EYðÞ ¼ 3:5μ1þ3:65μ2þ3:8μ3¼$6465
VarYðÞ ¼ 3:52σ2
1þ3:652σ2
2þ3:82σ2
3¼243, 864
SDYðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ243, 864p¼$493:83 ■
Example 4.19 Recall that a hypergeometric rv Xis the number of successes in a random sample of
sizenselected without replacement from a population of size Nconsisting of Msuccesses and N/C0M
failures. It is tricky to obtain the mean value and variance of Xdirectly from the pmf, and the
hypergeometric moment generating function is very complicated. We now show how the foregoing
proposition on linear combinations can be used to accomplish this task.
To this end, let X1¼1 if the ﬁrst individual or object selected is a success and X1¼0i fi ti sa
failure; deﬁne X2,X3,...,Xnanalogously for the second selection, third selection, and so on. Each Xi
is a Bernoulli rv, and each has the same marginal distribution: p(1)¼M/N andp(0)¼1/C0M/N (this
is obvious for X1, which is based on the very ﬁrst draw from the population, and can be veriﬁed for the266 4 Joint Probability Distributions and Their Applications
other draws as well). Thus E(Xi)¼0(1/C0M/N)+1 ( M/N)¼M/N. The total number of successes in
the sample is X¼X1+...+Xn(a 1 is added in for each success and a 0 for each failure), so
EXðÞ ¼ EX 1ðÞ þ ...þEX nðÞ ¼ M=NþM=Nþ...þM=N¼nM=NðÞ ¼ np
where pdenotes the success probability on any particular draw (trial). That is, just as in the case of a
binomial rv, the expected value of a hypergeometric rv is the success probability on any trial
multiplied by the number of trials. Notice that we were able to apply Statement 1 of the foregoing
theorem, even though the Xis are not independent.
However, the variance of Xhere is notthe same as the binomial variance, precisely because the
successive draws are not independent. Consider the joint distribution of X1andX2:
p1;1ðÞ ¼M
NM/C01
N/C01/C18/C19
,p0;0ðÞ ¼N/C0M
N/C18/C19N/C0M/C01
N/C01/C18/C19
,
p1;0ðÞ ¼ p0;1ðÞ ¼M
NN/C0M
N/C01/C18/C19
This is also the joint pmf of any pair Xi,Xj. A slightly tedious calculation then results in
Cov Xi;Xj/C0/C1
¼/C0p1/C0pðÞ
N/C01i6¼jðÞ
Applying the variance formula from statement 1 of the theorem eventually yields
VarXðÞ ¼ VarX1þ...þXn ðÞ ¼ nVarX1ðÞ þ nn/C01ðÞ Cov X1;X2 ðÞ ¼ np1/C0pðÞN/C0n
N/C01/C18/C19
This is quite close to the binomial variance provided that nis much smaller than Nso that the last
term in parentheses is close to 1. ■
The following corollary expresses the n¼2 case of the main theorem for ease of use, including
the important special cases of the sum and the difference of two random variables.
Corollary
For any two rvs X1andX2, and any constants a1,a2,b,
Ea 1X1þa2X2þb ðÞ ¼ a1EX 1ðÞ þ a2EX 2ðÞ þ b
and
Vara1X1þa2X2þb ðÞ ¼ a2
1VarX1ðÞ þ a2
2VarX2ðÞ þ 2a1a2Cov X1;X2 ðÞ
In particular, E(X1+X2)¼E(X1)+E(X2) and, if X1andX2are independent, Var( X1+X2)
¼Var(X1) + Var( X2).1Also, E(X1/C0X2)¼E(X1)/C0E(X2) and, if X1andX2are independent,
VarX1/C0X2 ðÞ ¼ VarX1ðÞ þ VarX2ðÞ:
1This property of independent rvs can also be written as SD( X1)2+ SD( X2)2¼[SD( X1+X2)]2. In part because the
formula has the format a2+b2¼c2, statisticians sometimes call this property the Pythagorean Theorem .4.3 Properties of Linear Combinations 267
The expected value of a difference is the difference of the two expected values, but the variance of
a difference between two independent variables is the sum, not the difference, of the two variances.
There is just as much variability in X1/C0X2as in X1+X2: writing X1/C0X2¼X1+ (–1) X2, the term
(–1)X2has the same amount of variability as X2itself.
Example 4.20 An automobile manufacturer equips a particular model with either a six-cylinder
engine or a four-cylinder engine. Let X1andX2be fuel efﬁciencies for independently and randomly
selected six-cylinder and four-cylinder cars, respectively. With μ1¼22,μ2¼26,σ1¼1.2, and
σ2¼1.5,
EX 1/C0X2 ðÞ ¼ μ1/C0μ2¼22/C026¼/C04 mpg
VarX1/C0X2 ðÞ ¼ σ2
1þσ2
2¼1:22þ1:52¼3:69
SDX1/C0X2 ðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
3:69p
¼1:92 mpg
If we relabel so that X1refers to the four-cylinder car, then E(X1/C0X2)¼26/C022¼4 mpg, but
the standard deviation of the difference is still 1.92 mpg. ■
4.3.1 The PDF of a Sum
Generally speaking, knowing the mean and standard deviation of a random variable Wis not enough
to specify its probability distribution and thus be able to compute probabilities such as P(W>10) or
P(W/C20–2). In the case of independent rvs, a general method exists for determining the pdf of the sum
X1+/C1/C1/C1+Xnfrom their marginal pdfs. We present ﬁrst the result for two random variables.
THEOREM
Suppose XandYare independent, continuous rvs with marginal pdfs fX(x) and fY(y), respec-
tively. Then the pdf of the rv W¼X+Y is given by
fWwðÞ ¼ð1
/C01fXxðÞfYw/C0xðÞ dx
[In mathematics, this integral operation is known as the convolution offX(x) and fY(y) and is
sometimes denoted fW¼fX«fY.] The limits of integration are determined by which xvalues
make both fX(x)>0 and fY(w/C0x)>0.
Proof Since XandYare independent, their joint pdf is given by fX(x)/C1fY(y). The cdf of Wis then
FWwðÞ ¼ PW/C20w ðÞ ¼ PXþY/C20w ðÞ
To calculate P(X+Y /C20w), we must integrate over the set of numbers {( x, y):x+y /C20w}, which
is the shaded region indicated in Fig. 4.6.
The resulting limits of integration are – 1<x<1and –1<y/C20w/C0x, and so268 4 Joint Probability Distributions and Their Applications
FWwðÞ ¼ P/C0
XþY/C20w/C1
¼ð1
/C01ðw/C0x
/C01fXxðÞfYyðÞdydx¼ð1
/C01fXxðÞðw/C0x
/C01fYyðÞdydx
¼ð1
/C01fXxðÞFYw/C0xðÞ dx
The pdf of Wis the derivative of this expression with respect to w; taking the derivative underneath
the integral sign yields the desired result. ■
By a similar argument, the pdf of W¼X+Y can be determined even when XandYare not
independent. Assuming XandYhave joint pdf f(x, y),
fWwðÞ ¼ð1
/C01fx,w/C0x ðÞ dx
Example 4.21 In astandby system , a component is used until it wears out and is then immediately
replaced by another, not necessarily identical, component. (The second component is said to be “in
standby mode,” i.e., waiting to be used.) The overall lifetime of a standby system is just the sum of the
lifetimes of its individual components. Let XandYdenote the lifetimes of the two components of a
standby system, and suppose XandYare independent exponentially distributed random variables
with expected lifetimes 3 weeks and 4 weeks, respectively. Let W¼X+Y , the lifetime of the
standby system.
Using the ﬁrst theorem of this section, the expected lifetime of the standby system is E(W)¼
E(X)+E(Y)¼3+4 ¼7 weeks. Since XandYare exponential, the variance of each one is the
square of its mean (9 and 16, respectively); since they are also independent,
VarWðÞ ¼ VarXðÞ þ VarYðÞ ¼ 32þ42¼25
It follows that SD( W)¼5 weeks. Since μW6¼σW,Wcannot itself be exponentially distributed,
but we can use the previous theorem to ﬁnd its pdf.
The marginal pdfs of XandYarefX(x)¼(1/3) e–x/3forx>0 and fY(y)¼(1/4) e–y/4fory>0.
Substituting y¼w/C0x, the inequalities x>0 and w/C0x>0 imply 0 <x<w, which are the
limits of integration of the convolution integral:
fWwðÞ ¼ð1
/C01fXxðÞfYw/C0xðÞ dx¼ðw
01=3ðÞ e/C0x=31=4ðÞ e/C0w/C0xðÞ =4dx
¼1
12e/C0w=4ðw
0e/C0x=12dx
¼e/C0w=41/C0e/C0w=12/C0/C1
,w>0y
xx + y = w
Fig. 4.6 Region of integration for P(X+Y /C20w)4.3 Properties of Linear Combinations 269
The pdf of Wappears in Fig. 4.7. As a check, the mean and variance of Wcan be veriﬁed directly
from its pdf.
The probability the standby system lasts more than its expected lifetime of 7 weeks is given by
PW >7 ðÞ ¼ð1
7fWwðÞdw¼ð1
7e/C0w=41/C0e/C0w=12/C16/C17
dw¼:4042 ■
As a generalization of the previous proposition, the pdf of the sum W¼X1+/C1/C1/C1+Xnof
nindependent, continuous rvs can be determined by successive convolution: fW¼f1«/C1/C1/C1«fn.I n
most situations, it isn’t practical to evaluate such a complicated object. Thankfully, as we’ll see next,
such tedious computations can sometimes be avoided with the use of moment generating functions.
4.3.2 Moment Generating Functions for Linear Combinations
A corollary in Sect. 4.2stated that the expected value of a product of functions of independent random
variables is the product of the individual expected values. We now use this to formulate the moment
generating function of a linear combination of independent random variables.
PROPOSITION
LetX1,X2,...,Xnbe independent random variables with moment generating functions MX1tðÞ,
MX2tðÞ,...,MXntðÞ, respectively. Then the moment generating function of the linear combina-
tionY¼a1X1+a2X2+/C1/C1/C1+anXn+bis
MYtðÞ¼ ebtMX1a1tðÞ /C1 MX2a2tð Þ/C1 /C1/C1/C1 /C1 MXnantðÞ
In the special case that a1¼a2¼ /C1/C1/C1 ¼ an¼1 and b¼0, so Y¼X1+/C1/C1/C1+Xn,
MYtðÞ¼ MX1tðÞ/C1MX2tðÞ/C1 /C1/C1/C1 /C1 MXntðÞ
That is, the mgf of a sum of independent rvs is the product of the individual mgfs.0.10
0.08
0.06
0.04
0.02
0
01 0 2 0wfW(w)
30Fig. 4.7 The pdf
ofW¼X+Y for
Example 4.21270 4 Joint Probability Distributions and Their Applications
Proof First, we write the moment generating function of Yas the expected value of a product.
MYtðÞ¼ E/C2
etY/C3
¼E/C2
eta1X1þa2X2þ/C1/C1/C1þ anXnþb ðÞ/C3
¼Eeta1X1þta2X2þ/C1/C1/C1þ tanXnþtb/C2/C3
¼ebtE/C2
ea1tX1/C1ea2tX2/C1/C1 /C1 /C1/C1 eantXn/C3
The last expression inside brackets is the product of functions of X1,X2,...,Xn. Since the Xis are
independent, the expected value can be distributed across this product:
ebtEea1tX1/C1ea2tX2/C1/C1/C1/C1/C1 eantXn ½/C138 ¼ ebtE/C2
ea1tX1/C3
/C1E/C2
ea2tX2/C3
/C1/C1/C1/C1/C1 E/C2
eantXn/C3
¼ebtMX1ða1tÞ/C1MX2ða2tÞ/C1/C1/C1/C1/C1 MXnðantÞ ■
Now suppose we wish to determine the pdf of some linear combination of independent rvs.
Provided we have their mgfs, the previous proposition makes it easy to determine the mgf of the
linear combination. Then, if we can recognize this mgf as belonging to some known distributional
family (binomial, exponential, etc.), the uniqueness property of mgfs guarantees that our linear
combination has that particular distribution. The next several propositions illustrate this technique.
PROPOSITION
IfX1,X2,...,Xnare independent, normally distributed rvs (with possibly different means and/or
sds), then any linear combination of the Xis also has a normal distribution. In particular, the sum
of independent normally distributed rvs itself has a normal distribution, and the difference
X1/C0X2between two independent, normally distributed variables is itself normally distributed.
Proof LetY¼a1X1+a 2X2+/C1/C1/C1+a nXn+b, where Xiis normally distributed with mean μiand
standard deviation σi, and the Xis are independent. From Sect. 3.3,MXitðÞ¼ eμitþσ2
it2=2:Therefore,
MYtðÞ¼ ebtMX1/C0
a1t/C1
/C1MX2/C0
a2t/C1
/C1/C1/C1/C1/C1 MXn/C0
ant/C1
¼ebteμ1a1tþσ2
1a2
1t2=2eμ2a2tþσ2
2a2
2t2=2/C1/C1/C1/C1/C1 eμnantþσ2
na2
nt2=2
¼eμ1a1þμ2a2þ/C1/C1/C1þ μnanþb ðÞ tþσ2
1a2
1þσ2
2a2
2þ/C1/C1/C1þ σ2
na2
n ðÞ t2=2
¼eμtþσ2t2=2
where μ¼a1μ1+a2μ2+/C1/C1/C1+anμn+band σ2¼a12σ12+a22σ22+/C1/C1/C1+an2σn2. We recognize this
function as the mgf of a normal random variable, and it follows by the uniqueness property of
mgfs that Yis normally distributed . Notice that the mean and variance are in agreement with the ﬁrst
proposition of this section. ■
Example 4.22 (Example 4.18 continued) The total revenue from the sale of the three grades of
gasoline on a particular day was Y¼3.5X1+ 3.65 X2+ 3.8 X3, and we calculated μY¼$6465 and
(assuming independence) σY¼$493.83. If the Xis are normally distributed, the probability that
revenue exceeds $5000 is
PY>5000ðÞ ¼ PZ >5000/C06465
493:83/C18/C19
¼PZ>/C02:967 ðÞ ¼ 1/C0Φ/C02:967ðÞ ¼ :9985 ■
This same method may be applied to Poisson rvs, as the next proposition indicates.4.3 Properties of Linear Combinations 271
PROPOSITION
Suppose X1,...,Xnare independent Poisson random variables, where Xihas mean μi. Then
Y¼X1+/C1/C1/C1+Xnalso has a Poisson distribution, with mean μ1+/C1/C1/C1+μn.
Proof From Sect. 2.7, the mgf of a Poisson rv with mean μiseμet/C01ðÞ:Since Yis the sum of the Xis,
and the Xis are independent,
MYtðÞ¼ MX1tðÞ /C1/C1/C1 MXntðÞ¼ eμ1et/C01ðÞ/C1/C1/C1eμnet/C01ðÞ¼eμ1þ/C1/C1/C1þ μn ðÞ et/C01ðÞ
This is the mgf of a Poisson rv with mean μ1+/C1/C1/C1+μn. Therefore, by the uniqueness property of
mgfs, Yhas a Poisson distribution with mean μ1+/C1/C1/C1+μn. ■
Example 4.23 During the open enrollment period at a large university, the number of freshmen
registering for classes through the online registration system in 1 h follows a Poisson distribution with
mean 80 students; denote this rv by X1. Deﬁne X2,X3, and X4similarly for sophomores, juniors, and
seniors, and suppose the corresponding means are 125, 118, and 140, respectively. Assume these four
counts are independent. The rv Y¼X1+X2+X3+X4represents the total number of undergraduate
students registering in 1 h; by the preceding proposition, Yis also a Poisson rv, but with mean
80 + 125 + 118 + 140 ¼463 students and standard deviationﬃﬃﬃﬃﬃﬃﬃﬃ
463p
¼21:5 students. The probabil-
ity that more than 500 students enroll during 1 h, exceeding the registration system’s capacity, is then
P(Y>500)¼1/C0P(Y/C20500)¼.042 (software was used to perform the calculation). ■
Because of the properties stated in the preceding two propositions, both the normal and Poisson
models are sometimes called additive distributions , meaning that the sum of independent rvs from
that family (normal or Poisson) will also belong to that family. The next proposition shows that not all
of the major probability distributions are additive; its proof is left as an exercise (Exercise 65).
PROPOSITION
Suppose X1,...,Xnare independent exponential random variables with common parameter λ.
Then Y¼X1+/C1/C1/C1+Xnhas a gamma distribution, with parameters α¼nandβ¼1/λ(aka the
Erlang distribution).
Notice this proposition requires the Xito have the same “rate” parameter λ, i.e., the Xis must be
independent andidentically distributed. As we saw in Example 4.21, the sum of two independent
exponential rvs with different parameters does not follow an exponential distribution.
4.3.3 Exercises: Section 4.3(43–65)
43. A shipping company handles containers in three different sizes: (1) 27 ft3(3/C23/C23),
(2) 125 ft3, and (3) 512 ft3. Let Xi(i¼1, 2, 3) denote the number of type icontainers shipped
during a given week. With μi¼E(Xi) and σi¼SD(Xi), suppose the mean values and standard
deviations are as follows:272 4 Joint Probability Distributions and Their Applications
μ1¼200 μ2¼250 μ3¼100
σ1¼10 σ2¼12 σ3¼8
(a) Assuming that X1,X2,X3are independent, calculate the expected value and standard
deviation of the total volume shipped. [ Hint: Volume ¼27X1+125X2+512X3.]
(b) Would your calculations necessarily be correct if the Xis were not independent? Explain.
(c) Suppose that the Xis are independent with each one having a normal distribution. What is the
probability that the total volume shipped is more than 100,000 ft3?
44. Let X1,X2, and X3represent the times necessary to perform three successive repair tasks at a
service facility. Suppose they are independent, normal rvs with expected values μ1,μ2, and μ3and
variances σ12,σ22, and σ32, respectively.
(a) If μ1¼μ2¼μ3¼60 and σ12¼σ22¼σ32¼15, calculate P(X1+X2+X3/C20200).
(b) Using the μis and σis given in part (a), what is P(150/C20X1+X2+X3/C20200)?
(c) Using the μis and σis given in part (a), calculate P55/C20/C22X ðÞ andP58/C20/C22X/C2062 ðÞ :[As noted
at the beginning of this section, /C22Xdenotes the sample mean, so here /C22X¼X1þX2þX3 ðÞ =3:]
(d) Using the μis and σis given in part (a), calculate P(–10/C20X1/C0.5X2/C0.5X3/C205).
(e) If μ1¼40, μ2¼50, μ3¼60, σ12¼10, σ22¼12, and σ32¼14, calculate both
P(X1+X2+X3/C20160) and P(X1+X 2/C212X3).
45. Five automobiles of the same type are to be driven on a 300-mile trip. The ﬁrst two have
six-cylinder engines, and the other three have four-cylinder engines. Let X1,X2,X3,X4, and X5
be the observed fuel efﬁciencies (mpg) for the ﬁve cars. Suppose these variables are independent
and normally distributed with μ1¼μ2¼20,μ3¼μ4¼μ5¼21, and σ2¼4 for the smaller
engines and 3.5 for the larger engines. Deﬁne an rv Yby
Y¼X1þX2
2/C0X3þX4þX5
3
so that Yis a measure of the difference in efﬁciency between the six-cylinder and four-cylinder
engines. Compute P(0/C20Y) and P(–1/C20Y/C201). [ Hint:Y¼a1X1+/C1/C1/C1+a 5X5, with
a1¼1
2,...,a5¼/C01
3:]
46. Exercise 28 introduced random variables XandY, the number of cars and buses, respectively,
carried by a ferry on a single trip. The joint pmf of XandYis given in the table in Exercise 9. It is
readily veriﬁed that XandYare independent.
(a) Compute the expected value, variance, and standard deviation of the total number of
vehicles on a single trip.
(b) If each car is charged $3 and each bus $10, compute the expected value, variance, and
standard deviation of the revenue resulting from a single trip.
47. A concert has three pieces of music to be played before intermission. The time taken to play each
piece has a normal distribution. Assume that the three times are independent of each other. The
mean times are 15, 30, and 20 min, respectively, and the standard deviations are 1, 2, and 1.5 min,
respectively. What is the probability that this part of the concert takes at most 1 h? Are there
reasons to question the independence assumption? Explain.
48. Refer to Exercise 3.
(a) Calculate the covariance between X1¼the number of customers in the express checkout
andX2¼the number of customers in the superexpress checkout.
(b) Calculate Var( X1+X 2). How does this compare to Var( X1)+Var(X2)?4.3 Properties of Linear Combinations 273
49. Suppose your waiting time for a bus in the morning is uniformly distributed on [0, 8], whereas
waiting time in the evening is uniformly distributed on [0, 10] independent of morning waiting
time.
(a) If you take the bus each morning and evening for a week, what is your total expected waiting
time? [ Hint: Deﬁne rvs X1,...,X10and use a rule of expected value.]
(b) What is the variance of your total waiting time?
(c) What are the expected value and variance of the difference between morning and evening
waiting times on a given day?
(d) What are the expected value and variance of the difference between total morning waiting
time and total evening waiting time for a particular week?
50. An insurance ofﬁce buys paper by the ream (500 sheets) for use in the copier, fax, and printer.
Each ream lasts an average of 4 days, with standard deviation 1 day. The distribution is normal,
independent of previous reams.
(a) Find the probability that the next ream outlasts the present one by more than 2 days.
(b) How many reams must be purchased if they are to last at least 60 days with probability at
least 80%?
51. If two loads are applied to a cantilever beam as shown in the accompanying drawing, the bending
moment at 0 due to the loads is a1X1+a 2X2.
X1 X2
a1 a2
0
(a) Suppose that X1andX2are independent rvs with means 2 and 4 kips, respectively, and
standard deviations .5 and 1.0 kip, respectively. If a1¼5 ft and a2¼10 ft, what is the
expected bending moment and what is the standard deviation of the bending moment?
(b) If X1andX2are normally distributed, what is the probability that the bending moment will
exceed 75 kip-ft?
(c) Suppose the positions of the two loads are random variables. Denoting them by A1andA2,
assume that these variables have means of 5 and 10 ft, respectively, that each has a standard
deviation of .5, and that all Ais and Xis are independent of each other. What is the expected
moment now?
(d) For the situation of part (c), what is the variance of the bending moment?
(e) If the situation is as described in part (a) except that Corr( X1,X2)¼.5 (so that the two loads
are not independent), what is the variance of the bending moment?
52. One piece of PVC pipe is to be inserted inside another piece. The length of the ﬁrst piece is
normally distributed with mean value 20 in. and standard deviation .5 in. The length of the second
piece is a normal rv with mean and standard deviation 15 in. and .4 in., respectively. The amount
of overlap is normally distributed with mean value 1 in. and standard deviation .1 in. Assuming
that the lengths and amount of overlap are independent of each other, what is the probability that
the total length after insertion is between 34.5 and 35 in.?
53. Two airplanes are ﬂying in the same direction in adjacent parallel corridors. At time t¼0, the
ﬁrst airplane is 10 km ahead of the second one. Suppose the speed of the ﬁrst plane (km/h) is
normally distributed with mean 520 and standard deviation 10 and the second plane’s speed,
independent of the ﬁrst, is also normally distributed with mean and standard deviation 500 and
10, respectively.274 4 Joint Probability Distributions and Their Applications
(a) What is the probability that after 2 h of ﬂying, the second plane has not caught up to the ﬁrst
plane?
(b) Determine the probability that the planes are separated by at most 10 km after 2 h.
54. Three different roads feed into a particular freeway entrance. Suppose that during a ﬁxed time
period, the number of cars coming from each road onto the freeway is a random variable, with
expected value and standard deviation as given in the table.
Road 1 Road 2 Road 3
Expected value 800 1000 600
Standard deviation 16 25 18
(a) What is the expected total number of cars entering the freeway at this point during the
period? [ Hint: Let Xi¼the number from road i.]
(b) What is the standard deviation of the total number of entering cars? Have you made any
assumptions about the relationship between the numbers of cars on the different roads?
(c) With Xidenoting the number of cars entering from road iduring the period, suppose that
Cov( X1,X2)¼80, Cov( X1,X3)¼90, and Cov( X2,X3)¼100 (so that the three streams of
trafﬁc are not independent). Compute the expected total number of entering cars and the
standard deviation of the total.
55. Suppose we take a random sample of size nfrom a continuous distribution having median 0 so
that the probability of any one observation being positive is .5. We now disregard the signs of the
observations, rank them from smallest to largest in absolute value, and then let W¼the sum of
the ranks of the observations having positive signs. For example, if the observations are –.3,+.7,
+2.1, and –2.5, then the ranks of positive observations are 2 and 3, so W¼5. In statistics
literature, Wis called Wilcoxon’s signed-rank statistic. W can be represented as follows:
W¼1/C1Y1þ2/C1Y2þ3/C1Y3þ/C1/C1/C1þ n/C1Yn¼Xn
i¼1i/C1Yi
where the Yis are independent Bernoulli rvs, each with p¼.5 (Yi¼1 corresponds to the
observation with rank ibeing positive). Compute the following:
(a) E(Yi) and then E(W) using the equation for W[Hint: The ﬁrst npositive integers sum to
n(n+1)/2.]
(b) Var( Yi) and then Var( W)[Hint: The sum of the squares of the ﬁrst npositive integers is
n(n+ 1)(2 n+1)/6.]
56. In Exercise 51, the weight of the beam itself contributes to the bending moment. Assume that the
beam is of uniform thickness and density so that the resulting load is uniformly distributed on the
beam. If the weight of the beam is random, the resulting load from the weight is also random;
denote this load by W(kip-ft).
(a) If the beam is 12 ft long, Whas mean 1.5 and standard deviation .25, and the ﬁxed loads are
as described in part (a) of Exercise 51, what are the expected value and variance of the
bending moment? [ Hint: If the load due to the beam were wkip-ft, the contribution to the
bending moment would be wÐ12
0xdx.]
(b) If all three variables ( X1,X2, and W) are normally distributed, what is the probability that the
bending moment will be at most 200 kip-ft?
57. A professor has three errands to take care of in the Administration Building. Let Xi¼the time
that it takes for the ith errand ( i¼1, 2, 3), and let X4¼the total time in minutes that she spends
walking to and from the building and between each errand. Suppose the Xis are independent,
normally distributed, with the following means and standard deviations: μ1¼15,σ1¼4,4.3 Properties of Linear Combinations 275
μ2¼5,σ2¼1,μ3¼8,σ3¼2,μ4¼12,σ4¼3. She plans to leave her ofﬁce at precisely
10:00 a.m. and wishes to post a note on her door that reads, “I will return by ta.m.” What time
tshould she write down if she wants the probability of her arriving after tto be .01?
58. In an area having sandy soil, 50 small trees of a certain type were planted, and another 50 trees
were planted in an area having clay soil. Let X¼the number of trees planted in sandy soil that
survive 1 year and Y¼the number of trees planted in clay soil that survive 1 year. If the
probability that a tree planted in sandy soil will survive 1 year is .7 and the probability of 1-year
survival in clay soil is .6, compute an approximation to P(–5/C20X/C0Y/C205). [Hint: Use a normal
approximation from Sect. 3.3. Do not bother with the continuity correction.]
59. Let XandYbe independent rvs, with X~N(0, 1) and Y~N(0, 1).
(a) Use convolution to show that X+Y is also normal, and identify its mean and standard
deviation.
(b) Use the additive property of the normal distribution presented in this section to verify your
answer to part (a).
60. Karen throws two darts at a board with radius 10 in.; let XandYdenote the distances of the two
darts from the center of the board. Under the system Karen uses, the score she receives depends
upon W¼X+Y , the sum of these two distances. Assume XandYare independent.
(a) Suppose XandYare both uniform on the interval [0, 10]. Use convolution to determine the
pdf of W¼X+Y. Be very careful with your limits of integration!
(b) Based on the pdf in part (a), calculate P(X+Y /C205).
(c) If Karen’s darts are equally likely to land anywhere on the board, it can be shown that the
pdfs of XandYarefX(x)¼x/50 for 0 /C20x/C2010 and fY(y)¼y/50 for 0 /C20y/C2010. Use
convolution to determine the pdf of W¼X+Y . Again, be very careful with your limits of
integration.
(d) Based on the pdf in part (c), calculate P(X+Y /C205).
61. Siblings Matt and Liz both enjoy playing roulette. One day, Matt brought $10 to the local casino
and Liz brought $15. They sat at different tables, and each made $1 wagers on red on consecutive
spins (10 spins for Matt, 15 for Liz). Let X¼the number of times Matt won and Y¼the number
of times Liz won.
(a) What is a reasonable probability model for X?[Hint: Successive spins of a roulette wheel
are independent, and P(land on red) ¼18/38.]
(b) What is a reasonable probability model for Y?
(c) What is a reasonable probability model for X+Y , the total number of times Matt and Liz
win that day? Explain. [ Hint: Since the siblings sat at different table, their gambling results
are independent.]
(d) Use moment-generating functions, along with your answers to (a) and (b), to show that your
answer to part (c) is correct.
(e) Generalize part (d): If X1,...,Xkare independent binomial rvs, with Xi~ Bin( ni,p), show
that their sum is also binomially distributed.
(f) Does the result of part (e) hold if the probability parameter pis different for each Xi(e.g., if
Matt bets on red but Liz bets on the number 27)?
62. The children attending Milena’s birthday party are enjoying taking swings at a pin ˜ata. Let
X¼the number of swings it takes Milena to hit the pin ˜ata once (since she’s the birthday girl,
she goes ﬁrst), and let Y¼the number of swings it takes her brother Lucas to hit the pin ˜ata once
(he goes second). Assume the results of successive swings are independent (the children don’t
improve, since they’re blindfolded), and that each child has a .2 probability of hitting the pin ˜ata
on any attempt.276 4 Joint Probability Distributions and Their Applications
(a) What is a reasonable probability model for X?
(b) What is a reasonable probability model for Y?
(c) What is a reasonable probability model for X+Y , the total number of swings taken by
Milena and Lucas? Explain. (Assume Milena’s and Lucas’ results are independent.)
(d) Use moment-generating functions, along with your answers to (a) and (b), to show that
X+Yhas a negative binomial distribution.
(e) Generalize part (d): If X1,...,Xrare independent geometric rvs with common parameter p,
show that their sum has a negative binomial distribution.
(f) Does the result of part (e) hold if the probability parameter pis different for each Xi(e.g., if
Milena has probability .4 on each attempt while Lucas’ success probability is only .1)?
63. Let X1,...,Xnbe independent rvs, with Xihaving a negative binomial distribution with
parameters riandp(i¼1,...,n). Use moment generating functions to show that X1+/C1/C1/C1+Xn
has a negative binomial distribution, and identify the parameters of this distribution. Explain why
this answer makes sense, based on the negative binomial model. [ Note: Each Ximay have a
different parameter ri, but all have the same pparameter.]
64. Let XandYbe independent gamma random variables, both with the same scale parameter β. The
value of the shape parameter is α1forXandα2forY. Use moment generating functions to show
thatX+Yis also gamma distributed, with shape parameter α1+α2and scale parameter β.I s
X+Ygamma distributed if the scale parameters are different? Explain.
65. Let XandYbe independent exponential random variables with common parameter λ.
(a) Use convolution to show that X+Yhas a gamma distribution, and identify the parameters of
that gamma distribution.
(b) Use the previous exercise to establish the same result.
(c) Generalize part (b): If X1,...,Xnare independent exponential rvs with common parameter λ,
what is the distribution of their sum?
4.4 Conditional Distributions and Conditional Expectation
The distribution of Ycan depend strongly on the value of another variable X. For example, if Xis
height and Yis weight, the distribution of weight for men who are 6 ft tall is very different from the
distribution of weight for short men. The conditional distribution ofYgiven X¼xdescribes for each
possible xvalue how probability is distributed over the set of yvalues. We deﬁne below the
conditional distribution of Ygiven X, but the conditional distribution of Xgiven Ycan be obtained
by just reversing the roles of XandY. Both deﬁnitions are analogous to that of the conditional
probability, P(A|B), as the ratio P(A\B)/P(B).
DEFINITION
LetXandYbe two discrete random variables with joint pmf p(x,y) and marginal XpmfpX(x).
Then for any xvalue such that pX(x)>0, the conditional probability mass function of Y
given X¼xis
pYjXyjxðÞ ¼px;yðÞ
pXxðÞ4.4 Conditional Distributions and Conditional Expectation 277
An analogous formula holds in the continuous case. Let XandYbe two continuous random
variables with joint pdf f(x,y) and marginal XpdffX(x). Then for any xvalue such that fX(x)>0,
theconditional probability density function of Ygiven X¼xis
fYjXyjxðÞ ¼fx;yðÞ
fXxðÞ
Example 4.24 For a discrete example, reconsider Example 4.1, where Xrepresents the deductible
amount on an automobile policy and Yrepresents the deductible amount on a homeowner’s policy.
Here is the joint distribution again.
y
p(x,y) 0 100 200
x100 .20 .10 .20 .50
250 .05 .15 .30 .50
.25 .25 .50
The distribution of Ydepends on X. In particular, let’s ﬁnd the conditional probability that Yis
200, given that Xis 250, ﬁrst using the deﬁnition of conditional probability from Sect. 1.4:
PY¼200jX¼250 ðÞ ¼PY¼200\X¼250 ðÞ
PX¼250ðÞ¼:30
:05þ:15þ:30¼:6
With our new deﬁnition we obtain the same result:
pYjX200j250 ðÞ ¼p250;200ðÞ
pX250ðÞ¼:30
:50¼:6
The conditional probabilities for the two other possible values of Yare
pYjX0j250ðÞ ¼p250;0ðÞ
pX250ðÞ¼:05
:50¼:1
pYjX100j250 ðÞ ¼p250;100ðÞ
pX250ðÞ¼:15
:50¼:3
Notice that pY|X(0j250) + pY|X(100j250) + pY|X(200j250)¼.1 + .3 + .6 ¼1. This is no coinci-
dence: conditional probabilities satisfy the properties of ordinary probabilities (i.e., they are nonneg-
ative and they sum to 1). Essentially, the denominator in the deﬁnition of conditional probability is
designed to make the total be 1.
Reversing the roles of XandY, we ﬁnd the conditional distribution for X, given that Y¼0:
pXjY100j0 ðÞ ¼p100;0ðÞ
pY0ðÞ¼:20
:20þ:05¼:8
pXjY250j0 ðÞ ¼p250;0ðÞ
pY0ðÞ¼:05
:20þ:05¼:2
Again, the conditional probabilities add to 1. ■278 4 Joint Probability Distributions and Their Applications
Example 4.25 For a continuous example, recall Example 4.5, where Xis the weight of almonds and
Yis the weight of cashews in a can of mixed nuts. The sum of X+Yis at most 1 lb, the total weight of
the can of nuts. The joint pdf of XandYis
fx;yðÞ ¼24xy
0/C26
0/C20x/C201, 0/C20y/C201,xþy/C201
otherwise
and in Example 4.5 it was shown that
fXxðÞ ¼12x1/C0xðÞ20/C20x/C201
0 otherwise/C26
Thus, the conditional pdf of Ygiven that X¼xis
fYjXyjxðÞ ¼fx;yðÞ
fXxðÞ¼24xy
12x1/C0xðÞ2¼2y
1/C0xðÞ20/C20y/C201/C0x
This can be used to get conditional probabilities for Y. For example,
PY/C20:25jX¼:5 ðÞ ¼ð:25
/C01fYjXyj:5ðÞ dy¼ð:25
02y
1/C0:5ðÞ2dy¼4y2/C2/C3 :25
0¼:25
Given that the weight of almonds ( X) is .5 lb, the probability is .25 for the weight of cashews ( Y)t o
be less than .25 lb.
Just as in the discrete case, the conditional distribution assigns a total probability of 1 to the set of
all possible Yvalues. That is, integrating the conditional density over its set of possible values should
yield 1:
ð1
/C01fYjXyjxðÞ dy¼ð1/C0x
02y
1/C0xðÞ2dy¼y2
1/C0xðÞ2"#1/C0x
0¼1
Whenever you calculate a conditional density, we recommend doing this integration as a validity
check. ■
4.4.1 Conditional Distributions and Independence
Recall that in Sect. 4.1two random variables were deﬁned to be independent if their joint pmf or pdf
factors into the product of the marginal pmfs or pdfs. We can understand this deﬁnition better with the
help of conditional distributions. For example, suppose there is independence in the discrete case.
Then
pYjXyjxðÞ ¼px;yðÞ
pXxðÞ¼pXxðÞpYyðÞ
pXxðÞ¼pYyðÞ
That is, independence implies that the conditional distribution of Yis the same as the unconditional
(i.e., marginal) distribution, and that this is true no matter the value of X. The implication works in the
other direction, too. If pY|X(y|x)¼pY(y), then4.4 Conditional Distributions and Conditional Expectation 279
px;yðÞ
pXxðÞ¼pYyðÞ
sop(x,y)¼pX(x)pY(y), and therefore XandYare independent.
In Example 4.7 we said that independence necessitates the region of positive density being a
rectangle (possibly inﬁnite in extent). In terms of conditional distributions, this region tells us the
domain of Yfor each possible xvalue. For independence we need to have the domain of Y(the interval
of positive density) be the same for each x, implying a rectangular region.
4.4.2 Conditional Expectation and Variance
Because the conditional distribution is a valid probability distribution, it makes sense to deﬁne the
conditional mean and variance.
DEFINITION
LetXandYbe two discrete random variables with conditional probability mass function
pY|X(y|x). Then the conditional expectation (orconditional mean )ofYgiven X¼xis
μYjX¼x¼EYjX¼x ðÞ ¼X
yy/C1pYjXyjxðÞ
Analogously, for two continuous rvs XandYwith conditional probability density function
fY|X(y|x),
μYjX¼x¼EYjX¼x ðÞ ¼ð1
/C01y/C1fYjXyjxðÞ dy
More generally, the conditional mean of any function h(Y) is given by
EhYðÞ jX¼x ðÞ ¼X
yhyðÞ /C1 pYjXyjxðÞ discrete caseðÞ
ð1
/C01hyðÞ /C1 fYjXyjxðÞ dy continous caseðÞ8
>><
>>:
In particular, the conditional variance of Ygiven X¼xis
σ2
YjX¼x¼VarYjX¼x ðÞ ¼ E½ðY/C0μYjX¼xÞ2jX¼x/C138
¼E/C0
Y2jX¼x/C1
/C0μ2
YjX¼x
Example 4.26 Having previously found the conditional distribution of Ygiven X¼250 in
Example 4.24, we now compute the conditional mean and variance.
μYjX¼250¼EYjX¼250 ðÞ ¼ 0/C1pYjX0j250ðÞ þ 100/C1pYjX100j250 ðÞ þ 200/C1pYjX200j250 ðÞ
¼0:1ðÞ þ 100:3ðÞ þ 200:6ðÞ ¼ 150
The average homeowner’s policy deductible, among customers with a $250 auto deductible, is
$150. Given that the possibilities for Yare 0, 100, and 200 and most of the probability is on the latter
two values, it is reasonable that the conditional mean should be between 100 and 200.280 4 Joint Probability Distributions and Their Applications
Using the alternative (shortcut) formula for the conditional variance requires ﬁrst obtaining the
conditional expectation of Y2:
EY2jX¼250/C0/C1
¼02pYjX0j250ðÞ þ 1002pYjX100j250 ðÞ þ 2002pYjX200j250 ðÞ
¼02:1ðÞ þ 1002:3ðÞ þ 2002:6ðÞ ¼ 27, 000
Thus,
σ2
YjX¼250¼VarYjX¼250 ðÞ ¼ EY2jX¼250/C0/C1
/C0μ2
YjX¼250¼27, 000 /C01502¼4500
Taking the square root gives σY|X¼250¼$67.08, which is in the right ballpark when we recall
that the possible values of Yare 0, 100, and 200. ■
Example 4.27 (Example 4.25 continued) Suppose a 1-lb can of mixed nuts contains .1 lbs of
almonds (i.e., we know that X¼.1). Given this information, the amount of cashews Yin the can is
constrained by 0 /C20y/C201/C0x¼.9, and the expected amount of cashews in such a can is
EYjX¼:1 ðÞ ¼ð:9
0y/C1fYjXyj:1ðÞ dy¼ð:9
0y/C12y
1/C0:1ðÞ2dy¼:6
The conditional variance of Ygiven that X¼.1 is
VarYjX¼:1 ðÞ ¼ð:9
0y/C0:6ðÞ2/C1fYjXyj:1ðÞ dy¼ð:9
0y/C0:6ðÞ2/C12y
1/C0:1ðÞ2dy¼:045
Using the aforementioned shortcut, this can also be calculated in two steps:
EY2jX¼:1/C0/C1
¼ð:9
0y2/C1fYjXyj:1ðÞ dy¼ð:9
0y2/C12y
1/C0:1ðÞ2dy¼:405
)VarYjX¼:1 ðÞ ¼ :405/C0/C0
:6/C12¼:045
More generally, conditional on X¼xlbs (where 0 <x<1), integrals similar to those above can
be used to show that the conditional mean amount of cashews is 2(1 /C0x)/3, and the corresponding
conditional variance is (1 /C0x)2/18. This formula implies that the variance gets smaller as the weight
of almonds in a can approaches 1 lb. Does this make sense? When the weight of almonds is 1 lb, the
weight of cashews is guaranteed to be 0, implying that the variance is 0. Indeed, Fig. 4.2shows that
the set of possible y-values narrows to 0 as xapproaches 1. ■
4.4.3 The Laws of Total Expectation and Variance
By the deﬁnition of conditional expectation, the rv Yhas a conditional mean for every possible value
xof the variable X. In Example 4.26, we determined the mean of Ygiven that X¼250, but a different
mean would result if we conditioned on X¼100. For the continuous rvs in Example 4.27, every
value xbetween 0 and 1 yielded a different conditional mean of Y(and, in fact, we even found a
general formula for this conditional expectation). As it turns out, these conditional means can be
related back to the unconditional mean of Y, i.e., μY. Our next example illustrates the connection.4.4 Conditional Distributions and Conditional Expectation 281
Example 4.28 Apartments in a certain city have x¼0, 1, 2, or 3 bedrooms (0 for a studio
apartment), and y¼1, 1.5, or 2 bathrooms. The accompanying table gives the proportions of
apartments for the various number of bedroom/number of bathroom combinations.
y
p(x, y) 1 1.5 2
0 .10 .00 .00 .1
1 .20 .08 .02 .3x
2 .15 .10 .15 .4
3 .05 .05 .10 .2
.50 .23 .27
LetXandYdenote the number of bedrooms and bathrooms, respectively, in a randomly selected
apartment in this city. The marginal distribution of Ycomes from the column totals in the joint
probability table, from which it is easily veriﬁed that E(Y)¼1.385 and Var( Y)¼.179275. The
conditional distributions (pmfs) of Ygiven that X¼xforx¼0, 1, 2, and 3 are as follows:
x¼0:pYjX¼01ðÞ ¼ 1 all studio apartments have one bathroomðÞ
x¼1:pYjX¼11ðÞ ¼ :667, pYjX¼11:5ðÞ ¼ :267, pYjX¼12ðÞ ¼ :067
x¼2:pYjX¼21ðÞ ¼ :375, pYjX¼21:5ðÞ ¼ :25, pYjX¼22ðÞ ¼ :375
x¼3:pYjX¼31ðÞ ¼ :25, pYjX¼31:5ðÞ ¼ :25, pYjX¼32ðÞ ¼ :50
From these conditional pmfs, we obtain the expected value of Ygiven X¼xfor each of the four
possible xvalues:
EYjX¼0 ðÞ ¼ 1,EYjX¼1 ðÞ ¼ 1:2,EYjX¼2 ðÞ ¼ 1:5,EYjX¼3 ðÞ ¼ 1:625
So, on the average, studio apartments have 1 bathroom, one-bedroom apartments have 1.2
bathrooms, 2-bedrooms have 1.5 baths, and luxurious 3-bedroom apartments have 1.625 baths.
Now, instead of writing E(Y|X¼x) for some speciﬁc value x, let’s consider the expected number
of bathrooms for an apartment of randomly selected size, X. This expectation, denoted E(Y|X), is itself
a random variable, since it is a function of the random quantity X. Its smallest possible value is
1, which occurs when X¼0, and that happens with probability .1 (the sum of probabilities in the ﬁrst
row of the joint probability table). Similarly, the random variable E(Y|X) takes on the value 1.2 with
probability pX(1)¼.3. Continuing in this manner, the probability distribution of the rv E(Y|X)i sa s
follows:
Value of EðYjXÞ 11 :21 :51 :625
Probability of value :1:3 :4 :2ð4:7Þ
The expected value of this random variable, denoted E[E(Y|X)], is computed by taking the
weighted average of the four values of E(Y|X¼x) against the probabilities speciﬁed by pX(x), as
suggested by (4.7):
EEY jXðÞ½/C138 ¼ 1:1ðÞ þ 1:2:3ðÞ þ 1:5:4ðÞ þ 1:625:2ðÞ ¼ 1:385
But this is exactly E(Y), the expected number of bathrooms. ■282 4 Joint Probability Distributions and Their Applications
LAW OF TOTAL EXPECTATION
For any two random variables XandY,
EEY jXðÞ½/C138 ¼ EYðÞ
(This is sometimes referred to as computing E(Y) by means of iterated expectation. )
The Law of Total Expectation says that E(Y) is a weighted average of the conditional means
E(Y|X¼x), where the weights are given by the pmf or pdf of X. It is analogous to the Law of Total
Probability, which describes how to ﬁnd P(B) as a weighted average of conditional probabilities P(B|Ai).
Proof Here is the proof when both rvs are discrete; in the jointly continuous case, simply replace
summation by integration and pmfs by pdfs.
E/C2
EYjXðÞ/C3
¼X
x2DXEYjX¼x ðÞ pXxðÞ ¼X
x2DXX
y2DYypYjXyjxðÞ pXxðÞ
¼X
x2DXX
y2DYypx;yðÞ
pXxðÞpXxðÞ ¼X
y2DYyX
x2DXpx;yðÞ
¼X
y2DYypYyðÞ ¼ EYðÞ ■
In Example 4.28, the use of iterated expectation to compute E(Y) is unnecessarily cumbersome;
working from the marginal pmf of Yis more straightforward. However, there are many situations in
which the distribution of a variable Yis only expressed conditional on the value of another variable X.
For these so-called hierarchical models , the Law of Total Expectation proves very useful.
Example 4.29 A ferry goes from the left bank of a small river to the right bank once an hour. The
ferry can accommodate at most two vehicles. The probability that no vehicles show up is .1, than
exactly one shows up is .7, and that two or more show up is .2 (but only two can be transported). The
fare paid for a vehicle depends upon its weight, and the average fare per vehicle is $25. What is the
expected fare for a single trip made by this ferry?
LetXrepresent the number of vehicles that show up, and let Ydenote the total fare for a single trip.
The conditional mean of Y, given X, is given by E(Y|X)¼25X. So, by the Law of Total Expectation,
EYðÞ ¼ E/C2
E/C0
YjX/C1/C3
¼E/C2
25X/C3
¼X2
x¼025x/C1pXxðÞ ½/C138
¼0ðÞ/C0
:1/C1
þ/C0
25/C1/C0
:7/C1
þ/C0
50/C1/C0
:2/C1
¼$27:50 ■
The next theorem provides a way to compute the variance of Yby conditioning on the value of X.
There are two contributions to Var( Y). The ﬁrst part is the variance of the random variable E(Y|X).
The second part involves the random variable Var( Y|X)—the variance of Yas a function of X—and in
particular the expected value of this random variable.
LAW OF TOTAL VARIANCE
For any two random variables XandY,
VarYðÞ ¼ VarEYjXðÞ½/C138 þ EVarYjXðÞ½/C1384.4 Conditional Distributions and Conditional Expectation 283
Proving the Law of Total Variance requires some more sophisticated algebra; see Exercise 84. For
those familiar with statistical methods, the Law of Total Variance is analogous to the famous
ANOVA identity, wherein the total variability in a response variable Ycan be decomposed into the
differences between group means (here, the term Var[ E(Y|X)]) and the variation of responses within
groups (represented by E[Var( Y|X)] above).
Example 4.30 Let’s verify the Law of Total Variance for the apartment scenario of Example 4.28.
The pmf of the rv E(Y|X) appears in (4.7), from which its variance is given by
VarEYjXðÞ½/C138 ¼ 1/C01:385 ðÞ2:1ðÞ þ/C0
1:2/C01:385/C12:3ðÞ
þ/C0
1:5/C01:385/C12:4ðÞ þ/C0
1:625/C01:385/C12:2ðÞ
¼0:0419
(Recall that 1.385 is the mean of the rv E(Y|X), which, by the Law of Total Expectation, is also
E(Y).) The second term in the Law of Total Variance involves the variable Var( Y|X), which requires
determining the conditional variance of Ygiven X¼xforx¼0, 1, 2, 3. Using the four conditional
distributions displayed in Example 4.28, these are
VarYjX¼0 ðÞ ¼ 0;VarYjX¼1 ðÞ ¼ :0933
VarYjX¼2 ðÞ ¼ :1875 ;VarYjX¼3 ðÞ ¼ :171875
The rv Var( Y|X) takes on these four values with probabilities .1, .4, .3, and .2, respectively (again,
these are inherited from the distribution of X). Thus,
EVarYjXðÞ½/C138 ¼ 0:1ðÞ þ :0933 :3ðÞ þ :1875 :4ðÞ þ :171875 :2ðÞ ¼ :137375
Combining, Var[ E(Y|X)] +E[Var( Y|X)]¼.0419 + .137375 ¼.179275
This is exactly Var( Y) computed using the marginal pmf of Yin Example 4.28, and the Law of
Total Variance is veriﬁed for this example. ■
The computation of Var( Y) in Example 4.30 is clearly not efﬁcient; it is much easier, given the
joint pmf of XandY, to determine the variance of Yfrom its marginal pmf. As with the Law of Total
Expectation, the real worth of the Law of Total Variance comes from its application to hierarchical
models, where the distribution of one variable ( Y, say) is only known conditional on the distribution
of another rv.
Example 4.31 In the manufacture of ceramic tiles used for heat shielding, the proportion of tiles
that meet the required thermal speciﬁcations varies from day to day. Let Pdenote the proportion of
tiles meeting speciﬁcations on a randomly selected day, and suppose Pcan be modeled by the
following pdf:
fpðÞ ¼ 9p80<p<1
At the end of each day, a random sample of n¼20 tiles is selected and each tile is tested.
LetYrepresent the number of tiles among the 20 that meet speciﬁcations; conditional on P¼p,
Y~ Bin(20, p). Find the expected number of tiles meeting thermal speciﬁcations in a daily sample of
20, and ﬁnd the corresponding standard deviation.
From the properties of the binomial distribution, we know that E(Y|P¼p)¼np¼20p,s o
E(Y|P)¼20P. Applying the Law of Total Expectation,284 4 Joint Probability Distributions and Their Applications
EYðÞ ¼ EEY jPðÞ½/C138 ¼ E20P½/C138 ¼ð1
020p/C1fpðÞdp¼ð1
0180p9dp¼18
This is reasonable: since E(P)¼.9 by integration, the expected proportion of good tiles is 90%,
and thus the expected number of good tiles in a random sample of 20 tiles is 18.
Determining the standard deviation of Yrequires the two pieces of the Law of Total Variance.
First, using the rescaling property of variance,
VarEYjPðÞ½/C138 ¼ Var 20 PðÞ ¼ 202VarPðÞ ¼ 400Var PðÞ
The variance of the proportion Pcan be determined directly from the pdf of Pvia integration. The
result is Var( P)¼9/1100, so Var[ E(Y|P)]¼400(9/1100) ¼36/11. Second, the binomial variance
formula np(1/C0p) implies that the conditional variance of Ygiven Pis Var( Y|P)¼20P(1/C0P), so
EVarYjPðÞ½/C138 ¼ E20P1/C0PðÞ½/C138 ¼ð1
020p1/C0pðÞ /C1 9p8dp¼18
11
Therefore, by the Law of Total Variance,
VarYðÞ ¼ VarEYjPðÞ½/C138 þ EVarYjPðÞ½/C138 ¼36
11þ18
11¼54
11¼4:909,
and the standard deviation of YisσY¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
4:909p
¼2:22:This “total” standard deviation accounts for
two effects: day-to-day variation in quality as modeled by P(the ﬁrst term in the variance expression),
and random variation in the number of observed good tiles as modeled by the binomial distribution
(the second term). ■
Here is an example where the Laws of Total Expectation and Variance are helpful in ﬁnding the
mean and variance of a random variable that is neither discrete nor continuous.
Example 4.32 The probability of a claim being ﬁled on an insurance policy is .1, and only one claim
can be ﬁled. If a claim is ﬁled, the amount is exponentially distributed with mean $1,000. Recall from
Sect. 3.4that the mean and standard deviation of the exponential distribution are the same, so the
variance is the square of this value. We want to ﬁnd the mean and variance of the amount paid. Let
Xbe the number of claims (0 or 1) and let Ybe the payment. We know that E(Y|X¼0)¼0 and also
E(Y|X¼1)¼1000. Also, Var( Y|X¼0)¼0 and Var( Y|X¼1)¼10002¼1,000,000. Here is a
table for the both the distribution of E(Y|X¼x) and that of Var( Y|X¼x):
xP (X¼x) E(Y|X¼x) Var( Y|X¼x)
0 .9 0 0
1 .1 1000 1,000,000
Therefore
EYðÞ ¼ EEY jXðÞ½/C138 ¼ EYjX¼0 ðÞ PX¼0ðÞ þ EYjX¼1 ðÞ PX¼1ðÞ
¼0:9ðÞ þ 1000 :1ðÞ ¼ 100
The average claim amount across all customers is $100. Next, the variance of the conditional
mean is
VarEYjXðÞ½/C138 ¼ 0/C0100ðÞ2:9ðÞ þ 1000/C0100 ðÞ2:1ðÞ ¼ 90,000,
and the expected value of the conditional variance is4.4 Conditional Distributions and Conditional Expectation 285
EVarYjXðÞ½/C138 ¼ 0:9ðÞ þ 1,000,000 :1ðÞ ¼ 100,000
Apply the Law of Total Variance to get Var( Y):
VarYðÞ ¼ VarEYjXðÞ½/C138 þ EVarYjXðÞ½/C138 ¼ 90,000 þ100,000 ¼190,000
Taking the square root gives the standard deviation, σY¼$435.89.
Suppose that we want to compute the mean and variance of Ydirectly. Notice that Xis discrete, but
the conditional distribution of Ygiven X¼1 is continuous. The random variable Yitself is neither
discrete nor continuous, because it has probability .9 of being 0, but the other .1 of its probability is
spread out from 0 to 1. Such “mixed” distributions may require a little extra effort to evaluate means
and variances, although it is not especially hard in this case (because the discrete mass is at 0 and
doesn’t contribute to expectations):
EYðÞ ¼/C0
:1/C1ð1
0y1
1000e/C0y=1000dy¼/C0
:1/C1/C0
1000/C1
¼100
EY2/C0/C1
¼/C0
:1/C1ð1
0y21
1000e/C0y=1000dy¼/C0
:1/C1/C0
2,000,000/C1
¼200,000
VarYðÞ ¼ EY2/C0/C1
/C0EYðÞ½/C1382¼200,000 /C010,000 ¼190,000
These agree with what we found using the theorems. ■
4.4.4 Exercises: Section 4.4(66–84)
66. Refer back to Exercise 1 of this chapter.
(a) Given that X¼1, determine the conditional pmf of Y—that is, pYjX(0j1) ,pYjX(1j1), and
pYjX(2j1).
(b) Given that two hoses are in use at the self-service island, what is the conditional pmf of the
number of hoses in use on the full-service island?
(c) Use the result of part (b) to calculate the conditional probability P(Y/C201jX¼2).
(d) Given that two hoses are in use at the full-service island, what is the conditional pmf of the
number in use at the self-service island?
67. A system consists of two components. Suppose the joint pdf of the lifetimes of the two
components in a system is given by f(x,y)¼c[10/C0(x+y)] for x>0,y>0,x+y<10,
where xandyare in months.
(a) If the ﬁrst component functions for exactly 3 months, what is the probability that the second
functions for more than 2 months?
(b) Suppose the system will continue to work only as long as both components function. Among
20 of these systems that operate independently of each other, what is the probability that at
least half work for more than 3 months?
68. The joint pdf of pressures for right and left front tires is given in Exercise 11.
(a) Determine the conditional pdf of Ygiven that X¼xand the conditional pdf of Xgiven that
Y¼y.
(b) If the pressure in the right tire is found to be 22 psi, what is the probability that the left tire
has a pressure of at least 25 psi? Compare this to P(Y/C2125).
(c) If the pressure in the right tire is found to be 22 psi, what is the expected pressure in the left
tire, and what is the standard deviation of pressure in this tire?286 4 Joint Probability Distributions and Their Applications
69. Suppose that Xis uniformly distributed between 0 and 1. Given X¼x,Yis uniformly distributed
between 0 and x2.
(a) Determine E(Y|X¼x) and then Var( Y|X¼x).
(b) Determine f(x,y) using fX(x) and fY|X(y|x).
(c) Determine fY(y).
70. Consider three Ping-Pong balls numbered 1, 2, and 3. Two balls are randomly selected with
replacement. If the sum of the two resulting numbers exceeds 4, two balls are again selected. This
process continues until the sum is at most 4. Let XandYdenote the last two numbers selected.
Possible ( X,Y) pairs are {(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1)}.
(a) Determine pX,Y(x,y).
(b) Determine pY|X(y|x).
(c) Determine E(Y|X¼x).
(d) Determine E(X|Y¼y). What special property of p(x,y) allows us to get this from (c)?
(e) Determine Var( Y|X¼x).
71. Let Xbe a random digit (0, 1, 2, ..., 9 are equally likely) and let Ybe a random digit not equal to
X. That is, the nine digits other than Xare equally likely for Y.
(a) Determine pX(x),pY|X(y|x),pX,Y(x,y).
(b) Determine a formula for E(Y|X¼x).
72. A pizza delivery business has two phones. On each phone the waiting time until the ﬁrst call is
exponentially distributed with mean 1 min. Each phone is not inﬂuenced by the other. Let Xbe
the shorter of the two waiting times and let Ybe the longer. Using techniques from Sect. 4.9, it can
be shown that the joint pdf of XandYisf(x,y)¼2e–(x+y)for 0 <x<y<1.
(a) Determine the marginal density of X.
(b) Determine the conditional density of Ygiven X¼x.
(c) Determine the probability that Yis greater than 2, given that X¼1.
(d) Are XandYindependent? Explain.
(e) Determine the conditional mean of Ygiven X¼x.
(f) Determine the conditional variance of Ygiven X¼x.
73. Teresa and Allison each have arrival times uniformly distributed between 12:00 and 1:00. Their
times do not inﬂuence each other. If Yis the ﬁrst of the two times and Xis the second, on a scale of
0 to 1, it can be shown that the joint pdf of XandYisf(x,y)¼2 for 0 <y<x<1.
(a) Determine the marginal density of X.
(b) Determine the conditional density of Ygiven X¼x.
(c) Determine the conditional probability that Yis between 0 and .3, given that Xis .5.
(d) Are XandYindependent? Explain.
(e) Determine the conditional mean of Ygiven X¼x.
(f) Determine the conditional variance of Ygiven X¼x.
74. Refer back to the previous exercise.
(a) Determine the marginal density of Y.
(b) Determine the conditional density of Xgiven Y¼y.
(c) Determine the conditional mean of Xgiven Y¼y.
(d) Determine the conditional variance of Xgiven Y¼y.
75. According to an article in the August 30, 2002 issue of the Chronicle of Higher Education , 30%
of ﬁrst-year college students are liberals, 20% are conservatives, and 50% characterize them-
selves as middle-of-the-road. Choose two students at random, let Xbe the number of liberals
among the two, and let Ybe the number of conservatives among the two.4.4 Conditional Distributions and Conditional Expectation 287
(a) Using the multinomial distribution from Sect. 4.1, give the joint probability mass function p
(x,y)o fXandYand the corresponding joint probability table.
(b) Determine the marginal probability mass functions by summing p(x,y) numerically. How
could these be obtained directly? [ Hint: What are the univariate distributions of XandY?]
(c) Determine the conditional probability mass function of Ygiven X¼xforx¼0, 1, 2. Com-
pare this to the binomial distribution with n¼2/C0xandp¼.2/(.2 + .5). Why should this
work?
(d) Are XandYindependent? Explain.
(e) Find E(Y|X¼x) for x¼0, 1, 2. Do this numerically and then compare with the use of the
formula for the binomial mean, using the binomial distribution given in part (c).
(f) Determine Var( Y|X¼x) for x¼0, 1, 2. Do this numerically and then compare with the use
of the formula for the binomial variance, using the binomial distribution given in part (c).
76. A class has 10 mathematics majors, 6 computer science majors, and 4 statistics majors. Two of
these students are randomly selected to make a presentation. Let Xbe the number of mathematics
majors and let Ybe the number of computer science majors chosen.
(a) Determine the joint probability mass function p(x,y). This generalizes the hypergeometric
distribution studied in Sect. 2.6. Give the joint probability table showing all nine values, of
which three should be 0.
(b) Determine the marginal probability mass functions by summing numerically. How could
these be obtained directly? [ Hint: What type of rv is X?Y?]
(c) Determine the conditional probability mass function of Ygiven X¼xforx¼0, 1, 2. Com-
pare with the h(y;2/C0x, 6, 10) distribution. Intuitively, why should this work?
(d) Are XandYindependent? Explain.
(e) Determine E(YjX¼x),x¼0, 1, 2. Do this numerically and then compare with the use of
the formula for the hypergeometric mean, using the hypergeometric distribution given in
part (c).
(f) Determine Var( YjX¼x),x¼0, 1, 2. Do this numerically and then compare with the use of
the formula for the hypergeometric variance, using the hypergeometric distribution given in
part (c).
77. A 1-ft-long stick is broken at a point X(measured from the left end) chosen randomly uniformly
along its length. Then the left part is broken at a point Ychosen randomly uniformly along its
length. In other words, Xis uniformly distributed between 0 and 1 and, given X¼x,Yis
uniformly distributed between 0 and x.
(a) Determine E(YjX¼x) and then Var( YjX¼x).
(b) Determine f(x,y) using fX(x) and fY|X(yjx).
(c) Determine fY(y).
(d) Use fY(y) from (c) to get E(Y) and Var( Y).
(e) Use (a) and the Laws of Total Expectation and Variance to get E(Y) and Var( Y).
78. Consider the situation in Example 4.29, and suppose further that the standard deviation for fares
per car is $4.
(a) Find the variance of the rv E(Y|X).
(b) Using Expression ( 4.6) from the previous section, the conditional variance of Ygiven X¼x
is 42x¼16x. Determine the mean of the rv Var( Y|X).
(c) Use the Law of Total Variance to ﬁnd σY, the unconditional standard deviation of Y.
79. This week the number Xof claims coming into an insurance ofﬁce has a Poisson distribution with
mean 100. The probability that any particular claim relates to automobile insurance is .6,288 4 Joint Probability Distributions and Their Applications
independent of any other claim. If Yis the number of automobile claims, then Yis binomial with
Xtrials, each with “success” probability .6.
(a) Determine E(Y|X¼x) and Var( Y|X¼x).
(b) Use part (a) to ﬁnd E(Y).
(c) Use part (a) to ﬁnd Var( Y).
80. In the previous exercise, show that the distribution of Yis Poisson with mean 60. [You will need
to recognize the Maclaurin series expansion for the exponential function.] Use the knowledge
thatYis Poisson with mean 60 to ﬁnd E(Y) and Var( Y).
81. The heights of American men follow a normal distribution with mean 70 in. and standard
deviation 3 in. Suppose that the weight distribution (lbs) for men that are xinches tall also has
a normal distribution, but with mean 4 x/C0104 and standard deviation .3 x/C017. Let Ydenote the
weight of a randomly selected American man. Find the (unconditional) mean and standard
deviation of Y.
82. A statistician is waiting behind one person to check out at a store. The check-out time for the ﬁrst
person, X, can be modeled by an exponential distribution with some parameter λ>0. The
statistician observes the ﬁrst person’s check-out time, x; being a statistician, she surmises that
her check-out time Ywill follow an exponential distribution with mean x.
(a) Determine E(Y|X¼x) and Var( Y|X¼x).
(b) Use the Laws of Total Expectation and Variance to ﬁnd E(Y) and Var( Y).
(c) Write out the joint pdf of XandY.[Hint: You have fX(x) and fY|X(y|x).] Then write an integral
expression for the marginal pdf of Y(from which, at least in theory, one could determine the
mean and variance of Y). What happens?
83. In the game Plinko on the television game show The Price is Right , contestants have the
opportunity to earn “chips” (ﬂat, circular disks) that can be dropped down a peg board into
slots labeled with cash amounts. Every contestant is given one chip automatically and can earn up
to four more chips by correctly guessing the prices of certain small items. If we let pdenote the
probability a contestant correctly guesses the price of a prize, then the number of chips a
contestant earns, X, can be modeled as X¼1+N, where N~ Bin(4, p).
(a) Determine E(X) and Var( X).
(b) For each chip, the amount of money won on the Plinko board has the following distribution:
Value $0 $100 $500 $1000 $10,000
Probability .39 .03 .11 .24 .23
Determine the mean and variance of the winnings from a single chip.
(c) Let Ydenote the total winnings of a randomly selected contestant. Using results from the
previous section, the conditional mean and variance of Y, given a player gets xchips, are μx
andσ2x, respectively, where μandσ2are the mean and variance for a single chip computed
in (b). Find expressions for the (unconditional) mean and standard deviation of Y.[Note:
Your answers will be functions of p.]
(d) Evaluate your answers to part (c) for p¼0, .5, and 1. Do these answers make sense?
Explain.
84. Let XandYbe any two random variables.
(a) Show that E[Var( Y|X)]¼E[Y2]/C0Eμ2
YjX.[Hint: Use the variance shortcut formula and
apply the Law of Total Expectation to the ﬁrst term.]
(b) Show that Var( E[Y|X])¼Eμ2
YjX/C0(E[Y])2.[Hint: Use the variance shortcut formula again;
this time, apply the Law of Total Expectation to the second term.]
(c) Combine the previous two results to establish the Law of Total Variance.4.4 Conditional Distributions and Conditional Expectation 289
4.5 Limit Theorems (What Happens as nGets Large)
Many problems in probability and statistics involve either a sum or an average of random variables. In
this section we consider what happens as n, the number of variables in such sums and averages, gets
large. The most important result of this type is the celebrated Central Limit Theorem, according to
which the approximate distribution is normal when nis sufﬁciently large.
4.5.1 Random Samples
The random variables from which our sums and averages will be created must satisfy two general
conditions.
DEFINITION
The rvs X1,X2,...,Xnare said to be independent and identically distributed (iid) if
1. The Xis are independent rvs.
2. Every Xihas the same probability distribution.
Such a collection of rvs is also called a (simple) random sample of size n.
For example, X1,X2,...Xnmight be a random sample from a normal distribution with mean
100 and standard deviation 15; then the Xis are independent and each one has the speciﬁed normal
distribution. Similarly, for these variables to constitute a random sample from an exponential
distribution, they must be independent and the value of the exponential parameter λmust be the
same for each variable.
The notion of iid rvs is meant to resemble (simple) random sampling from a population: X1is the
value of some variable for the ﬁrst individual or object selected, X2is the value of that same variable
for the second selected individual or object, and so on. If sampling is either with replacement or from
a (potentially) inﬁnite population, Conditions 1 and 2 are satisﬁed exactly. These conditions will be
approximately satisﬁed if sampling is without replacement, yet the sample size nis much smaller than
the population size N. In practice, if n/N/C20.05 (at most 5% of the population is sampled), we proceed
as if the Xis form a random sample.
Throughout this section, we will be primarily interested in the properties of two particular rvs
derived from random samples: the sample total Tand the sample mean /C22X:
T¼X1þ/C1/C1/C1þ Xn¼Xn
i¼1Xi, /C22X¼X1þ/C1/C1/C1þ Xn
n¼T
n:
Note that both Tand /C22Xare linear combinations of the Xis.290 4 Joint Probability Distributions and Their Applications
PROPOSITION
Suppose X1,X2,...,Xnare iid with common mean μand common standard deviation σ.Tand /C22X
have the following properties:
1.E(T)¼nμ
2. Var( T)¼nσ2and SD TðÞ ¼ﬃﬃﬃnpσ
3. If the Xis are normally distributed, then Tis
also normally distributed.1.E/C22XðÞ ¼ μ
2. Var /C22XðÞ ¼σ2
nand SD /C22XðÞ ¼σﬃﬃﬃnp
3. If the Xis are normally distributed, then /C22Xis
also normally distributed.
Proof Recall from the main theorem of Sect. 4.3that the expected value of a sum is the sum of
individual expected values; moreover, when the variables in the sum are independent, the variance of
the sum is the sum of the individual variances:
ETðÞ ¼ E/C0
X1þ/C1/C1/C1þ Xn/C1
¼E/C0
X1/C1
þ/C1/C1/C1þ E/C0
Xn/C1
¼μþ/C1/C1/C1þ μ¼nμ
VarTðÞ ¼ Var/C0
X1þ/C1/C1/C1þ Xn/C1
¼Var/C0
X1/C1
þ/C1/C1/C1þ Var/C0
Xn/C1
¼σ2þ/C1/C1/C1þ σ2¼nσ2
SDTðÞ ¼ﬃﬃﬃﬃﬃﬃﬃ
nσ2p
¼ﬃﬃﬃnpσ
The corresponding results for /C22Xcan be derived by writing /C22X¼1
n/C1Tand using basic rescaling
properties, such as E(cY)¼cE(Y). Property 3 is a consequence of the more general result from
Sect. 4.3that any linear combination of independent normal rvs is normal. ■
According to Property 1, the distribution of /C22Xis centered precisely at the mean of the population
from which the sample has been selected. If the sample mean is used to compute an estimate
(educated guess) of the population mean μ, there will be no systematic tendency for the estimate to
be too large or too small.
Property 2 shows that the /C22Xdistribution becomes more concentrated about μas the sample size
nincreases, because its standard deviation decreases. In marked contrast, the distribution of
Tbecomes more spread out as nincreases. Averaging moves probability in toward the middle,
whereas totaling spreads probability out over a wider and wider range of values. The expression σ=ﬃﬃﬃnp
for the standard deviation of /C22Xis called the standard error of the mean , and it indicates the typical
amount by which a value of /C22Xwill deviate from the true mean, μ(in contrast, σitself represents the
typical difference between an individual X iandμ).
When σis unknown, as is usually the case when μis unknown and we are trying to estimate it, we
may substitute the sample standard deviation, s, of our sample into the standard error formula and say
that an observed value of /C22Xwill typically differ by about s=ﬃﬃﬃnpfrom μ. This is the estimated standard
error formula presented in Sects. 2.8and3.8.
Finally, Property 3 says that we know everything there is to know about the /C22XandTdistributions
when the population distribution is normal. In particular, probabilities such as Pa/C20/C22X/C20b ðÞ and
P(c/C20T/C20d) can be obtained simply by standardizing. Figure 4.8illustrates the /C22Xpart of the
proposition.4.5 Limit Theorems (What Happens as nGets Large) 291
Example 4.33 The amount of time that a patient undergoing a particular procedure spends in a
certain outpatient surgery center is a random variable with a mean value of 4.5 h and a standard
deviation of 1.4 h. Let X1,...,X25be the times for a random sample of 25 patients. Then the expected
total time for the 25 patients is E(T)¼nμ¼25(4.5) ¼112.5 h, whereas the expected sample mean
amount of time is E/C22XðÞ ¼ μ¼4:5 hours :The standard deviations of Tand /C22Xare
σT¼ﬃﬃﬃnpσ¼ﬃﬃﬃﬃﬃ
25p
1:4ðÞ ¼ 7 hours
σ/C22X¼σﬃﬃﬃnp¼1:4ﬃﬃﬃﬃﬃ
25p¼:28 hours
Suppose further that such patient times follow a normal distribution, i.e., Xi~N(4.5, 1.4). Then the
total time spent by 25 randomly selected patients in this center is also normal: T~N(112.5, 7). The
probability their total time exceeds 5 days (120 h) is
PT>120ðÞ ¼ 1/C0PT/C20120ðÞ ¼ 1/C0Φ120/C0112:5
7/C18/C19
¼1/C0Φ1:07ðÞ ¼ :1423
This same probability can be reframed in terms of /C22X: for 25 patients, a total time of 120 h equates to
an average time of 120/25 ¼4.8 h, and since /C22X/C24N4:5;:28ðÞ ,
P/C22X>4:8 ðÞ ¼ 1/C0Φ4:8/C04:5
:28/C18/C19
¼1/C0Φ1:07ðÞ ¼ :1423 ■
Example 4.34 Resistors used in electronics manufacturing are labeled with a “nominal” resistance
as well as a percentage tolerance. For example, a 330-ohm resistor with a 5% tolerance is anticipated
to have an actual resistance between 313.5 Ωand 346.5 Ω. Consider ﬁve such resistors, randomly
selected from the population of all resistors with those speciﬁcations, and model the resistance of each
by a uniform distribution on [313.5, 346.5]. If these are connected in series, the resistance Rof the
system is given by R¼X1+/C1/C1/C1+X5, where the Xiare the iid uniform resistances.
A random variable uniformly distributed on [ A, B] has mean ( A+B )/2 and standard deviation
B/C0AðÞ =ﬃﬃﬃﬃﬃ
12p
:For our uniform model, the mean resistance is E(Xi)¼(313.5 + 346.5)/2 ¼330Ω,
the nominal resistance, with a standard deviation of 346 :5/C0313:5 ðÞ =ﬃﬃﬃﬃﬃ
12p
¼9:526Ω:The system’s
resistance has mean and standard deviation
ERðÞ ¼ nμ¼5 330ðÞ ¼ 1650Ω,S D RðÞ ¼ﬃﬃﬃnpσ¼ﬃﬃﬃ
5p
9:526ðÞ ¼ 21:3Ω
But what is the probability distribution of R?I sRalso uniformly distributed? Determining the
exact pdf of Ris difﬁcult (it requires four convolutions). And the mgf of R, while easy to obtain, is not
recognizable as coming from any particular family of known distributions. Instead, we resort to a
simulation of R, the results of which appear in Fig. 4.9. For 10,000 iterations in R (appropriately), ﬁveX distribution
when n = 10
X distribution
when n = 4
Population
distributionFig. 4.8 A normal
population distribution and
/C22Xsampling distributions292 4 Joint Probability Distributions and Their Applications
independent uniform variates on [313.5, 346.5] were created and summed; see Sect. 3.8for
information on simulating a uniform distribution. The histogram in Fig. 4.9clearly indicates that
Ris not uniform; in fact, if anything, Rappears (from the simulation, anyway) to be approximately
normal! ■
4.5.2 The Central Limit Theorem
When iid Xis are normally distributed, so are Tand /C22Xfor every sample size n. The simulation results
from Example 4.34 suggest that even when the population distribution is not normal, summing
(or averaging) produces a distribution more bell-shaped than the one being sampled. Upon reﬂection,
this is quite intuitive: in order for Rto be near 5(346.5) ¼1732.5, its theoretical maximum, all ﬁve
randomly selected resistors would have to exert resistances at the high end of their common range
(i.e., every Xiwould have to be near 346.5). Thus, R-values near 1732.5 are unlikely, and the same
applies to R’s theoretical minimum of 5(313.5) ¼1567.5. On the other hand, there are many ways for
Rto be near the mean value of 1650: all ﬁve resistances in the middle, two low and one middle and
two high, and so on. Thus, Ris more likely to be “centrally” located than out at the extremes. (This is
analogous to the well-known fact that rolling a pair of dice is far more likely to result in a sum of
7 than 2 or 12, because there are more ways to obtain 7.)
This general pattern of behavior for sample totals and sample means is formalized by the most
important theorem of probability, the Central Limit Theorem (CLT). A proof of this theorem is
beyond the scope of this book, but interested readers may consult the text by Devore and Berk listed
in the references.Histogram of R
RFrequency
1600 1650 1700050010001500
Fig. 4.9 Simulated distribution of the random variable Rin Example 4.344.5 Limit Theorems (What Happens as nGets Large) 293
CENTRAL LIMIT THEOREM
LetX1,X2,...,Xnbe a random sample from a distribution with mean μand standard deviation
σ. Then, in the limit as n!1 , the standardized versions of Tand /C22Xhave the standard normal
distribution. That is,
lim
n!1PT/C0nμﬃﬃﬃnpσ/C20z/C18/C19
¼PZ/C20zðÞ ¼ ΦzðÞ
and
lim
n!1P/C22X/C0μ
σ=ﬃﬃﬃnp/C20z/C18/C19
¼PZ/C20zðÞ ¼ ΦzðÞ
where Zis a standard normal rv. It is customary to say that Tand /C22Xareasymptotically normal .
Thus when nis sufﬁciently large, the sample total Thas approximately a normal distribution
with mean μT¼nμand standard deviation σT¼ﬃﬃﬃnpσ:Equivalently, for large nthe sample
mean /C22Xhas approximately a normal distribution with mean μ/C22X¼μand standard deviation
σ/C22X¼σ=ﬃﬃﬃnp:
Figure 4.10 illustrates the Central Limit Theorem for /C22X:According to the CLT, when nis large and
we wish to calculate a probability such as Pa/C20/C22X/C20b ðÞ orP(c/C20T/C20d),we need only “pretend”
that /C22XorTis normal, standardize it, and use software or the standard normal table. The resulting
answer will be approximately correct. The exact answer could be obtained only by ﬁrst ﬁnding the
distribution of Tor/C22X,so the CLT provides a truly impressive shortcut.
A practical difﬁculty in applying the CLT is in knowing when nis “sufﬁciently large.” The
problem is that the accuracy of the approximation for a particular ndepends on the shape of the
original underlying distribution being sampled. If the underlying distribution is symmetric and there
is not much probability far out in the tails, then the approximation will be good even for a small n,
whereas if it is highly skewed or has “heavy” tails, then a large nwill be required. For example, if the
distribution is uniform on an interval, then it is symmetric with no probability in the tails, and the
normal approximation is very good for nas small as 10 (in Example 4.34, even for n¼5, the
distribution of the sample total appeared rather bell-shaped). However, at the other extreme, a
distribution can have such fat tails that its mean fails to exist and the Central Limit Theorem does
not apply, so no nis big enough. A popular, although frequently somewhat conservative, convention
is that the Central Limit Theorem may be safely applied when n>30. Of course, there are
exceptions, but this rule applies to most distributions of real data.
X distribution for
small to moderate n
Population
distributionX distribution for
large n (approximately normal)
mFig. 4.10 The Central
Limit Theorem for /C22X
illustrated294 4 Joint Probability Distributions and Their Applications
Example 4.35 When a batch of a certain chemical product is prepared, the amount of a particular
impurity in the batch is a random variable with mean value 4.0 g and standard deviation 1.5 g. If
50 batches are independently prepared, what is the (approximate) probability that the total amount of
impurity is between 175 and 190 g? According to the convention mentioned above, n¼50 is large
enough for the CLT to be applicable. The total Tthen has approximately a normal distribution with
mean value μT¼50(4.0) ¼200 g and standard deviation σT¼ﬃﬃﬃﬃﬃ
50p
1:5ðÞ ¼ 10:6066 g :So, with
Zdenoting a standard normal rv,
P175/C20T/C20190 ðÞ /C25 P175/C0200
10:6066/C20Z/C20190/C0200
10:6066/C18/C19
¼Φ/C0:94ðÞ /C0 Φ/C02:36ðÞ ¼ :1645
Notice that nothing was said initially about the shape of the underlying impurity distribution. It
could be normally distributed, or uniform, or positively skewed—regardless, the CLT ensures that the
distribution of their total, T, is approximately normal. ■
Example 4.36 Suppose the number of times a randomly selected customer of a large bank uses the
bank’s ATM during a particular period is a random variable with a mean value of 3.2 and a standard
deviation of 2.4. Among 100 randomly selected customers, how likely is it that the sample mean
number of times the bank’s ATM is used exceeds 4? Let Xidenote the number of times the ith
customer in the sample uses the bank’s ATM. Notice that Xiis a discrete rv, but the CLT is not limited
to continuous random variables. Also, although the fact that the standard deviation of this nonnega-
tive variable is quite large relative to the mean value suggests that its distribution is positively
skewed, the large sample size implies that /C22Xdoes have approximately a normal distribution. Using
μ/C22X¼μ¼3:2 and σ/C22X¼σ=ﬃﬃﬃnp¼2:4=ﬃﬃﬃﬃﬃﬃﬃﬃ
100p
¼:24,
P/C22X>4ðÞ /C25 PZ >4/C03:2
:24/C18/C19
¼1/C0Φ3:33ðÞ ¼ :0004 ■
Example 4.37 Consider the distribution shown in Fig. 4.11 for the amount purchased (rounded to the
nearest dollar) by a randomly selected customer at a particular gas station (a similar distribution for
5 1 01 52 02 53 03 54 04 55 05 56 00.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00Probability
Purchase amount ( x)
Fig. 4.11 Probability distribution of X¼amount of gasoline purchased ($) in Example 4.374.5 Limit Theorems (What Happens as nGets Large) 295
purchases in Britain (in £) appeared in the article “Data Mining for Fun and Proﬁt,” Statistical
Science , 2000: 111 /C0131; there were big spikes at the values 10, 15, 20, 25, and 30). The distribution
is obviously quite non-normal.
We asked Matlab to select 1000 different samples, each consisting of n¼15 observations, and
calculate the value of the sample mean for each one. Figure 4.12 is a histogram of the resulting 1000
values; this is the approximate distribution of /C22Xunder the speciﬁed circumstances. This distribution is
clearly approximately normal even though the sample size is not all that large. As further evidence for
normality, Fig. 4.13 shows a normal probability plot of the 1000 /C22xvalues; the linear pattern is very
prominent. It is typically not non-normality in the central part of the population distribution that
causes the CLT to fail, but instead very substantial skewness or heavy tails. ■20 25 30 35 400.0010.0030.010.020.050.100.250.500.750.900.950.980.990.9970.999
DataProbabilityNormal Probability Plot Fig. 4.13 Normal
probability plot from
Matlab of the 1000 /C22xvalues
based on samples of size
n¼150.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
18 21 24 27 30 33 36Density
Mean purchase amount ( x)Fig. 4.12 Approximate
sampling distribution of the
sample mean amount
purchased when n¼15
and the population
distribution is as shown in
Fig.4.11296 4 Joint Probability Distributions and Their Applications
The CLT can also be generalized so it applies to non-identically distributed independent random
variables and certain linear combinations. Roughly speaking, if nis large and no individual term is
likely to contribute too much to the overall value, then asymptotic normality prevails (see Exercise
190). It can also be generalized to sums of variables which are not independent provided the extent of
dependence between most pairs of variables is not too strong.
4.5.3 Other Applications of the Central Limit Theorem
The CLT can be used to justify the normal approximation to the binomial distribution discussed in
Sect. 3.3. Recall that a binomial variable Xis the number of successes in a binomial experiment
consisting of nindependent success/failure trials with p¼P(success) for any particular trial. Deﬁne
new rvs X1,X2,...,Xnby
Xi¼1 if the ith trial results in a success
0 if the ith trial results in a failure/C26
i¼1,...,n ðÞ
Because the trials are independent and P(success) is constant from trial to trial, the Xis are iid
(a random sample from a Bernoulli distribution). When the Xis are summed, a 1 is added for every
success that occurs and a 0 for every failure ,soX¼X1+/C1/C1/C1+Xn, their total .The sample mean of
theXisi s /C22X¼X=n, the sample proportion of successes, which in previous discussions we have
denoted ^P:The Central Limit Theorem then implies that if nis sufﬁciently large, both Xand ^Pare
approximately normal when nis large. We summarize properties of the ^Pdistribution in the following
corollary; Statements 1 and 2 were derived in Sect. 2.4.
COROLLARY
Consider an event Ain the sample space of some experiment with p¼P(A). Let X¼the
number of times Aoccurs when the experiment is repeated nindependent times, and deﬁne
^P¼^PAðÞ ¼X
n
Then
1.μ^P¼E^P/C0/C1
¼p
2.σ^P¼SD ^P/C0/C1
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1/C0pðÞ
nq
3. As nincreases, the distribution of ^Papproaches a normal distribution.
In practice, Property 3 is taken to say that ^Pis approximately normal, provided that np/C2110
andn(1/C0p)/C2110.
The necessary sample size for this approximation depends on the value of p: when pis close to .5,
the distribution of each Xiis reasonably symmetric (see Fig. 4.14), whereas the distribution is quite
skewed when pis near 0 or 1. Using the approximation only if both np/C2110 and n(1/C0p)/C2110
ensures that nis large enough to overcome any skewness in the underlying Bernoulli distribution.4.5 Limit Theorems (What Happens as nGets Large) 297
Example 4.38 A computer simulation in the style of Sect. 1.6is used to determine the probability
that a complex system of components operates properly throughout the warranty period. Unknown to
the investigator, the true probability is P(A)¼.18. If 10,000 simulations of the underlying process
are run, what is the chance the estimated probability ^PAðÞwill lie within .01 of the true probability
P(A)?
Apply the preceding corollary, with n¼10,000 and p¼P(A)¼.18. The expected value of the
estimator ^PAðÞisp¼.18, and the standard deviation is σ^P¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:18:82ðÞ =10, 000p
¼:00384 :Since
np¼1800/C2110 and n(1/C0p)¼8200/C2110, a normal distribution can safely be used to
approximate the distribution of ^PAðÞ. This sample proportion is within .01 of the true probability,
.18, iff .17 <^PAðÞ<:19, so the desired likelihood is approximately
P:17<^P<:19/C0/C1
/C25P:17/C0:18
:00384<Z<:19/C0:18
:00384/C18/C19
¼Φ2:60ðÞ /C0 Φ/C02:60ðÞ ¼ :9906 ■
The normal distribution serves as a reasonable approximation to the binomial pmf when nis large
because the binomial distribution is additive , i.e., a binomial rv can be expressed as the sum of other,
iid rvs. Other additive distributions include the Poisson, negative binomial, gamma, and (of course)
normal distributions; some of these were discussed at the end of Sect. 4.3. In particular, CLT justiﬁes
normal approximations to the following distributions:
• Poisson, when μis large
• Negative binomial, when ris large
• Gamma, when αis large
As a ﬁnal application of the CLT, ﬁrst recall from Sect. 3.5thatXhas a lognormal distribution if
ln(X) has a normal distribution.
PROPOSITION
LetX1,X2,...,Xnbe a random sample from a distribution for which only positive values are
possible [ P(Xi>0)¼1]. Then if nis sufﬁciently large, the product Y¼X1X2/C1/C1/C1Xnhas
approximately a lognormal distribution; that is, ln( Y) has approximately a normal distribution.
To verify this, note that
lnYðÞ ¼ lnX1ðÞ þ lnX2ð Þþ/C1/C1/C1þ lnXnðÞ
Since ln( Y) is a sum of independent and identically distributed rvs [the ln( Xi)s], it is approximately
normal when nis large, so Yitself has approximately a lognormal distribution. As an example of the
applicability of this result, it has been argued that the damage process in plastic ﬂow and crack
propagation is a multiplicative process, so that variables such as percentage elongation and rupture
strength have approximately lognormal distributions.01 01b a Fig. 4.14 Two Bernoulli
distributions: ( a)p¼.4
(reasonably symmetric);
(b)p¼.1 (very skewed)298 4 Joint Probability Distributions and Their Applications
4.5.4 The Law of Large Numbers
In the simulation sections of Chaps. 1–3, we described how a sample proportion ^Pcould estimate a
true probability p, and a sample mean /C22Xserved to approximate a theoretical expected value μ.
Moreover, in both cases the precision of the estimation improves as the number of simulation runs, n,
increases. We would like to be able to say that our estimates “converge” to the correct values in some
sense. Such a convergence statement is justiﬁed by another important theoretical result, called the
Law of Large Numbers .
To begin, recall the ﬁrst proposition in this section: If X1,X2,...,Xnis a random sample from a
distribution with mean μand standard deviation σ, then E/C22XðÞ ¼ μand Var /C22XðÞ ¼ σ2=n:Asnincreases,
the expected value of /C22Xremains at μbut the variance approaches zero; that is, E/C22X/C0μðÞ½/C1382¼Var /C22XðÞ
¼σ2=n!0:We say that /C22X converges in mean square toμbecause the mean of the squared difference
between /C22Xandμgoes to zero. This is one form of the Law of Large Numbers.
Another form of convergence states that as the sample size nincreases, /C22Xis increasingly unlikely
to differ by any set amount from μ. More precisely, let εbe a positive number close to 0, such as .01 or
.001, and consider Pj/C22X/C0μj/C21ε ðÞ , the probability that /C22Xdiffers from μby at least ε(at least .01, at
least .001, etc.). We will prove shortly with the help of Chebyshev’s inequality that, no matter how
small the value of ε, this probability will approach zero as n!1 . Because of this, statisticians say
that /C22X converges to μin probability.
The two forms of the Law of Large Numbers are summarized in the following theorem.
LAW OF LARGE NUMBERS
IfX1,X2,...,Xnis a random sample from a distribution with mean μand ﬁnite variance, then /C22X
converges to μ
1. In mean square: E /C22X/C0μðÞ2hi
!0a sn!1
2. In probability: Pj/C22X/C0μj/C21ε ðÞ ! 0a sn!1 for any ε>0
Proof The proof of Statement 1 appears a few paragraphs above. For Statement 2, recall
Chebyshev’s inequality, which states that for any rv Y,P(|Y/C0μY|/C21kσY)/C201/k2for any k/C211
(i.e., the probability that Yis at least kstandard deviations away from its mean is at most 1/ k2). Let
Y¼/C22X,s o μY¼E/C22XðÞ ¼ μand σY¼SD /C22XðÞ ¼ σ=ﬃﬃﬃnp. Now, for any ε>0, determine the value of
ksuch that ε¼kσY¼kσ=ﬃﬃﬃnp:Solving for kyields k¼εﬃﬃﬃnp=σ, which for sufﬁciently large nwill
exceed 1. Apply Chebyshev’s inequality:
PjY/C0μYj/C21kσY ðÞ /C201
k2)Pj/C22X/C0μj/C21εﬃﬃﬃnp
σ/C1σﬃﬃﬃnp/C18/C19
/C201
εﬃﬃﬃnp=σ ðÞ2
)Pj/C22X/C0μj/C21ε ðÞ /C20σ2
ε2n!0a sn!1
That is, Pj/C22X/C0μj/C21ε ðÞ ! 0a sn!1 for any ε>0. ■
Convergence of /C22Xtoμin probability actually holds even if the variance σ2does not exist (a heavy-
tailed distribution) as long as μis ﬁnite. But then Chebyshev’s inequality cannot be used, and the
proof is much more complicated.4.5 Limit Theorems (What Happens as nGets Large) 299
An analogous result holds for proportions. If the Xiare iid Bernoulli( p) rvs, then similar to the
discussion earlier in this section we may write /C22Xas^P, and μ¼E(Xi)¼p. It follows that the sample
proportion ^Pconverges to the “true” proportion p
1. In mean square: E ^P/C0p/C0/C1 2hi
!0a sn!1 , and
2. In probability: Pj^P/C0pj/C21ε/C0/C1
!0a sn!1 for any ε>0.
In statistical language, the Law of Large Numbers states that /C22Xis aconsistent estimator of μ, and ^P
is a consistent estimator of p. This consistency property also applies to other estimators. For
example, it can be shown that the sample variance S2¼PXi/C0/C22X ðÞ2=n/C01ðÞ converges in probabil-
ity to the population variance σ2.
4.5.5 Exercises: Section 4.5(85–102)
85. The inside diameter of a randomly selected piston ring is a random variable with mean value
12 cm and standard deviation .04 cm.
(a) If /C22Xis the sample mean diameter for a random sample of n¼16 rings, where is the sampling
distribution of /C22Xcentered, and what is the standard deviation of the /C22Xdistribution?
(b) Answer the questions posed in part (a) for a sample size of n¼64 rings.
(c) For which of the two random samples, the one of part (a) or the one of part (b), is /C22Xmore
likely to be within .01 cm of 12 cm? Explain your reasoning.
86. Refer to the previous exercise. Suppose the distribution of diameter is normal.
(a) Calculate P11:99/C20/C22X/C2012:01 ðÞ when n¼16.
(b) How likely is it that the sample mean diameter exceeds 12.01 when n¼25?
87. Suppose that the fracture angle under pure compression of a randomly selected specimen of ﬁber
reinforced polymer-matrix composite material is normally distributed with mean value 53 and
standard deviation 1 (suggested in the article “Stochastic Failure Modelling of Unidirectional
Composite Ply Failure,” Reliability Engr. and System Safety , 2012: 1–9; this type of material is
used extensively in the aerospace industry).
(a) If a random sample of 4 specimens is selected, what is the probability that the sample mean
fracture angle is at most 54? Between 53 and 54?
(b) How many such specimens would be required to ensure that the ﬁrst probability in (a) is at
least .999?
88. The time taken by a randomly selected applicant for a mortgage to ﬁll out a certain form has a
normal distribution with mean value 10 min and standard deviation 2 min. If ﬁve individuals ﬁll
out a form on 1 day and six on another, what is the probability that the sample average amount of
time taken on each day is at most 11 min?
89. The lifetime of a type of battery is normally distributed with mean value 10 h and standard
deviation 1 h. There are four batteries in a package. What lifetime value is such that the total
lifetime of all batteries in a package exceeds that value for only 5% of all packages?
90. The National Health Statistics Reports dated Oct. 22, 2008 stated that for a sample size
of 277 18-year-old American males, the sample mean waist circumference was 86.3 cm. A some-
what complicated method was used to estimate various population percentiles, resulting in the
following values:300 4 Joint Probability Distributions and Their Applications
5th 10th 25th 50th 75th 90th 95th
69.6 70.9 75.2 81.3 95.4 107.1 116.4
(a) Is it plausible that the waist size distribution is at least approximately normal? Explain your
reasoning. If your answer is no, conjecture the shape of the population distribution.
(b) Suppose that the population mean waist size is 85 cm and that the population standard
deviation is 15 cm. How likely is it that a random sample of 277 individuals will result in a
sample mean waist size of at least 86.3 cm?
(c) Referring back to (b), suppose now that the population mean waist size is 82 cm (closer to
the sample median than the sample mean). Now what is the (approximate) probability that
the sample mean will be at least 86.3? In light of this calculation, do you think that 82 is a
reasonable value for μ?
91. A friend commutes by bus to and from work 6 days per week. Suppose that waiting time is
uniformly distributed between 0 and 10 min, and that waiting times going and returning on
various days are independent of each other. What is the approximate probability that total waiting
time for an entire week is at most 75 min?
92. There are 40 students in an elementary statistics class. On the basis of years of experience, the
instructor knows that the time needed to grade a randomly chosen paper from the ﬁrst exam is a
random variable with an expected value of 6 min and a standard deviation of 6 min.
(a) If grading times are independent and the instructor begins grading at 6:50 p.m. and grades
continuously, what is the (approximate) probability that he is through grading before the
11:00 p.m. TV news begins?
(b) If the sports report begins at 11:10, what is the probability that he misses part of the report if
he waits until grading is done before turning on the TV?
93. The tip percentage at a restaurant has a mean value of 18% and a standard deviation of 6%.
(a) What is the approximate probability that the sample mean tip percentage for a random
sample of 40 bills is between 16 and 19%?
(b) If the sample size had been 15 rather than 40, could the probability requested in part (a) be
calculated from the given information?
94. A small high school holds its graduation ceremony in the gym. Because of seating constraints,
students are limited to a maximum of four tickets to graduation for family and friends. The vice
principal knows that historically 30% of students want four tickets, 25% want three, 25% want
two, 15% want one, and 5% want none.
(a) Let X¼the number of tickets requested by a randomly selected graduating student, and
assume the historical distribution applies to this rv. Find the mean and standard deviation
ofX.
(b) Let T¼the total number of tickets requested by the 150 students graduating this year.
Assuming all 150 students’ requests are independent, determine the mean and standard
deviation of T.
(c) The gym can seat a maximum of 500 guests. Calculate the (approximate) probability that all
students’ requests can be accommodated. [ Hint: Express this probability in terms of T. What
distribution does Thave?]
95. Let Xrepresent the amount of gasoline (gallons) purchased by a randomly selected customer at a
gas station. Suppose that the mean and standard deviation of Xare 11.5 and 4.0, respectively.
(a) In a sample of 50 randomly selected customers, what is the approximate probability that
the sample mean amount purchased is at least 12 gallons?
(b) In a sample of 50 randomly selected customers, what is the approximate probability that
the total amount of gasoline purchased is at least 600 gallons?4.5 Limit Theorems (What Happens as nGets Large) 301
(c) What is the approximate value of the 95th percentile for the total amount purchased by
50 randomly selected customers?
96. For males the expected pulse rate is 70 per second and the standard deviation is 10 per second.
For women the expected pulse rate is 77 per second and the standard deviation is 12 per second.
Let /C22X¼the sample average pulse rate for a random sample of 40 men and let /C22Y¼the sample
average pulse rate for a random sample of 36 women.
(a) What is the approximate distribution of /C22X?O f /C22Y?
(b) What is the approximate distribution of /C22X/C0/C22Y?Justify your answer.
(c) Calculate (approximately) the probability P/C02/C20/C22X/C0/C22Y/C201 ðÞ .
(d) Calculate (approximately) P/C22X/C0/C22Y/C20/C015 ðÞ . If you actually observed /C22X/C0/C22Y/C20/C015,
would you doubt that μ1/C0μ2¼–7? Explain.
97. The ﬁrst assignment in a statistical computing class involves running a short program. If past
experience indicates that 40% of all students will make no programming errors, use an
appropriate normal approximation to compute the probability that in a class of 50 students
(a) At least 25 will make no errors.
(b) Between 15 and 25 (inclusive) will make no errors.
98. The number of parking tickets issued in a certain city on any given weekday has a Poisson
distribution with parameter μ¼50. What is the approximate probability that
(a) Between 35 and 70 tickets are given out on a particular day?
(b) The total number of tickets given out during a 5-day week is between 225 and 275? [For
parts (a) and (b), use an appropriate CLT approximation.]
(c) Use software to obtain the exact probabilities in (a) and (b), and compare to the
approximations.
99. Suppose the distribution of the time X(in hours) spent by students at a certain university on a
particular project is gamma with parameters α¼50 and β¼2. Use CLT to compute the
(approximate) probability that a randomly selected student spends at most 125 h on the project.
100. The Central Limit Theorem says that /C22Xis approximately normal if the sample size is large. More
speciﬁcally, the theorem states that the standardized /C22Xhas a limiting standard normal distribu-
tion. That is, /C22X/C0μðÞ =σ=ﬃﬃﬃnpðÞ has a distribution approaching the standard normal. Can you
reconcile this with the Law of Large Numbers?
101. It can be shown that if Ynconverges in probability to a constant τ, then h(Yn) converges to h(τ)
for any function hthat is continuous at τ. Use this to obtain a consistent estimator for the rate
parameter λof an exponential distribution. [ Hint: How does μfor an exponential distribution
relate to the exponential parameter λ?]
102. Let X1,...,Xnbe a random sample from the uniform distribution on [0, θ]. Let Ynbe the
maximum of these observations: Yn¼max( X1,...,Xn). Show that Ynconverges in probability
toθ, that is, that P(|Yn/C0θ|/C21ε)!0a snapproaches 1.[Hint: We shall show in Sect. 4.9that
the pdf of Ynisf(y)¼nyn–1/θnfor 0/C20y/C20θ.]
4.6 Transformations of Jointly Distributed Random Variables
In the previous chapter we discussed the problem of starting with a single random variable X, forming
some function of X, such as Y¼X2orY¼eX, and investigating the distribution of this new random
variable Y. We now generalize this scenario by starting with more than a single random variable.
Consider as an example a system having a component that can be replaced just once before the system302 4 Joint Probability Distributions and Their Applications
itself expires. Let X1denote the lifetime of the original component and X2the lifetime of the
replacement component. Then any of the following functions of X1andX2may be of interest to an
investigator:
1. The total lifetime, X1+X2.
2. The ratio of lifetimes X1/X2(for example, if the value of this ratio is 2, the original component
lasted twice as long as its replacement).
3. The ratio X1/(X1+X2), which represents the proportion of system lifetime during which the
original component operated.
4.6.1 The Joint Distribution of Two New Random Variables
Given two random variables X1andX2, consider forming two new random variables Y1¼u1(X1,X2)
andY2¼u2(X1,X2). Our focus is on ﬁnding the joint distribution of these two new variables. Since
most applications assume that the Xis are continuous, we restrict ourselves to that case. Some notation
is needed before a general result can be given. Let
f(x1,x2)¼the joint pdf of the two original variables
g(y1,y2)¼the joint pdf of the two new variables
Theu1(/C1) and u2(/C1) functions express the new variables in terms of the original ones. The general
result presumes that these functions can be inverted to solve for the original variables in terms of the
new ones:
X1¼v1Y1;Y2 ðÞ ,X2¼v2Y1;Y2 ðÞ
For example, if
y1¼x1þx2andy2¼x1
x1þx2
then multiplying y2byy1gives an expression for x1, and then we can substitute this into the
expression for y1and solve for x2:
x1¼y1y2¼v1y1;y2ðÞ x2¼y11/C0y2 ðÞ ¼ v2y1;y2ðÞ
In a ﬁnal burst of notation, let
S¼ x1;x2ðÞ :fx1;x2ðÞ >0 fg T¼ y1;y2ðÞ :gy1;y2ðÞ >0 fg
That is, Sis the region of positive density for the original variables and Tis the region of positive
density for the new variables; Tis the “image” of Sunder the transformation.
TRANSFORMATION THEOREM (bivariate case)
Suppose that the partial derivative of each vi(y1,y2) with respect to both y1andy2exists and is
continuous for every ( y1,y2)2T. Form the 2 /C22 matrix4.6 Transformations of Jointly Distributed Random Variables 303
M¼∂v1y1;y2ðÞ
∂y1∂v1y1;y2ðÞ
∂y2
∂v2y1;y2ðÞ
∂y1∂v2y1;y2ðÞ
∂y20
BBB@1
CCCA
The determinant of this matrix, called the Jacobian ,i s
detMðÞ ¼∂v1
∂y1/C1∂v2
∂y2/C0∂v1
∂y2/C1∂v2
∂y1
The joint pdf for the new variables then results from taking the joint pdf f(x1,x2) for the
original variables, replacing x1andx2by their expressions in terms of y1andy2, and ﬁnally
multiplying this by the absolute value of the Jacobian:
gy1;y2ðÞ ¼ fv1y1;y2ðÞ ,v2y1;y2ðÞ ðÞ /C1 j detMðÞ j y1;y2ðÞ 2 T
The theorem can be rewritten slightly by using the notation
detMðÞ ¼∂x1;x2ðÞ
∂y1;y2ðÞ/C12/C12/C12/C12/C12/C12/C12/C12
Then we have
gy1;y2ðÞ ¼ fx1;x2ðÞ∂x1;x2ðÞ
∂y1;y2ðÞ/C12/C12/C12/C12/C12/C12/C12/C12:
which is the natural extension of the univariate transformation theorem fYyðÞ¼fXxðÞ/C1dx=dyjj
discussed in Chap. 3.
Example 4.39 Continuing with the component lifetime situation, suppose that X1and X2are
independent, each having an exponential distribution with parameter λ. Let’s determine the joint
pdf of
Y1¼u1X1;X2 ðÞ ¼ X1þX2and Y2¼u2X1;X2 ðÞ ¼X1
X1þX2:
We have already inverted this transformation:
x1¼v1y1;y2ðÞ ¼ y1y2 x2¼v2y1;y2ðÞ ¼ y11/C0y2 ðÞ
The image of the transformation, i.e., the set of ( y1,y2) pairs with positive density, is y1>0 and
0<y2<1. The four relevant partial derivatives are
∂v1
∂y1¼y2∂v1
∂y2¼y1∂v2
∂y1¼1/C0y2∂v2
∂y2¼/C0y1
from which the Jacobian is det( M)¼y1y2/C0y1(1/C0y2)¼/C0 y1.
Since the joint pdf of X1andX2is
fx1;x2ðÞ ¼ λe/C0λx1/C1λe/C0λx2¼λ2e/C0λx1þx2ðÞ x1>0,x2>0
we have, by the Transformation Theorem,304 4 Joint Probability Distributions and Their Applications
gy1;y2ðÞ ¼ λ2e/C0λy1y2þy11/C0y2ðÞ ðÞ/C1/C0y1jj ¼λ2y1e/C0λy1¼λ2y1e/C0λy1/C11y1>0, 0<y2<1
In the last step, we’ve factored the joint pdf into two parts: the ﬁrst part is a gamma pdf with
parameters α¼2 and β¼1/λ, and the second part is a uniform pdf on (0, 1). Since the pdf factors
and the region of positive density is rectangular, we have demonstrated that
1. The distribution of system lifetime X1+X2is gamma (with α¼2,β¼1/λ);
2. The distribution of the proportion of system lifetime during which the original component
functions is uniform on (0, 1); and
3.Y1¼X1+X2andY2¼X1/(X1+X2) are independent of each other. ■
In the foregoing example, because the joint pdf factored into one pdf involving y1alone and
another pdf involving y2alone, the individual (i.e., marginal) pdfs of the two new variables were
obtained from the joint pdf without any further effort. Often this will not be the case—that is, Y1and
Y2will not be independent. Then to obtain the marginal pdf of Y1, the joint pdf must be integrated over
all values of the second variable. In fact, in many applications an investigator wishes to obtain the
distribution of a single function u1(X1,X2) of the original variables. To accomplish this, a second
function Y2¼u2(X1,X2) is selected, the joint pdf is obtained, and then y2integrated out. There are of
course many ways to select the second function. The choice should be made so that the transformation
can be easily inverted andthe integration in the last step is straightforward.
Example 4.40 Consider a rectangular coordinate system with a horizontal x1axis and a vertical x2
axis as shown in Fig. 4.15a . First a point ( X1,X2) is randomly selected, where the joint pdf of X1,X2is
fx1;x2ðÞ ¼x1þx2 0<x1<1, 0 <x2<1
0 otherwise/C26
Then a rectangle with vertices (0, 0), ( X1, 0), (0, X2), and ( X1,X2) is formed as shown in Fig. 4.15a .
What is the distribution of X1X2, the area of this rectangle? To answer this question, let
Y1¼X1X2 Y2¼X2
so
y1¼u1x1;x2ðÞ ¼ x1x2 y2¼u2x1;x2ðÞ ¼ x2
1 0x2 y2
1
0 x1 y1
1 01
0A possible
rectangleb a
(X1, X2)Fig. 4.15 Regions of
positive density for
Example 4.404.6 Transformations of Jointly Distributed Random Variables 305
Then
x1¼v1y1;y2ðÞ ¼y1
y2x2¼v2y1;y2ðÞ ¼ y2
Notice that because x2(¼y2) is between 0 and 1 and y1is the product of the two xis, it must be the
case that 0 <y1<y2. The region of positive density for the new variables is then
T¼ y1;y2ðÞ :0<y1<y2,0<y2<1 fg
which is the triangular region shown in Fig. 4.15b .
Since∂v2/∂y1¼0, the product of the two off-diagonal elements in the matrix Mwill be 0, so only
the two diagonal elements contribute to the Jacobian:
M¼1
y2/C0y1
y2
2010
@1
A, jdetMðÞ j ¼1
y2
The joint pdf of the two new variables is now
gy1;y2ðÞ ¼ fy1
y2;y2/C18/C19
/C1detMðÞjj
¼y1
y2þy2/C18/C19
/C11
y20<y1<y2<1
0 otherwise8
><
>:
To obtain the marginal pdf of Y1alone, we must now ﬁx y1at some arbitrary value between 0 and
1, and integrate out y2. Figure 4.15b shows that for any value of y1, the values of y2range from y1to 1:
g1y1ðÞ ¼ð1
y1y1
y2þy2/C18/C19
/C11
y2dy2¼21/C0y1 ðÞ 0<y1<1
This marginal pdf can now be integrated to obtain any desired probability involving the area. For
example, integrating from 0 to .5 gives P(area <.5)¼.75. ■
4.6.2 The Joint Distribution of More Than Two New Variables
Consider now starting with three random variables X1,X2, and X3, and forming three new variables
Y1,Y2, and Y3. Suppose again that the transformation can be inverted to express the original variables
in terms of the new ones:
x1¼v1y1;y2;y3 ðÞ ,x2¼v2y1;y2;y3 ðÞ ,x3¼v3y1;y2;y3 ðÞ
Then the foregoing theorem can be extended to this new situation. The Jacobian matrix has
dimension 3 /C23, with the entry in the ith row and jth column being ∂vi/∂yj. The joint pdf of the
new variables results from replacing each xiin the original pdf f(/C1) by its expression in terms of the yjs
and multiplying by the absolute value of the Jacobian.
Example 4.41 Consider n¼3 identical components with independent lifetimes X1,X2,X3, each
having an exponential distribution with parameter λ. If the ﬁrst component is used until it fails,306 4 Joint Probability Distributions and Their Applications
replaced by the second one which remains in service until it fails, and ﬁnally the third component is
used until failure, then the total lifetime of these components is Y3¼X1+X2+X3. (This design
structure, where one component is replaced by the next in succession, is called a standby system. )T o
ﬁnd the distribution of total lifetime, let’s ﬁrst deﬁne two other new variables: Y1¼X1andY2¼
X1+X2(so that Y1<Y2<Y3). After ﬁnding the joint pdf of all three variables, we integrate out the
ﬁrst two variables to obtain the desired information. Solving for the old variables in terms of the new
gives
x1¼y1 x2¼y2/C0y1 x3¼y3/C0y2
It is obvious by inspection of these expressions that the three diagonal elements of the Jacobian
matrix are all 1s and that the elements above the diagonal are all 0s, so the determinant is 1, the
product of the diagonal elements. Since
fx1;x2;x3 ðÞ ¼ λ3e/C0λx1þx2þx3 ðÞx1>0,x2>0,x3>0
by substitution,
gy1;y2;y3 ðÞ ¼ λ3e/C0λy3 0<y1<y2<y3
Integrating this joint pdf ﬁrst with respect to y1between 0 and y2and then with respect to y2
between 0 and y3(try it!) gives
g3y3ðÞ ¼λ3
2y2
3e/C0λy3 y3>0
which is the gamma pdf with α¼3 and β¼1/λ. This result and Example 3.39 are both special cases
of a proposition from Sect. 4.3, stating that the sum of niid exponential rvs has a gamma distribution
with α¼n. ■
4.6.3 Exercises: Section 4.6(103–110)
103. Let X1andX2be independent, standard normal rvs.
(a) Deﬁne Y1¼X1+X2andY2¼X1/C0X2. Determine the joint pdf of Y1andY2.
(b) Determine the marginal pdf of Y1.[Note: We know the sum of two independent normal rvs
is normal, so you can check your answer against the appropriate normal pdf.]
(c) Are Y1andY2independent?
104. Consider two components whose lifetimes X1and X2are independent and exponentially
distributed with parameters λ1and λ2, respectively. Obtain the joint pdf of total lifetime
X1+X2and the proportion of total lifetime X1/(X1+X2) during which the ﬁrst component
operates.
105. Let X1denote the time (hr) it takes to perform a ﬁrst task and X2denote the time it takes to
perform a second one. The second task always takes at least as long to perform as the ﬁrst task.
The joint pdf of these variables is
fx1;x2ðÞ ¼2x1þx2 ðÞ 0/C20x1/C20x2/C201
0 otherwise/C26
(a) Obtain the pdf of the total completion time for the two tasks.
(b) Obtain the pdf of the difference X2/C0X1between the longer completion time and the
shorter time.4.6 Transformations of Jointly Distributed Random Variables 307
106. An exam consists of a problem section and a short-answer section. Let X1denote the amount of
time (h) that a student spends on the problem section and X2represent the amount of time the
same student spends on the short-answer section. Suppose the joint pdf of these two times is
fx1;x2ðÞ ¼cx1x2x1
3<x2<x1
2,0 <x1<1
0 otherwise(
(a) What is the value of c?
(b) If the student spends exactly .25 h on the short-answer section, what is the probability that
at most .60 h was spent on the problem section? [ Hint: First obtain the relevant conditional
distribution.]
(c) What is the probability that the amount of time spent on the problem part of the exam
exceeds the amount of time spent on the short-answer part by at least .5 h?
(d) Obtain the joint distribution of Y1¼X2/X1, the ratio of the two times, and Y2¼X2. Then
obtain the marginal distribution of the ratio.
107. Consider randomly selecting a point ( X1,X2,X3) in the unit cube {( x1,x2,x3): 0<x1<1,
0<x2<1, 0<x3<1} according to the joint pdf
fx1;x2;x3 ðÞ ¼8x1x2x3 0<x1<1, 0 <x2<1, 0 <x3<1
0 otherwise/C26
(so the three variables are independent). Then form a rectangular solid whose vertices are (0, 0,
0), (X1, 0, 0), (0, X2, 0), ( X1,X2, 0), (0, 0, X3), (X1,0 ,X3), (0, X2,X3), and ( X1,X2,X3). The
volume of this cube is Y3¼X1X2X3. Obtain the pdf of this volume. [ Hint: Let Y1¼X1and
Y2¼X1X2.]
108. Let X1andX2be independent, each having a standard normal distribution. The pair ( X1,X2)
corresponds to a point in a two-dimensional coordinate system. Consider now changing to polar
coordinates via the transformation,
Y1¼X2
1þX2
2
Y2¼arctanX2
X1/C18/C19
X1>0,X2/C210
arctanX2
X1/C18/C19
þ2πX1>0,X2<0
arctanX2
X1/C18/C19
þπX1<0
0 X1¼08
>>>>>>>>>><
>>>>>>>>>>:
from which X1¼ﬃﬃﬃﬃﬃY1pcosY2ðÞ,X2¼ﬃﬃﬃﬃﬃY1psinY2ðÞ:Obtain the joint pdf of the new variables
and then the marginal distribution of each one. [ Note: It would be nice if we could simply let
Y2¼arctan( X2/X1), but in order to insure invertibility of the arctan function, it is deﬁned to take
on values only between /C0π/2 and π/2. Our speciﬁcation of Y2allows it to assume any value
between 0 and 2 π.]
109. The result of the previous exercise suggests how observed values of two independent standard
normal variables can be generated by ﬁrst generating their polar coordinates with an exponential
rv with λ¼1
2and an independent Unif(0, 2 π) rv: Let U1andU2be independent Unif(0, 1) rvs,
and then let308 4 Joint Probability Distributions and Their Applications
Y1¼/C02l n U1ðÞ Y2¼2πU2,
Z1¼ﬃﬃﬃﬃﬃ
Y1p
cosY2ðÞ Z2¼ﬃﬃﬃﬃﬃ
Y1p
sinY2ðÞ
Show that the Zis are independent standard normal. [ Note: This is called the Box-Muller
transformation after the two individuals who discovered it. Now that statistical software
packages will generate almost instantaneously observations from a normal distribution with
any mean and variance, it is thankfully no longer necessary for people like you and us to carry
out the transformations just described—let the software do it!]
110. Let X1andX2be independent random variables, each having a standard normal distribution.
Show that the pdf of the ratio Y¼X1/X2is given by f(y)¼1/[π(1 + y2)] for /C01 <y<1.
(This is called the standard Cauchy distribution ; its density curve is bell-shaped, but the tails are
so heavy that μdoes not exist.)
4.7 The Bivariate Normal Distribution
Perhaps the most useful joint distribution is the bivariate normal. Although the formula may seem
rather complicated, it is based on a simple quadratic expression in the standardized variables (subtract
the mean and then divide by the standard deviation). The bivariate normal density is
fx;yðÞ ¼1
2πσ1σ2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0ρ2p exp/C01
21/C0ρ2 ðÞx/C0μ1
σ1/C18/C192
/C02ρx/C0μ1
σ1/C18/C19y/C0μ2
σ2/C18/C19
þy/C0μ2
σ2/C18/C192"# !
The notation used here for the ﬁve parameters reﬂects the roles they play. Some tedious integration
shows that μ1andσ1are the mean and standard deviation, respectively, of X,μ2andσ2are the mean
and standard deviation, respectively, of Y, and ρis the correlation coefﬁcient between the two
variables. The integration required to do bivariate normal probability calculations is quite difﬁcult.
Computer code is available for calculating P(X/C20x,Y/C20y) approximately using numerical integra-
tion, and some software packages, including Matlab and R, incorporate this feature (see the end of
this section).
The density surface in three dimensions looks like a mountain with elliptical cross-sections, as
shown in Fig. 4.16a . The vertical cross-sections are all proportional to normal densities. If we set
f(x,y)¼cto investigate the contours (curves along which the density is constant), this amounts to
xyab
xy
f(x, y)
Fig. 4.16 (a) A graph of the bivariate normal pdf; ( b) contours of the bivariate normal pdf4.7 The Bivariate Normal Distribution 309
equating the exponent of the joint pdf to a constant. The contours are then concentric ellipses centered
at (x, y)¼(μ1,μ2), as shown in Fig. 4.16b .
Ifρ¼0, then f(x,y)¼fX(x)fY(y), where X/C24N(μ1,σ1) and Y/C24N(μ2,σ2). That is, XandYhave
independent normal distributions. In this case the elliptical contours reduce to circles. Recall that in
Sect. 4.2we emphasized that independence of XandYimplies ρ¼0 but, in general, ρ¼0 does not
imply independence. However, we have just seen that when XandYare bivariate normal, ρ¼0 does
imply independence. Therefore, in the bivariate normal case ρ¼0 if and only if the two rvs are
independent.
Regardless of whether or not ρ¼0, the marginal distribution fX(x) is just a normal pdf with mean
μ1and standard deviation σ1:
fXxðÞ ¼1
σ1ﬃﬃﬃﬃﬃ
2πp e/C0x/C0μ1ðÞ2=2σ2
1ðÞ
The integration to show this [integrating f(x,y)o nyfrom – 1to1] is rather messy. Likewise, the
marginal distribution of YisN(μ2,σ2). These two marginal pdfs are, in fact, just special cases of a
much stronger result.
PROPOSITION
IfXandYhave a bivariate normal distribution, then any linear combination of XandYis also
normal. That is, for any constants a, b, c, the random variable aX + bY + c has a normal
distribution.
This proposition can be proved using the transformation techniques of Sect. 4.6along with some
extremely tedious algebra. Setting a¼1 and b¼c¼0, we have that Xis normally distributed;
a¼0,b¼1,c¼0 yields the same result for Y. To ﬁnd the mean and standard deviation of a general
linear combination, one can use the rules for linear combinations established in Sect. 4.3.
Example 4.42 Many students applying for college take the SAT, which until 2016 consisted of three
components: Critical Reading, Mathematics, and Writing. While some colleges use all three
components to determine admission, many only look at the ﬁrst two (reading and math). Let Xand
Ydenote the Critical Reading and Mathematics scores, respectively, for a randomly selected student.
According to the College Board website, the population of students taking the exam in Fall 2012 had
the following results:
μ1¼496, σ1¼114, μ2¼514, σ2¼117
Suppose that XandYhave approximately (because both XandYare discrete) a bivariate normal
distribution with correlation coefﬁcient ρ¼.25. Let’s determine the probability that a student’s total
score across these two components exceeds 1250, the minimum admission score for a particular
university.
Our goal is to calculate P(X+Y >1250). Using the bivariate normal pdf, the desired probability
is a daunting double integral:
1
2π114ðÞ 117ðÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0:252pð1
/C01ð1
1250/C0y
e/C0 x/C0496ðÞ =114 ½/C1382/C02:25ðÞ x/C0496ðÞ y/C0514ðÞ =114ðÞ 117ðÞ þ y/C0514ðÞ =117 ½/C1382fg =21/C0:252ðÞ½/C138dxdy310 4 Joint Probability Distributions and Their Applications
This is not a practical way to solve this problem! Instead, recognize X+Y as a linear combination
ofXandY; by the preceding proposition, X+Yhas a normal distribution. The mean and variance of
X+Yare calculated using the formulas from Sect. 4.3:
EXþYðÞ ¼ E/C0
X/C1
þE/C0
Y/C1
¼μ1þμ2¼496þ514¼1010
VarXþYðÞ ¼ Var/C0
X/C1
þVar/C0
Y/C1
þ2Cov/C0
X,Y/C1
¼σ2
1þσ2
2þ2ρσ1σ2¼1142þ1172þ2:25ðÞ/C0
114/C1/C0
117/C1
¼33, 354
Therefore,
PXþY>1250 ðÞ ¼ 1/C0Φ1250/C01010ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ33, 354p/C18/C19
¼1/C0Φ1:31ðÞ ¼ :0951 :
Suppose instead we wish to determine P(X<Y), the probability a student scores better on math
than on reading. If we rewrite this probability as P(X/C0Y<0), then we may apply the preceding
proposition to the linear combination X/C0Y. With E(X/C0Y)¼–18 and Var( X/C0Y)¼20,016,
PX<YðÞ ¼PX/C0Y<0 ðÞ ¼Φ0/C0/C0 18ðÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ20, 016p/C18/C19
¼Φ0:13ðÞ ¼:5517 : ■
4.7.1 Conditional Distributions of Xand Y
As in Sect. 4.4, the conditional density of Ygiven X¼xresults from dividing the marginal density of
Xintof(x,y). The algebra is again a mess, but the result is fairly simple.
PROPOSITION
LetXandYhave a bivariate normal distribution. Then the conditional distribution of Y, given
X¼x, is normal with mean and variance
μYjX¼x¼EYjX¼x ðÞ ¼ μ2þρσ2x/C0μ1
σ1
σ2
YjX¼x¼VarYjX¼x ðÞ ¼ σ2
21/C0ρ2/C0/C1
Notice that the conditional mean of Yis a linear function of x, and the conditional variance of
Ydoesn’t depend on xat all. When ρ¼0, the conditional mean is the mean of Y,μ2, and the
conditional variance is just the variance of Y,σ22. In other words, if ρ¼0, then the conditional
distribution of Yis the same as the unconditional distribution of Y. When ρis close to 1 or –1 the
conditional variance will be much smaller than Var( Y), which says that knowledge of Xwill be very
helpful in predicting Y.I fρis near 0 then XandYare nearly independent and knowledge of Xis not
very useful in predicting Y.
Example 4.43 LetXand Ybe the heights of a randomly selected mother and her daughter,
respectively. A similar situation was one of the ﬁrst applications of the bivariate normal distribution,
by Francis Galton in 1886, and the data were found to ﬁt the distribution very well. Suppose a
bivariate normal distribution with mean μ1¼64 in. and standard deviation σ1¼3 in. for Xand mean
μ2¼65 in. and standard deviation σ2¼3 in. for Y. Here μ2>μ1, which is in accord with the
increase in height from one generation to the next. Assume ρ¼.4. Then4.7 The Bivariate Normal Distribution 311
μYjX¼x¼μ2þρσ2x/C0μ1
σ1¼65þ:43ðÞx/C064
3¼65þ:4x/C064ðÞ ¼ :4xþ39:4
σ2
YjX¼x¼VarYjX¼x ðÞ ¼ σ2
21/C0ρ2/C0/C1
¼91/C0:42/C0/C1
¼7:56 and σYjX¼x¼2:75
Notice that the conditional variance is 16% less than the variance of Y. Squaring the correlation
gives the percentage by which the conditional variance is reduced relative to the variance of Y.■
4.7.2 Regression to the Mean
The formula for the conditional mean can be reexpressed as
μYjX¼x/C0μ2
σ2¼ρ/C1x/C0μ1
σ1
In words, when the formula is expressed in terms of standardized quantities, the standardized
conditional mean is just ρtimes the standardized x. In particular, for the height scenario
μYjX¼x/C065
3¼:4/C1x/C064
3
If the mother is 5 in. above the mean of 64 in. for mothers, then the daughter’s conditional expected
height is just 2 in. above the mean for daughters. In this example, with equal standard deviations for
YandX, the daughter’s conditional expected height is always closer to its mean than the mother’s
height is to its mean. One can think of the conditional expectation as falling back toward the mean,
and that is why Galton called this regression to the mean .
Regression to the mean occurs in many contexts. For example, let Xbe a baseball player’s average
for the ﬁrst half of the season and let Ybe the average for the second half. Most of the players with a
high X(above .300) will not have such a high Y. The same kind of reasoning applies to the
“sophomore jinx,” which says that if a player has a very good ﬁrst season, then the player is unlikely
to do as well in the second season.
4.7.3 The Multivariate Normal Distribution
The multivariate normal distribution extends the bivariate normal distribution to situations involving
models for nrandom variables X1,X2,...,Xnwith n>2. The joint density function is quite
complicated; the only way to express it compactly is to make use of matrix algebra notation, and
probability calculations based on this distribution are extremely complex. Here are some of the most
important properties of the distribution:
• The distribution of any linear combination of X1,X2,...,Xnis normal
• The marginal distribution of any Xiis normal
• The joint distribution of any pair Xi,Xjis bivariate normal
• The conditional distribution of any Xigiven values of the other n/C01 variables is normal
Many procedures for the analysis of multivariate data (observations simultaneously on three or
more variables) are based on assuming that the data were selected from a multivariate normal
distribution. We recommend Methods of Multivariate Analysis , 3rd ed., by Rencher for more
information on multivariate analysis and the multivariate normal distribution.312 4 Joint Probability Distributions and Their Applications
4.7.4 Bivariate Normal Calculations with Software
Matlab will compute probabilities under the bivariate normal pdf using the mvncdf command
(“mvn” abbreviates multivariate normal). This function is illustrated in the next example.
Example 4.44 Consider the SAT reading/math scenario of Example 4.42. What is the probability
that a randomly selected student scored at most 650 on both components, i.e., what is P(X/C20650\
Y/C20650)?
The desired probability cannot be expressed in terms of a linear combination of XandY, and so the
technique of the earlier example does not apply. Figure 4.17 shows the required Matlab code. The ﬁrst
two inputs are the desired cdf values ( x, y)¼(650, 650) and the means ( μ1,μ2)¼(496, 514),
respectively. The third input is called the covariance matrix ofXandY, deﬁned by
CX;YðÞ ¼VarXðÞ Cov X;YðÞ
Cov X;YðÞ VarYðÞ/C20/C21
¼σ2
1 ρσ1σ2
ρσ1σ2 σ2
2/C20/C21
Matlab returns an answer of .8097, so for XandYhaving a bivariate normal distribution with the
parameters speciﬁed in Example 4.42, P(X/C20650\Y/C20650)¼.8097. About 81% of students
scored 650 or below on both the Critical Reading and Mathematics components, according to this
model. ■
Thepmvnorm function in R will perform the same calculation with the same inputs (the
covariance matrix is labeled sigma ). Users must install the mvtnorm package to access this
function.
4.7.5 Exercises: Section 4.7(111–120)
111. Example 4.42 introduced a bivariate normal model for X¼SAT Critical Reading score and
Y¼SAT Mathematics score. Let W¼SAT Writing score (the third component of the SAT),
which has mean 488 and standard deviation 114. Suppose XandWhave a bivariate normal
distribution with ρX,W¼Corr( X, W )¼.5.
(a) An English department plans to use X+W , a student’s total score on the non-math
sections of the SAT, to help determine admission. Determine the distribution of X+W .
(b) Calculate P(X+W >1200).
(c) Suppose the English department wishes to admit only those students who score in the top
10% on this Critical Reading + Writing criterion. What combined score separates the top
10% of students from the rest?
112. In the context of the previous exercise, let T¼X+Y+W , a student’s grand total score on the
three components of the SAT.
(a) Find the expected value of T.
(b) Assume Corr( Y, W )¼.2. Find the variance of T.[Hint: Use Expression ( 4.5) from
Sect. 4.3.]mu=[496, 514];
C=[114^2, .25*114*117; .25*114*117, 117^2];
mvncdf([650, 650],mu,C)Fig. 4.17 Matlab code for
Example 4.444.7 The Bivariate Normal Distribution 313
(c) Suppose X, Y, W have a multivariate normal distribution, in which case Tis also normally
distributed. Determine P(T>2000).
(d) What is the 99th percentile of SAT grand total scores, according to this model?
113. Let X¼height (inches) and Y¼weight (lbs) for an American male. Suppose XandYhave a
bivariate normal distribution, the mean and sd of heights are 70 in and 3 in. the mean and sd of
weights are 170 lbs and 20 lbs, and the correlation coefﬁcient is ρ¼.9.
(a) Determine the distribution of Ygiven X¼68, i.e., the weight distribution for 5’8’’
American males.
(b) Determine the distribution of Ygiven X¼70, i.e., the weight distribution for 5’10’’
American males. In what ways is this distribution similar to that of part (a), and how are
they different?
(c) Calculate P(Y<180jX¼72), the probability that a 6-ft-tall American male weighs less
than 180 lb.
114. In electrical engineering, the unwanted “noise” in voltage or current signals is often modeled by
a Gaussian (i.e., normal) distribution. Suppose that the noise in a particular voltage signal has a
constant mean of 0.9 V, and that two noise instances sampled τseconds apart have a bivariate
normal distribution with covariance equal to 0.04 e–jτj/10. Let XandYdenote the noise at times
3 s and 8 s, respectively.
(a) Determine Cov( X, Y).
(b) Determine σXandσY.[Hint: Var(X)¼Cov( X, X).]
(c) Determine Corr( X, Y).
(d) Find the probability we observe greater voltage noise at time 3 s than at time 8 s.
(e) Find the probability that the voltage noise at time 3 s is more than 1 V above the voltage
noise at time 8 s.
115. For a Calculus I class, the ﬁnal exam score Yand the average Xof the four earlier tests have a
bivariate normal distribution with mean μ1¼73, standard deviation σ1¼12, mean μ2¼70,
standard deviation σ2¼15. The correlation is ρ¼.71. Determine
(a) μYjX¼x
(b) σ2
YjX¼x
(c) σYjX¼x
(d) P(Y>90jX¼80), i.e., the probability that the ﬁnal exam score exceeds 90 given that the
average of the four earlier tests is 80
116. Refer to the previous exercise. Suppose a student’s Calculus I grade is determined by 4 X+Y,
the total score across ﬁve tests.
(a) Find the mean and standard deviation of 4 X+Y .
(b) Determine P(4X+Y<320).
(c) Suppose the instructor sets the curve in such a way that the top 15% of students, based on
total score across the ﬁve tests, will receive As. What point total is required to get an A in
Calculus I?
117. Let XandY, reaction times (sec) to two different stimuli, have a bivariate normal distribution
with mean μ1¼20 and standard deviation σ1¼2 for Xand mean μ2¼30 and standard
deviation σ2¼5 for Y. Assume ρ¼.8. Determine
(a) μYjX¼x
(b) σ2
YjX¼x
(c) σYjX¼x
(d) P(Y>46jX¼25)314 4 Joint Probability Distributions and Their Applications
118. Refer to the previous exercise.
(a) One researcher is interested in X+Y , the total reaction time to the two stimuli. Determine
the mean and standard deviation of X+Y .
(b) If XandYwere independent, what would be the standard deviation of X+Y ? Explain why
it makes sense that the sd in part (a) is much larger than this.
(c) Another researcher is interested in Y/C0X, the difference in the reaction times to the two
stimuli. Determine the mean and standard deviation of Y/C0X.
(d) If XandYwere independent, what would be the standard deviation of Y/C0X? Explain why
it makes sense that the sd in part (c) is much smaller than this.
119. Let XandYbe the times for a randomly selected individual to complete two different tasks, and
assume that ( X,Y) has a bivariate normal distribution with μ1¼100, σ1¼50,μ2¼25,
σ2¼5,ρ¼.4. From statistical software we obtain P(X<100, Y<25)¼.3333, P(X<50,
Y<20)¼.0625, P(X<50,Y<25)¼.1274, and P(X<100, Y<20)¼.1274.
(a) Determine P(50<X<100, 20 <Y<25).
(b) Leave the other parameters the same but change the correlation to ρ¼0 (independence).
Now recompute the probability in part (a). Intuitively, why should the original be larger?
120. One of the propositions of this section gives an expression for E(YjX¼x).
(a) By reversing the roles of XandYgive a similar formula for E(XjY¼y).
(b) Both E(YjX¼x) and E(XjY¼y) are linear functions. Show that the product of the two
slopes is ρ2.
4.8 Reliability
Reliability theory is the branch of statistics and operations research devoted to studying how long
systems will function properly. A “system” can refer to a single device, such as a DVR, or a network
of devices or objects connected together (e.g., electronic components or stages in an assembly line).
For any given system, the primary variable of interest is T¼the system’s lifetime, i.e., the duration
of time until the system fails (either permanently or until repairs/upgrades are made). Since
Tmeasures time, we always have T/C210. Most often, Tis modeled as a continuous rv on (0, 1),
though occasionally lifetimes are modeled as discrete or, at least, having positive probability of
equaling zero (such as a light bulb that never turns on). The probability distribution of Tis often
described in terms of its reliability function.
4.8.1 The Reliability Function
DEFINITION
LetTdenote the lifetime (i.e., the time to failure) of some system. The reliability function of
T(or of the system), denoted by R(t), is deﬁned for t/C210b y
R(t)¼P(T>t)¼1/C0F(t),
where F(t) is the cdf of T. That is, R(t) is the probability that the system lasts more that ttime
units. The reliability function is sometimes also called the survival function ofT.4.8 Reliability 315
Properties of F(t) and the relation R(t)¼1/C0F(t) imply that
1. If Tis a continuous rv on [0, 1), then R(0)¼1.
2.R(t) is a non-increasing function of t.
3.R(t)!0a st!1 .
Example 4.45 The exponential distribution serves as one of the most common lifetime models
in engineering practice. Suppose the lifetime T, in hours, of a certain drill bit is exponential
with parameter λ¼.01 (equivalently, mean 100). From Sect. 3.4, we know that Thas cdf
F(t)¼1/C0e–.01t, so the reliability function of Tis
RtðÞ¼ 1/C0FtðÞ¼ e/C0:01tt/C210
This function satisﬁes properties 1–3 above. A graph of R(t) appears in Fig. 4.18a .
Now suppose instead that 5% of these drill bits shatter upon initial use, so that P(T¼0)¼.05,
while the remaining 95% of such drill bits follow the aforementioned exponential distribution. Since
Tcannot be negative, R(0)¼P(T>0)¼1/C0P(T¼0)¼.95. For t>0, the reliability function of
Tis determined as follows:
RtðÞ¼ P/C0
T>t/C1
¼Pbit doesn’t shatterðÞ P/C0
T>tjbit doesn’t shatter/C1
¼:95ðÞ/C0
e/C0:01t/C1
¼:95e/C0:01t
The expression e–.01tcomes from the previous reliability function calculation. Since this expres-
sion for R(t) equals .95 at t¼0, we have for all t/C210 that R(t)¼.95e–.01t(see Fig. 4.18b ). This, too,
is a non-increasing function of twith R(t)!0a st!1 , but property 1 does not hold because Tis
not a continuous rv (it has a “mass” of .05 at t¼0).
Example 4.46 The Weibull family of distributions offers a broader class of models than does the
exponential family. Recall from Sect. 3.5that the cdf of a Weibull rv is F(x)¼1/C0exp(–( x/β)α),
where αis the shape parameter and βis the scale parameter (both >0). If a system’s time to failure
follows a Weibull distribution, then the reliability function is1
0 100 200t0.95
0 100 200tR(t) R(t)ab
Fig. 4.18 Reliability functions: ( a) a continuous lifetime distribution; ( b) lifetime with positive probability of failure
att¼0 ■316 4 Joint Probability Distributions and Their Applications
RtðÞ¼ 1/C0FtðÞ¼ exp/C0t=βðÞαðÞ
Several examples of Weibull reliability functions are illustrated in Fig. 4.19. The α¼1 case
corresponds to an exponential distribution with λ¼1/β. Interestingly, models with larger values of α
have higher reliability for small values of t(to be precise, t<β) but lower reliability for larger tthan
do Weibull models with small αparameter values.
■
4.8.2 Series and Parallel Designs
Now consider assessing the reliability of systems conﬁgured in series and/or parallel designs.
Figure 4.20 illustrates the two basic designs: a series system works if and only if all of its components
work, while a parallel system continues to function as long as at least one of its components is still
functioning. Let T1,...,Tndenote the ncomponent lifetimes and let Ri(t)¼P(Ti>t) be the
reliability function of the ith component. A standard assumption in reliability theory is that the
ncomponents operate independently , i.e., that the Tis are independent rvs.
LetTdenote the lifetime of the series system depicted in Fig. 4.20a . Under the assumption of
component independence, the system reliability function is
RtðÞ¼ P/C0
T>t/C1
¼P/C0
the system’s lifetime exceeds t/C1
¼Pallncomponent lifetimes exceed t ðÞ series system
¼PT 1>t\...\Tn>t ðÞ
¼PT 1>t ðÞ /C1 .../C1P/C0
Tn>t/C1
by independence
¼R1tðÞ/C1 .../C1Rn/C0
t/C1
That is, for a series design, the system reliability function equals the product of the component
reliability functions. On the other hand, the reliability function for the parallel system in Fig. 4.20b is
given byR(t)
t1
a = .5a = 3
a = 2
a = 1Fig. 4.19 Reliability
functions for Weibull
lifetime distributions4.8 Reliability 317
RtðÞ¼ P/C0
the system’s lifetime exceeds t/C1
¼Pat least one component lifetime exceeds t ðÞ parallel system
¼1/C0Pall component lifetimes are /C20t ðÞ
¼1/C0PT 1/C20t\...\Tn/C20t ðÞ
¼1/C0PT 1/C20t ðÞ /C1 .../C1P/C0
Tn/C20t/C1
by independence
¼1/C0/C2
1/C0R1tðÞ/C3
/C1.../C1/C2
1/C0Rn/C0
t/C1/C3
These two results are summarized in the following proposition.
PROPOSITION
Suppose a system consists of nindependent components with reliability functions R1(t),...,
Rn(t).
1. If the ncomponents are connected in series, the system reliability function is
RtðÞ¼Yn
i¼1RitðÞ
2. If the ncomponents are connected in parallel, the system reliability function is
RtðÞ¼ 1/C0Yn
i¼11/C0RitðÞ ½/C138
Example 4.47 Consider three independently operating devices, each of whose lifetime (in hours) is
exponentially distributed with mean 100. From the previous example, R1(t)¼R2(t)¼R3(t)¼e–.01t.
If these three devices are connected in series, the reliability function of the resulting system is
RtðÞ¼Y3
i¼1RitðÞ¼ e/C0:01t/C0/C1
e/C0:01t/C0/C1
e/C0:01t/C0/C1
¼e/C0:03t
In contrast, a parallel system using these three devices as its components has reliability function
RtðÞ¼ 1/C0Y3
i¼11/C0RitðÞ ½/C138 ¼ 1/C01/C0e/C0:01t/C0/C1 3a
bFig. 4.20 Basic system
designs: ( a) series
connection; ( b) parallel
connection318 4 Joint Probability Distributions and Their Applications
These two reliability functions are graphed on the same set of axes in Fig. 4.21. Both functions
obey properties 1–3 from p. 383, but for any t>0 the parallel system reliability exceeds that of the
series system, as it logically should. For example, the probability the series system’s lifetime exceeds
100 h (the expected lifetime of a single component) is R(100) ¼e–.03(100)¼e–3¼.0498, while the
corresponding reliability for the parallel system is R(100) ¼1/C0(1/C0e/C0.01(100))3¼1/C0
(1/C0e/C01)3¼.7474.
Example 4.48 Consider the system depicted below, which consists of a combination of series and
parallel elements. Using previous notation and assuming component lifetimes are independent, let’s
determine the reliability function of this system. More than one method may be applied here; we will
rely on the Addition Rule:
32 1
RtðÞ¼ P/C0/C2
T1>t\T2>t/C3
[T3>t/C1
¼PT 1>t\T2>t ðÞ þ P/C0
T3>t/C1
/C0P/C0
T1>t\T2>t\T3>t/C1
Addition Rule
¼PT 1>t ðÞ P/C0
T2>t/C1
þP/C0
T3>t/C1
/C0P/C0
T1>t/C1
P/C0
T2>t/C1
P/C0
T3>t/C1
independence
¼R1tðÞR2/C0
t/C1
þR3/C0
t/C1
/C0R1/C0
t/C1
R2/C0
t/C1
R3/C0
t/C1
It can be shown that this reliability function satisﬁes properties 1–3 (the ﬁrst and last are quite
easy). If all three components have common reliability function Ri(t)¼e–.01tas in Example 4.47,
the system reliability function becomes R(t)¼e–.02t+e–.01t/C0e–.03t, which lies in between the two
reliability functions of Example 4.47 for all t>0. ■1
0.6R(t)
0.8
0.4
0.2
0
0 200 100t
300 400
Fig. 4.21 Reliability functions for the series ( solid ) and parallel ( dashed ) systems of Example 4.47 ■4.8 Reliability 319
4.8.3 Mean Time to Failure
IfTdenotes a system’s lifetime, i.e., its time until failure, then the mean time to failure (mttf) of the
system is simply E(T). The following proposition relates mean time to failure to the reliability
function.
PROPOSITION
Suppose a system has reliability function R(t) for t/C210. Then the system’s mean time to failure
is given by
μT¼ð1
01/C0FtðÞ ½/C138 dt¼ð1
0RtðÞdt ð4:8Þ
Expression ( 4.8) was established for all non-negative random variables in Exercises 38 and 150 of
Chap. 3.
As a simple demonstration of this proposition, consider a single exponential lifetime with mean
100 h. We have already seen that R(t)¼e–.01tfor this particular lifetime model; integrating the
reliability function yields
ð1
0RtðÞdt¼ð1
0e/C0:01tdt¼e/C0:01t
/C0:01/C12/C12/C12/C121
0¼0/C01
/C0:01¼100
which is indeed the mean lifetime (aka mean time to failure) in this situation. The advantage of using
Eq. ( 4.8) instead of the deﬁnition of E(T) from Chap. 3is that the former is usually an easier integral
to calculate than the latter. Here, for example, direct computation of the mean time to failure would be
ETðÞ ¼ð1
0t/C1ftðÞdt¼ð1
0:01te/C0:01tdt,
which requires integration by parts (while the preceding computation did not).
Example 4.49 Consider again the series and parallel systems of Example 4.47. Using Eq. ( 4.8), the
mttf of the series system is
μT¼ð1
0RtðÞdt¼ð1
0e/C0:03tdt¼1
:03/C2533:33hours
More generally, if nindependent components are connected in series, and each one has an
exponentially distributed lifetime with common mean μ, then the system’s mean time to failure is
μ/n.
In contrast, mttf for the parallel system is given by
ð1
0RtðÞdt¼ð1
01/C01/C0e/C0:01t/C2/C3 3/C16/C17
dt¼ð1
0/C0
3e/C0:01t/C03e/C0:02tþe/C0:03t/C1
dt
¼3
:01/C03
:02þ1
:03¼550
3/C25183:33hours
There is no simple formula for the mttf of a parallel system, even when the components have
identical exponential distributions. ■320 4 Joint Probability Distributions and Their Applications
4.8.4 Hazard Functions
The reliability function of a system speciﬁes the likelihood that the system will last beyond a
prescribed time, t. An alternative characterization of reliability, called the hazard function, conveys
information about the likelihood of imminent failure at any time t.
DEFINITION
LetTdenote the lifetime of a system. If the rv Thas pdf f(t) and cdf F(t), the hazard function is
deﬁned by
htðÞ¼ftðÞ
1/C0FtðÞ
IfThas reliability function R(t), the hazard function may also be written as htðÞ¼ ftðÞRtðÞ:
Since the pdf f(t) is not a probability, neither is the hazard function h(t). To get a sense of what the
hazard function represents, consider the following question: Given that the system has survived past
time t, what is the probability the system will fail within the next Δttime units (an imminent failure)?
Such a probability may be computed as follows:
PT/C20tþΔtjT>t ðÞ ¼PT/C20tþΔt\T>t ðÞ
PT>tðÞ¼ðtþΔt
tftðÞdt
RtðÞ/C25ftðÞ/C1Δt
RtðÞ¼htðÞ/C1Δt
Rearranging, we have h(t)/C25P(T/C20t+ΔtjT>t)/Δt; more precisely, h(t) is the limit of the right-
hand side as Δt!0. This suggests that the hazard function h(t) is a density function, like f(t), except
thath(t) relates to the conditional probability that the system is about to fail.
Example 4.50 Once again, consider an exponentially distributed lifetime, T, but with arbitrary
parameter λ. The pdf and reliability function of Tareλe–λtande–λt, respectively, so the hazard
function of Tis
htðÞ¼ftðÞ
RtðÞ¼λe/C0λt
e/C0λt¼λ
In other words, a system whose time to failure follows an exponential distribution will have a
constant hazard function. (The converse is true, too; we’ll see how to recover f(t) from h(t) shortly.)
This relates to the memoryless property of the exponential distribution: given the system has
functioned for thours thus far, the chance of surviving any additional amount of time is independent
oft. As mentioned in Sect. 3.4, this suggests the system does not “wear out” as time progresses (which
may be realistic for some devices in the short term, but not in the long term). ■
Example 4.51 Suppose instead that we model a system’s lifetime with a Weibull distribution. From
the formulas presented in Sect. 3.5,
htðÞ¼ftðÞ
1/C0FtðÞ¼α=βαðÞ tα/C01e/C0t=βðÞα
1/C01/C0e/C0t=βðÞα/C2/C3 ¼α
βαtα/C01
Forα¼1, this is identical to the exponential distribution hazard function (with β¼1/λ). For
α>1,h(t) is an increasing function of t, meaning that we are more likely to observe an imminent4.8 Reliability 321
failure as time progresses (this is equivalent to the system wearing out). For 0 <α<1,h(t)
decreases with t, which would suggest that failures become less likely as tincreases! This can
actually be realistic for small values of t: for many devices, manufacturing ﬂaws cause a handful to
fail very early, and those that survive this initial “burn in” period are actually more likely to survive a
while longer (since they presumably don’t have severe faults). ■
Figure 4.22 shows a prototype hazard function, popularly called a “bathtub” shape. The function
can be divided into three time intervals: (1) a “burn in” period of early failures due to manufacturing
errors; (2) a “stable” period where failures are due primarily to chance; and (3) a “burn out” period
with an increasing failure rate due to devices wearing out. In practice, most hazard functions exhibit
one or more of these behaviors.
There is a one-to-one correspondence between the pdf of a system lifetime, f(t), and its hazard
function, h(t). The deﬁnition of the hazard function shows how one may derive h(t) from f(t); the
following proposition reverses the process.
PROPOSITION
Suppose a system has a continuous lifetime distribution on [0, 1) with hazard function h(t).
Then its lifetime (aka time to failure) has reliability function R(t) given by
RtðÞ¼ e/C0Ðt
0huðÞdu
and pdf f(t) given by
ftðÞ¼/C0 R0tðÞ¼ htðÞe/C0Ðt
0huðÞdu
Proof Since R(t)¼1/C0F(t),R0(t)¼–f(t), and the hazard function may be written as h(t)¼
/C0R0(t)/R(t). Now integrate both sides:
ðt
0huðÞdu¼/C0ðt
0R0uðÞ
RuðÞdu¼/C0 lnRuðÞ½/C138 jt
0¼/C0 lnRtðÞ½/C138 þ lnR0ðÞ½/C138
Since the system lifetime is assumed to be continuous on [0, 1),R(0)¼1 and ln[ R(0)]¼0. This
leaves the equationburn
inburn
outth(t)
stableFig. 4.22 A prototype
hazard function322 4 Joint Probability Distributions and Their Applications
/C0lnRtðÞ½/C138 ¼ðt
0huðÞdu,
and the formula for R(t) follows by solving for R(t). The formula for f(t) follows from the previous
observation that R0(t)¼–f(t), so f(t)¼–R0(t), and then applying the chain rule:
ftðÞ¼/C0 R0tðÞ¼/C0d
dte/C0Ðt
0huðÞdu/C20/C21
¼/C0e/C0Ðt
0huðÞdu/C1d
dt/C0ðt
0huðÞdu/C20/C21
¼e/C0Ðt
0huðÞdu/C1htðÞ
The last step utilizes the Fundamental Theorem of Calculus. ■
The formulas for R(t) and f(t) in the preceding proposition can be easily modiﬁed for the case
where T¼0 with some positive probability, and so R(0)<1 (see Exercise 132).
Example 4.52 A certain type of high-quality transistors has hazard function h(t)¼1+t6fort/C210,
where tis measured in thousands of hours. This function is illustrated in Fig. 4.23a ; notice there is no
“burn in” period, but we see a fairly stable interval followed by burnout. The corresponding pdf for
transistor lifetimes is
ftðÞ¼ htðÞe/C0Ðt
0huðÞdu¼1þt6/C0/C1
e/C0Ðt
01þu6ðÞ du¼1þt6/C0/C1
e/C0tþt7=7 ðÞ
This pdf appears in Fig. 4.23b .
4.8.5 Exercises: Section 4.8(121–132)
121. The lifetime, in thousands of hours, of the motor in a certain brand of kitchen blender has a
Weibull distribution with α¼2 and β¼1.
(a) Determine the reliability function of such a motor and then graph it.
(b) What is the probability a motor of this type will last more than 1,500 h?
(c) Determine the hazard function of such a motor and then graph it.
(d) Find the mean time to failure of such a motor. Compare your answer to the expected value
of a Weibull distribution given in Sect. 3.5.[Hint: Letu¼x2, and apply the gamma
integral formula ( 3.5) to the resulting integral.]
122. High-speed Internet customers are often frustrated by modem crashes. Suppose the time to
“failure” for one particular brand of cable modem, measured in hundreds of hours, follows a
gamma distribution with α¼β¼2.0123 000.20.40.60.81
123th(t) f(t)
tab Fig. 4.23 (a) Hazard
function and ( b) pdf for
Example 4.524.8 Reliability 323
(a) Determine and graph the reliability function for this brand of cable modem.
(b) What is the probability such a modem does not need to be refreshed for more than 300 h?
(c) Find the mean time to “failure” for such a modem. Verify that your answer matches the
formula for the mean of a gamma rv given in Sect. 3.4.
(d) Determine and graph the hazard function for this type of modem.
123. Empirical evidence suggests that the electric ignition on a certain brand of gas stove has the
following lifetime distribution, measured in thousands of days:
ftðÞ¼:375t20/C20t/C202
0 otherwise/C26
(Notice that the model indicates that all such ignitions expire within 2,000 days, a little less than
6 years.)
(a) Determine and graph the reliability function for this model, for all t/C210.
(b) Determine and graph the hazard function for 0 /C20t/C202.
(c) What happens to the hazard function for t>2?
124. The manufacture of a certain children’s toy involves an assembly line with ﬁve stations. The
lifetimes of the equipment at these stations are independent and all exponentially distributed;
the mean time to failure at the ﬁrst three stations (in hundreds of hours) is 1.5, while the mttf at
the last two stations is 2.4.
(a) Determine the reliability function for each of the ﬁve individual stations.
(b) Determine the reliability function for the assembly line. [ Hint: An assembly line is an
example of what type of design?]
(c) Find the mean time to failure for the assembly line.
(d) Determine the hazard function for the assembly line.
125. A local bar owns four of the blenders described in Exercise 121, each having a Weibull(2, 1)
lifetime distribution. During peak hours, these blenders are in continuous use, but the bartenders
can keep making blended drinks (margaritas, etc.) provided that at least one of the four blenders
is still functional. Deﬁne the “system” to be the four blenders under continuous use as described
above, and deﬁne the lifetime of the system to be the length of time that at least one of the
blenders is still functional. (Assume none of the blenders is replaced until all four have worn
out.)
(a) What sort of system design do we have in this example?
(b) Find the reliability function of the system.
(c) Find the hazard function of the system.
(d) Find the mean time to failure of the system. [See the hint from Exercise 121(d).]
126. Consider the six-component system displayed below. Let R1(t),...,R6(t) denote the reliability
functions of the components. Assume the six components operate independently.
456123
(a) Find the system reliability function.
(b) Assuming all six components have exponentially distributed lifetimes with mean 100 h,
ﬁnd the mean time to failure for the system.324 4 Joint Probability Distributions and Their Applications
127. Consider the six-component system displayed below. Let R1(t),...,R6(t) denote the component
reliability functions. Assume the six components operate independently.
135
246
(a) Find the system reliability function.
(b) Assuming all six components have exponentially distributed lifetimes with mean 100 h,
ﬁnd the mean time to failure for the system.
128. A certain machine has the following hazard function:
htðÞ¼:002 0 <t/C20200
:001 t>200/C26
This corresponds to a situation where a device with an exponentially distributed lifetime is
replaced after 200 h of operation by another, better device also having an exponential lifetime
distribution.
(a) Determine and graph the reliability function.
(b) Determine the probability density function of the machine’s lifetime.
(c) Find the mean time to failure.
129. Suppose the hazard function of a device is given by
htðÞ¼α1/C0t
β/C18/C19
0/C20t/C20β
0 otherwise8
<
:
for some α,β>0. This model asserts that if a device lasts βhours, it will last forever (while
seemingly unreasonable, this model can be used to study just “initial wearout”).
(a) Find the reliability function.
(b) Find the pdf of device lifetime.
130. Suppose nindependent devices are connected in series and that the ith device has an exponential
lifetime distribution with parameter λi.
(a) Find the reliability function of the series system.
(b) Show that the system lifetime also has an exponential distribution, and identify its
parameter in terms of λ1,...,λn.
(c) If the mean lifetimes of the individual devices are μ1,...,μn, ﬁnd an expression for the
mean lifetime of the series system.
(d) If the same devices were connected in parallel, would the resulting system’s lifetime also
be exponentially distributed? How can you tell?
131. Show that a device whose hazard function is constant must have an exponential lifetime
distribution.
132. Reconsider the drill bits described in Example 4.45, of which 5% shatter instantly (and
so have lifetime T¼0). It was established that the reliability function for this scenario is
R(t)¼.95e–.01t,t/C210.
(a) A generalized version of expected value that applies to distributions with both discrete and
continuous elements can be used to show that the mean lifetime of these drill bits is
(.05)(0) + (.95)(100) ¼95 h. Verify that Eq. ( 4.8) applied to R(t) gives the same answer.4.8 Reliability 325
[This suggests that our proposition about mttf can be used even when the lifetime
distribution assigns positive probability to 0.]
(b) For t>0, the expression h(t)¼–R0(t)/R(t) is still valid. Find the hazard function for
t>0.
(c) Find a formula for R(t) in terms of h(t) that applies in situations where R(0)<1. Verify
that you recover R(t)¼.95e–.01twhen your formula is applied to h(t) from part (b). [ Hint:
Look at the earlier proposition in this section. What one change needs to occur to
accommodate R(0)<1?]
4.9 Order Statistics
Many situations arise in practice that involve ordering sample observations from smallest to largest
and then manipulating these ordered values in various ways. For example, once the bidding has closed
in a hidden-bid auction (one in which bids are submitted independently of one another), the largest
bid in the resulting sample is the amount paid for the item being auctioned, and the difference
between the largest and second largest bids can be regarded as the amount that the successful bidder
has overpaid.
Suppose that X1,X2,...,Xnis a random sample from a continuous distribution. Because of
continuity, for any i,jwith i6¼j,P(Xi¼Xj)¼0. This implies that with probability 1, the
nsample observations will all be different (of course, in practice all measuring instruments have
accuracy limitations, so tied values may in fact result).
DEFINITION
The order statistics from a random sample are the random variables Y1,...Yngiven by
Y1¼the smallest among X1,X2,...,Xni:e:, the sample minimumðÞ
Y2¼the second smallest among X1,X2,...,Xn
⋮
Yn¼the largest among X1,X2,...,Xnthe sample maximumðÞ
Thus, with probability 1, Y1<Y2<...<Yn/C01<Yn.
Thesample median (i.e., the middle value in the ordered list) is then Y(n+ 1)/2 when nis odd, while
thesample range isYn/C0Y1.
4.9.1 The Distributions of Ynand Y1
The key idea in obtaining the distribution of the sample maximum Ynis the observation that Ynis at
most yif and only if every one of the Xis is at most y.Similarly, the distribution of Y1is based on the
fact that it will exceed yif and only if all Xis exceed y.
Example 4.53 Consider 5 identical components connected in parallel as shown in Fig. 4.20b . Let Xi
denote the lifetime, in hours, of the ith component ( i¼1, 2, 3, 4, 5). Suppose that the Xis are
independent and that each has an exponential distribution with λ¼.01, so the expected lifetime of326 4 Joint Probability Distributions and Their Applications
any particular component is 1/ λ¼100 h. Because of the parallel conﬁguration, the system will
continue to function as long as at least one component is still working, and will fail as soon as the last
component functioning ceases to do so. That is, the system lifetime is Y5, the largest order statistic in a
sample of size 5 from the speciﬁed exponential distribution. Now Y5will be at most yif and only if
every one of the ﬁve Xis is at most y. With G5(y) denoting the cdf of Y5,
G5yðÞ ¼ PY 5/C20y ðÞ ¼ PX 1/C20y\X2/C20y\...\X5/C20y ðÞ
¼PX 1/C20y ðÞ /C1 PX 2/C20y ð Þ/C1/C1/C1/C1/C1 PX 5/C20y ðÞ
For every one of the Xis,P(Xi/C20y)¼F(y)¼Ð
0y.01e/C0.01xdx¼1/C0e/C0.01y; this is the common
cdf of the Xis evaluated at y. Hence, G5(y)¼(1/C0e/C0.01y)/C1/C1/C1(1/C0e/C0.01y)¼(1/C0e/C0.01y)5. The pdf of
Y5can now be obtained by differentiating the cdf with respect to y.
Suppose instead that the ﬁve components are connected in series rather than in parallel
(Fig. 4.20a ). In this case the system lifetime will be Y1, the smallest of the ﬁve order statistics,
since the system will crash as soon as a single one of the individual components fails. Note that
system lifetime will exceed yhours if and only if the lifetime of every component exceeds yhours.
Thus, the cdf of Y1is
G1yðÞ ¼ PY 1/C20y ðÞ ¼ 1/C0PY 1>y ðÞ ¼ 1/C0PX 1>y\X2>y\...\X5>y ðÞ
¼1/C0PX 1>y ðÞ /C1 PX 2>y ð Þ/C1/C1/C1/C1/C1 PX 5>y ðÞ ¼ 1/C0e/C0:01y/C0/C1 5¼1/C0e/C0:05y
This is the form of an exponential cdf with parameter .05. More generally, if the ncomponents in a
series connection have lifetimes that are independent, each exponentially distributed with the same
parameter λ, then the system lifetime will be exponentially distributed with parameter nλ. We saw a
similar result in Example 4.49. The expected system lifetime will then be 1/( nλ), much smaller than
the expected lifetime of an individual component. ■
An argument parallel to that of the previous example for a general sample size nand an arbitrary
pdff(x) gives the following general results.
PROPOSITION
LetY1andYndenote the smallest and largest order statistics, respectively, based on a random
sample from a continuous distribution with cdf F(x) and pdf f(x). Then the cdf and pdf of Ynare
GnyðÞ ¼ FyðÞ½/C138n,gnyðÞ ¼ nFyðÞ½/C138n/C01/C1fyðÞ
The cdf and pdf of Y1are
G1yðÞ ¼ 1/C01/C0FyðÞ ½/C138n,g1yðÞ ¼ n1/C0FyðÞ ½/C138n/C01/C1fyðÞ
Example 4.54 LetXdenote the contents of a one-gallon container, and suppose that its pdf is
f(x)¼2xfor 0/C20x/C201 (and 0 otherwise) with corresponding cdf F(x)¼x2on [0, 1]. Consider a
random sample of four such containers. The order statistics Y1andY4represent the contents of the
least-ﬁlled container and the most-ﬁlled container, respectively. The pdfs of Y1andY4are
g1yðÞ ¼ 41/C0y2/C0/C1 3/C12y¼8y1/C0y2/C0/C1 30/C20y/C2014.9 Order Statistics 327
g4yðÞ ¼ 4y2/C0/C1 3/C12y¼8y70/C20y/C201
The corresponding density curves appear in Fig. 4.24.
Let’s determine the expected value of Y4/C0Y1, the difference between the contents of the most-
ﬁlled container and the least-ﬁlled container; Y4/C0Y1is just the sample range. Apply linearity of
expectation:
EY 4/C0Y1 ðÞ ¼ E/C0
Y4/C1
/C0E/C0
Y1/C1
¼ð1
0y/C18y7dy/C0ð1
0y/C18y1/C0y2/C0/C1 3dy
¼8
9/C0384
945¼:889/C0:406¼:483
If random samples of four containers were repeatedly selected and the sample range of contents
determined for each one, the long run average value of the range would be .483. ■
4.9.2 The Distribution of the ith Order Statistic
We have already obtained the (marginal) distribution of the largest order statistic Ynand also that of
the smallest order statistic Y1. A generalization of the argument used previously results in the
following proposition; Exercise 140 suggests how this result can be derived.
PROPOSITION
Suppose X1,X2,...,Xnis a random sample from a continuous distribution with cdf F(x) and pdf
f(x). The pdf of the ith smallest order statistic Yiis
giyðÞ ¼n!
i/C01ðÞ !n/C0iðÞ !FyðÞ½/C138i/C011/C0FyðÞ ½/C138n/C0ifyðÞ ð 4:9Þ
An intuitive justiﬁcation for Expression ( 4.9) will be given shortly. Notice that it is consistent with
the pdf expressions for g1(y) and gn(y) given previously; just substitute i¼1 and i¼n, respectively.1.0 0.8 0.6 0.4 0.2 0.02.0
1.5
1.0
0.5
0.0
Y11.0 0.8 0.6 0.4 0.2 0.08
6
4
2
0
Y4g1(y)a g4(y)b
y y
Fig. 4.24 Density curves for the order statistics ( a)Y1and ( b)Y4in Example 4.54328 4 Joint Probability Distributions and Their Applications
Example 4.55 Suppose that component lifetime is exponentially distributed with parameter λ. For a
random sample of n¼5 components, the expected value of the sample median lifetime is
EY 3ðÞ ¼ð1
0y/C1g3yðÞdy¼ð1
0y/C15!
2!/C12!1/C0e/C0λy/C0/C1 2e/C0λy/C0/C1 2/C1λe/C0λydy
Expanding out the integrand and integrating term by term, the expected value is .783/ λ. The
median of the exponential distribution is, from solving F(η)¼.5,η¼–ln(.5)/ λ¼.693/ λ. Thus if
sample after sample of ﬁve components is selected, the long run average value of the sample median
will be somewhat larger than the median value of the individual lifetime distribution. This is because
the exponential distribution has a positive skew. ■
Here is the intuitive derivation of Eq. ( 4.9). Let Δbe a number quite close to 0, and consider
the three intervals ( /C01,y], (y,y+Δ], and ( y+Δ,1). For a single X, the probabilities of these
three intervals are
p1¼P(X/C20y)¼F(y) p2¼P(y<X/C20y+Δ)¼Ð
yy+Δf(x)dx/C25f(y)/C1Δ
p3¼P(X>y+Δ)¼1/C0F(y+Δ)
For a random sample of size n, it is very unlikely that two or more Xs will fall in the middle
interval, since its width is only Δ. The probability that the ith order statistic falls in the middle interval
is then approximately the probability that i/C01 of the Xs are in the ﬁrst interval, one is in the middle,
and the remaining n/C0iare in the third. This is just a trinomial probability:
Py<Yi/C20yþΔ ðÞ /C25n!
i/C01ðÞ !1!n/C0iðÞ !FyiðÞ½/C138i/C01/C1fyðÞ /C1Δ/C11/C0FyþΔðÞ ½/C138n/C0i
Dividing both sides by Δand taking the limit as Δ!0 gives exactly ( 4.9). That is, we may
interpret the pdf gi(y) as loosely specifying that i/C01 of the original observations are below y, one is
“at”y, and the other n/C0iare above y.
Similar reasoning works to intuitively derive the joint pdf of YiandYj(i<j). In this case there are
ﬁve relevant intervals: ( /C01,yi], (yi,yi+Δ1], (yi+Δ1,yj], (yj,yj+Δ2], and ( yj+Δ2,1).
4.9.3 The Joint Distribution of the nOrder Statistics
We now develop the joint pdf of Y1,Y2,...,Yn. Consider ﬁrst a random sample X1,X2,X3of fuel
efﬁciency measurements (mpg). The joint pdf of this random sample is
fx1;x2;x3 ðÞ ¼ fx1ðÞ /C1 fx2ðÞ /C1 fx3ðÞ
The joint pdf of Y1,Y2,Y3will be positive only for values of y1,y2,y3satisfying y1<y2<y3.
What is this joint pdf at the values y1¼28.4, y2¼29.0, y3¼30.5? There are six different ways to
obtain these ordered values:4.9 Order Statistics 329
X1¼28.4, X2¼29.0, X3¼30.5
X1¼28.4, X2¼30.5, X3¼29.0
X1¼29.0, X2¼28.4, X3¼30.5
X1¼29.0, X2¼30.5, X3¼28.4
X1¼30.5, X2¼28.4, X3¼29.0
X1¼30.5, X2¼29.0, X3¼28.4
These six possibilities come from the 3! ways to order the three numerical observations once their
values are ﬁxed. Thus
g28:4;29:0;30:5 ðÞ ¼ f28:4ðÞ /C1 f29:0ðÞ /C1 f30:5ð Þþ/C1/C1/C1þ f30:5ðÞ /C1 f29:0ðÞ /C1 f28:4ðÞ
¼3!f28:4ðÞ /C1 f29:0ðÞ /C1 f30:5ðÞ
Analogous reasoning with a sample of size nyields the following result:
PROPOSITION
Letg(y1,y2,...,yn) denote the joint pdf of the order statistics Y1,Y2,...,Ynresulting from a
random sample of Xis from a pdf f(x). Then
gy1;y2;...;yn ðÞ ¼n!fy1ðÞ /C1 f/C0
y2/C1
/C1/C1/C1/C1/C1 f/C0
yn/C1
y1<y2<...<yn
0 otherwise(
For example, if we have a random sample of component lifetimes and the lifetime distribution is
exponential with parameter λ, then the joint pdf of the order statistics is
gy1;...;yn ðÞ ¼ n!λne/C0λy1þ/C1/C1/C1þ yn ðÞ0<y1<y2</C1/C1/C1<yn
Example 4.56 Suppose X1,X2,X3, and X4are independent random variables, each uniformly
distributed on the interval from 0 to 1. The joint pdf of the four corresponding order statistics Y1,
Y2,Y3, and Y4isg(y1,y2,y3,y4)¼4!∙1 for 0 <y1<y2<y3<y4<1. The probability that every
pair of Xis is separated by more than .2 is the same as the probability that Y2/C0Y1>.2,Y3/C0Y2
>.2, and Y4/C0Y3>.2. This latter probability results from integrating the joint pdf of the Yis over the
region .6 <y4<1, .4 <y3<y4/C0.2, .2 <y2<y3/C0.2, 0 <y1<y2/C0.2:
PY 2/C0Y1>:2,Y3/C0Y2>:2,Y4/C0Y3>:2 ðÞ ¼ð1
:6ðy4/C0:2
:4ðy3/C0:2
:2ðy2/C0:2
04!dy1dy2dy3dy4
The inner integration gives 4!( y2/C0.2), and this must then be integrated between .2 and y3/C0.2.
Making the change of variable z2¼y2/C0.2, the integration of z2is from 0 to y3/C0.4. The result of
this integration is 4! ∙(y3/C0.4)2/2. Continuing with the third and fourth integration, each time making
an appropriate change of variable so that the lower limit of each integration becomes 0, the result is
PY 2/C0Y1>:2,Y3/C0Y2>:2,Y4/C0Y3>:2 ðÞ ¼ :44¼:0256
A more general multiple integration argument for nindependent uniform [0, B] rvs shows that the
probability that all values are separated by more than some distance dis330 4 Joint Probability Distributions and Their Applications
Pall values are separated by more than d ðÞ ¼/C2
1/C0n/C01ðÞ d=B/C3n0/C20d/C20B=/C0
n/C01/C1
0 d>B=n/C01ðÞ/C26
As an application, consider a year that has 365 days, and suppose that the birth time of someone
born in that year is uniformly distributed throughout the 365-day period. Then in a group of
10 independently selected people born in that year, the probability that all of their birth times are
separated by more than 24 h ( d¼1 day) is (1 /C0(10/C01)(1)/365)10¼.779. Thus the probability that
at least two of the 10 birth times are separated by at most 24 h is .221. As the group size nincreases, it
becomes more likely that at least two people have birth times that are within 24 h of each other (but
not necessarily on the same day). For n¼16, this probability is .467, and for n¼17 it is .533. So
with as few as 17 people in the group, it is more likely than not that at least two of the people were
born within 24 h of each other.
Coincidences such as this are not as surprising as one might think. The probability that at least two
people are born on the same calendar day (assuming equally likely birthdays) is much easier to
calculate than what we have shown here; see the Birthday Problem in Example 1.22. ■
4.9.4 Exercises: Section 4.9(133–142)
133. A friend of ours takes the bus 5 days per week to her job. The ﬁve waiting times until she can
board the bus are a random sample from a uniform distribution on the interval from 0 to 10 min.
(a) Determine the pdf and then the expected value of the largest of the ﬁve waiting times.
(b) Determine the expected value of the difference between the largest and smallest times.
(c) What is the expected value of the sample median waiting time?
(d) What is the standard deviation of the largest time?
134. Refer back to Example 4.54. Because n¼4, the sample median is the average of the two middle
order statistics, ( Y2+Y3)/2. What is the expected value of the sample median, and how does it
compare to the median of the population distribution?
135. An insurance policy issued to a boat owner has a deductible amount of $1000, so the amount of
damage claimed must exceed this deductible before there will be a payout. Suppose the amount
(thousands of dollars) of a randomly selected claim is a continuous rv with pdf f(x)¼3/x4for
x>1. Consider a random sample of three claims.
(a) What is the probability that at least one of the claim amounts exceeds $5000?
(b) What is the expected value of the largest amount claimed?
136. A store is expecting ndeliveries between the hours of noon and 1 p.m. Suppose the arrival time
of each delivery truck is uniformly distributed on this 1-h interval and that the times are
independent of each other. What are the expected values of the ordered arrival times?
137. Let Xbe the amount of time an ATM is in use during a particular 1-h period, and suppose that
Xhas the cdf F(x)¼xθfor 0 <x<1 (where θ>1). Give expressions involving the gamma
function for both the mean and variance of the ith smallest amount of time Yifrom a random
sample of nsuch time periods.
138. The logistic pdf f(x)¼e/C0x/(1 + e/C0x)2for/C01 <x<1is sometimes used to describe the
distribution of measurement errors.
(a) Graph the pdf. Does the appearance of the graph surprise you?
(b) For a random sample of size n, obtain an expression involving the gamma function for the
moment generating function of the ith smallest order statistic Yi. This expression can then
be differentiated to obtain moments of the order statistics. [ Hint: Set up the appropriate
integral, and then let u¼1/(1 + e/C0x).]4.9 Order Statistics 331
139. Let Xrepresent a measurement error. It is natural to assume that the pdf f(x) is symmetric about
0, so that the density at a value /C0cis the same as the density at c(an error of a given magnitude
is equally likely to be positive or negative). Consider a random sample of nmeasurements,
where n¼2k+ 1, so that Yk+1is the sample median. What can be said about E(Yk+1)? If the
Xdistribution is symmetric about some other value, so that value is the median of the distribu-
tion, what does this imply about E(Yk+1)? [Hints : For the ﬁrst question, symmetry implies that
1/C0F(x)¼P(X>x)¼P(X</C0x)¼F(/C0x). For the second question, consider W¼X/C0η;
what is the median of the distribution of W?]
140. The pdf of the second-largest order statistic, Yn–1can be obtained using reasoning analogous to
how the pdf of Ynwas ﬁrst obtained.
(a) For any number y,Yn–1/C20yif and only if at least n /C01 of the original Xs are/C20y. (Do you
see why?) Use this fact to derive a formula for the cdf of Yn–1in terms of F, the cdf of the
Xs. [Hint: Separate “at least n/C01” into two cases and apply the binomial distribution
formula.]
(b) Differentiate the cdf in part (a) to obtain the pdf of Yn–1. Simplify and verify it matches the
formula for gn–1(y) provided in this section.
141. Use the intuitive argument sketched in this section to obtain the following general formula for
the joint pdf of two order statistics YiandYjwith i<j:
gyi;yj/C0/C1
¼
n!
i/C01ðÞ !j/C0i/C01 ðÞ !n/C0jðÞ !FyiðÞi/C01FyjðÞ /C0 FyiðÞ ½/C138j/C0i/C011/C0FyjðÞ ½/C138n/C0j/C1f/C0yi/C1
f/C0yj/C1
foryi<yj
142. Consider a sample of size n¼3 from the standard normal distribution, and obtain the expected
value of the largest order statistic. What does this say about the expected value of the largest
order statistic in a sample of this size from anynormal distribution? [ Hint: With ϕ(x) denoting
the standard normal pdf, use the fact that ( d/dx)ϕ(x)¼/C0xϕ(x) along with integration by parts.]
4.10 Simulation of Joint Probability Distributions and System Reliability
In Chaps. 2and 3, we saw several methods for simulating “generic” discrete and continuous
distributions (in addition to built-in functions for binomial, Poisson, normal, etc.). Unfortunately,
most of these general methods do not carry over easily to joint distributions or else require signiﬁcant
re-tooling. In this section, we brieﬂy survey some simulation techniques for general bivariate discrete
and continuous distributions and discuss how to simulate normal distributions in more than one
dimension. We then consider simulations for the lifetimes of interconnected systems, in order to
understand the reliability of such systems.
4.10.1 Simulating Values from a Joint PMF
Simulating two dependent discrete rvs XandYcan be rather tedious and is easier to understand with
a speciﬁc example in mind. Suppose we desire to simulate ( X, Y) values from the joint pmf in
Example 4.1:332 4 Joint Probability Distributions and Their Applications
y
p(x,y) 0 100 200
x100 .20 .10 .20
250 .05 .15 .30
Theexhaustive search approach uses the inverse cdf method of Sect. 2.8by reformatting the table
as a single row of ( x, y) pairs along with cumulative probabilities. Starting in the upper left corner and
going across, create “cumulative” probabilities for the entire table:
(x, y) (100, 0) (100, 100) (100, 200) (250, 0) (250, 100) (250, 250)
cum. prob. .20 .30 .50 .55 .70 1
Be careful not to interpret these increasing decimals as cumulative probabilities in the traditional
sense, e.g., it is notthe case that .70 in the preceding table represents P(X/C20250\Y/C20100).
Now the simulation proceeds similarly to those illustrated in Fig. 2.10 for simulating a single
discrete random variable: use if-else statements, specifying the pair of values ( x, y) for each range of
standard uniform random numbers. Figure 4.25 provides the needed Matlab and R code.
In both languages, executing the code in Fig. 4.25 results in two vectors xandythat, when
regarded as paired values, form a simulation of the original joint pmf. That is to say, if xandywere
laid in parallel roughly 20% of the paired values would be (100, 0), about 10% would be (100, 100),
and so on.
At the end of Sect. 2.8we mentioned that both Matlab and R have built-in functions to speed up the
inverse cdf method ( randsample and sample , respectively) for a single discrete
rv. Unfortunately, these are not designed to take pairs of values as an input, and so the lengthier
code is required. You might be tempted to use these built-in functions to simulate the (marginal) pmfs
ofXandYseparately, but beware: by design, the resulting simulated values of XandYwould be
independent, and the rvs displayed in the original joint pmf are clearly dependent. For example,
(100, 0) ought to appear roughly 20% of the time in a simulation; however, separate simulations of
XandYwill result in about 50% 100s for Xand 25% 0s for Y,independently , meaning the pair (100, 0)
will appear in approximately (.5)(.25) ¼12.5% of simulated ( X, Y) values.
x=zeros(10000,1); y=x;
for i=1:10000
    u=rand;
    if u<.2
        x(i)=100; y(i)=0;
    elseif u<.3
        x(i)=100; y(i)=100;
    elseif u<.5
        x(i)=100; y(i)=200;
    elseif u<.55
        x(i)=250; y(i)=0;
    elseif u<.7
        x(i)=250; y(i)=100;
    else
        x(i)=250; y(i)=200;
    end
end x <- NULL; y <- NULL
for (i in 1:10000){
      u=runif(1)
      if (u<.2){
            x[i]<-100; y[i]<-0}
      else if (u<.3){
            x[i]<-100; y[i]<-1}
      else if (u<.5){
            x[i]<-100; y[i]<-2}
      else if (u<.55){
            x[i]<-250; y[i]<-0}
      else if (u<.7){
            x[i]<-250; y[i]<-100}
      else{
            x[i]<-250; y[i]<-200}
}ab
Fig. 4.25 The exhaustive search method for simulating two discrete rvs: ( a) Matlab; ( b)R4.10 Simulation of Joint Probability Distributions and System Reliability 333
It’s worth noting that the choice to add across rows ﬁrst was arbitrary. We could just as well have
added down the left-most column ( Y¼0) of the original joint pmf table, then the middle column,
then the right column to create “cumulative” probabilities and then rewritten our code accordingly.
4.10.2 Simulating Values from a Joint PDF
As in the discrete case, a pair of independent continuous rvs XandYcan be simulated separately using
any of the methods from Sect. 3.8(inverse cdf, accept–reject). In the general case, however, the
inverse cdf method breaks down in two or more dimensions, because we cannot “invert” the joint cdf
ofXandY. Hence, we rely primarily on the accept–reject method. The following proposition repeats
the algorithm from Sect. 3.8but expands it to two dimensions; a simulation scheme for three or more
dependent rvs would be analogous.
ACCEPT–REJECT METHOD (bivariate case)
It is desired to simulate nvalues from a joint pdf f(x, y). Let g1(x) and g2(y) be two univariate
pdfs such that the ratio f/[g1g2] is bounded from above, i.e., there exists a constant csuch that
f(x, y)/[g1(x)g2(y)]/C20cfor all xandy. Proceed as follows:
1. Generate a variate, x*, from the distribution g1;independently , generate a variate, y*, from
the distribution g2. This pair ( x*, y* ) is our candidate.
2. Generate a standard uniform variate, u.
3. If u/C1c/C1g1(x*)g2(y*)/C20f(x*, y* ), then assign ( x, y)¼(x*, y* ), i.e., “accept” the candidate.
Otherwise, reject ( x*, y* ) and return to step 1.
These steps are repeated until ncandidate pairs have been accepted. The resulting accepted
pairs ( x1,y1),...,(xn,yn) constitute a simulation of a pair of random variables ( X, Y) with the
original joint pdf, f(x, y).
As in the one-dimensional case, the accept–reject method hinges on generating some other
distribution on the same set of values as the “target” pdf. What’s special here is that we leverage
our ability to simulate univariate distributions—namely, g1(x) and g2(y)—and create a candidate pair
(x*, y* ) from two independent rvs. In particular, the product g1(x*)g2(y*) that appears in the
algorithm is the joint pdf of two independent rvs having marginal distributions g1andg2, respectively.
Example 4.57 It is desired to simulate values from the following joint pdf, introduced in
Exercise 11:
fx;yðÞ ¼kx2þy2ðÞ 20/C20x/C2030, 20 /C20y/C2030
0 otherwise/C26
(Determining the constant of integration, k, won’t be necessary.) Since both XandYare bounded
between 20 and 30, a sensible choice for both g1andg2is the uniform distribution on [20, 30]. That is,
g1(x)¼1/(30–20) ¼.1 for 20 /C20x/C2030, and g2(y)¼g1(y). The majorization constant cis deter-
mined by requiring
fx;yðÞ
g1xðÞg2yðÞ¼kx2þy2ðÞ
:1ðÞ:1ðÞ/C20cfor 20 /C20x/C2030, 20 /C20y/C2030334 4 Joint Probability Distributions and Their Applications
The left-hand expression is obviously maximized at x¼y¼30, from which we have c/C21
k(302+3 02)/(.1)2¼180,000 k. Setting c¼180,000 k, the accept–reject scheme for this joint pdf
proceeds as follows:
1. Generate independent x* ~ Unif[20, 30] and y* ~ Unif[20, 30].
2. Generate a standard uniform variate, u.
3. Accept ( x*, y* ) iffu/C1c/C1g1(x*)g2(y*)/C20f(x*, y* ), i.e., u/C1180,000 k/C1(.1)(.1) /C20k((x*)2+(y*)2).
This is algebraically equivalent to u/C20((x*)2+(y*)2)/1800.
Figure 4.26 provides Matlab and R code for this example. The output of either one is a pair of
vectors, xandy, whose paired values simulate the original joint pdf.
Figure 4.27 shows the joint pdf f(x, y) alongside a “three-dimensional histogram” of 10,000 ( x, y)
values simulated in Matlab (the latter was created using the hist3 command). Observe that both
show a slight rise as the x-o ry-values increase from 20 to 30.
As indicated in Sect. 3.8, it can be shown that the majorization constant cis also the expected
number of candidates required to generate a single accepted value (here, a pair). In the preceding
example, the numerical value of cturns out to be c¼27/19 /C251.421, so we expect our programs to
require about 14,210 iterations of the while loop to create 10,000 simulated valued of ( X, Y).x=zeros(10000,1); y=x;
i=0;
while i<10000
    xstar=unifrnd(20,30);
    ystar=unifrnd(20,30);
    u=rand;
    if u<=(xstar^2+ystar^2)/1800
        i=i+1;
        x(i)=xstar;
        y(i)=ystar;
    end
endx <- NULL; y <- NULL
i <- 0
while (i <10000){
      xstar <- runif(1,20,30)
      ystar <- runif(1,20,30)
      u <- runif(1)
      if (u<=(xstar^2+ystar^2)/1800){
            i <- i+1
            x[i] <- xstar
            y[i] <- ystar
      }
}b a
Fig. 4.26 Simulation code for Example 4.57: ( a) Matlab; ( b)R
0.014
0.012
0.010
0.008f(x, y)
0.006
0.004
0.002
0
29
2930
25
20 20222426283050
40
30
20
10
0
27
2725
25
xy2323 2121ab
Fig. 4.27 Joint pdf ( a) and histogram of simulated values ( b) for Example 4.57 ■4.10 Simulation of Joint Probability Distributions and System Reliability 335
As an alternative to the accept–reject method, a technique based on conditional distributions can
be employed. The basic idea is this: suppose Xhas pdf f(x) and, conditional on X¼x,Yhas
conditional distribution f(yjx). Then one can simulate ( X, Y) by ﬁrst simulating from f(x) using the
techniques of Sect. 3.8and then, given the simulated value of x, simulating a value yfrom f(yjx).
Example 4.58 Consider the following joint pdf, introduced in Exercise 14:
fx;yðÞ ¼xe/C0x1þyðÞx/C210 and y/C210
0 otherwise/C26
Straightforward integration shows the marginal pdf of Xto be fX(x)¼e–x, from which the
conditional distribution of Ygiven X¼xis
fyjxðÞ ¼fx;yðÞ
fXxðÞ¼xe/C0x1þyðÞ
e/C0x¼xe/C0xy
Each of these has an algebraically simple cdf, so we will employ the inverse cdf method for each
step. The cdf of XisF(x)¼1/C0e–x, whose inverse is given by x¼–ln(1 /C0u). Similarly, the
conditional cdf of Ygiven X¼xisF(yjx)¼1/C0e–xy, whose inverse function (with respect to y)i s
y¼/C0(1/x)ln(1/C0u). The resulting simulation code, in Matlab and R, appears in Fig. 4.28. Notice in
each program that twostandard uniform variates, uandv, are required: one to simulate x, and another
to simulate ygiven x.
Some simpliﬁcations can be made to the preceding code. As in many other simulations, the for
loop can be vectorized (summoning all 10,000 simulated values at once). Additionally, you might
recognize the pdfs under consideration: the marginal distribution of Xis exponential with λ¼1,
while Ygiven X¼xis exponential with parameter λ¼x. Hence, we could exploit Matlab’s or R’s
built-in exponential distribution simulator, rather than ﬁnding and inverting the cdfs. ■
This method can also be extended to three or more variables, but ﬁnding the required conditional
pdfs from the joint pdf can be difﬁcult. This conditional distributions method is best suited to
so-called hierarchical models, where the distribution of each rv is speciﬁed conditional on its
predecessors, e.g., we are provided initially with f(x),f(yjx),f(zjy,x), and so on.
The conditional distributions approach may also be implemented to simulate a joint discrete
distribution; see Exercise 149.
4.10.3 Simulating a Bivariate Normal Distribution
The prevalence of normal distributions makes the ability to simulate both univariate and multivariate
normal rvs especially important. A simple method exists for simulating pairs from an arbitrary
bivariate normal distribution, as indicated in the following proposition.x=zeros(10000,1); y=x;
for i=1:10000
u=rand;
x(i)=-log(1-u);
v=rand;
y(i)=-(1/x(i))*log(1-v);
endx <- NULL; y <- NULL
for (i in 1:10000){
u<-runif(1)
x[i]<- -log(1-u)
v<-runif(1)
y[i]<- -(1/x[i])*log(1-v)
}ab Fig. 4.28 Simulation
code for Example 4.58:
(a) Matlab; ( b)R336 4 Joint Probability Distributions and Their Applications
PROPOSITION
LetZ1andZ2be independent standard normal rvs and let
W1¼Z1, W2¼ρ/C1Z1þﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0ρ2p
Z2
Then W1andW2have a bivariate normal distribution, each having mean 0 and standard
deviation 1, and Corr( W1,W2)¼ρ.
This result can be proved using the transformation methods of Sect. 4.6. The means, variances, and
correlation coefﬁcient of W1andW2are established in Exercise 161.
Now suppose we wish to simulate from a bivariate normal distribution with an arbitrary set of
parameters μ1,σ1,μ2,σ2, and ρ. Deﬁne XandYby
X¼μ1þσ1W1¼μ1þσ1Z1,
Y¼μ2þσ2W2¼μ2þσ2ρZ1þﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0ρ2p
Z2/C16/C17ð4:10Þ
Since XandYin Expression ( 4.10) are just linear functions of W1andW2, it follows from Sect. 4.2
that Corr( X, Y)¼Corr( W1,W2)¼ρ. Moreover, since W1andW2have mean zero and standard
deviation 1, these linear transformations give XandYthe desired means and standard deviations. So,
to simulate a bivariate normal distribution, create a pair of independent standard normal variates z1
andz2, and then apply the formulas for XandYin Eq. ( 4.10).
Example 4.59 Consider the joint distribution of SAT reading and math scores described in Example
4.42. Using the parameters from that example, Eq. ( 4.10) becomes
X¼496þ114Z1, Y¼514þ117 :25Z1þﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0:252p
Z2/C16/C17
Figure 4.29 shows this transformation implemented in Matlab and R; both programs have been
vectorized and produce 10,000 ( X, Y) pairs.
Now deﬁne a new rv R¼Y/X, the ratio of a student’s SAT Math and Critical Reading scores.
Arguably, this measures a student’s math ability relative to her or his reading skills. Determining the
pdf of Ris simply not feasible, especially since XandYare dependent. But the above simulation,
along with the command r¼y/x (in R, or r¼y./x in Matlab) gives us information about its
distribution. A histogram of the simulated values of Rappears in Fig. 4.30. For these 10,000 simulated
values, the sample mean and standard deviation are /C22r¼1:0161 and s¼0.1677. So, we estimate the
true expected ratio E(R) for all students that took the SAT in Fall 2012 is 1.0161, with an estimated
standard error of s=ﬃﬃﬃnp¼0:1677 =ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ10, 000p¼:001677 :z1=normrnd(0,1,[10000 1]);
z2=normrnd(0,1,[10000 1]);
x=496+114*Z1;
y=514+117*(.25*z1+sqrt(1-.25^2)*z2);z1 <- rnorm(10000)
z2 <- rnorm(10000)
x <- 496+114*z1
y <- 514+117*(.25*z1+sqrt(1-.25^2)*z2)b a
Fig. 4.29 Code for Example 4.59: ( a) Matlab ( b)R4.10 Simulation of Joint Probability Distributions and System Reliability 337
Matlab and R also have built-in programs to simulate multivariate normal distributions that work
for 2 or more dimensions and do not rely on the preceding proposition. In fact, users have created
several such tools in R; we illustrate here the function available in the mvtnorm package. The
mvnrnd function in Matlab and the rmvnorm function in R take three inputs: the desired number of
simulated values (in the bivariate case, simulated pairs), a vector of means, and a covariance matrix
(see the end of Sect. 4.7). Figure 4.31 illustrates these commands for the distribution speciﬁed in
Example 4.59.
4.10.4 Simulation Methods for Reliability
One area of application for the simulation methods presented in this section is to the lifetime distributions
of complex systems. It can sometimes be difﬁcult to derive the exact pdf of the lifetime of a system
comprised of many components (in series and/or parallel), but simulation provides a way out.
Example 4.60 Consider the system described in Example 4.48; this is actually a comparatively
simple conﬁguration. Let T1,T2, and T3denote the lifetimes of the three components. Since
components 1 and 2 are connected in series, the “1–2 subsystem” functions only as long as the
smaller of T1andT2, e.g., if T1¼135 h and T2¼119 h, then the lifetime of the 1–2 subsystem is
119 h. The lifetime of the 1–2 subsystem, therefore, can be expressed mathematically as min( T1,T2).mu=[496, 514];
C=[114^2, .25*114*117;
.25*114*117, 117^2];
x=mvnrnd(mu,C,10000)mu <- c(496,514)
C <- matrix(c(114^2, .25*114*117,
.25*114*117, 117^2),2,2)
x <- rmvnorm(10000,mu,C)ab
Fig. 4.31 Built-in multivariate normal simulations: ( a) Matlab; ( b)R0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5R 050100150200250300350400450Frequency
Fig. 4.30 Histogram of R¼Y/Xfrom Example 4.59 ■338 4 Joint Probability Distributions and Their Applications
Similarly, the 1–2 subsystem is linked in parallel with component 3, and so the lifetime of the
overall system is the larger of the lifetimes of the two pieces (the 1–2 subsystem and component 3).
For example, if the lifetime of the 1–2 subsystem is 119 h and the lifetime of component 3 is 127 h,
then the overall system lifetime is 127 h. If we let Tsysdenote the system lifetime, then we have
Tsys¼max 1 /C02 subsystem lifetime, component 3 lifetimeðÞ
¼max/C0
min T1;T2 ðÞ ,T3/C1
This expression combining max and min functions can be used to simulate the system lifetime,
provided we have models (that we can simulate) for the lifetimes of the three individual components.
Figure 4.32 shows example code for simulating the system lifetime assuming each of the three
components has an exponentially distributed lifetime with mean 100 h. The 10,000 simulation runs
have been vectorized to accelerate the process. Notice that the R code requires using the pmax and
pmin functions, which treat their inputs as parallel vectors and ﬁnd the “row-wise” maximum or
minimum.
A histogram of 10,000 simulated values of Tsysfrom R appears in Fig. 4.33; this should look
similar to the pdf of that rv. We can use these same simulated values to estimate the expectation and
standard deviation of Tsys: for our run, the sample mean and standard deviation were 115.37 h and
93.49 h, respectively.
Histogram of Tsys
3000
2500
2000
1500Frequency
1000
500
0 200 400
Tsys600 8000
Fig. 4.33 A histogram of simulated values of Tsysin Example 4.60 ■ab
T1=exprnd(100,10000,1);
T2=exprnd(100,10000,1);
T3=exprnd(100,10000,1);
Tsys=max(min(T1,T2),T3);T1 <- rexp(10000,1/100)
T2 <- rexp(10000,1/100)
T3 <- rexp(10000,1/100)
Tsys <- pmax(pmin(T1,T2),T3)
Fig. 4.32 Simulation code for Example 4.60: ( a) Matlab; ( b)R4.10 Simulation of Joint Probability Distributions and System Reliability 339
4.10.5 Exercises: Section 4.10 (143–153)
143. Consider the service station scenario presented in Exercise 1 of this chapter.
(a) Write a program to simulate the rvs ( X, Y) described in that exercise.
(b) Use your program to estimate P(X/C201 and Y/C201), and compare your estimate to the exact
answer from the joint pmf. Use at least 10,000 simulation runs.
(c) Deﬁne a new variable D¼jX/C0Yj, the (absolute) difference in the number of hoses in use
at the two gas pumps. Use your program (with at least 10,000 runs) to simulate D, and
estimate both the mean and standard deviation of D.
144. Refer back to the quiz scenario of Exercise 24.
(a) Write a program to simulate students’ scores ( X, Y) on the two parts of the quiz.
(b) Use your program to estimate the probability that a student’s total score is at least 20 points.
How does your estimate compare to the exact answer from the joint pmf?
(c) Deﬁne a new rv M¼the maximum of the two scores. Use your program to simulate M,
and estimate both the mean and standard deviation of M.
145. Consider the situation presented in Example 4.13: the joint pdf of the amounts XandYof
almonds and cashews, respectively, in a 1-lb can of nuts is
fx;yðÞ ¼24xy0/C20x/C201, 0 /C20y/C201, xþy/C201
0 otherwise/C26
With the prices speciﬁed in that example, the total cost of the contents of one can is
W¼3.5 + 2.5 X+ 6.5 Y.
(a) Write a program implementing the accept–reject method of this section to simulate ( X, Y).
(b) On the average, how many iterations will your program require to generate 10,000
“accepted” ( X, Y) pairs?
(c) Use your program to simulate the rv W. Create a histogram of the simulated values of W,
and report estimates of the mean and standard deviation of W. How close is your sample
mean to the value E(W)¼$7.10 determined in Example 4.13?
(d) Use your simulation in part (c) to estimate the probability that the cost of the contents of a
can of nuts exceeds $8.
146. Suppose a randomly chosen individual’s verbal score Xand quantitative score Yon a nationally
administered aptitude examination, each scaled down to [0, 1], have joint pdf
fx;yðÞ ¼2
52xþ3y ðÞ 0/C20x/C201, 0 /C20y/C201
0 otherwise(
(a) Write a program implementing the accept–reject method of this section to simulate ( X, Y).
(b) The engineering school at a certain university uses a weighted total T¼3X+7Yas part of
its admission process. Use your program in part (a) to simulate the rv T, and estimate
P(T/C219).
(c) Suppose the engineering school decides to only admit students whose weighted totals are
above the 85th percentile for the national distribution. That is, if η.85is the 85th percentile
of the distribution of T, a student’s weighted total must exceed η.85for admission. Use your
simulated values of Tfrom part (b) to estimate η.85.[Hint: η.85separates the bottom 85% of
theTdistribution from the remaining 15%. What value separates the lowest 85% of
simulated T values from the rest?]340 4 Joint Probability Distributions and Their Applications
147. Refer back to Exercise 145.
(a) Determine the marginal pdf of Xand the conditional pdf of Ygiven X¼x.
(b) Write a program to simulate ( X, Y) using the conditional distributions method presented
in this section.
(c) What advantage does this method have over the accept–reject approach used in
Exercise 145?
148. Consider the situation in Example 4.31: the proportion Pof tiles meeting thermal speciﬁcations
varies according to the pdf f(p)¼9p8,0<p<1; conditional on P¼p, the number of
inspected tiles that meet speciﬁcations is a rv Y~ Bin(20, p).
(a) Write a program to simulate Y. Your program will ﬁrst need to simulate a value of P, and
then generate a variate from the appropriate binomial distribution. [ Hint: Use your
software’s built-in binomial simulation tool.]
(b) Simulate (at least) 10,000 values of Y, and report estimates of both E(Y) and Var( Y). How
do these compare to the exact answers found in Example 4.31?
(c) Use your simulation to estimate both P(Y¼18) and P(Y/C2118).
149. The conditional distributions method described in this section can also be applied to joint
discrete rvs. Refer back to the joint pmf presented in this section, which is originally from
Example 4.1.
(a) Determine the marginal pmf of X. (This should be very easy.)
(b) Determine the conditional pmfs of Ygiven X¼100 and given X¼250.
(c) Write a program that ﬁrst simulates Xusing its marginal pmf, then simulates Yvia the
appropriate conditional pmf. [ Hint: For each stage, use your program’s built-in discrete
simulator ( randsample in Matlab, sample in R).]
(d) Use your program in part (c) to simulate at least 10,000 ( X, Y) pairs. Verify that the relative
frequencies of the six possible pairs in your sample are close to the probabilities speciﬁed
in the original joint pmf table.
150. Refer back to Exercise 113, which speciﬁes a bivariate normal distribution for the rvs X¼
height (inches) and Y¼weight (lbs) for American males. The parameters of that model were
μ1¼70,σ1¼3,μ2¼170, σ2¼20, and ρ¼.9.
(a) Use your software’s built-in multivariate normal simulation function to generate (at least)
10,000 ( X, Y) pairs according to this bivariate normal model.
(b) A person’s body-mass index (BMI) is determined by the formula 703 Y/X2. Use the result of
part (a) to create a histogram of BMIs for the population of American males.
(c) BMI scores between 18.5 and 25 are considered healthy. By that criterion, what proportion
of American males are healthy? Report both an estimate of this proportion and its
estimated standard error.
151. The conditional distributions method of this section can be implemented to simulate a
bivariate normal distribution, providing an alternative to built-in multivariate simulation tools
or Expression ( 4.10). Let XandYhave a bivariate normal distribution with parameters μ1,σ1,μ2,
σ2, and ρ.
(a) What are the marginal distribution of Xand the conditional distribution of Ygiven X¼x?
[Hint: Refer back to Sect. 4.7.]
(b) Write a program to simulate ( X, Y) values from a bivariate normal distribution by ﬁrst
simulating Xand then YjX¼x. The inputs to your program should be the ﬁve parameters
and the desired number of simulated values; the outputs should be vectors containing the
simulated values of XandY.4.10 Simulation of Joint Probability Distributions and System Reliability 341
(c) Use your program to simulate the height-weight distribution from the previous exercise.
Verify that the sample mean and standard deviation of your simulated Yvalues are roughly
170 and 20, respectively.
152. Consider the system design illustrated in Exercise 126. Suppose that components 1, 2, and
3 have exponential lifetimes with mean 250 h, while components 4, 5, and 6 have exponential
lifetimes with mean 300 h.
(a) Write a program to simulate the lifetime of the system.
(b) Let μdenote the true mean system lifetime. Provide an estimate of μ, along with its
estimated standard error.
(c) Let pdenote the true probability that the system lasts more than 200 h. Provide an estimate
ofp, along with its estimated standard error.
153. Consider the system design illustrated in Exercise 127. Suppose the odd-numbered components
have exponential lifetimes with mean 250 h, while the even-numbered components have gamma
lifetime distributions with α¼2 and β¼125. (This second distribution also has mean 250 h.)
(a) Write a program to simulate the lifetime of the system. [You might want to use your
software’s built-in gamma random number generator.]
(b) Let μdenote the true mean system lifetime. Provide an estimate of μ, along with its
estimated standard error.
(c) Let pdenote the true probability that the system fails prior to 400 h. Provide an estimate of
p, along with its estimated standard error.
4.11 Supplementary Exercises (154–192)
154. Suppose the amount of rainfall in one region during a particular month has an exponential
distribution with mean value 3 in., the amount of rainfall in a second region during that same
month has an exponential distribution with mean value 2 in., and the two amounts are indepen-
dent of each other. What is the probability that the second region gets more rainfall during this
month than does the ﬁrst region?
155. Two messages are to be sent. The time (min) necessary to send each message has an exponential
distribution with parameter λ¼1, and the two times are independent of each other. It costs $2
per minute to send the ﬁrst message and $1 per minute to send the second. Obtain the density
function of the total cost of sending the two messages. [ Hint: First obtain the cumulative
distribution function of the total cost, which involves integrating the joint pdf.]
156. A restaurant serves three ﬁxed-price dinners costing $20, $25, and $30. For a randomly selected
couple dining at this restaurant, let X¼the cost of the man’s dinner and Y¼the cost of the
woman’s dinner. The joint pmf of XandYis given in the following table:
y
p(x, y) 2 02 53 0
20 .05 .05 .10
x 25 .05 .10 .35
30 0 .20 .10
(a) Compute the marginal pmfs of XandY.
(b) What is the probability that the man’s and the woman’s dinner cost at most $25 each?342 4 Joint Probability Distributions and Their Applications
(c) Are XandYindependent? Justify your answer.
(d) What is the expected total cost of the dinner for the two people?
(e) Suppose that when a couple opens fortune cookies at the conclusion of the meal, they ﬁnd
the message “You will receive as a refund the difference between the cost of the more
expensive and the less expensive meal that you have chosen.” How much does the
restaurant expect to refund?
157. A health-food store stocks two different brands of a type of grain. Let X¼the amount (lb) of
brand A on hand and Y¼the amount of brand B on hand. Suppose the joint pdf of XandYis
fx;yðÞ ¼kxy x /C210, y/C210, 20 /C20xþy/C2030
0 otherwise/C26
(a) Draw the region of positive density and determine the value of k.
(b) Are XandYindependent? Answer by ﬁrst deriving the marginal pdf of each variable.
(c) Compute P(X+Y/C2025).
(d) What is the expected total amount of this grain on hand?
(e) Compute Cov( X,Y) and Corr( X,Y).
(f) What is the variance of the total amount of grain on hand?
158. Let X1,X2,...,Xnbe random variables denoting nindependent bids for an item that is for sale.
Suppose each Xiis uniformly distributed on the interval [100, 200]. If the seller sells to the
highest bidder, how much can he expect to earn on the sale? [ Hint: Let Y¼max( X1,X2,...,Xn).
Use the results of Sect. 4.9to ﬁnd E(Y).]
159. Suppose a randomly chosen individual’s verbal score Xand quantitative score Yon a nationally
administered aptitude examination have joint pdf
fx;yðÞ ¼2
52xþ3y ðÞ 0/C20x/C2010 /C20y/C201
0 otherwise8
<
:
You are asked to provide a prediction tof the individual’s total score X+Y. The error of
prediction is the mean squared error E[(X+Y/C0t)2]. What value of tminimizes the error of
prediction?
160. Let X1andX2be quantitative and verbal scores on one aptitude exam, and let Y1andY2be the
corresponding scores on another exam. If Cov( X1,Y1)¼5, Cov( X1,Y2)¼1, Cov( X2,Y1)¼2,
and Cov( X2,Y2)¼8, what is the covariance between the two total scores X1+X2andY1+Y2?
161. Let Z1andZ2be independent standard normal rvs and let
W1¼Z1 W2¼ρ/C1Z1þﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0ρ2p
Z2
(a) By deﬁnition, W1has mean 0 and standard deviation 1. Show that the same is true for W2.
(b) Use the properties of covariance to show that Cov( W1,W2)¼ρ.
(c) Show that Corr( W1,W2)¼ρ.
162. You are driving on a highway at speed X1. Cars entering this highway after you travel at speeds
X2,X3,.... Suppose these Xis are independent and identically distributed. Unfortunately there is
no way for a faster car to pass a slower one—it will catch up to the slower one and then travel at
the same speed. For example, if X1¼52.3, X2¼37.5, and X3¼42.8, then no car will catch up
to yours, but the third car will catch up to the second. Let N¼the number of cars that ultimately
travel at your speed (in your “cohort”), including your own car. Possible values of Nare 1, 2,
3,.... Show that the pmf of Nisp(n)¼1/[n(n+ 1)], and then determine the expected number
of cars in your cohort. [ Hint:N¼3 requires that X1<X2,X1<X3,X4<X1.]4.11 Supplementary Exercises (154–192) 343
163. Suppose the number of children born to an individual has pmf p(x). A Galton–Watson
branching process unfolds as follows: At time t¼0, the population consists of a single
individual. Just prior to time t¼1, this individual gives birth to X1individuals according to
the pmf p(x), so there are X1individuals in the ﬁrst generation. Just prior to time t¼2, each of
these X1individuals gives birth independently of the others according to the pmf p(x), resulting
inX2individuals in the second generation (e.g., if X1¼3, then X2¼Y1+Y2+Y3, where Yiis
the number of progeny of the ith individual in the ﬁrst generation). This process then continues
to yield a third generation of size X3, and so on.
(a) If X1¼3,Y1¼4,Y2¼0,Y3¼1, draw a tree diagram with two generations of branches
to represent this situation.
(b) Let Abe the event that the process ultimately becomes extinct (one way for Ato occur
would be to have X1¼3 with none of these three second-generation individuals having
any progeny) and let p*¼P(A). Argue that p* satisﬁes the equation
p*¼X
p*ðÞx/C1pxðÞ
[Hint:A¼[ x¼01(A\X1¼x), so the Law of Total Probability can be applied. Now
given that X1¼3,Awill occur if and only if each of the three separate branching processes
starting from the ﬁrst generation ultimately becomes extinct; what is the probability of this
happening?]
(c) Verify that one solution to the equation in (b) is p*¼1. It can be shown that this equation
has just one other solution, and that the probability of ultimate extinction is in fact the
smaller of the two roots. If p(0)¼.3,p(1)¼.5, and p(2)¼.2, what is p*? Is this
consistent with the value of μ, the expected number of progeny from a single individual?
What happens if p(0)¼.2,p(1)¼.5, and p(2)¼.3?
164. Let f(x) and g(y) be pdfs with corresponding cdfs F(x) and G(y), respectively. With cdenoting a
numerical constant satisfying jcj/C201, consider
fx;yðÞ ¼ fxðÞgyðÞ1þc2FxðÞ /C0 1 ½/C138 2GyðÞ /C0 1 ½/C138 fg
(a) Show that f(x,y) satisﬁes the conditions necessary to specify a joint pdf for two
continuous rvs.
(b) What is the marginal pdf of the ﬁrst variable X? Of the second variable Y?
(c) For what values of careXandYindependent?
(d) If f(x) and g(y) are normal pdfs, is the joint distribution of XandYbivariate normal?
165. The joint cumulative distribution function of two random variables XandY, denoted by
F(x,y), is deﬁned by
Fx;yðÞ ¼ PX/C20xðÞ \ Y/C20yðÞ ½/C138 /C0 1 <x<1,/C01 <y<1
(a) Suppose that XandYare both continuous variables. Once the joint cdf is available, explain
how it can be used to determine P((X,Y)2A), where Ais the rectangular region {( x,y):
a/C20x/C20b, c/C20y/C20d}.
(b) Suppose the only possible values of XandYare 0, 1, 2, ...and consider the values a¼5,
b¼10,c¼2, and d¼6 for the rectangle speciﬁed in (a). Describe how you would use
the joint cdf to calculate the probability that the pair ( X,Y) falls in the rectangle. More
generally, how can the rectangular probability be calculated from the joint cdf if a,b,c, and
dare all integers?344 4 Joint Probability Distributions and Their Applications
(c) Determine the joint cdf for the scenario of Example 4.1. [ Hint: First determine F(x,y) for
x¼100, 250 and y¼0, 100, and 200. Then describe the joint cdf for various other ( x,y)
pairs.]
(d) Determine the joint cdf for the scenario of Example 4.3 and use it to calculate the
probability that XandYare both between .25 and .75. [ Hint: For 0 /C20x/C201 and 0 /C20y
/C201,F(x,y)¼Ð
0xÐ
0yf(u,v)dvdu .]
(e) Determine the joint cdf for the scenario of Example 4.4. [ Hint: Proceed as in (d), but be
careful about the order of integration and consider separately ( x,y) points that lie inside the
triangular region of positive density and then points that lie outside this region.]
166. A circular sampling region with radius Xis chosen by a biologist, where Xhas an exponential
distribution with mean value 10 ft. Plants of a certain type occur in this region according to a
(spatial) Poisson process with “rate” .5 plant per square foot. Let Ydenote the number of plants
in the region.
(a) Find E(YjX¼x) and Var( YjX¼x).
(b) Use part (a) to ﬁnd E(Y).
(c) Use part (a) to ﬁnd Var( Y).
167. The number of individuals arriving at a post ofﬁce to mail packages during a certain period is a
Poisson random variable Xwith mean value 20. Independently of each other, any particular
customer will mail either 1, 2, 3, or 4 packages with probabilities .4, .3, .2, and .1, respectively.
LetYdenote the total number of packages mailed during this time period.
(a) Find E(YjX¼x) and Var( YjX¼x).
(b) Use part (a) to ﬁnd E(Y).
(c) Use part (a) to ﬁnd Var( Y).
168. Sandstone is mined from two different quarries. Let X¼the amount mined (in tons) from the
ﬁrst quarry in one day and Y¼the amount mined (in tons) from the second quarry in one day.
The variables XandYare independent, with μX¼12,σX¼4,μY¼10,σY¼3.
(a) Find the mean and standard deviation of the variable X+Y , the total amount of sandstone
mined in a day.
(b) Find the mean and standard deviation of the variable X/C0Y, the difference in the mines’
outputs in a day.
(c) The manager of the ﬁrst quarry sells sandstone at $25/t, while the manager of the second
quarry sells sandstone at $28/t. Find the mean and standard deviation for the combined
amount of money the quarries generate in a day.
(d) Assuming XandYare both normally distributed, ﬁnd the probability the quarries generate
more than $750 revenue in a day.
169. The article “Stochastic Modeling for Pavement Warranty Cost Estimation” ( J. of Constr. Engr.
and Mgmt. , 2009: 352 –359) proposes the following model for the distribution of Y¼time to
pavement failure. Let X1be the time to failure due to rutting, and X2be the time to failure due to
transverse cracking; these two rvs are assumed independent. Then Y¼min(X1,X2). The
probability of failure due to either one of these distress modes is assumed to be an increasing
function of time t. After making certain distributional assumptions, the following form of the
cdf for each mode is obtained:
Φaþbtﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
cþdtþet2p/C18/C19
where Φis the standard normal cdf. Values of the ﬁve parameters a,b,c,d, and eare –25.49,
1.15, 4.45, –1.78, and .171 for cracking and –21.27, .0325, .972, –.00028, and .00022 for rutting.
Determine the probability of pavement failure within t¼5 years and also t¼10 years.4.11 Supplementary Exercises (154–192) 345
170. Consider a sealed-bid auction in which each of the nbidders has his/her valuation (assessment
of inherent worth) of the item being auctioned. The valuation of any particular bidder is not
known to the other bidders. Suppose these valuations constitute a random sample X1,...,Xn
with corresponding order statistics Y1/C20Y2/C20 /C1/C1/C1 /C20 Yn. The rent of the winning bidder is the
difference between the winner’s valuation and the price. The article “Mean Sample Spacings,
Sample Size and Variability in an Auction-Theoretic Framework” ( Oper. Res. Lett. , 2004: 103–
108) argues that the rent is just Yn/C0Yn–1(do you see why?).
(a) Suppose that the valuation distribution is uniform on [0, 100]. What is the expected rent
when there are n¼10 bidders?
(b) Referring back to (a), what happens when there are 11 bidders? More generally, what is the
relationship between the expected rent for nbidders and for n+ 1 bidders? Is this intuitive?
[Note: The cited article presents a counterexample.]
171. Suppose two identical components are connected in parallel, so the system continues to function
as long as at least one of the components does so. The two lifetimes are independent of each
other, each having an exponential distribution with mean 1000 h. Let Wdenote system lifetime.
Obtain the moment generating function of W, and use it to calculate the expected lifetime.
172. Let Y0denote the initial price of a particular security and Yndenote the price at the end of
nadditional weeks for n¼1, 2, 3, .... Assume that the successive price ratios Y1/Y0,Y2/Y1,Y3/
Y2,...are independent of one another and that each ratio has a lognormal distribution with
μ¼.4 and σ¼.8 (the assumptions of independence and lognormality are common in such
scenarios).
(a) Calculate the probability that the security price will increase over the course of a week.
(b) Calculate the probability that the security price will be higher at the end of the next week,
be lower the week after that, and then be higher again at the end of the following week.
[Hint: What does “higher” say about the ratio Yi+1/Yi?]
(c) Calculate the probability that the security price will have increased by at least 20% over the
course of a ﬁve-week period. [ Hint: Consider the ratio Y5/Y0, and write this in terms of
successive ratios Yi+1/Yi.]
173. In cost estimation, the total cost of a project is the sum of component task costs. Each of these
costs is a random variable with a probability distribution. It is customary to obtain information
about the total cost distribution by adding together characteristics of the individual component
cost distributions—this is called the “roll-up” procedure. For example, E(X1+/C1/C1/C1+X n)¼
E(X1)+/C1/C1/C1+E(Xn), so the roll-up procedure is valid for mean cost. Suppose that there are two
component tasks and that X1andX2are independent, normally distributed random variables. Is
the roll-up procedure valid for the 75th percentile? That is, is the 75th percentile of the
distribution of X1+X 2the same as the sum of the 75th percentiles of the two individual
distributions? If not, what is the relationship between the percentile of the sum and the sum
of percentiles? For what percentiles is the roll-up procedure valid in this case?
174. Suppose that for a certain individual, calorie intake at breakfast is a random variable with
expected value 500 and standard deviation 50, calorie intake at lunch is random with expected
value 900 and standard deviation 100, and calorie intake at dinner is a random variable with
expected value 2000 and standard deviation 180. Assuming that intakes at different meals are
independent of each other, what is the probability that average calorie intake per day over the
next (365-day) year is at most 3500? [ Hint: Let Xi,Yi, and Zidenote the three calorie intakes on
dayi. Then total intake is given by ∑(Xi+Yi+Zi).]346 4 Joint Probability Distributions and Their Applications
175. The mean weight of luggage checked by a randomly selected tourist-class passenger ﬂying
between two cities on a certain airline is 40 lb, and the standard deviation is 10 lb. The mean and
standard deviation for a business-class passenger are 30 lb and 6 lb, respectively.
(a) If there are 12 business-class passengers and 50 tourist-class passengers on a particular
ﬂight, what are the expected value of total luggage weight and the standard deviation of
total luggage weight?
(b) If individual luggage weights are independent, normally distributed rvs, what is the
probability that total luggage weight is at most 2500 lb?
176. Random sums. IfX1,X2,...,Xnare independent rvs, each with the same mean value μand
variance σ2, then we have seen that E(X1+X2+/C1/C1/C1+Xn)¼nμand Var( X1+X2+/C1/C1/C1+Xn)
¼nσ2. In some applications, the number of Xis under consideration is not a ﬁxed number nbut
instead a rv N. For example, let Nbe the number of components of a certain type brought into a
repair shop on a particular day and let Xirepresent the repair time for the ith component. Then
the total repair time is TN¼X1+X2+/C1/C1/C1+XN, the sum of a random number of rvs.
(a) Suppose that Nis independent of the Xis. Use the Law of Total Expectation to obtain an
expression for E(TN) in terms of μandE(N).
(b) Use the Law of Total Variance to obtain an expression for Var( TN) in terms of μ,σ2,E(N),
and Var( N).
(c) Customers submit orders for stock purchases at a certain online site according to a Poisson
process with a rate of 3 per hour. The amount purchased by any particular customer
(in thousands of dollars) has an exponential distribution with mean 30. What is the
expected total amount ($) purchased during a particular 4-h period, and what is the
standard deviation of this total amount?
177. Suppose the proportion of rural voters in a certain state who favor a particular gubernatorial
candidate is .45 and the proportion of suburban and urban voters favoring the candidate is .60. If
a sample of 200 rural voters and 300 urban and suburban voters is obtained, what is the
approximate probability that at least 250 of these voters favor this candidate?
178. Let μdenote the true pH of a chemical compound. A sequence of nindependent sample pH
determinations will be made. Suppose each sample pH is a random variable with expected value
μand standard deviation .1. How many determinations are required if we wish the probability
that the sample average is within .02 of the true pH to be at least .95? What theorem justiﬁes
your probability calculation?
179. The amount of soft drink that Ann consumes on any given day is independent of consumption on
any other day and is normally distributed with μ¼13 oz and σ¼2. If she currently has two
six-packs of 16-oz bottles, what is the probability that she still has some soft drink left at the end
of 2 weeks (14 days)? Why should we worry about the validity of the independence assumption
here?
180. A large university has 500 single employees who are covered by its dental plan. Suppose the
number of claims ﬁled during the next year by such an employee is a Poisson rv with mean value
2.3. Assuming that the number of claims ﬁled by any such employee is independent of the
number ﬁled by any other employee, what is the approximate probability that the total number
of claims ﬁled is at least 1200?
181. A student has a class that is supposed to end at 9:00 a.m. and another that is supposed to begin at
9:10 a.m. Suppose the actual ending time of the 9 a.m. class is a normally distributed rv X1with
mean 9:02 and standard deviation 1.5 min and that the starting time of the next class is also a
normally distributed rv X2with mean 9:10 and standard deviation 1 min. Suppose also that the
time necessary to get from one classroom to the other is a normally distributed rv X3with mean4.11 Supplementary Exercises (154–192) 347
6 min and standard deviation 1 min. What is the probability that the student makes it to the
second class before the lecture starts? (Assume independence of X1,X2, and X3, which is
reasonable if the student pays no attention to the ﬁnishing time of the ﬁrst class.)
182. This exercise provides an alternative approach to establishing the properties of correlation.
(a) Use the general formula for the variance of a linear combination to write an expression
for Var( aX+Y). Then let a¼σY/σX, and show that ρ/C21–1. [Hint: Variance is always /C21
0, and Cov( X,Y)¼σX/C1σY/C1ρ.]
(b) By considering Var( aX/C0Y), conclude that ρ/C201.
(c) Use the fact that Var( W)¼0 only if Wis a constant to show that ρ¼1 only if
Y¼aX+b.
183. A rock specimen from a particular area is randomly selected and weighed two different times.
LetWdenote the actual weight and X1andX2the two measured weights. Then X1¼W+E1
andX2¼W+E2, where E1andE2are the two measurement errors. Suppose that the Eis are
independent of each other and of Wand that Var( E1)¼Var(E2)¼σE2.
(a) Express ρ, the correlation coefﬁcient between the two measured weights X1andX2,i n
terms of σW2, the variance of actual weight, and σX2, the variance of measured weight.
(b) Compute ρwhen σW¼1 kg and σE¼.01 kg.
184. Let Adenote the percentage of one constituent in a randomly selected rock specimen, and
letBdenote the percentage of a second constituent in that same specimen. Suppose Dand
Eare measurement errors in determining the values of AandBso that measured values are
X¼A+D andY¼B+E , respectively. Assume that measurement errors are independent of
each other and of actual values.
(a) Show that
Corr X;YðÞ ¼ Corr A;BðÞ /C1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Corr X1;X2 ðÞp
/C1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Corr Y1;Y2 ðÞp
where X1andX2are replicate measurements on the value of A, and Y1andY2are deﬁned
analogously with respect to B. What effect does the presence of measurement error have on
the correlation?
(b) What is the maximum value of Corr( X,Y) when Corr( X1,X2)¼.8100 and Corr( Y1,Y2)
¼.9025? Is this disturbing?
185. Let X1,...,Xnbe independent rvs with mean values μ1,...,μnand variances σ12,...,σn2.
Consider a function h(x1,...,xn), and use it to deﬁne a new rv Y¼h(X1,...,Xn). Under rather
general conditions on the hfunction, if the σis are all small relative to the corresponding μis, it
can be shown that E(Y)/C25h(μ1,...,μn) and
VarYðÞ /C25∂h
∂x1/C18/C192
/C1σ2
1þ/C1/C1/C1þ∂h
∂xn/C18/C192
/C1σ2
n
where each partial derivative is evaluated at ( x1,...,xn)¼(μ1,...,μn). Suppose three resistors
with resistances X1,X2,X3are connected in parallel across a battery with voltage X4. Then by
Ohm’s law, the current is
Y¼X41
X1þ1
X2þ1
X3/C18/C19
Letμ1¼10Ω,σ1¼1.0Ω,μ2¼15Ω,σ2¼1.0Ω,μ3¼20Ω,σ3¼1.5Ω,μ4¼120 V,
σ4¼4.0 V. Calculate the approximate expected value and standard deviation of the current
(suggested by “Random Samplings,” CHEMTECH , 1984: 696–697).348 4 Joint Probability Distributions and Their Applications
186. A more accurate approximation to E[h(X1,...,Xn)] in the previous exercise is
hμ1;...;μn ðÞ þ1
2σ2
1∂2h
∂x2
1/C18/C19
þ/C1/C1/C1þ1
2σ2
n∂2h
∂x2
n/C18/C19
Compute this for Y¼h(X1,X2,X3,X4) given in the previous exercise, and compare it to the
leading term h(μ1,...,μn).
187. Let Y1andYnbe the smallest and largest order statistics, respectively, from a random sample of
sizen.
(a) Use the result of Exercise 141 to determine the joint pdf of Y1andYn. (Your answer will
include the pdf fand cdf Fof the original random sample.)
(b) Let W1¼Y1andW2¼Yn/C0Y1(the latter is the sample range). Use the method of
Sect. 4.6to obtain the joint pdf of W1andW2, and then derive an expression involving
an integral for the pdf of the sample range.
(c) For the case in which the random sample is from a uniform distribution on [0, 1], carry out
the integration of (b) to obtain an explicit formula for the pdf of the sample range. [ Hint:
For the Unif[0, 1] distribution, what are fandF?]
188. Consider independent and identically distributed random variables X1,X2,X3,...where each Xi
has a discrete uniform distribution on the integers 0, 1, 2, ..., 9; that is, P(Xi¼k)¼1/10 for
k¼0, 1, 2, ..., 9. Now form the sum
Un¼Xn
i¼11
10ðÞiXi¼:1X1þ:01X2þ/C1/C1/C1þ :1ðÞnXn:
Intuitively, this is just the ﬁrst ndigits in the decimal expansion of a random number on the
interval [0, 1]. Show that as n!1 ,P(Un/C20u)!P(U/C20u) where U/C24Unif[0, 1] (this is
called convergence in distribution , the type of convergence involved in the CLT) by showing
that the moment generating function of Unconverges to the moment generating function of U.
[The argument for this appears on p. 52 of the article “A Few Counter Examples Useful in
Teaching Central Limit Theorems,” The American Statistician , Feb. 2013.]
189. The following example is based on “Conditional Moments and Independence” ( The American
Statistician , 2008: 219). Consider the following joint pdf of two rvs XandY:
fx;yðÞ ¼1
2πe/C0lnxðÞ2þlnyðÞ2½/C138 =2
xy1þsin 2πlnx ðÞ sin 2πlny ðÞ ½/C138 forx>0,y>0
(a) Show that the marginal distribution of each rv is lognormal. [ Hint: When obtaining the
marginal pdf of X, make the change of variable u¼ln(y).]
(b) Obtain the conditional pdf of Ygiven that X¼x. Then show that for every positive integer
n,E(YnjX¼x)¼E(Yn). [Hint: Make the change of variable ln( y)¼u+nin the second
integrand.]
(c) Redo (b) with XandYinterchanged.
(d) The results of (b) and (c) suggest intuitively that XandYare independent rvs. Are they in
fact independent?
190. Let X1,X2,...be a sequence of independent, but not necessarily identically distributed random
variables, and let T¼X1+/C1/C1/C1+Xn.Lyapunov’s Theorem states that the distribution of the
standardized variable ( T/C0μT)/σTconverges to a N(0, 1) distribution as n!1 , provided that4.11 Supplementary Exercises (154–192) 349
lim
n!1Xn
i¼1EjXi/C0μij3/C0/C1
σ3
T¼0
where μi¼E(Xi). This limit is sometimes referred to as the Lyapunov condition for
convergence.
(a) Assuming E(Xi)¼μiand Var( Xi)¼σi2, write expressions for μTandσT.
(b) Show that the Lyapunov condition is automatically met when the Xis are iid. [ Hint: Let
τ¼E(jXi/C0μij3), which we assume is ﬁnite, and observe that τis the same for every Xi.
Then simplify the limit.]
(c) Let X1,X2,...be independent random variables, with Xihaving an exponential distribution
with mean i. Show that X1+/C1/C1/C1+Xnhas an approximately normal distribution as
nincreases.
(d) An online trivia game presents progressively harder questions to players; speciﬁcally, the
probability of answering the ith question correctly is 1/ i. Assume any player’s successive
answers are independent, and let Tdenote the number of questions a player has right out of
the ﬁrst n. Show that Thas an approximately normal distribution for large n.
191. This exercise and the next complete our investigation of the Coupon Collector’s Problem begun
in the book’s Introduction. A box of a certain brand of cereal marketed for children is equally
likely to contain one of 10 different small toys. Suppose someone purchases boxes of this cereal
one by one, stopping only when all 10 toys have been obtained.
(a) After obtaining a toy in the ﬁrst box, let Y2be the subsequent number of boxes purchased
until a toy different from the one in the ﬁrst box is obtained. Argue that this rv has a
geometric distribution, and determine its expected value.
(b) Let Y3be the number of additional boxes purchased to get a third type of toy once two types
have been obtained. What kind of a distribution does this rv have, and what is its expected
value?
(c) Analogous to Y2andY3, deﬁne Y4,...,Y10as the numbers of additional boxes purchased to
get a new type of toy. Express the total number of boxes purchased in terms of the Yis and
determine its expected value.
(d) Determine the standard deviation of the total number of boxes purchased. [ Hint: TheYis
are independent.]
192. Return to the scenario described in the previous problem. Suppose an individual purchases
25 boxes of this cereal.
(a) Let X1¼1 if at least one type 1 toy is included in the 25 boxes and X1¼0 otherwise
(a Bernoulli rv). Determine E(X1).
(b) Deﬁne X2,...,X10analogously to X1for the other nine types of toys. Express the number
of different toys obtained from the 25 boxes in terms of the Xis and determine its expected
value.
(c) What happens to the expected value in (b) as the number of boxes purchased increases? As
the number of different toys available increases?
(d) Show that, for i6¼j,
Cov Xi;Xj/C0/C1
¼8
10/C18/C1925
/C09
10/C18/C1950
:
Then determine the variance of the number of different toys obtained from the 25 boxes by
applying Eq. ( 4.5) to the expression from part (b). [ Hint: Refer back to Example 4.19 for
the required method.]350 4 Joint Probability Distributions and Their Applications
The Basics of Statistical Inference5
The overarching objective of statistical inference is to draw conclusions (make inferences) based on
available sample data. In this chapter we generally assume that data have been acquired by observing
the values of a random sample X1,X2,...,Xn; recall from Sect. 4.5that a random sample consists of
rvs that are independent and have the same underlying probability distribution (what we also called
iid). For example, highway fuel efﬁciency of a certain type of vehicle might have a normal distribu-
tion with mean μand standard deviation σ. Then each observed fuel efﬁciency value would come
from this normal distribution, with the various observed values obtained independently of one
another—a normal random sample. Or the number of blemishes on a new type of DVD might have
a Poisson distribution with mean value μ.I fnof these disks were to be randomly selected and the
number of blemishes on each one counted, the result would be data from a Poisson random sample. In
either example, the values of the parameters would typically not be known to an investigator. The
sample data would then be used to draw some type of conclusion about these values.
In this chapter we introduce several different inferential procedures. The ﬁrst, point estimation ,
involves using the available data to obtain a single number that can be regarded as an educated guess
for the value of some parameter (“point” refers to the fact that a single number corresponds to a single
point on a number line). Thus we might offer up 31.2 mpg as a sensible estimate of population mean
fuel efﬁciency, or 0.8 as an estimate of the true mean number of blemishes per DVD. Section 5.1
introduces some general concepts of point estimation and methods for assessing the quality of an
estimate, while Sect. 5.2discusses a popular method for producing point estimates.
A point estimate by itself, being a single number, does not provide any information as to how close
the estimate might be to the value of the parameter being estimated. This deﬁciency can be remedied
by calculating an entire set of plausible values for the parameter of interest, called a conﬁdence
interval . For example, it might be reported with a high degree of conﬁdence—more precisely, a
conﬁdence level of 95%—that the true average breaking strength of hockey sticks made from a
certain type of graphite-Kevlar composite is estimated to be between 459.5 and 466.2 N. Later in the
chapter we consider conﬁdence intervals for a population mean and also a population proportion (e.g.,
the proportion of all college students who regularly text during class).
Rather than estimating the value of some parameter, we may wish to decide which of two
contradictory claims about the parameter is correct. Suppose, for example, that 1,000,000 signatures
have been submitted in support of putting a particular initiative on a statewide ballot. State law
requires that more than 500,000 of these signatures be valid. If we let pdenote the proportion of valid
signatures among those submitted, then the initiative qualiﬁes if p>.5 and does not qualify if p/C20.5.
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_5351
Because it is extremely tedious and time consuming to check all one million signatures, it is
customary to select a random sample, determine how many of those are valid, and then use the result
as a basis for deciding between the two contradictory hypotheses p>.5 and p/C20.5. In this chapter
we shall consider methods for “testing” hypotheses (that is, deciding which of two hypotheses is more
plausible) about a population mean and also a population proportion.
Thus far our paradigm for inference has been to regard a parameter such as μas having a ﬁxed but
unknown value. A different perspective, referred to as the Bayesian method, views any parameter
whose value is unknown as being a random variable with some type of “prior” probability distribu-
tion. Once sample data is available, Bayes’ theorem can be used to obtain the “posterior” distribution
of the parameter conditional on the observed data. Adherents of the Bayesian method of inference
then use this posterior distribution to draw some type of conclusion about the unknown parameter.
The last section of this chapter introduces Bayesian methodology.
5.1 Point Estimation
Recall that a parameter is a numerical characteristic of a probability distribution. Often the
distribution under consideration furnishes a model for how some variable is distributed in a popula-
tion of interest. Examples include the distribution of yield strength values in a population of building-
grade steel bars, or the distribution of time-to-recovery from a dental anesthetic in the conceptual
population of all individuals given the anesthetic (we say “conceptual” here because the population
includes both past and future recipients of the treatment). One parameter in the anesthetic scenario is
the population mean recovery time μ, while in the steel bar population an investigator might focus on
the parameter η.95, the 95th percentile of the distribution (i.e., the yield strength that separates the
strongest 5% of steel bars from the other 95%).
Statistical inference is frequently directed toward drawing some type of conclusions about one or
more parameters. To do so requires that an investigator obtain sample data from the underlying
distribution. If the sample consists of observations on some random variable X, we will denote the
number of sample observations (the sample size) by n, the ﬁrst observation by x1, the second by x2,
and so on, with the last observation represented by xn. The subscripts on xgenerally have no
relationship to the magnitudes of the observations. They are often listed in the order in which they
were acquired by an investigator. Conclusions (inferences) about the population distribution can then
be based on the computed values of various sample quantities.
DEFINITION
Astatistic is any random variable whose value can be computed from sample data.
Example 5.1 Zinfandel is a popular red wine varietal produced almost exclusively in California. It is
rather controversial among wine connoisseurs because its alcohol content varies rather substantially
from one producer to another. We went to the website klwines.com, randomly selected 10 from
among the 325 available zinfandels, and obtained the following values of alcohol content (%):
x1¼14:8x2¼14:5x3¼16:1x4¼14:2x5¼15:9
x6¼13:7x7¼16:2x8¼14:6x9¼13:8x10¼15:0
Here are examples of some statistics and their values calculated from the foregoing data:352 5 The Basics of Statistical Inference
(a) The sample mean /C22X, the arithmetic average of the nobservations:
/C22X¼X1þ/C1/C1/C1þ Xn
n¼X
Xi
n
We encountered /C22Xpreviously in Sect. 4.5; this is the most frequently used measure of center for
sample data. The calculated value of the sample mean for the given data is
/C22x¼X
xi
n¼14:8þ14:5þ...þ15:0
10¼148:8
10¼14:88
Another sample of 10 such wines might yield /C22x¼15:23, and yet another give /C22x¼14:70. Prior to
obtaining the data, there is uncertainty in what the value of the sample mean will be; hence we
think of it as a random variable.
(b) The value of the sample mean can be unduly inﬂuenced by even a single unusually large or small
observation, e.g., a sample of incomes that includes Bill Gates, or a sample of cities’ populations
that includes Shanghai. An alternative measure of center is the sample median eX: list the
nobservations in increasing order from smallest to largest; then if nis an odd number, the
median is the middle value in this ordered list [the ( n+ 1)/2th value in from either end], and if
nis even, the median is the average of the two middle values. Clearly several extreme values on
either end of the ordered list will have no impact on the median. The ordered observations in our
sample are
13.7 13.8 14.2 14.5 14.6 14.8 15.0 15.9 16.1 16.2
Because n¼10, the calculated value of the sample median is the average of the ﬁfth and sixth
largest values: ex¼14:6þ14:8 ðÞ =2¼14:70. Again, a second sample might result in ex¼15:15,
a third sample in ex¼14:95, and so on. Prior to obtaining data, there is uncertainty in what value
of the sample median will result, so the sample median is regarded as a random variable.
(c) The sample mean and sample median are both assessments of where the sample is centered—a
typical or representative value. Another important characteristic of data is the extent to which
the observations spread out about the center. The simplest measure of spread (dispersion,
variability) is the sample range W: the difference between the largest and smallest observations.
For our data, w¼16.2/C013.7¼2.5. A second sample might yield 14.1 and 15.8 as the
smallest and largest observations, giving w¼1.7. Clearly the value of the sample range varies
from one sample to another, so before data is available it is viewed as a random variable.
(d) In the context of simulation in Chaps. 2and3, we previously introduced another measure of
variability, the sample standard deviation S:
S¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n/C01Xn
i¼1Xi/C0/C22X ðÞ2s
(The sample variance is deﬁned as S2.) For our data, the observed value of the sample standard
deviation is
s¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
10/C01X10
i¼1xi/C014:88 ðÞ2vuut¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
914:8/C014:88 ðÞ2þ/C1/C1/C1þ 15:0/C014:88 ðÞ2hir
¼0:915
As with the previous examples of statistics, this value is particular to our data; a different random
sample of 10 zinfandel wines might provide s¼1.018, another s¼0.882, and so on. Thus,
prior to obtaining data, we regard Sas a random variable.5.1 Point Estimation 353
(e) Finally, consider the random variable
Z¼/C22X/C0μ
σ=ﬃﬃﬃnp,
which expresses the distance between the sample mean and its expected value μin standard
deviations (e.g., if z¼3, then the value of the sample mean is three standard deviations larger
than would be expected). This rv is nota statistic unless the values of μandσare known; without
those values, the sample does not provide enough information to calculate z. ■
5.1.1 Estimates and Estimators
When discussing general concepts and methods of inference, it is convenient to have a generic
symbol for the parameter of interest. We will use the Greek letter θfor this purpose. The objective of
point estimation is to select a single number, based on sample data, that represents a sensible value for
θ. Suppose, for example, that the parameter of interest is μ, the true average lifetime of batteries of a
certain type. A random sample of n¼3 batteries might yield observed lifetimes (hours) x1¼5.0,
x2¼6.4,x3¼5.9. The computed value of the sample mean lifetime is /C22x¼5:77, and it is reasonable
to regard 5.77 h as a plausible value of μ, our “best guess” for the value of μbased on the available
sample information.
DEFINITION
A point estimate of a parameter θis a single number that can be regarded as a sensible value for
θ. A point estimate is obtained by selecting a suitable statistic and computing its value from the
given sample data. The selected statistic is called the point estimator ofθ.
In the battery scenario just described, the point estimator (i.e., the statistic) used to obtain the point
estimate of μwas /C22X, and the point estimate of μwas 5.77. If the three observed lifetimes had instead
been x1¼5.6,x2¼4.5, and x3¼6.1, using the same estimator /C22Xwould have resulted in a different
estimate ,/C22x¼5:6þ4:5þ6:1 ðÞ =3¼5:40h.
The symbol ^θ(“theta hat”) is customarily used to denote the point estimate resulting from a given
sample; we shall also use it to denote the estimator, as using an uppercase ^Θis somewhat awkward to
write. Thus ^μ¼/C22Xis read as “the point estimator of μis the sample mean /C22X.” The statement “the point
estimate of μis 5.77 h” can be written concisely as ^μ¼/C22x¼5:77. Notice that in writing a statement
such as ^θ¼72:5, there is no indication of how this point estimate was obtained (i.e., what statistic
was used). It is recommended that both the estimator and the resulting estimate be reported.
Example 5.2 The National Health and Nutrition Examination Survey (NHANES) collects demo-
graphic, socioeconomic, dietary, and health-related information on an annual basis. Here is a sample
of 20 observations on HDL-cholesterol level (mg/dl) obtained from the 2009–2010 survey (HDL is
“good” cholesterol, and the higher the value, the lower the risk for heart disease):
35 49 52 54 65 51 51 47 86 36 46 33 39 45 39 63 95 35 30 48
Figure 5.1shows both a normal probability plot and a brief descriptive summary of the data.354 5 The Basics of Statistical Inference
(a) Let’s ﬁrst consider estimating the population mean HDL level μ. The natural estimator is of
course the sample mean /C22X. The resulting point estimate is
^μ¼/C22x¼X
xi
n¼35þ49þ/C1/C1/C1þ 48
20¼49:95 mg =dl
The NHANES data ﬁle contained 7846 HDL observations. We could regard our sample of size
20 as coming from the population consisting of these 7846 values. The population mean is then
known to be μ¼52.6 mg/dl, so our estimate of 49.95 is somewhat smaller than the value of the
parameter we are trying to estimate. We extracted a second sample of size 20 from the
population; for this sample, ^μ¼/C22x¼57:40, a substantial overestimate of μ.
(b) Now let’s consider estimating the population median η, the value that separates the smallest 50%
of all HDL levels in the population from the largest 50%. The natural statistic for estimating this
parameter is the sample median eXdescribed previously. The estimate here is the average of the
10th and 11th values in the ordered list of sample observations:
^η¼ex¼47þ48
2¼47:5m g =dl
This is somewhat smaller than the sample mean because the sample has somewhat of a positive
skew—values on the upper end stretch out more than do values on the lower end, and these pull
the mean rightward compared to the median. If for the moment we regard the NHANES data set
as constituting the population, the population median is 51.0 (again somewhat smaller than the
population mean because of a positive skew). Our estimate of 47.5 is also smaller than what we
are attempting to estimate (51.0). For the second sample alluded to in part (a), the sample median
was 57.0, an overestimate of the population median.
(c) To estimate the HDL population standard deviation σ, it is natural to use the sample standard
deviation Sas our point estimator. The resulting point estimate of σis
^σ¼s¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
20/C0135/C049:95 ðÞ2þ...þ48/C049:95 ðÞ2hir
¼16:81 mg =dl100 90 80 70 60 50 40 30 20 1099
95
90
80
70
60
50
40
30
20
10
5
1
HDLPercent
Count   Mean  StDev  SE Mean  Minimum     Q1  Median     Q3  Maximum   IQR
   20  49.95  16.81     3.76    30.00  36.75   47.50  53.50    95.00 16.75
Fig. 5.1 Normal probability plot and descriptive summary of the HDL sample5.1 Point Estimation 355
Roughly speaking, the sample SD describes the size of a typical deviation within the sample
from the sample mean. A second sample from the same population would almost surely give a
somewhat different value of s, and thus a different point estimate of σ.
(d) An HDL level of at least 60 mg/dl is considered desirable, as it corresponds to a signiﬁcantly
lower risk of heart disease. How can we estimate the proportion pof the population having an
HDL level of at least 60? If we think of a sample observation of at least 60 as being a “success,”
then a natural estimator of pis the sample proportion of successes:
^P¼# of successes in the sample
n
(We have encountered ^Pseveral times already, both in the context of simulation and of the
Central Limit Theorem.) Four of the 20 sample observations are at least 60. Thus our point
estimate is ^p¼4=20¼:20. That is, we estimate that 20% of the individuals in the population
have an HDL level of at least 60. If a second sample is selected, it may be that 7 of the
20 individuals have such a level. Use of the same estimator then gives the point estimate
7/20¼.35. Just as with the other estimators proposed in this example, the value of the estimator
^Pwill in general vary from one sample to another. ■
The foregoing example may have suggested that point estimation is deceptively straightforward:
once the parameter to be estimated is identiﬁed, use intuition to specify a suitable estimator (statistic)
and then just calculate. However, there are at least two major problems with this strategy. The ﬁrst is
that intuition may not be up to the task of identifying an estimator. For example, suppose a materials
engineer is willing to assume (based on subject matter expertise and an appropriate probability plot)
that the data she collected were sampled from a Weibull distribution. This distribution has two
parameters, αandβ, which appear in the Weibull pdf in a rather complicated way. Furthermore, the
mean μand standard deviation σboth involve the gamma function. So the sample mean and sample
SD estimate complicated functions of the two parameters; it is not at all obvious how to sensibly
estimate αandβ. In the next section we introduce a constructive method for producing estimators that
will generally be reliable.
The second problem with relying solely on intuition is that, in many situations, there are two or
more estimators for a particular parameter that could sensibly be used. For example, suppose an
investigator is quite convinced (again, by a combination of subject matter expertise and a probability
plot) that available data was generated by a normal distribution. A major objective of the investiga-
tion is to estimate the parameter μ. Since μis the mean value of the normal population distribution, it
certainly makes sense to use the sample mean /C22Xas its estimator. However, because any normal
density curve is symmetric, μis also the median of the normal population distribution. It is then
sensible to think of using the sample median eXas an estimator. Two other potential estimators of μare
themid-range , the average of the largest and smallest observations, and a trimmed mean , obtained by
eliminating a speciﬁed percentage of the values from each end of the ordered list and averaging those
values that remain.
As a second example of competing estimators, consider data resulting from a Poisson random
sample. This distribution has one parameter, μ, which is both the mean and the variance of the Poisson
model. So one sensible estimator of μis the sample mean, another is the sample variance, and a third
is the average of these two. The choice between competing estimators such as these cannot usually be
based on intuitive reasoning. Instead we need to introduce desirable properties for an estimator and
then try to ﬁnd one that satisﬁes the properties.356 5 The Basics of Statistical Inference
5.1.2 Assessing Estimators: Accuracy and Precision
When a particular statistic is selected to estimate an unknown parameter, two criteria often used to
assess the quality of that estimator are its accuracy and its precision. Loosely speaking, an estimator is
accurate if it has no systematic tendency, across repeated values of the estimator calculated from
different samples, to overestimate or underestimate the value of the parameter. An estimator is
precise if those same repeated values are “close together,” so that two statisticians using the same
estimator formula (but two different random samples) are liable to get similar point estimates.
The notions of accuracy and precision are made more rigorous by the following deﬁnitions.
DEFINITION
A point estimator ^θis said to be an unbiased estimator ofθifE^θ/C0/C1
¼θfor every possible value
ofθ.I f ^θis not unbiased, the difference E^θ/C0/C1
/C0θis called the bias of^θ.
The standard error of^θis its standard deviation, σ^θ¼SD ^θ/C0/C1
. If the standard error itself
involves unknown parameters whose values can be estimated, substitution of these estimates
intoσ^θyields the estimated standard error of^θ. The estimated standard error can be denoted
by either ^σ^θor by s^θ.
The bias of an estimator ^θquantiﬁes its accuracy be measuring how far, on the average, ^θdiffers
from θ. The standard error of ^θquantiﬁes its precision by measuring the variability of ^θacross
different possible realizations (i.e., different random samples). It is important to note that both bias
and standard error are properties of an estimator (the random variable), such as /C22X, and not of any
speciﬁc value or estimate ,/C22x.
Figure 5.2illustrates bias and standard error for three potential estimators of a population
parameter θ. Figure 5.2a shows the distribution of an estimator ^θ1whose expected value is very
qq1qpdf of q2
q2
qpdf of q3
q3b a
cpdf of q1
Fig. 5.2 Three potential types of estimators: ( a) accurate, but not precise; ( b) precise, but not accurate; ( c) both
accurate and precise5.1 Point Estimation 357
close to θbut whose distribution is quite dispersed. Hence, ^θ1has low bias but relatively high standard
error. In contrast, the distribution of ^θ2displayed in Fig. 5.2b is very concentrated but is “off target”:
the values of ^θ2across different random samples will systematically over-estimate θby a large
amount. So, ^θ2has low standard error but high bias. The “ideal” estimator is illustrated in Fig. 5.2c:^θ3
has a mean roughly equal to θ, so it has low bias, and it also has a relatively small standard error.
Example 5.3 Consider the scenario of Example 5.1, wherein a sample mean /C22Xfrom a random sample
ofn¼10 observations will be used to estimate the population mean alcohol content μof all zinfandel
wines. In Sect. 4.5, we showed that the expected value and standard deviation of /C22Xareμandσ=ﬃﬃﬃnp,
respectively, where σis the population standard deviation (i.e., the SD of the alcohol content of all
zinfandel wines). Hence, the bias of /C22Xin estimating μis
E/C22XðÞ /C0 μ¼μ/C0μ¼0
That is, /C22Xis an unbiased estimator of μ. This is true for any random sample and for any sample size,
n. The standard error of /C22Xis simply SD /C22XðÞ ¼ σ=ﬃﬃﬃnp¼σ=ﬃﬃﬃﬃﬃ
10p
; clearly the precision of /C22Xwould be
improved (i.e., the standard error reduced) by increasing the sample size n.
Since the value of σis almost always unknown, we can estimate the standard error of /C22Xby
^σ/C22X¼s=ﬃﬃﬃnp, where sdenotes the sample standard deviation, as we did in the context of simulations in
Sects. 2.8and3.8. For the random sample of 10 wines presented in Example 5.1, we have a point
estimate ^μ¼/C22x¼14:88 with an estimated standard error of s=ﬃﬃﬃnp¼0:915=ﬃﬃﬃﬃﬃ
10p
¼0:29. The latter
indicates that, based on the available data, we believe our estimate of μis liable to differ by about
/C60.29 from the actual value of μ. ■
Example 5.4 Consider once again estimating a population proportion of “successes” p(for example,
the proportion of all engineering graduates who have taken a statistics course, or the proportion of all
vehicle accidents in which cell phone use was not a factor). The natural estimator of pis the sample
proportion of successes ^P¼X=n, where Xdenotes the number of successes in the sample. Using the
fact that X~ Bin( n,p), we showed in Sect. 2.4that the mean and standard error of ^Pare
E^P/C0/C1
¼pand SD ^P/C0/C1
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1/C0pðÞ
nr
The ﬁrst equation tells us that ^Pis an unbiased estimator for p, and that this is true no matter the
sample size. As for the standard error, since pis unknown (else why estimate?), we substitute ^p¼x=n
into σ^P, yielding the estimated standard error ^σ^P¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1/C0^pðÞ =np
. This was used several times in the
context of simulation in earlier chapters, and we will see this expression again in Sect. 5.5. When n¼25
and ^p¼:6, this gives ^σ^P¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:6ðÞ:4ðÞ=25p
¼:098. Alternatively, since the largest value of p(1/C0p)i s
attained when p¼.5, an upper bound on the standard error isﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:5ðÞ:5ðÞ=np
¼1=2ﬃﬃﬃnpðÞ . ■
Example 5.5 The time a customer spends in service after waiting in a queue is often modeled with an
exponential distribution. Recall that the exponential model has a single parameter, λ, and that the
mean of the exponential distribution is 1/ λ. Thus, since λ¼1/μ, a reasonable estimator of λmight be358 5 The Basics of Statistical Inference
^λ¼1
/C22X
where /C22Xis the average of a random sample of wait times X1,...,Xnfrom the aforementioned single-
server queue. How accurate is ^λas an estimator of λ? How precise is it?
It can be shown (Exercise 11) that the mean and variance of ^λ¼1=/C22Xare
E^λ/C0/C1
¼nλ
n/C01and Var ^λ/C0/C1
¼n2λ2
n/C01ðÞ2n/C02ðÞ
The bias of ^λas an estimator of λis therefore E^λ/C0/C1
/C0λ¼λ=n/C01ðÞ . We see that ^λis not an
unbiased estimator of λ; since λ/(n/C01)>0, we say that ^λisbiased high , meaning it will tend to
systematically over-estimate λ. Thankfully, the bias approaches 0 as nincreases.
The standard error of ^λis the square root of the variance expression above. It can be estimated by
replacing the unknown λwith the calculated value of ^λ,1=/C22x, resulting in
^σ^λ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n2^λ2
n/C01ðÞ2n/C02ðÞs
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n2
n/C01ðÞ2n/C02ðÞ /C22x2s
■
As mentioned before, in some situations more than one estimator might be proposed for the same
parameter. It is sometimes the case in such scenarios that one estimator is more accurate (lower bias)
while the other is more precise (smaller standard error). Which consideration should prevail?
PRINCIPLE OF UNBIASED ESTIMATION
When choosing among several different estimators of θ, select one that is unbiased.
According to this principle, the sample mean /C22Xwould be selected as an estimator of a population
mean μover any biased estimator (see Example 5.3), and a sample proportion ^Pis preferred over any
biased estimator of a true proportion p(Example 5.4). In contrast, the estimator ^λof Example 5.5 is
notunbiased; if we can ﬁnd some other estimator of λwhich is unbiased, we would choose this latter
estimator over ^λ.
If two or more estimators of a parameter are unbiased, then naturally one selects the estimator
among them with the smallest standard error. For example, we previously proposed several different
estimators for the mean μof a normal distribution. When the sampled distribution is continuous and
symmetric, all four of the proposed estimators—the sample mean /C22X, the sample median eX, the
midrange, and a trimmed mean—are unbiased estimators of μ(provided μis ﬁnite). Using some
sophisticated mathematics, it can be shown that when drawing from a normal distribution, /C22Xhas the
smallest standard error not only among these four estimators but in fact among allunbiased
estimators of μ. For this reason, /C22Xis referred to as the minimum variance unbiased estimator
(MVUE) of μwhen sampling from a normally distributed population.
An alternative approach to the Principle of Unbiased Estimation is to combine the considerations
of accuracy (bias) and precision (standard error) into a single measure, which can be achieved through
themean squared error ; see Exercise 22. Under this method, the estimator with the smallest mean
squared error is selected, even if it is biased and other estimators are not.5.1 Point Estimation 359
5.1.3 Exercises: Section 5.1(1–23)
1. A study of children’s intelligence and behavior included the following IQ data for 33 ﬁrst-graders
that participated in the study.
82 96 99 102 103 103 106 107 108 108 108
108 109 110 110 111 113 113 113 113 115 115
118 118 119 121 122 122 127 132 136 140 146
(a) Calculate a point estimate of the mean IQ for the conceptual population of all ﬁrst graders in
this school, and state which estimator you used.
(b) Calculate a point estimate of the IQ value that separates the lowest 50% of all such students
from the highest 50%, and state which estimator you used.
(c) Calculate and interpret a point estimate of the population standard deviation σ. Which
estimator did you use?
(d) Calculate a point estimate of the proportion of all such students whose IQ exceeds 100.
[Hint: Think of an observation as a “success” if it exceeds 100.]
(e) Calculate a point estimate of the population coefﬁcient of variation , 100 σ/μ, and state what
estimator you used.
2. A sample of 20 students who had recently taken elementary statistics yielded the following
information on brand of calculator owned (T ¼Texas Instruments, H ¼Hewlett-Packard, C ¼
Casio, S ¼Sharp):
TTHTCTTSCH
SSTHCTTTHT
(a) Estimate the true proportion of all such students who own a Texas Instruments calculator.
(b) Of the 10 students who owned a TI calculator, 4 had graphing calculators. Estimate the
proportion of students who do not own a TI graphing calculator.
3. Consider the following sample of observations on coating thickness for low-viscosity paint
(“Achieving a Target Value for a Manufacturing Process: A Case Study,” J. Qual. Technol. ,
1992: 22–26):
.83 .88 .88 1.04 1.09 1.12 1.29 1.31
1.48 1.49 1.59 1.62 1.65 1.71 1.76 1.83
Assume that the distribution of coating thickness is normal (a normal probability plot strongly
supports this assumption).
(a) Calculate a point estimate of the mean value of coating thickness, and state which estimator
you used.
(b) Calculate a point estimate of the median of the coating thickness distribution, and state
which estimator you used.
(c) Calculate a point estimate of the value that separates the largest 10% of all values in the
thickness distribution from the remaining 90%, and state which estimator you used. [ Hint:
Express what you are trying to estimate in terms of μandσ.]
(d) Estimate P(X<1.5), i.e., the proportion of all thickness values less than 1.5. [ Hint: If you
knew the values of μand σ, you could calculate this probability. These values are not
available, but they can be estimated.]
(e) What is the estimated standard error of the estimator that you used in (b)?
4. The data set mentioned in Exercise 1 also includes these third-grade IQ observations for males:360 5 The Basics of Statistical Inference
117 103 121 112 120 132 113 117 132
149 125 131 136 107 108 113 136 114
and females:
114 102 113 131 124 117 120 90
114 109 102 114 127 127 103
Prior to obtaining data, denote the male values by X1,...,Xmand the female values by Y1,...,Yn.
Suppose that the Xis constitute a random sample from a distribution with mean μ1and standard
deviation σ1and that the Yis form a random sample (independent of the Xis) from another
distribution with mean μ2and standard deviation σ2.
(a) Show that /C22X/C0/C22Yis an unbiased estimator of μ1/C0μ2. Then calculate the estimate for the
given data.
(b) Use rules of variance from Chap. 4to obtain an expression for the standard error of the
estimator in (a), and then compute the estimated standard error.
(c) Calculate a point estimate of the ratio σ1/σ2of the two standard deviations.
(d) Suppose one male third-grader and one female third-grader are randomly selected. Calculate
a point estimate of the variance of the difference X/C0Ybetween their IQs.
5. As an example of a situation in which several different statistics could reasonably be used to
calculate a point estimate, consider a population of Ninvoices. Associated with each invoice is its
“book value,” the recorded amount of that invoice. Let τdenote the total book value, a known
amount. Some of these book values are erroneous. An audit will be carried out by randomly
selecting ninvoices and determining the audited (correct) value for each one. Suppose that the
sample gives the following results (in dollars).
Invoice
1234 5
Book value 300 720 526 200 127
Audited value 300 520 526 200 157
Error 0 200 0 0 /C030
Let /C22X¼the sample mean book value, /C22Y¼the sample mean audited value, and /C22D¼the sample
mean error. Propose three different statistics for estimating the total audited (i.e., correct) value
θ—one involving just Nand /C22X, another involving N,τ, and /C22D, and the last involving τand /C22X=/C22Y.
Then calculate the resulting estimates when N¼5000 and τ¼1,761,300 (The article “Statisti-
cal Models and Analysis in Auditing,” Statistical Science , 1989: 2–33 discusses properties of
these estimators.)
6. Consider the accompanying observations on stream ﬂow (thousands of acre-feet) recorded at a
station in Colorado for the period April 1–August 31 over a 31-year span (from an article in the
1974 volume of Water Resources Res .).
127.96 210.07 203.24 108.91 178.21
285.37 100.85 89.59 185.36 126.94
200.19 66.24 247.11 299.87 109.64
125.86 114.79 109.11 330.33 85.54
117.64 302.74 280.55 145.11 95.36
204.91 311.13 150.58 262.09 477.08
94.33
An appropriate probability plot supports the use of the lognormal distribution (see Sect. 3.5)a sa
reasonable model for stream ﬂow.5.1 Point Estimation 361
(a) Estimate the parameters of the distribution. [ Hint: Remember that Xhas a lognormal
distribution with parameters μand σif ln( X) is normally distributed with mean μand
standard deviation σ.]
(b) Use the estimates of part (a) to calculate an estimate of the expected value of stream ﬂow.
[Hint: What is the expression for E(X)?]
7.(a) A random sample of 10 houses in a particular area, each of which is heated with natural gas,
is selected and the amount of gas (therms) used during the month of January is determined
for each house. The resulting observations are 103, 156, 118, 89, 125, 147, 122, 109, 138,
99. Let μdenote the average gas usage during January by all houses in this area. Compute a
point estimate of μ.
(b) Suppose there are 10,000 houses in this area that use natural gas for heating. Let τdenote the
total amount of gas used by all of these houses during January. Estimate τusing the data of
(a). What estimator did you use in computing your estimate?
(c) Use the data in (a) to estimate p, the proportion of all houses that used at least 100 therms.
(d) Give a point estimate of the population median usage based on the sample of (a). What
estimator did you use?
8. In a random sample of 80 components of a certain type, 12 are found to be defective.
(a) Give a point estimate of the proportion of all such components that are notdefective.
(b) A system is to be constructed by randomly selecting two of these components and
connecting them in series, as shown here.
The series connection implies that the system will function if and only if neither component
is defective (i.e., both components work properly). Estimate the proportion of all such
systems that work properly. [ Hint: Ifpdenotes the probability that a component works
properly, how can P(system works) be expressed in terms of p?]
(c) Let ^pbe the sample proportion of successes. Is ^p2an unbiased estimator for p2?[Hint: For
any rv Y,E(Y2)¼Var(Y)+[E(Y)]2.]
9. Each of 150 newly manufactured items is examined and the number of scratches per item is
recorded (the items are supposed to be free of scratches), yielding the following data:
Number of scratches per item 01234 5 6 7
Observed frequency 18 37 42 30 13 7 2 1
LetX¼the number of scratches on a randomly chosen item, and assume that Xhas a Poisson
distribution with parameter μ.
(a) Find an unbiased estimator of μand compute the estimate for the data.
(b) What is the standard error of your estimator? Compute the estimated standard error.
[Hint:σX2¼μforXPoisson.]
10. Let X1,...,Xnbe a random sample from a distribution with mean μand variance σ2.
(a) Show thatPXi/C0/C22X ðÞ2¼PX2
i/C0/C1
/C0n/C22X2.
(b) Show that EPX2
i/C0/C1
¼n(μ2+σ2). [Hint: Use linearity of expectation, along with the relation
E(Y2)¼Var(Y)+[E(Y)]2.]
(c) Show that E/C0
n/C22X2/C1
¼nμ2þσ2.[Hint: Apply the relation given in the previous hint, but this
time to Y¼/C22X.]
(d) Combine parts (a)–(c) to show that S2is an unbiased estimator of σ2.362 5 The Basics of Statistical Inference
(e) Does it follow that the sample standard deviation, S, of a random sample is an unbiased
estimator of σ? Why or why not?
11. Example 5.5 considered the estimator ^λ¼1=/C22Xfor the unknown parameter λof an exponential
distribution, based on a random sample X1,X2,...,Xnfrom that distribution.
(a) Show using a moment generating function argument that /C22Xhas a gamma distribution, with
parameters α¼nandβ¼1/(nλ).
(b) Find the expected value of ^λ.[Hint: The goal is to ﬁnd E(1/Y), where Y~ gamma( n,1 / (nλ)).
Use the gamma pdf to determine this expected value.]
(c) Find the variance of ^λ.[Hint: Now ﬁnd E(1/Y2). Then apply the variance shortcut formula.]
12. Using a long rod that has length μ, you are going to lay out a square plot in which the length of
each side is μ. Thus the area of the plot will be μ2. However, you do not know the value of μ,s o
you decide to make nindependent measurements X1,X2,...Xnof the length. Assume that each Xi
has mean μ(unbiased measurements) and variance σ2.
(a) Show that /C22X2is not an unbiased estimator for μ2.[Hint: Apply the hint from Exercises 8 and
10 with Y¼/C22X.]
(b) For what value of kis the estimator /C22X2/C0kS2unbiased for μ2?[Hint: Compute E/C22X2/C0kS2/C0/C1
,
using the result of Exercise 10(d).]
13. Of n1randomly selected male smokers, X1smoked ﬁlter cigarettes, whereas of n2randomly
selected female smokers, X2smoked ﬁlter cigarettes. Let p1andp2denote the probabilities that a
randomly selected male and female, respectively, smoke ﬁlter cigarettes.
(a) Show that ( X1/n1)/C0(X2/n2) is an unbiased estimator for p1/C0p2.[Hint: What type of rvs
areX1andX2?]
(b) What is the standard error of the estimator in (a)?
(c) How would you use the observed values x1andx2to estimate the standard error of your
estimator?
(d) If n1¼n2¼200, x1¼127, and x2¼176, use the estimator of (a) to obtain an estimate of
p1/C0p2.
(e) Use the result of (c) and the data of (d) to estimate the standard error of the estimator.
14. Suppose a certain type of fertilizer has an expected yield per acre of μ1with variance σ2, whereas
the expected yield for a second type of fertilizer is μ2with the same variance σ2. Let S12andS22
denote the sample variances of yields based on sample sizes n1andn2, respectively, of the two
fertilizers. Use the result of Exercise 10(d) to show that the pooled (combined) estimator
^σ2¼n1/C01 ðÞ S2
1þn2/C01 ðÞ S2
2
n1þn2/C02
is an unbiased estimator of σ2.
15. Consider a random sample X1,...,Xnfrom the pdf
fx;θðÞ ¼ :51þθx ðÞ /C0 1/C20x/C201
where /C01/C20θ/C201 (this distribution arises in particle physics). Show that ^θ¼3/C22Xis an unbiased
estimator of θ.[Hint: First determine μ¼EXðÞ ¼ E/C22XðÞ.]
16. A sample of ncaptured jet ﬁghters results in serial numbers x1,x2,x3,...,xn. The CIA knows that
the aircraft were numbered consecutively at the factory starting with αand ending with β, so that
the total number of planes manufactured is β/C0α+ 1 (e.g., if α¼17 and β¼29, then
29/C017 + 1 ¼13 planes having serial numbers 17, 18, 19, ..., 28, 29 were manufactured).5.1 Point Estimation 363
However, the CIA does not know the values of αorβ. A CIA statistician suggests using the
estimator max( Xi)/C0min(Xi) + 1 to estimate the total number of planes manufactured.
(a) If n¼5,x1¼237, x2¼375, x3¼202, x4¼525, and x5¼418, what is the
corresponding estimate?
(b) Under what conditions on the sample will the value of the estimate be exactly equal to the
true total number of planes? Will the estimate ever be larger than the true total? Do you think
the estimator is unbiased for estimating β/C0α+ 1? Explain in one or two sentences.
(A similar method was used to estimate German tank production in World War II.)
17. Let X1,X2,...,Xnrepresent a random sample from a Rayleigh distribution with pdf
fx;θðÞ ¼x
θe/C0x2=2θðÞx>0
(a) It can be shown that E(X2)¼2θ. Use this fact to construct an unbiased estimator of θbased
onPX2
i(and use rules of expected value to show that it is unbiased).
(b) Estimate θfrom the following measurements of blood plasma beta concentration
(in pmol/L) for n¼10 men.
16.88 10.23 4.59 6.66 13.68
14.23 19.87 9.40 6.51 10.95
18. Suppose the true average growth μof one type of plant during a 1-year period is identical to that
of a second type, but the variance of growth for the ﬁrst type is σ2, whereas for the second type
the variance is 4 σ2. Let X1,...,Xmbemindependent growth observations on the ﬁrst type
[soE(Xi)¼μ, Var( Xi)¼σ2], and let Y1,...,Ynbenindependent growth observations on
the second type [ E(Yi)¼μ, Var( Yi)¼4σ2].
Let cbe a numerical constant and consider the estimator ^μ¼c/C22Xþ1/C0cðÞ /C22Y. For
anycbetween 0 and 1, this is a weighted average of the two sample means, e.g., :7/C22Xþ:3/C22Y.
(a) Show that for any cthe estimator is unbiased.
(b) For ﬁxed mandn, what value cminimizes the standard error of ^μ?[Hint: The estimator is a
linear combination of the two sample means and these means are independent. Once you
have an expression for the variance, differentiate with respect to c.]
19. In Chap. 2, we deﬁned a negative binomial rv as the number of trials required to achieve the rth
success in a sequence of independent and identical success/failure trials. The probability mass
function (pmf) of Xis
nb x ;r;pðÞ ¼x/C01
r/C01/C18/C19
pr1/C0pðÞx/C0rx¼r,rþ1,rþ2,...
0 otherwise8
<
:
(a) Suppose that r/C212. Show that
^P¼r/C01ðÞ =X/C01ðÞ
is an unbiased estimator for p.[Hint: Write out E^P/C0/C1
as a sum, then make the substitutions
y¼x/C01 and s¼r/C01.]
(b) A reporter wishing to interview ﬁve individuals who support a certain candidate begins
asking people whether ( S) or not ( F) they support the candidate. If the sequence of responses
isSFFSFFFSSS , estimate p¼the true proportion who support the candidate.364 5 The Basics of Statistical Inference
20. Suppose that X, the reaction time to a stimulus, has a uniform distribution on the interval from
0 to an unknown upper limit θ. An investigator wants to estimate θon the basis of a random
sample X1,X2,...,Xnof reaction times. Consider two possible estimators:
^θ1¼max X1;...;Xn ðÞ ^θ2¼2/C22X
(a) The following observed reaction times, in seconds, are for a sample of n¼5 subjects:
x1¼4.2,x2¼1.7,x3¼2.4,x4¼3.9,x5¼1.3. Calculate a point estimate of θbased on ^θ1
and a point estimate of θbased on ^θ2.
(b) The techniques of Sect. 4.9imply that the pdf of ^θ1isf(y)¼nyn/C01/θnfor 0/C20y/C20θ(we’re
using yas the argument instead of ^θ1so that the notation is less confusing). Use this to obtain
the mean and variance of ^θ1.
(c) Is ^θ1an unbiased estimator of θ? Explain why this is reasonable. [ Hint: If the population
maximum is θ, what must be true of the sample maximum?]
(d) The mean and variance of a uniform distribution on [0, θ] are θ/2 and θ2/12, respectively.
Use these and the properties of /C22Xto ﬁnd the mean and variance of ^θ2.
(e) If a statistician elected to apply the Principle of Unbiased Estimation, which estimator
would she select? Why?
(f) Find a constant ksuch that ^θ3¼k/C1^θ1is unbiased for θ, and compare the standard error of ^θ3
to the standard error of ^θ2.
21. An investigator wishes to estimate the proportion of students at a certain university who have
violated the honor code. Having obtained a random sample of nstudents, she realizes that asking
each, “Have you violated the honor code?” will probably result in some untruthful responses.
Consider the following scheme, called a randomized response technique. The investigator
makes up a deck of 100 cards, of which 50 are of Type I and 50 are of Type II.
Type I: Have you violated the honor code (yes or no)?
Type II: Is the last digit of your telephone number a 0, 1, or 2 (yes or no)?
Each student in the random sample is asked to mix the deck, draw a card, and answer the
resulting question truthfully. Because of the irrelevant question on Type II cards, a yes response
no longer stigmatizes the respondent, so we assume that responses are truthful. Let pdenote the
proportion of honor-code violators (i.e., the probability of a randomly selected student being a
violator), and let λ¼P(yes response). Then λandpare related by λ¼.5p+ (.5)(.3).
(a) Let Ydenote the number of yes responses, so Y~ Bin( n,λ). Thus Y/nis an unbiased
estimator of λ. Derive an estimator for pbased on Y.I fn¼80 and y¼20, what is your
estimate? [ Hint: Solve λ¼.5p+ .15 for pand then substitute Y/nforλ.]
(b) Use the fact that E(Y/n)¼λto show that your estimator is unbiased for p.
(c) If there were 70 Type I and 30 Type II cards, what would be your estimator for p?
22. The mean squared error of an estimator ^θis deﬁned by
MSE ^θ/C0/C1
¼E ^θ/C0θ/C0/C1 2hi
(a) Show that MSE ^θ/C0/C1
¼E^θ/C0/C1
/C0θ/C2/C3 2þVar ^θ/C0/C1
by expanding out the quadratic expression
“inside” the expected value operation in the deﬁnition of MSE and then using linearity of
expectation.
(b) If ^θis an unbiased estimator of the parameter θ, how does MSE( ^θ) simplify?
(c) Refer back to Example 5.5. Determine the mean squared error of the estimator ^λusing the
mean and variance expressions provided in that example.5.1 Point Estimation 365
(d) Consider an alternative estimator, ^λa, deﬁned by
^λa¼n/C01X
Xi¼n/C01
n/C11
/C22X¼n/C01
n^λ
Obtain the mean, variance, and MSE of ^λa.[Hint: Use rescaling properties.]
(e) Which of the two estimators, ^λor^λa, is preferable? Explain your reasoning.
23. Return to the problem of estimating a population proportion p, and consider the following
adjusted estimator:
^Pa¼Xþﬃﬃﬃﬃﬃﬃﬃﬃ
n=4p
nþﬃﬃﬃnp
The justiﬁcation for this estimator comes from the Bayesian approach to point estimation to be
introduced in Sect. 5.6.
(a) Determine the mean, variance, and mean squared error of this estimator. What do you ﬁnd
interesting about this MSE?
(b) Compare the MSE of this estimator to the MSE of the usual estimator ^P(the sample
proportion).
5.2 Maximum Likelihood Estimation
The point estimators introduced in Sect. 5.1were obtained via intuition and/or educated guesswork.
We now introduce a “constructive” method for obtaining point estimators: the method of maximum
likelihood. By constructive we mean that the general deﬁnition of a maximum likelihood estimator
suggests explicitly how to obtain the estimator in any speciﬁc problem.
The method of maximum likelihood was ﬁrst introduced by R. A. Fisher, a geneticist and
statistician, in the 1920s. Most statisticians recommend this method, at least when the sample size
is large, since the resulting estimators have certain desirable efﬁciency properties (see the proposition
on large-sample behavior toward the end of this section).
Example 5.6 The best protection against hacking into an online account is to use a password that has
at least eight characters containing upper and lower case letters, a numeral, and a special character.
Suppose that ten individuals who have a certain type of email account are selected, and it is found that
the ﬁrst, third, and tenth individuals have such strong protection, whereas the others do not (the
January 2012 issue of Consumer Reports reported that only 25% of individuals surveyed used a strong
password). Let p¼P(strong protection), i.e., pis the proportion of all account holders having strong
protection. Deﬁne Bernoulli random variables X1,X2,...,X10by
Xi¼1 if the ith person has strong protection
0 if not/C26
i¼1, 2, ...10
Then for the obtained sample, X1¼X3¼X10¼1 and the other seven Xis are all zero. The
probability mass function of any particular Xiispxi1/C0pðÞ1/C0xi, which becomes pifxi¼1 and 1 /C0p
when xi¼0. Finally, the strengths of various passwords are presumably independent of one another,366 5 The Basics of Statistical Inference
so that the Xis are independent and their joint probability mass function is the product of the
individual pmfs. Thus the joint pmf evaluated at the observed Xisi s
p/C11/C0pðÞ /C1 p/C11/C0pðÞ /C1 1/C0pðÞ /C1 /C1 /C1 p¼p31/C0pðÞ7ð5:1Þ
Suppose that p¼.25. Then the probability of observing the sample that we actually obtained is
(.25)3(.75)7¼.002086. If instead p¼.50, then this probability is (.50)3(.50)7¼.000977. So, we are
more likely to observe this sample if p¼.25 than if p¼.50. For what value of pis the obtained
sample most likely to have occurred? That is, what value of pmaximizes the joint pmf in Eq. ( 5.1)?
Figure 5.3shows a graph of Eq. ( 5.1) as a function of p; i.e., a graph of the likelihood of our particular
sample as a function of the unknown population parameter. It appears that the graph reaches its peak
above p¼.3, which is the proportion of strong passwords in the sample. The second ﬁgure shows a
graph of the natural logarithm of Eq. ( 5.1); since ln[ g(u)] is a strictly increasing function of g(u),
ﬁnding uto maximize the function g(u) is the same as ﬁnding uto maximize ln[ g(u)].
We can verify our visual impression by using calculus to ﬁnd the value of pthat maximizes
Eq. (5.1). Working with the natural log of the joint pmf is often easier than working with the joint pmf
itself, since the joint pmf is typically a product so its logarithm will be a sum. Here
lnp31/C0pðÞ7hi
¼3l n pðÞ þ 7l n 1 /C0pðÞ
Thus
d
dplnp31/C0pðÞ7hi
¼d
dp3l n pðÞ þ 7l n 1 /C0pðÞ ½/C138 ¼3
pþ7
1/C0p/C01ðÞ ¼3
p/C07
1/C0p
The (/C01) comes from the chain rule in calculus. Equating this derivative to 0 and solving for
pgives 3(1 /C0p)¼7p, from which 3 ¼10pand so p¼3/10¼.30 as conjectured. That is, our point
estimate is ^p¼:30:It is called the maximum likelihood estimate because it is the parameter value that
maximizes the likelihood (joint pmf) of the observed sample. In general, the second derivative should
be examined to make sure a maximum has been obtained, but here this is obvious from the ﬁgure.
Suppose that rather than being told the condition of every password, we had only been informed
that three of the ten were strong. Then we would have the observed value of a binomial random
variable X¼the number of strong passwords. The pmf of Xis10
x/C18/C19
px1/C0pðÞ10/C0x. For x¼3, this.0010
.0005
0.0020
.0015.0025
0 .2 .4 .6 .8 1.0 0. 2. 4. 6. 8 1 . 0Likelihoodb a
p−5
−10
−15
−20
−25
−30
−35ln(likelihood)
p
Fig. 5.3 Likelihood and log likelihood plotted against p5.2 Maximum Likelihood Estimation 367
becomes10
3/C18/C19
p31/C0pðÞ7. The binomial coefﬁcient10
3/C18/C19
is irrelevant to the maximization, so the
value of pthat maximizes the likelihood of observing X¼3 is again ^p¼:30: ■
DEFINITION
LetX1,...,Xnhave a joint distribution (i.e., a joint pmf or pdf) that depends on a parameter θ
whose value is unknown. This joint distribution, regarded as a function of θ, is called the
likelihood function and denoted by L(θ). The maximum likelihood estimate (mle)^θis the
value of θthat maximizes the likelihood function.
Echoing the terminology from the previous section, we call ^θa maximum likelihood estimate if it’s
expressed in terms of our observed sample data and a maximum likelihood estimator if^θis regarded
as a function of the random variables X1,...,Xn.
In Example 5.6, the joint pmf of X1,...,X10became p3(1/C0p)7once the observed values of the Xis
were substituted. So, the likelihood function would be written L(p)¼p3(1/C0p)7. If we take the
perspective that our data consists of a single binomial observation, then LpðÞ ¼10
3/C18/C19
p31/C0pðÞ7.I n
either case, the value of pthat maximizes L(p)i s ^p¼:3.
The likelihood function tells us how likely the observed sample is as a function of the possible
parameter values. Maximizing the likelihood gives the parameter value for which the observed
sample is most likely to have been generated, that is, the parameter value that “agrees most closely”
with the observed data. Maximizing the likelihood is equivalent to maximizing the logarithm of the
likelihood, and the latter is typically computationally more straightforward.
Example 5.7 Suppose X1,...,Xnis a random sample from an exponential distribution with
parameter λ. Because of independence, the likelihood function is a product of the individual pdfs:
fx1;...;xn;λ ðÞ ¼ λe/C0λx1/C0/C1
/C1/C1/C1/C1/C1 λe/C0λxn/C0/C1
¼λne/C0λΣxi¼LλðÞ
The log of the likelihood function is
lnLλðÞ½/C138 ¼ nlnλðÞ /C0 λX
xi
Equating ( d/dλ)ln[L(λ)] to zero results in n/λ/C0Pxi¼0, or λ¼n=Pxi¼1=/C22x. Thus the mle is
^λ¼1=/C22X. As we saw in Example 5.5, ^λis unfortunately not an unbiased estimator, since
E1=/C22XðÞ 6¼1=E/C22XðÞ. ■
Example 5.8 In Chap. 2, we indicated that the Poisson distribution could be used for modeling the
number of events of some sort that occur in a two-dimensional region (e.g., the occurrence of
tornadoes during a particular time period). Assume that when the region Rbeing sampled has area
a(R), the number Xof events occurring in Rhas a Poisson distribution with mean λ/C1a(R), where λis
the expected number of events per unit area, and that nonoverlapping regions yield independent
Xs. (This is called a spatial Poisson process .)
Suppose an ecologist selects nnonoverlapping regions R1,...,Rnand counts the number of plants
of a certain species found in each region. The joint pmf (likelihood) is then368 5 The Basics of Statistical Inference
px1;...;xn;λ ðÞ ¼λ/C1aR 1ðÞ½/C138x1e/C0λ/C1aR 1ðÞ
x1!/C1/C1/C1/C1/C1λ/C1aR nðÞ½/C138xne/C0λ/C1aR nðÞ
xn!
¼aR 1ðÞ½/C138x1/C1/C1/C1/C1/C1 aR nðÞ½/C138xn/C1λΣxi/C1e/C0λΣaR iðÞ
x1!/C1/C1/C1/C1/C1 xn!¼LλðÞ
The log-likelihood is
lnLλðÞ½/C138 ¼X
xilnaR iðÞ½/C138 þ lnλðÞ /C1X
xi/C0λX
aR iðÞ /C0X
lnxi!ðÞ
Taking the derivative with respect to λand equating it to zero yields
0þX
xi
λ/C0X
aR iðÞ /C0 0¼0)λ¼X
xiX
aR iðÞ
The mle is then ^λ¼PXi=PaR iðÞ. This is intuitively reasonable because λis the true density
(plants per unit area), whereas ^λis the sample density: ∑Xiis the number of plants counted, and
∑a(Ri) is just the total area sampled. Because E(Xi)¼λ/C1a(Ri), the estimator is unbiased.
Sometimes an alternative sampling procedure is used. Instead of ﬁxing regions to be sampled, the
ecologist will select npoints in the entire region of interest and let yi¼the distance from the ith point
to the nearest plant. The cdf of Y¼distance to the nearest plant is
FYyðÞ ¼ P/C0
Y/C20y/C1
¼1/C0P/C0
Y>y/C1
¼1/C0Pno plants in a
circle of radius y/C18/C19
¼1/C0e/C0λπy2λπy2ðÞ0
0!¼1/C0e/C0λπy2
Taking the derivative of FY(y) with respect to yyields
fYy;λðÞ ¼2πλye/C0λπy2y/C210
0 otherwise/C26
If we now form the likelihood L(λ)¼fY(y1;λ)/C1/C1 /C1 /C1/C1fY(yn;λ), differentiate ln[ L(λ)], and so on, the
resulting mle is
^λ¼n
πX
Y2
i¼number of plants observed
total area sampled
which is also a sample plant density. It can be shown that in a sparse environment (small λ), the
distance method is in a certain sense better, whereas in a dense environment, the ﬁrst sampling
method is better. ■
The deﬁnition of maximum likelihood estimates can be extended in the natural way to distribu-
tional families that include two or more parameters. The mles of parameters θ1,...,θmare those
values ^θ1,...,^θmthat maximize the likelihood function L(θ1,...,θm).
Example 5.9 LetX1,...,Xnbe a random sample from a normal distribution, which includes the two
parameters μandσ. The likelihood function is5.2 Maximum Likelihood Estimation 369
fx1;...;xn;μ;σ ðÞ ¼1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πσ2p e/C0x1/C0μðÞ2=2σ2ðÞ/C1/C1/C1/C1/C11ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πσ2p e/C0xn/C0μðÞ2=2σ2ðÞ
¼2πσ2ðÞ/C0n=2e/C0P
xi/C0μðÞ2=2σ2ðÞ¼Lμ;σðÞ
so
lnLμ;σðÞ½/C138 ¼ /C0n
2ln 2πðÞ /C0 nlnσ/C01
2σ2X
xi/C0μðÞ2
To ﬁnd the maximizing values of μand σ, we must take the partial derivatives of ln( L) with
respect to both μandσ, equate them to zero, and solve the resulting two equations:
∂
∂μln/C2
Lμ;σðÞ/C3
¼/C02
2σ2X/C0
xi/C0μ/C1/C0
/C01/C1
¼1
σ2X/C0
xi/C0μ/C1
¼0
∂
∂σln/C2
Lμ;σðÞ/C3
¼/C0n
σþ1
σ3X/C0
xi/C0μ/C12¼0
The ﬁrst equation implies that ∑(xi/C0μ)¼0, from which ∑xi/C0nμ¼0 and ﬁnally μ¼Pxi=n¼/C22x. The mle of μis the sample mean, independent of what the mle of σturns out to
be. Solving the second equation for σyields σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPxi/C0μðÞ2=nq
; we must then substitute the
solution from the ﬁrst equation into this expression in order to get the simultaneous solution to the two
partial derivative equations. Thus the maximum likelihood estimators of the two parameters are
^μ¼/C22X ^σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃX
Xi/C0/C22X ðÞ2
ns
Notice that the mle of σisnotthe sample standard deviation, S, since the denominator in the latter
isn/C01 and not n. ■
Example 5.10 LetX1,...,Xnbe a random sample from a Weibull pdf
fx;α;βðÞ ¼α
βα/C1xα/C01/C1e/C0x=βðÞαx/C210
0 otherwise(
Writing the likelihood as Land log-likelihood as ln( L), then setting both partial derivatives
(∂/∂α)[ln(L)]¼0 and ( ∂/∂β)[ln(L)]¼0 yields the equations
α¼X
xα
i/C1lnxiðÞ/C2/C3
X
xα
i/C0X
lnxiðÞ
n"# /C01
β¼X
xα
i
n ! 1=α
These two equations cannot be solved explicitly to give general formulas for the mles ^αand ^β.
Instead, for each sample x1,...,xn, the equations must be solved using an iterative numerical
procedure.
The iterative mle computations can be done using statistical software. In Matlab, the command
wblﬁt(x) will return ^αand ^βassuming the data is stored in the vector x. The R command ﬁtdistr
(x,"weibull") performs the same estimation (the MASS package must be installed ﬁrst). As an
example, consider the following survival time data alluded to in Example 3.28:370 5 The Basics of Statistical Inference
152 115 109 94 88 137 152 77 160 165
125 40 128 123 136 101 62 153 83 69
A Weibull probability plot supports the plausibility of assuming that survival time has a Weibull
distribution. The maximum likelihood estimates of the Weibull parameters are ^α¼3:799 and
^β¼125:88. Figure 5.4shows the Weibull log likelihood as a function of both αandβ. The surface
near the top has a rounded shape, allowing the maximum to be found easily, but for some distributions
the surface can be much more irregular, and the maximum may be hard to ﬁnd.
Sometimes calculus cannot be used to obtain mles.
Example 5.11 Suppose the waiting time for a bus is uniformly distributed on [0, θ] and the results x1,
...,xnof a random sample from this distribution have been observed. Since f(x;θ)¼1/θfor
0/C20x/C20θand 0 otherwise,
LθðÞ ¼ fx1;...;xn;θ ðÞ ¼1
θn 0/C20x1/C20θ,...,0/C20xn/C20θ
0 otherwise8
<
:
As long as θ/C21max( xi), the likelihood is 1/ θn, which is positive, but as soon as θ<max( xi), the
likelihood drops to 0. This is illustrated in Fig. 5.5. Calculus will not work because the maximum of
the likelihood occurs at a point of discontinuity, but the ﬁgure shows that ^θ¼max xiðÞ. Thus if the
waiting times are 2.3, 3.7, 1.5, .4, and 3.2, then the mle is ^θ¼3:7. Note that the mle is biased (see
Exercise 20(b)).
Log likelihood
3.0−101−100−99
3.5
ab
4.04.5120125130135
Fig. 5.4 Weibull log likelihood for Example 5.10 ■
max(xi)qL(q)
Fig. 5.5 The likelihood function for Example 5.11 ■5.2 Maximum Likelihood Estimation 371
5.2.1 Some Properties of MLEs
In Example 5.9, we obtained the mle of σwhen the underlying distribution is normal. The mle of σ2,
as well as many other mles, can be easily derived using the following proposition.
PROPOSITION (MLE INVARIANCE PRINCIPLE)
Let ^θ1,^θ2,...,^θmbe the mles of the parameters θ1,θ2,...,θm. Then the mle of any function
h(θ1,θ2,...,θm) of these parameters is the function h^θ1;^θ2;...;^θm/C0/C1
, of the mles.
Proof For an intuitive idea of the proof, consider the special case m¼1, with θ1¼θ, and assume
thath(/C1) is a one-to-one function. On the graph of the likelihood as a function of the parameter θ, the
highest point occurs where θ¼^θ. Now consider the graph of the likelihood as a function of h(θ). In
the new graph the same heights occur, but the height that was previously plotted at θ¼ais now
plotted at h(θ)¼h(a), and the highest point is now plotted at hθðÞ ¼ h^θ/C0/C1
:Thus, the maximum
remains the same, but it now occurs at h^θ/C0/C1
. ■
Example 5.12 (Example 5.9 continued) In the case of a random sample from a normal pdf, the mles
ofμandσare ^μ¼/C22Xand ^σ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃX
Xi/C0/C22X ðÞ2=nq
. To obtain the mle of the function h(μ,σ)¼σ2,
substitute the mles into the function:
bσ2¼^σ2¼1
nX
Xi/C0/C22X ðÞ2
The mle of σ2is not the unbiased estimator (the sample variance S2; see Exercise 10), although
they are close unless nis quite small. Similarly, the mle of the population coefﬁcient of variation ,
deﬁned by h(μ,σ)¼100μ/σ, is 100 ^μ=^σ. ■
Example 5.13 (Example 5.10 continued) The mean value of an rv Xthat has a Weibull distribution is
μ¼β/C1Γ1þ1=α ðÞ
The mle of μis therefore ^μ¼^β/C1Γ1þ1=^α ðÞ , where ^αand ^βare the mles of αandβ. In particular, /C22X
is not the mle of μ, although it is an unbiased estimator. At least for large n,^μis a better estimator than
/C22X. ■
The method of maximum likelihood estimation has considerable intuitive appeal. The following
proposition provides additional rationale for the use of mles.
PROPOSITION
Under very general conditions on the joint distribution of the sample, when the sample size is
large, the maximum likelihood estimator of any particular θ
• Is highly likely to be close to θ(consistency);
• Is either unbiased or at least approximately unbiased E^θ/C0/C1
/C25θ/C2/C3
; and
• Has variance that is either as small or nearly as small as can be achieved by any unbiased
estimator.372 5 The Basics of Statistical Inference
Because of this result and the fact that calculus-based techniques can usually be used to derive the
mles (although often numerical methods, such as Newton–Raphson, are necessary), maximum
likelihood estimation is the most widely used estimation technique among statisticians. Obtaining
an mle, however, does require that the underlying distribution be speciﬁed. For example, the mle of
the mean value of a Weibull distribution is different from the mle of the mean value of a Gamma
distribution.
Suppose X1,X2,...,Xnis a random sample from a pdf f(x;θ) that is symmetric about θ, but the
investigator is unsure of the form of the ffunction. It is then desirable to use an estimator that is
robust , that is, one that performs well for a wide variety of underlying pdfs. One such estimator, called
anM-estimator , is based on a generalization of maximum likelihood estimation. Instead of
maximizing the log-likelihood ∑ln[f(x;θ)] for a speciﬁed f, one maximizes ∑ψ(xi;θ), where the
“objective function” ψis selected to yield an estimator with good robustness properties. The book by
David Hoaglin et al. (see the references) contains a good exposition on this subject.
5.2.2 Exercises: Section 5.2(24–36)
24. Let Xrepresent the error in making a measurement of a physical characteristic or property (e.g.,
the boiling point of a particular liquid). It is often reasonable to assume that E(X)¼0 and that
Xhas a normal distribution. Thus the pdf of any particular measurement error is
fxðÞ ¼1ﬃﬃﬃﬃﬃﬃﬃﬃ
2πθp e/C0x2=2θ
where θdenotes the population variance. Now suppose that nindependent measurements are
made, resulting in measurement errors X1¼x1,X2¼x2,...,Xn¼xn.
(a) Determine the likelihood function of θ.
(b) Find and simplify the log-likelihood function.
(c) Differentiate (b) to determine the mle of θ.
(d) The precision of a normal distribution is deﬁned to be τ¼1/θ. Find the mle of τ.
25. A random sample of nbike helmets manufactured by a company is selected. Let X¼the
number among the nthat are ﬂawed, and let p¼P(ﬂawed). Assume that only Xis observed,
rather than the sequence of Ss and Fs.
(a) Derive the maximum likelihood estimator of p.I fn¼20 and x¼3, what is the estimate?
(b) Is the estimator of (a) unbiased?
(c) If n¼20 and x¼3, what is the mle of the probability (1 /C0p)5that none of the next ﬁve
helmets examined is ﬂawed?
26. Let Xdenote the proportion of allotted time that a randomly selected student spends working on
a certain aptitude test. Suppose the pdf of Xis
fx;θðÞ ¼θþ1ðÞ xθ0/C20x/C201
0 otherwise(
where /C01<θ. A random sample of ten students yields data x1¼.92,x2¼.79,x3¼.90,
x4¼.65,x5¼.86,x6¼.47,x7¼.73,x8¼.97,x9¼.94,x10¼.77.
Obtain the maximum likelihood estimator of θ, and then compute the estimate for the
given data.5.2 Maximum Likelihood Estimation 373
27. Two different computer systems are monitored for a total of nweeks. Let Xidenote the number
of breakdowns of the ﬁrst system during the ith week, and suppose the Xis are independent and
drawn from a Poisson distribution with parameter μ1. Similarly, let Yidenote the number of
breakdowns of the second system during the ith week, and assume independence with each Yi
Poisson with parameter μ2. Derive the mles of μ1,μ2, and μ1/C0μ2.[Hint: Using independence,
write the joint pmf (likelihood) of the Xis and Yis together.]
28. Six Pepperidge Farm bagels were weighed, yielding the following data (grams):
117.6 109.5 111.6 109.2 119.1 110.8
(a) Assuming that the six bagels are a random sample and that weights are normally
distributed, estimate the true average weight and standard deviation of the weight using
maximum likelihood.
(b) Again assuming a normal distribution, estimate the weight below which 95% of all bagels
will have their weights. [ Hint: What is the 95th percentile in terms of μandσ? Now use the
invariance principle.]
(c) Suppose we choose another bagel and weigh it. Let X¼weight of the bagel. Use the given
data to obtain the mle of P(X/C20113.4). [ Hint:P(X/C20113.4) ¼Φ[(113.4 /C0μ)/σ].]
29. Refer to Exercise 25. Instead of selecting n¼20 helmets to examine, suppose we examine
helmets in succession until we have found r¼3 ﬂawed ones. If the 20th helmet is the third
ﬂawed one, what is the mle of p? Is this the same as the estimate in Exercise 25? Why or why
not?
30. Let X1,...,Xnbe a random sample from a gamma distribution with parameters αandβ.
(a) Derive the equations whose solution yields the maximum likelihood estimators of αandβ.
Do you think they can be solved explicitly?
(b) Show that the mle of μ¼αβis^μ¼/C22X.
31. Let X1,X2,...,Xnrepresent a random sample from the Rayleigh distribution with density
function given in Exercise 17.
(a) Determine the maximum likelihood estimator of θand then calculate the estimate for the
vibratory stress data given in that exercise. Is this estimator the same as the unbiased
estimator suggested in Exercise 17?
(b) Determine the mle of the median of the vibratory stress distribution. [ Hint: First express
the median ηin terms of θ.]
32. Consider a random sample X1,X2,...,Xnfrom the shifted exponential pdf
fx;λ;θðÞ ¼λe/C0λx/C0θðÞx/C21θ
0 otherwise(
Taking θ¼0 gives the pdf of the exponential distribution considered previously (with positive
density to the right of zero). An example of the shifted exponential distribution appeared in
Example 3.5, in which the variable of interest was time headway in trafﬁc ﬂow and θ¼.5 was
the minimum possible time headway.
(a) Obtain the maximum likelihood estimators of θandλ.
(b) If n¼10 time headway observations are made, resulting in the values 3.11, .64, 2.55, 2.20,
5.44, 3.42, 10.39, 8.93, 17.82, and 1.30, calculate the estimates of θandλ.374 5 The Basics of Statistical Inference
33. The article “A Model of Pedestrians’ Waiting Times for Street Crossings at Signalized
Intersections” ( Transportation Research , 2013: 17–28) suggested that under some circumstances
the distribution of waiting time Xcould be modeled with the following pdf:
fx;θ;τðÞ ¼θ
τ1/C0x=τ ðÞθ/C010/C20x<τ
0 otherwise8
<
:where θ>0
(a) Suppose we observe a random sample of waiting times X1,...,Xn, and assume that the value
of the parameter τis known. Find the mle of θ.
(b) Suppose instead that θis known but τis unknown. Determine an equation whose solution is
the mle of τ.
34. Twenty identical components are simultaneously tested. The lifetime distribution of each
is exponential with parameter λ. The experimenter then leaves the test facility unmonitored.
On his return 24 h later, the experimenter immediately terminates the test after noticing that
y¼15 of the 20 components are still in operation (so 5 have failed). Derive the mle of λ.[Hint:
LetY¼the number that survive 24 h. Then Y~Bin(n,p). What is the mle of p? Now notice that
p¼P(Xi/C2124), where Xiis exponentially distributed. This relates λtop, so the former can be
estimated once the latter has been.]
35. Consider randomly selecting nsegments of pipe and determining the corrosion loss (mm) in the
wall thickness for each one. Denote these corrosion losses by Y1,...,Yn. The article “A
Probabilistic Model for A Gas Explosion Due to Leakages in the Grey Cast Iron Gas Mains”
(Reliability Engr. and System Safety 2013:270–279) proposes a linear corrosion model Yi¼tiRi,
where tiis the age of the pipe and Ri, the corrosion rate, is exponentially distributed with
parameter λ. Obtain the maximum likelihood estimator of λ(the resulting mle appears in the
cited article). [ Hint: First determine the pdf of Yi.]
36. A method that is often used to estimate the size of a wildlife population involves performing a
capture/recapture experiment. In this experiment, an initial sample of Manimals is captured,
each of these animals is tagged, and the animals are then returned to the population. After
allowing enough time for the tagged individuals to mix into the population, another sample of
sizenis captured. With X¼the number of tagged animals in the second sample, the objective is
to use the observed xto estimate the population size N.
(a) What is the probability distribution of X?
(b) Set L(N) equal to the distribution speciﬁed in (a); this is the likelihood function. Since Ncan
only assume integer values, using calculus to maximize L(N) would present difﬁculties.
Instead, determine the mle of Nbe considering the ratio L(N)/L(N/C01). [Hint: the mle can
be found by determining when this ratio is greater than 1 or less than 1 (do you see why?).]
(c) If 200 ﬁsh are taken from a lake and tagged, then subsequently 100 ﬁsh are recaptured and
among the 100 there are 11 tagged ﬁsh, what is the mle of the size of the ﬁsh population in
this lake? Does your answer make intuitive sense?
5.3 Confidence Intervals for a Population Mean
A point estimate, because it is a single number, by itself provides no information about the precision
and reliability of estimation. Consider, for example, using the statistic /C22Xto calculate a point estimate
for the true average breaking strength of paper towels of a certain brand, and suppose that a particular5.3 Confidence Intervals for a Population Mean 375
random sample yields /C22x¼9322 :7g. Because of sampling variability, it is virtually never the case that
/C22x¼μ, and the point estimate alone says nothing about how close it might be to μ. An alternative to
reporting a single sensible value for the parameter being estimated is to calculate and report an entire
interval of plausible values—an interval estimate orconﬁdence interval (CI).
A conﬁdence interval is always calculated by ﬁrst selecting a conﬁdence level , which is a measure
of the degree of reliability of the interval. A conﬁdence interval with a 95% conﬁdence level for the
true average breaking strength might have a lower limit of 9162.5 g and an upper limit of 9482.9 g.
Then at the “95% conﬁdence level,” any value of μbetween 9162.5 and 9482.9 is plausible. A
conﬁdence level of 95% implies that 95% of all samples would give an interval that includes μ,o r
whatever other parameter is being estimated, and only 5% of all samples would yield an erroneous
interval. The most frequently used conﬁdence levels are 95, 99, and 90%. The higher the conﬁdence
level, the more strongly we believe that the value of the parameter being estimated lies within the
interval.
Information about the precision of an interval estimate is conveyed by the width of the interval. If
the conﬁdence level is high and the resulting interval is quite narrow, our knowledge of the value of
the parameter is reasonably precise. A very wide conﬁdence interval, however, gives the message that
there is a great deal of uncertainty concerning the value of what we are estimating. Figure 5.6shows
95% conﬁdence intervals for true average breaking strengths of two different brands of paper towels.
One of these intervals suggests precise knowledge about μ, whereas the other suggests a very wide
range of plausible values.
5.3.1 A Confidence Interval for a Normal Population Mean
For much of this section we will assume that the available data results from a random sample X1,X2,
...,Xnselected from a normal population distribution. The plausibility of assuming a normal
population distribution can of course be checked by examining a normal probability plot of the
data. Particularly when the sample size is small, the conﬁdence interval to be developed here should
not be used if the plot shows a substantial departure from a linear pattern. We’ll comment later on
what might be done in the presence of non-normality.
Recall from the previous chapter that as a consequence of our normality assumption, the sample
mean /C22Xalso is normally distributed, with mean value μ(the mean of the population from which the
sample was selected) and standard deviation σ=ﬃﬃﬃnp. We now standardize /C22Xto obtain a random variable
having a standard normal distribution:
Z¼/C22X/C0μ
σ=ﬃﬃﬃnp
Unfortunately this standardized variable cannot serve as a basis for deriving a conﬁdence interval
forμunless the value of the population standard deviation σhappens to be known. So instead let’s
consider the standardized variable obtained by replacing σinZby the sample standard deviation S.
Deﬁne a new random variable TbyBrand 1:
Brand 2:Strength
Strength()
()
Fig. 5.6 Conﬁdence intervals indicating precise (brand 1) and imprecise (brand 2) information about μ376 5 The Basics of Statistical Inference
T¼/C22X/C0μ
S=ﬃﬃﬃnp
It is important to contrast the behavior of Zin repeated sampling with that of T. The only
variability in Zfrom one sample to another is because /C22Xin the numerator varies in value. However,
there are two sources of sample-to-sample variability in T: both /C22Xin the numerator and Sin the
denominator. Because of this extra variation in T, it stands to reason that the distribution of Tshould
be more spread out than that of Z. That is, the density curve for Tshould be more spread out than the
standard normal curve.
At this point we need to introduce a new (to the reader) family of probability distributions that
describes how Tvaries from one sample to another. This is the family of tdistributions . The formula
for the density function that speciﬁes a tdistribution is quite complicated (see the reference by Devore
and Berk, where the formula and a derivation appear). Fortunately for our purpose we need only be
acquainted with some general properties.
PROPERTIES OF TDISTRIBUTIONS
1. Any particular tdistribution is obtained by specifying the value of a single parameter ν,
called the number of degrees of freedom (df) of the distribution. Any positive integer is a
possible value of ν, so there is a tdistribution with 1 df, another with 2 df, and so on.
2. Each tνdensity curve is bell shaped and centered at 0, just like the standard normal ( z) curve.
3. Each tνdensity curve is more spread out than the zcurve.
4. As νincreases, the spread of the tνcurve decreases (so the t1curve is the most spread out, the
t2curve is next most spread out, and so on).
5. As ν!1 , the sequence of tνcurves approaches the zcurve (for this reason, the zcurve is
often called the tcurve with df ¼1 ).
Figure 5.7shows several different tdensity curves and the zcurve to illustrate how the curves
compare and change as df increases.
Appendix Table A.5 displays what are called t critical values ; these are numbers on the horizontal
axis that capture certain central areas under tcurves. For example, looking down the left column to
ν¼24 and then over to the column headed 95%, we learn that 95% of the area under the tcurve with
24 df lies between /C02.064 and 2.064. Notice that in any particular column of the table, the numbers.2
.1
0.4
.3.5
−5 −3 −11 3 5t5 df
1 dfz20 dff (t) Fig. 5.7 Comparison
of several tcurves
and the zcurve5.3 Confidence Intervals for a Population Mean 377
decrease as we move down; this is because the spread of tcurves decreases as df increases. And the
numbers in any row increase from right to left because a larger central area is being captured. Also
note that toward the bottom of the table df skips from 30 to 40 to 60 to 120 to 1. Once past 30 df, the
tcurves do not change all that much, so it is not worth continuing to tabulate in increments of 1 df. For
an intermediate number of df, linear interpolation can be used to get a reasonable approximation, or
appropriate software will produce an exact value. Lastly, the tdistribution with an inﬁnite number of
df is actually the standard normal distribution. Thus the bottom row of the table contains standard
normal critical values ; for example, 95% of the area under the zcurve lies between /C01.96 and 1.96.
With information about tdistributions in hand, we are now ready for the key theoretical result on
which our conﬁdence interval will be based. This result was originally discovered in 1908 by William
Sealy Gosset, a statistician at the Guinness Brewery in Dublin, Ireland.
GOSSET’S THEOREM
LetX1,...,Xnbe a random sample from a normal population distribution having mean μ,w i t h
corresponding sample mean /C22Xand sample standard deviation S. Then the random variable
T¼/C22X/C0μ
S=ﬃﬃﬃnp
has a tdistribution with n/C01 degrees of freedom.
An intuitive justiﬁcation for degrees of freedom here is that although there are ndeviations
X1/C0/C22X,X2/C0/C22X,...,Xn/C0/C22Xfrom the sample mean, it is easily veriﬁed thatPXi/C0/C22X ðÞ ¼ 0:This
implies that any particular one of the deviations can be obtained from the other n/C01 deviations. For
example, in the case n¼5, if the ﬁrst four deviations are /C02, 5, 1, and /C08, then the last one must be
4 to produce a sum of zero. The number of df here is the number of “freely varying” deviations that
are inputs to the sample standard deviation.
Consider for the moment a sample size of n¼25, for which the standardized variable Tis based
on 24 df. Then 95% of the area under this tcurve lies between /C02.064 and 2.064. The foregoing
theorem then allows us to make the following probability statement:
P/C02:064</C22X/C0μ
S=ﬃﬃﬃﬃﬃ
25p <2:064 !
¼:95
Let’s now manipulate the inequalities inside the parentheses to isolate μin the middle. This
requires three steps: (1) multiply all three terms by S=ﬃﬃﬃnp, (2) subtract /C22Xfrom all three terms, and
(3) multiply through by /C01 to eliminate the negative sign in front of μ. The last step will reverse the
direction of each inequality, resulting in
/C22Xþ2:064Sﬃﬃﬃﬃﬃ
25p >μ>/C22X/C02:064Sﬃﬃﬃﬃﬃ
25p
These new inequalities are completely equivalent to those in the original probability statement, so
P /C22X/C02:064Sﬃﬃﬃﬃﬃ
25p <μ</C22Xþ2:064Sﬃﬃﬃﬃﬃ
25p/C18/C19
¼:95378 5 The Basics of Statistical Inference
To interpret this latter probability, think of obtaining sample after sample of size 25 from a normal
population distribution, calculating the sample mean and sample standard deviation for each one, and
then forming the lower limit /C22x/C02:064s=ﬃﬃﬃﬃﬃ
25p
and the upper limit /C22xþ2:064s=ﬃﬃﬃﬃﬃ
25p
. Both the center of
the interval ( /C22x) and its width will vary from sample to sample. In the long run, 95% of such samples
will result in the value of μbeing captured in between the lower limit and the upper limit—the long-
run capture percentage for the sequence of intervals is 95%. Any particular one of these intervals is
called a conﬁdence interval for μwith conﬁdence level 95% .
Generalizing the foregoing derivation for an arbitrary sample size leads to the following conﬁ-
dence interval formula.
ONE-SAMPLE TCONFIDENCE INTERVAL
Let /C22xandsbe the sample mean and sample standard deviation of a random sample of size
nselected from a normal population distribution. Then a conﬁdence interval (interval of
plausible values) for the population mean μhas endpoints
/C22x/C6t*/C1sﬃﬃﬃnp
where t*is the appropriate tcritical value with n/C01 df from Table A.5.
Example 5.14 Have you ever dreamed of owning a Porsche? Even though academic salaries leave
little room for luxuries, the authors thought maybe the purchase of a used Boxster, the least expensive
Porsche model, might be feasible. So on December 30, 2012 we went to www.cars.com to peruse
prices. The news was discouraging, so we instead selected a random sample of 16 such vehicles and
obtained the following odometer readings (miles):
1445 25,822 26,892 29,860 35,285 47,874 49,544 64,763
72,698 75,732 84,457 91,577 93,000 109,538 113,399 137,652
Figure 5.8shows a normal probability plot of the data; this version includes a superimposed line
which makes it easier to judge whether the pattern in the plot is reasonably linear. Very clearly that is
the case. It is therefore quite plausible that the distribution of odometer readings is (at least approxi-
mately) normal, which validates the use of the one-sample tconﬁdence interval to estimate the
population mean odometer reading, μ.
160000 120000 80000 40000 099
95
90
80
70
60
50
40
30
20
10
5
1
odometerPercentFig. 5.8 Normal
probability plot of the
Boxster odometer
reading data5.3 Confidence Intervals for a Population Mean 379
The sample mean and sample standard deviation are 66,221.1 and 37,683.1672, respectively, and
the (estimated) standard error of the mean is s=ﬃﬃﬃnp¼9420 :7918. Table A.5 shows that the tcritical
value for a conﬁdence level of 95% when df ¼16/C01¼15 is t*¼2.131. The conﬁdence interval
is then
/C22x/C6t*/C1sﬃﬃﬃnp¼66, 221 :1/C62:131ðÞ 9420 :7918 ðÞ ¼ 66, 221 :1/C620, 075 :7
¼46, 145 :4, 86, 296 :8 ðÞ
That is, we can say with a conﬁdence level of 95% that 46,145.4 <μ<86,296.8.
Note that it is not correct at this point to write P(46,145.4 <μ<86,296.8) ¼.95, because
nothing inside the parentheses is random. The interval we have calculated may or may not include
the actual value of μ. If we were to obtain sample after sample of size 16 from this population
distribution and for each one use the given formula with t*¼2.131, in the long run 95% of the
calculated CIs would include μwhereas 5% would not. Without knowing the value of μ, we can’t
know whether the particular interval we have calculated is one of the “good” 95% or the “bad” 5%. ■
5.3.2 A Large-Sample Confidence Interval for m
When the sample size nis sufﬁciently large, the Central Limit Theorem says that /C22Xhas approximately
a normal distribution even when the population distribution is not normal. Furthermore, it can be
shown in this case that the standardized variable /C22X/C0μðÞ =S=ﬃﬃﬃnpðÞ has approximately a standard
normal distribution; using Sin place of σin the denominator does not appreciably increase the
variability of Zwhen nis large. This in turn implies that for large n, a legitimate conﬁdence interval
for a population mean μis
/C22x/C6z*/C1sﬃﬃﬃnp ð5:2Þ
where the zcritical values for the most frequently employed conﬁdence levels appear in the bottom
row of Appendix Table A.5 (or can be extracted from the ztable). For example, the zcritical value for
95% conﬁdence, the most common level used in practice, is z*¼1.96.
Example 5.15 Magnetic resonance imaging is a commonly used noninvasive technique for assessing
the extent of cartilage damage. However, there is concern that the MRI sizing of articular cartilage
defects may not be accurate. The article “Preoperative MRI Underestimates Articular Cartilage
Defect Size Compared with Findings at Arthroscopic Knee Surgery” ( Amer. J. of Sports Med. ,
2013: 590–595) reported on a study involving a sample of 92 cartilage defects. For each one, the
size of the lesion area was determined by an MRI analysis and also during arthroscopic surgery. Each
MRI value was then subtracted from the corresponding arthroscopic value to obtain a difference
value; this is commonly referred to as “paired difference” data. The sample mean difference was
calculated to be 1.04 cm2, with a sample standard deviation of 1.67. Let’s now calculate a conﬁdence
interval using a conﬁdence level of (at least approximately) 95% for μD, the mean difference for the
population of all such defects (as did the authors of the cited article). Using the z*¼1.96 and
Expression ( 5.2), the CI is380 5 The Basics of Statistical Inference
1:04/C61:96/C11:67ﬃﬃﬃﬃﬃ
92p¼1:04/C6:34¼:70, 1:38 ðÞ
At the 95% conﬁdence level, we conclude that .70 <μD<1.38. Perhaps the most important
aspect of this interval is that 0 is not included; only certain positive values of μDare plausible. It is this
fact that led the investigators to conclude that MRIs tend to underestimate defect size. ■
Many statisticians do not use Expression ( 5.2) unless their sample size is extremely large, electing
instead to use the one-sample tinterval for virtually all cases. In Example 5.15, for instance,
z*¼1.96 would be replaced by the more conservative tcritical value at 91 df, which happens to
be 1.986. This would make very little practical change to the resulting interval. In the simulation
sections of Chaps. 2–4, where the “sample” size was typically 10,000 or more, there would be no
controversy in using /C22x/C61:96s=ﬃﬃﬃnpas a 95% CI for the unknown mean μof the rv being simulated.
When the sample size is small and the population distribution is substantially non-normal, neither
the one sample tinterval nor Expression ( 5.2) should be used. In this case there are other techniques
for obtaining a valid CI. One relatively recent, computationally intensive such method is called a
bootstrap conﬁdence interval . This entails obtaining a large number of samples of size nby
resampling with replacement from the sample that was actually obtained—e.g., if the sample size
is 20, a bootstrap might be based on 1000 samples of size 20, each obtained with replacement from the
original sample. Details can be found in the book by Devore and Berk listed in the references.
5.3.3 Software for Confidence Interval Calculation
It should be no surprise that modern software can compute conﬁdence intervals automatically once
we have supplied the software with our data. In R, the t.test function takes in a vector of data and
returns, among other things, a one-sample t95% conﬁdence interval for the population mean μ. The
optional argument conf.level can be used to select any other conﬁdence level (the default is
conf.level ¼.95). The analogous function in Matlab is ttest , although the inputs and outputs
are managed differently. Both are illustrated in Fig. 5.9.
Notice that both R and Matlab give a CI of (46,147, 86,303) for the true mean odometer reading.
This is roughly what we computed in Example 5.14, and the disparity is primarily due to rounding in
the critical value t*. The other information provided by R relates to hypothesis testing , which we will
discuss in Sect. 5.4.>> x=[1445,25882,26892,29860,35285,
47874,49544,64763,72698,75732,84457,
91577,93000,109538,113399,137652];
>> [~,~,CI]=ttest(x)
CI =
   46147        86303> x<-c(1445,25882,26892,29860,35285,
47874,49544,64763,72698,75732,
84457,91577,93000,109538,113399,137652)
> t.test(x)
        One Sample t-test
data:  x 
t = 7.0305, df = 15, 
p-value = 4.068e-06
alternative hypothesis: true mean is 
not equal to 0 
95 percent confidence interval:
 46147.22 86302.53ab
Fig. 5.9 One-sample tintervals for μusing the data in Example 5.14: ( a) Matlab; ( b)R5.3 Confidence Intervals for a Population Mean 381
To simply ﬁnd the tcritical value for a particular df, the inverse cdf commands can be
implemented, but with one proviso: for the central area of a tcurve to equal some conﬁdence level
C, the cumulative probability from /C01 to the critical value must be
Cþ1/C0C
2¼1þC
2
e.g., for 95% conﬁdence, C¼.95, and the cumulative probability is (1 + .95)/2 ¼.975. In Matlab,
the command icdf(’t’,.975,15) returns 2.1314, the tcritical value at 15 df that we used in
Example 5.14. In R, qt(.975,15) gives this same value.
To construct the interval in Expression (5.2) for a population mean μ, use the command ztest in
Matlab; the z-based CI for μis not implemented in the R base package.
5.3.4 Exercises: Section 5.3(37–50)
37. Determine the tcritical value for a one-sample tconﬁdence interval in each of the following
situations.
(a) Conﬁdence level ¼95%, df ¼10
(b) Conﬁdence level ¼95%, df ¼15
(c) Conﬁdence level ¼99%, df ¼15
(d) Conﬁdence level ¼99%, n¼5
(e) Conﬁdence level ¼98%, df ¼24
(f) Conﬁdence level ¼99%, n¼38
38. According to the article “Fatigue Testing of Condoms” ( Polymer Testing , 2009: 567–571), “tests
currently used for condoms are surrogates for the challenges they face in use,” including a test for
holes, an inﬂation test, a package seal test, and tests of dimensions and lubricant quality (all
fertile territory for the use of statistical methodology!). The investigators developed a new test
that adds cyclic strain to a level well below breakage and determines the number of cycles to
break. A sample of 20 condoms of one particular type resulted in a sample mean number of 1584
and a sample standard deviation of 607. Calculate and interpret a conﬁdence interval at the 99%
conﬁdence level for the true average number of cycles to break. [ Note: The article presented the
results of hypothesis tests based on the tdistribution; the validity of these depends on assuming
normal population distributions.]
39. Here is a sample of ACT scores (average of the Math, English, Social Science, and Natural
Science scores) for students taking college freshman calculus:
24.00 28.00 27.75 27.00 24.25 23.50 26.25
24.00 25.00 30.00 23.25 26.25 21.50 26.00
28.00 24.50 22.50 28.25 21.25 19.75
(a) Using an appropriate graph, see if it is plausible that the observations were selected from a
normal distribution.
(b) Calculate a 95% conﬁdence interval for the population mean.
(c) The university ACT average for entering freshmen that year was about 21. Are the calculus
students better than average, as measured by the ACT?
40. Even as traditional markets for sweetgum lumber have declined, large section solid timbers
traditionally used for construction bridges and mats have become increasingly scarce. The article
“Development of Novel Industrial Laminated Planks from Sweetgum Lumber” ( J. of Bridge
Engr. , 2008: 64–66) described the manufacturing and testing of composite beams designed to382 5 The Basics of Statistical Inference
add value to low-grade sweetgum lumber. Here is data on the modulus of rupture (psi; the article
contained summary data expressed in MPa):
6807.99 7637.06 6663.28 6165.03 6991.41 6992.23
6981.46 7569.75 7437.88 6872.39 7663.18 6032.28
6906.04 6617.17 6984.12 7093.71 7659.50 7378.61
7295.54 6702.76 7440.17 8053.26 8284.75 7347.95
7422.69 7886.87 6316.67 7713.65 7503.33 7674.99
(a) Verify the plausibility of assuming a normal population distribution.
(b) Estimate the true average modulus of rupture in a way that conveys information about
precision and reliability.
41. A sample of 26 offshore oil workers took part in a simulated escape exercise, resulting in the
accompanying data on time (seconds) to complete the escape (“Oxygen Consumption and
Ventilation During Escape from an Offshore Platform,” Ergonomics , 1997: 281–292):
389 356 359 363 375 424 325 394 402
373 373 370 364 366 364 325 339 393
392 369 374 359 356 403 334 397
(a) Calculate a 99% conﬁdence interval for the population mean escape time.
(b) Would a 90% CI based on the same data be wider or narrower? Explain.
42. A study of the ability of individuals to walk in a straight line (“Can We Really Walk Straight?”
Amer. J. Phys. Anthropol ., 1992: 19–27) reported the accompanying data on cadence (strides per
second) for a sample of n¼20 randomly selected healthy men.
.95 .85 .92 .95 .93 .86 1.00 .92 .85 .81
.78 .93 .93 1.05 .93 1.06 1.06 .96 .81 .96
A normal probability plot gives substantial support to the assumption that the population
distribution of cadence is approximately normal. Calculate and interpret a 95% conﬁdence
interval for population mean cadence.
43. The article “Measuring and Understanding the Aging of Kraft Insulating Paper in Power
Transformers” ( IEEE Electrical Insul. Mag. , 1996: 28–34) contained the following observations
on degree of polymerization for paper specimens for which viscosity times concentration fell in a
certain middle range:
418 421 421 422 425 427 431
434 437 439 446 447 448 453
454 463 465
(a) Is it plausible that the given sample observations were selected from a normal distribution?
(b) Calculate a 95% conﬁdence interval for true average degree of polymerization (as did the
authors of the article). Does the interval suggest that 440 is a plausible value for true average
degree of polymerization? What about 450?
44. Silicone implant augmentation rhinoplasty is used to correct congenital nose deformities. The
success of the procedure depends on various biomechanical properties of the human nasal
periosteum and fascia. The article “Biomechanics in Augmentation Rhinoplasty” ( J. of Med.
Engr. and Tech. , 2005: 14–17) reported that for a sample of 15 (newly deceased) adults, the mean
failure strain (%) was 25.0, and the standard deviation was 3.5. Assuming a normal distribution
for failure strain, estimate true average strain in a way that conveys information about precision
and reliability.5.3 Confidence Intervals for a Population Mean 383
45. A more extensive tabulation of tcritical values than what appears in this book shows that for the
tdistribution with 20 df, the areas to the right of the values .687, .860, and 1.064 are .25, .20, and
.15, respectively. What is the conﬁdence level for each of the following three conﬁdence
intervals for the mean μof a normal population distribution? Which of the three intervals
would you recommend be used, and why?
(a) /C22x/C0:687s=ﬃﬃﬃﬃﬃ
21p
,/C22xþ1:725s=ﬃﬃﬃﬃﬃ
21p /C0/C1
(b) /C22x/C0:860s=ﬃﬃﬃﬃﬃ
21p
,/C22xþ1:325s=ﬃﬃﬃﬃﬃ
21p /C0/C1
(c) /C22x/C01:064s=ﬃﬃﬃﬃﬃ
21p
,/C22xþ1:064s=ﬃﬃﬃﬃﬃ
21p /C0/C1
46. In many applications, it sufﬁces to have a reliable lower bound for the mean μ, because
underestimating μwould be far more serious that overestimating it. This gives rise to the idea
of a lower conﬁdence bound forμ: a quantity Lso that we can say with 95% conﬁdence (for
example) that L<μ.
(a) Let t*be a value such that
P/C22X/C0μ
S=ﬃﬃﬃnp<t*/C18/C19
¼:95
Manipulate the inequality inside the parentheses to isolate μ, and thus conclude that L¼
/C22x/C0t* s=ﬃﬃﬃnpðÞ is a 95% lower conﬁdence bound for μ.
(b) Notice that the expression given in (a) speciﬁes that the area from /C01 tot* under the
appropriate tcurve is .95; equivalently, the upper tail area designated by t* is .05. What is
the appropriate tcritical value for a 95% lower conﬁdence bound with df ¼10? df ¼15?
[Hint: Do not use the header row in the ttable as a reference; those conﬁdence levels refer to
central areas, or equivalently two-sided conﬁdence intervals.]
(c) A sample of 14 joint specimens of a particular type gave a sample mean proportional limit
stress of 8.48 MPa and a sample standard deviation of .79 MPa (“Characterization of
Bearing Strength Factors in Pegged Timber Connections,” J. Struct. Engr. , 1997: 326–
332). Assuming the data are drawn from a normally distributed population, calculate and
interpret a 95% lower conﬁdence bound for the true average proportional limit stress of all
such joints.
47. An upper conﬁdence bound ,U, for μis obtained by replacing the /C0sign with a + sign in the
expression for Lfrom the previous exercise: U¼/C22xþt*s=ﬃﬃﬃnpðÞ . As in the previous exercise, the
tcritical value is determined by a one-tail area, not a central area.
Consider the following sample of fat content (in percentage) of n¼10 randomly selected hot
dogs (“Sensory and Mechanical Assessment of the Quality of Frankfurters,” J. Texture Stud. ,
1990: 395-409):
25.2 21.3 22.8 17.0 29.8 21.0 25.5 16.0 20.9 19.5
Assuming that these were selected from a normal population distribution, calculate and interpret
a 99% upper conﬁdence bound for the population mean fat content.
48. When the sample size nis very large, lower and upper conﬁdence bounds for μcan be obtained by
replacing t* with z* in the expressions from the previous two exercises. For example, a large-
sample lower conﬁdence bound for μis given by L¼/C22x/C0z*s=ﬃﬃﬃnp, where z* satisﬁes the relation
P(Z<z*)¼cwhen Z~N(0, 1) and 100 c% is the prescribed conﬁdence level (e.g., c¼.95).
(a) Show that the zcritical value for a one-sided (i.e., upper or lower) conﬁdence bound for μ,
with conﬁdence level 100 c%, is given by z*¼Φ/C01(c).
(b) Find the one-sided zcritical values for 90, 95, and 99% conﬁdence.384 5 The Basics of Statistical Inference
(c) A certain random variable is simulated 10,000 times. The sample mean and standard
deviation of the resulting 10,000 values are 41.63 and 8.05, respectively. Calculate and
interpret a 95% lower conﬁdence bound for the true expected value of this rv.
49. Often an investigator wishes to predict a single value of a variable to be observed at some future
time, rather than to estimate the mean value of that variable. Suppose we will observe the values
of a random sample X1,...,Xnfrom a normal population with mean μand standard deviation σ,
and from these we wish to predict the value of a future independent observation Xn+1.
(a) Show that
Z¼/C22X/C0Xnþ1
σﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1þ1
nq
has a standard normal distribution, where /C22Xis the sample mean of X1,...,Xn.[Hint: Since
the population is normal, the linear combination /C22X/C0Xnþ1is normal. Show that /C22X/C0Xnþ1has
mean 0 and variance σ2(1 + 1/ n), then standardize.]
(b) If we replace σwith the sample standard deviation Sin the expression for Zfrom (a), it can
be shown that the resulting quantity has a tdistribution with n/C01 df. Use this fact and a
derivation similar to the one presented in this section to show that a prediction interval (PI)
for a single future observation Xn+1is given by
/C22x/C6t*/C1sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1þ1
nr
(c) Use the previous expression, along with the data in Exercise 47, to provide a 95% prediction
interval for the fat content of a randomly selected hot dog you will consume at some
future time.
50. Independent observations X1,...,Xn~N(μ1,σ1) and Y1,...,Ym~N(μ2,σ2) will be taken. For
example the heights of nmen and mwomen might be recorded, where the height distribution of
each gender is normally distributed but with unknown parameters. Of interest is the difference
between the two unknown population means, μ1/C0μ2.
(a) The logical estimator of μ1/C0μ2is/C22X/C0/C22Y, the difference of the sample averages of the two
samples. By determining the mean and variance of /C22X/C0/C22Y, show that
Z¼/C22X/C0/C22YðÞ /C0 μ1/C0μ2 ðÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
1
nþσ2
2
mr
has a standard normal distribution.
(b) Let z* be the zcritical value such that P(/C0z*<Z<z*)¼c, where Zis the expression
above and 100 c% is the prescribed conﬁdence level. Rewrite the inequalities to provide a
conﬁdence interval for the difference of population means μ1/C0μ2.
(c) If nandmare large (say, /C2140), replacing σ1andσ2with S1andS2(the two sample standard
deviations) under the square root in (a) adds little extra variability; the resulting
standardized variable still has approximately a standard normal distribution. Make this
substitution to obtain a large-sample zconﬁdence interval for μ1/C0μ2that can be
implemented in practice.
(d) The article “Gender Differences in Individuals with Comorbid Alcohol Dependence and
Post-Traumatic Stress Disorder” ( Amer. J. Addiction , 2003: 412–423) reported the
accompanying data in total score in the Obsessive-Compulsive Drinking Scale.5.3 Confidence Intervals for a Population Mean 385
Gender Sample size Sample mean Sample SD
Male 44 19.93 7.74
Female 40 16.26 7.58
Calculate and interpret a 95% conﬁdence interval for the difference in the true mean scores
for males and females.
5.4 Testing Hypotheses About a Population Mean
We have seen that a parameter can be estimated from sample data, either by a single number (a point
estimate) or an entire interval of plausible values (a conﬁdence interval). Frequently, however, the
objective of an investigation is not to estimate a parameter but to decide which of two contradictory
claims about the parameter is correct. Methods for accomplishing this comprise the part of statistical
inference called hypothesis testing .
5.4.1 Hypotheses and Test Procedures
Astatistical hypothesis , or just hypothesis , is a claim or assertion either about the value of a single
parameter (population characteristic or characteristic of a probability distribution), about the values
of several parameters, or about the form of an entire probability distribution. Examples include
• The claim that μ¼$350, where μis the true average one-term textbook expenditure for students at
a university
• The assertion that p<.50, where pis the proportion of children who have a food allergy of some
sort
• The claim that μ1/C0μ2>3, where μ1is the true average fuel efﬁciency (mpg) of all current model
year Honda Accords equipped with a 4-cylinder engine and μ2is the analogous characteristic for
Accords equipped with a 6-cylinder engine.
In any hypothesis-testing problem, there are two contradictory hypotheses under consideration.
One hypothesis might be the claim μ¼$350 and the other μ6¼$350, or the two contradictory
statements might be p/C21.50 and p<.50. The objective is to decide, based on sample information,
which of the two hypotheses is correct. There is a familiar analogy to this in a criminal trial. One
claim is the assertion that the accused individual is innocent. In the US judicial system, this is
the claim that is initially believed to be true. Only in the face of strong evidence to the contrary should
the jury reject this claim in favor of the alternative assertion that the accused is guilty. In this sense,
the claim of innocence is the favored or protected hypothesis, and the burden of proof is placed on
those who believe in the alternative claim.
Similarly, in testing statistical hypotheses, the problem will be formulated so that one of the claims
is initially assumed to be true. This initial claim will not be rejected in favor of the alternative claim
unless sample evidence contradicts it and provides strong support for the alternative assertion.386 5 The Basics of Statistical Inference
DEFINITION
Thenull hypothesis , denoted by H0, is the claim that is initially assumed to be true (the “prior
belief” claim). The alternative hypothesis , denoted by Ha, is the assertion that is contradictory
toH0.
The null hypothesis will be rejected in favor of the alternative hypothesis only if sample
evidence suggests that H0is false. If the sample does not strongly contradict H0, we will
continue to believe in the plausibility of the null hypothesis. The two possible conclusions from
a hypothesis-testing analysis are then reject H 0orfail to reject H 0.
Atest of hypotheses is a method for using sample data to decide whether the null hypothesis
should be rejected. Thus we might test H0:μ¼350 against the alternative Ha:μ6¼350. Only if
sample data strongly suggests that μis something other than 350 should the null hypothesis be
rejected. In the absence of such evidence, H0should not be rejected since it is still judged to be
plausible.
Sometimes an investigator does not want to accept a particular assertion unless and until data can
provide strong support for the assertion; in that situation, this assertion will be the investigator’s
alternative hypothesis Ha. As an example, suppose a company is considering putting a new additive in
the dried fruit that it produces. The true average shelf life with the current additive is known to be
200 days. With μdenoting the true average shelf life with the newadditive, the company would not
want to make a change unless evidence strongly suggested that μexceeds 200. An appropriate
problem formulation would involve testing H0:μ¼200 against Ha:μ>200. The conclusion that
a change is justiﬁed is identiﬁed with Ha, and it would take conclusive evidence to justify rejecting H0
and switching to the new additive.
Scientiﬁc research often involves trying to decide whether a current theory should be replaced by a
more plausible and satisfactory explanation of the phenomenon under investigation. A conservative
approach is to identify the current theory with H0and the researcher’s alternative explanation with Ha.
Rejection of the current theory will then occur only when evidence is much more consistent with the
new theory. In many situations, Hais referred to as the “research hypothesis,” since it is the claim that
the researcher would really like to validate. The word null means “of no value, effect, or conse-
quence,” which suggests that H0should be identiﬁed with the hypothesis of no change (from current
opinion), no difference, no improvement, and so on. Suppose, for example, that 10% of all computer
circuit boards produced by a manufacturer during a recent period were defective. An engineer has
suggested a change in the production process in the belief that it will result in a reduced defective rate.
Letpdenote the true proportion of defective boards resulting from the changed process. Then the
research hypothesis, on which the burden of proof is placed, is the assertion that p<.10. Thus the
alternative hypothesis is Ha:p<.10.
In our treatment of hypothesis testing, H0will generally be stated as an equality claim. When the
parameter of interest is a population mean μ, the null hypothesis will have the form H0:μ¼μ0, where
μ0is a speciﬁed number called the null value (value claimed for μby the null hypothesis). For
example, let μrepresent the true average breaking strength of nylon string of a certain type. If a
particular application requires that μexceed 100 N and the string will not be used unless there is
compelling evidence that this is the case, the natural alternative hypothesis is Ha:μ>100. It would
then make sense to select as the null hypothesis the assertion that μ/C20100. However, we will instead
simplify the null hypothesis to H0:μ¼100. The rationale for using this simpliﬁed null hypothesis is5.4 Testing Hypotheses About a Population Mean 387
that any reasonable decision procedure for deciding between H0:μ¼100 and Ha:μ>100 will also
be reasonable for deciding between the claim that μ/C20100 and Ha, and should lead to exactly the
same conclusion for any particular sample. The use of a simpliﬁed H0is preferred because it has
certain technical beneﬁts, which will be apparent shortly.
The alternative to the null hypothesis H0:μ¼μ0will look like one of the following three
assertions:
1.Ha:μ>μ0(in which case the implicit null hypothesis is μ/C20μ0)
2.Ha:μ<μ0(so the implicit null hypothesis states that μ/C21μ0)
3.Ha:μ6¼μ0
5.4.2 Test Procedures for Hypotheses About a Population Mean m
The decision as to whether H0should be rejected is based on the analysis of data x1,x2,...,xn
resulting from a random sample of the population. A sensible strategy at this point would be to
calculate the sample mean /C22xand reject the null hypothesis if its value is too far from μ0in the
appropriate direction. For example, in the scenario involving breaking strength of nylon string, a
value of /C22xconsiderably larger than 100 would suggest that H0is false and should be rejected. But an /C22x
value lessthan 100 would not incline us to reject H0in favor of Ha, since a sample mean less than
100 would certainly not convince us that the population mean μis more than 100.
Rather than base a decision on /C22xitself, let’s standardize /C22xassuming that the null hypothesis is true:
t¼/C22x/C0μ0
s=ﬃﬃﬃnp
If we knew the value of the population standard deviation σ, we’d use it rather than the sample
standard deviation s, but in practice this is almost never the case. Continuing with the nylon string
scenario, t¼ /C22x/C0100ðÞ =s=ﬃﬃﬃnpðÞ . For n¼25 and sample data /C22x¼108:5,s¼12.14, we calculate
t¼8.5/2.428 ¼3.50. The interpretation is that the value of the sample mean, 108.5, is 3.5 estimated
standard errors from what we’d expect it to be if the null hypothesis were true . In general, tis the
distance between the sample mean and what we’d expect it to be if H0were true, expressed in
standard deviations.
Now let’s see if we can identify which values of tare at least as contradictory to H0as the value
calculated from the available sample data. Again focusing on the nylon string situation, because the
alternative hypothesis states that the population mean exceeds 100, any value of /C22xgreater than 108.5
argues even more strongly against H0than does the 108.5 resulting from our sample. And any /C22x
greater than 108.5 corresponds to a value of tthat exceeds 3.50. So values of tthat are at least 3.50 are
at least as contradictory to H0as 3.50 itself.
As another example, now suppose that μrepresents the mean IQ for a large population of children,
and consider the rival hypotheses H0:μ¼100 and Ha:μ6¼100. Because 100 is the generally
accepted value of mean IQ in the USA, the alternative hypothesis here states that the average for
the designated population of children is different from this accepted value. Suppose a sample of
225 children gives a sample mean IQ of 98.6 and a sample standard deviation of 16.15, from which
t¼98:6/C0100 ðÞ =16:15=ﬃﬃﬃﬃﬃﬃﬃﬃ
225p/C0/C1
¼/C01:30. The average IQ in the sample is 1.3 estimated standard388 5 The Basics of Statistical Inference
errors smaller than what would be expected were the null hypothesis true. To decide which values
oftare at least as contradictory to H0as/C01.30, ﬁrst consider which values of /C22xare at least
as contradictory to H0as 98.6. Not only is any value 98.6 or smaller in this category, but also
any value that is at least 101.4—that is, any value at least as far from 100 in either direction
(because 6¼appears in the alternative hypothesis). Thus any value of tthat is either /C20/C01.30 or
/C211.30 is at least as contradictory to H0as our calculated t¼/C01.30.
5.4.3 P-Values and the One-Sample tTest
Before data have been obtained, the sample mean and sample standard deviation are random
variables, which we have previously denoted by /C22XandS, respectively. Substituting these for /C22xand
sin the formula for tgives what is called the test statistic
T¼/C22X/C0μ0
S=ﬃﬃﬃnp,
which is also a random variable (that is, its value is subject to uncertainty prior to obtaining the
sample data).
If the population distribution is normal, Gosset’s Theorem from Sect. 5.3implies that when the
null hypothesis is true ,T has a t distribution with n /C01degrees of freedom . In the case of the nylon
string scenario, T¼ /C22X/C0100ðÞ =S=ﬃﬃﬃnpðÞ would have a tn/C01distribution when H0:μ¼100 is true
(assuming that the population strength distribution is normal). For the previously given sample
information, the calculated value of the test statistic was 3.50. Now consider the probability,
calculated assuming that the null hypothesis is true, of obtaining a test statistic value at least as
contradictory to the null hypothesis as the value 3.50 resulting from our sample data:
PH0T/C213:50 ðÞ ¼ Pat24random variable is at least 3 :50 ðÞ
¼the area under the t24curve to the right of 3 :50
¼:001 from softwareðÞ
That is, if the null hypothesis is true, there is only a .1% chance of obtaining a sample at least as
contradictory to the null hypothesis as our sample. So our sample is among the .1% of all samples
most contradictory to H0.
Recall that t¼/C01.30 in the IQ scenario. Again assuming a normal population distribution, the
probability of getting a value of Tat least as contradictory to H0when H0is true is
PH0T/C20/C01:30 or T/C211:30 ðÞ ¼ Pat224rv is/C20/C01:30 or/C211:30 ðÞ
¼area under the t224curve to the left of /C01:30 ðÞ þ
area under the t224curve to the right of 1 :30 ðÞ
¼2 area under the t224curve to the right of 1 :30 ðÞ
/C252 area under the zcurve to the right of 1 :30 ðÞ
¼:1936
So when the null hypothesis is true, almost 20% of all samples would result in a test statistic value
that is at least as contradictory to H0as the one resulting from our sample. This implies that our
sample is not very contradictory to H0.5.4 Testing Hypotheses About a Population Mean 389
DEFINITION
TheP-value is the probability, calculated assuming that the null hypothesis is true, of obtaining
a value of the test statistic at least as contradictory to H0as the value calculated from the
available sample. The smaller the P-value, the more the data contradicts the null hypothesis, so
H0should be rejected in favor of Haif the P-value is sufﬁciently small.
More speciﬁcally, select a number αreasonably close to 0; then reject the null hypothesis if
P-value /C20αand do not reject the null hypothesis if P-value >α. The selected αis called the
signiﬁcance level of the test.
The most frequently employed values of the signiﬁcance level are α¼.05, .01, and .001. We shall
say more about the choice of αshortly.
ONE-SAMPLE TTEST
Consider testing the null hypothesis H0:μ¼μ0based on a random sample X1,X2,...,Xnfrom a
normal population distribution (the plausibility of the normality assumption should be checked
by examining a normal probability plot). The test statistic is
T¼/C22X/C0μ0
S=ﬃﬃﬃnp
The calculated value of this test statistic is t¼ /C22x/C0μ0 ðÞ =s=ﬃﬃﬃnpðÞ . The determination of the
P-value depends on the choice of Haas follows:
Alternative Hypothesis P-value
Ha:μ>μ0 Area under the tn/C01curve to the right of t
Ha:μ<μ0 Area under the tn/C01curve to the left of t
H0:μ6¼μ0 2/C1(Area under the tn/C01curve to the right of | t|)
The test procedure when the alternative hypothesis is Ha:μ>μ0is referred to as an upper-tailed
test, because the P-value is the area captured in the upper tail of the relevant tcurve (i.e., to the right
oft). Analogously, the test procedure for the second case is called a lower-tailed test , and the
procedure in the third case is a two-tailed test . Figure 5.10 illustrates the determination of the
P-value in the three different cases.
Appendix Table A.6 provides information about tail areas under various tcurves. The calculated
value of t(to the accuracy of the tenths digit) appears along the left margin, and there is a different
column for each number of df. For example, the entry at the intersection of the t¼2.4 row and the
15 df column is .015, the area under the 15 df tcurve to the right of 2.4. By symmetry, this is also the
area under the 15 df tcurve to the left of /C02.4. Various software packages will allow for more decimal
accuracy in tand the corresponding areas.
Example 5.16 Correct alignment of the tibial and femoral components is an important factor in
determining favorable long-term results of total knee arthroplasty (TKA). It is generally accepted that
the tibial component should be placed perpendicular to the anatomical axis of the tibia. The article
“Simple Method for Conﬁrming Tibial Osteotomy During Total Knee Arthroplasty” ( Sports390 5 The Basics of Statistical Inference
Medicine, Arthroscopy, Rehabilitation, Therapy, and Technology , 2012, 4:44) reported that for a
sample of 35 TKAs, the sample mean varus angle of the tibial osteotomy was 89.45/C14and the sample
standard deviation was 1.62/C14. The authors of the cited article carried out a one-sample ttest to see
whether the true average angle differed from 90/C14(presumably after examining a normal probability
plot of the data). The relevant hypotheses are H0:μ¼90 versus Ha:μ6¼90.
The calculated value of the test statistic is
t¼89:46/C090 ðÞ =1:62=ﬃﬃﬃﬃﬃ
35p/C16/C17
¼/C01:97
The inequality in Haimplies that the test is two-tailed, so the P-value is twice the area under the t34
curve to the right of 1.97. The entry in the 2.0 row and 35 df column of Table A.6 is .027, so the P-
value is approximately 2(.027) ¼.054 (the article reports .055; notice that we have had to round both
the test statistic and the df in order to use the ttable).
Thus with a signiﬁcance level of .05, the null hypothesis cannot be rejected because P-value
¼.054 >.05¼α. This is what allowed the investigators to conclude that “there was no signiﬁcant
difference from the target angle of 90/C14.” ■
Recall from the previous section on conﬁdence intervals that when the sample size nis large, the
standardized variable /C22X/C0μðÞ =S=ﬃﬃﬃnpðÞ has approximately a standard normal distribution even if the
population distribution is not normal. The implication here is that we can relabel our test statistic as
Z¼ /C22X/C0μ0 ðÞ =S=ﬃﬃﬃnpðÞ . Then the prescription in the one-sample tbox for obtaining the P-value is1. Upper-tailed test
Ha contains the inequality >t curve for relevant df
t curve for relevant df
t curve for relevant d fP-value = area in upper tail
P-value = area in lower tail
P-value = sum of area in two tailsCalculated t
0
00
Calculated t
Calculated t, −t2. Lower-tailed test
Ha contains the inequality <
3. Two-tailed test
Ha contains the inequality ≠
Fig. 5.10 P-values for ttests5.4 Testing Hypotheses About a Population Mean 391
modiﬁed by replacing tn/C01andtbyz. That is, the P-value for these large-sample tests is an
appropriate zcurve area.
Example 5.17 The recommended daily intake of calcium for adults ages 18–30 is 1000 mg/day. The
article “Dietary and Total Calcium Intakes Are Associated with Lower Percentage Total Body and
Truncal Fat in Young, Healthy Adults” ( J. of the Amer. College of Nutr. , 2011: 484–490) reported the
following summary data for a sample of 76 healthy Caucasian males from southwestern Ontario,
Canada: n¼76, /C22x¼1093, s¼477. Let’s carry out a test at signiﬁcance level .01 to see whether the
population mean daily intake exceeds the recommended value. The relevant hypotheses are H0:
μ¼1000 versus Ha:μ>1000.
The calculated value of the test statistic is
z¼1093/C01000 ðÞ =477=ﬃﬃﬃﬃﬃ
76p/C16/C17
¼1:70
The resulting P-value is the area under the standard normal curve to the right of 1.70 (the
inequality in Haimplies that the test is upper-tailed). From Table A.3, this area is 1 /C0Φ(1.70) ¼
1/C0.9554 ¼.0446. Because this P-value is larger than .01, H0cannot be rejected. There is not
compelling evidence to conclude at signiﬁcance level .01 that the population mean daily intake
exceeds the recommended value (even though the sample mean does so). Note that the opposite
conclusion would result from using a signiﬁcance level of .05. But the smaller αthat we used requires
more persuasive evidence from the data before rejecting H0. ■
5.4.4 Errors in Hypothesis Testing and the Power of a Test
When a jury is called upon to render a verdict in a criminal trial, there are two possible erroneous
conclusions to be considered: convicting an innocent person, or letting a guilty person go free.
Similarly, in statistical hypothesis testing there are two potential errors whose consequences must be
considered when reaching a conclusion.
DEFINITION
AType I error involves rejecting the null hypothesis H0when it is true.
AType II error involves not rejecting H0when it is false.
Since in the US judicial system the null hypothesis (a priori belief) is that the accused is innocent, a
Type I error is analogous to convicting an innocent person. It would be nice if test procedures could
be developed that offered 100% protection against committing both a Type I error and a Type II error.
This is an impossible goal, however, because a conclusion is based on sample data rather than a
census of the entire population. There is always some chance that the sample will be unrepresentative
of the population and lead to an incorrect conclusion. The best we can hope for is a test procedure for
which it is unlikely that either a Type I or a Type II error will be committed.
Let’s reconsider the calcium intake scenario of the previous example. We employed a signiﬁcance
level of α¼.01, and so we could reject H0only if P-value /C20.01. The P-value in the case of this
upper-tailed large-sample test is the area under the standard normal curve to the right of the calculated
z. Table A.3 shows that the z-value 2.33 captures an upper-tail area of .01 (look inside the table for a
cumulative area of .9900). The P-value (captured upper-tail area) will therefore be at most .01 if and
only if zis at least 2.33; see Fig. 5.11.392 5 The Basics of Statistical Inference
Thus the probability of committing a Type I error—rejecting H0when it is true—is the probability
that the value of the test statistic Zwill be at least 2.33 when H0is true. Now the key fact: because we
created Zby subtracting the null value μ0when standardizing, Zhas a standard normal distribution
when H0is true. So
PType I errorðÞ ¼ Prejecting H0when H0is true ðÞ
¼PZ/C212:33 when Zis a standard normal rv ðÞ ¼ :01¼α
This is true not only for the ztest of Example 5.17 but also for the ttests described earlier and, in
fact, for any test procedure.
PROPOSITION
The signiﬁcance level αthat is employed when H0is rejected iff P-value /C20αis also the
probability that the test results in a Type I error.
Thus a test with signiﬁcance level .01 is one for which there is a 1% chance of committing a Type I
error, whereas using a signiﬁcance level of .05 results in a test with a Type I error probability of .05.
The smaller the signiﬁcance level, the less likely it is that the null hypothesis will be rejected when it
is true. A smaller signiﬁcance level makes it harder for the null hypothesis to be rejected and therefore
less likely that a Type I error will be committed.
It is natural to ask at this point why a signiﬁcance level of .05 should ever be employed when a
signiﬁcance level of .01 can be used. More generally, why use a test with a larger signiﬁcance level—
larger probability of a Type I error—when a smaller level is available? The answer lies in something
that we have not yet explicitly considered: the likelihood of committing a Type II error. Let’s denote
the probability of a Type II error by β. That is,
β¼P/C0
not rejecting H0when Hais true/C1
This notation is actually somewhat misleading: whereas for any particular test there is a single
value of α(a consequence of having H0be a statement of equality), there are in fact many different
values of β. The alternative hypothesis in the calcium intake situation was Ha:μ>1000. So this
would be true if μwere 1010 or 1050 or 1100 or in fact any value exceeding 1000. Nevertheless, for
any particular way of H0being false and Hatrue, it can be shown that αandβare inversely related:
changing the test procedure by decreasing αin order to make the chance of a Type I error smaller has
the inevitable consequence of making a Type II error more likely. Conversely, using a larger
signiﬁcance level will make it less likely that the null hypothesis will fail to be rejected when, in
fact, it is false.z2.33P-value < .01
z2.33P-value > .01a b
Fig. 5.11 P-values for an upper-tailed large-sample test: ( a)P-value <.01 if z>2.33; ( b)P-value >.01 if z<2.335.4 Testing Hypotheses About a Population Mean 393
Letμ0denote some particular value of μfor which Hais true. For example, for the hypotheses H0:
μ¼90 versus Ha:μ6¼90 from Example 5.16, we might be interested in determining βwhen the true
angle is 91/C14. Then μ0¼91 and we wish β(91). The value of βdepends on several factors:
• How far the alternative value of interest μ0is from μ0[β(μ0) decreases as μ0moves further away
from μ0]
• The sample size n[β(μ0) decreases as n, and therefore df, increases]
• The value of the population standard deviation σ[the larger the value of σ, the more difﬁcult it is
forH0to be rejected, and so the larger is β(μ0)]
• The signiﬁcance level α[making αsmaller increases β(μ0)]
Calculating βfor the one-sample ttest by hand is quite difﬁcult. This is because when μ¼μ0
rather than the null value μ0, the density function that describes the distribution of the test statistic Tis
exceedingly complicated. Fortunately statistical software comes to our rescue. Rather than work
directly with β, the most commonly used software packages involve a quantity called power .
DEFINITION
Suppose the null and alternative hypotheses are assertions about the value of some parameter θ,
with the null hypothesis having the form H0:θ¼θ0and the alternative hypothesis obtained by
replacing ¼inH0by one of the three inequalities >,<or6¼. Let θ0denote some particular value
ofθfor which Hais true. Then the power at the value θ0for a test of these hypotheses is the
probability of rejecting H0when θ¼θ0, which is 1 /C0β(θ0). The power of the test when
θ¼θ0is also the probability that H0is rejected, which in this case is the signiﬁcance level α.
Thus we want the power to be close to 0 when the null hypothesis is true and close to 1 when the
null hypothesis is false. A “powerful” test is one that has high power for alternative values of the
parameter, and thus good ability to detect departures from the null hypothesis.
Example 5.18 The true average voltage drop from collector to emitter of insulated gate bipolar
transistors of a certain type is supposed to be at most 2.5 V. An investigator selects a sample of
n¼10 such transistors and uses the resulting voltages as a basis for testing H0:μ¼2.5 versus Ha:
μ>2.5 using a ttest with signiﬁcance level α¼.05. If the standard deviation of the voltage
distribution is σ¼.1 V, how likely is it that H0will not be rejected when in fact μ¼2.55 or when
μ¼2.6? And what happens to the power and βif the sample size is increased to 20?
Thesampsizepwr function in Matlab provides the following information:
μ0n Power
2.55 10 .4273
2.55 20 .6951
2.6 10 .8975
2.6 20 .9961
So in the case μ0¼2.55, βis roughly .57 when the sample size is 10 and roughly .30 when the
sample size is 20. Clearly these Type II error probabilities are rather large. If it is important to detect
such a departure from H0, the test does not have good power to do so. Software can also be used to
determine what value of the sample size nis necessary to produce a sufﬁciently large power and
correspondingly small β. For example, when μ0¼2.55, a sample size of n¼36 is required to
produce a power of .90. ■394 5 The Basics of Statistical Inference
As Example 5.18 illustrates, the power of a test can be disappointingly small for an alternative
value of the parameter that represents an important departure from the null hypothesis. Too often
investigators are content to specify a comfortingly small value of αwithout paying attention to power
andβ. This can easily result in a test which has poor ability to detect when H0is false. Given the
availability and capabilities of statistical software packages, such a sin is unpardonable!
5.4.5 Software for Hypothesis Test Calculation
Thet.test andttest functions in R and Matlab, respectively, mentioned at the end of Sect. 5.3
can be used to automatically perform the one-sample ttest described in this section (in fact, that is the
primary purpose of these functions).
Example 5.19 The accompanying data on cube compressive strength (MPa) of concrete specimens
appeared in the article “Experimental Study of Recycled Rubber-Filled High-Strength Concrete”
(Magazine of Concrete Res. , 2009: 549–556):
112.3 97.0 92.7 86.0 102.0
99.2 95.8 103.5 89.0 86.7
Suppose the concrete will be used for a particular application unless there is strong evidence that
the true average strength is less than 100 MPa. Should the concrete be used? Test at the .05
signiﬁcance level.
Letμdenote the true average cube compressive strength of this concrete. We wish to test the
hypotheses H0:μ¼100 versus Ha:μ<100. A probability plot indicates the data are consistent with
a normally distributed population. Figure 5.12 shows the hypothesis test implemented in R and
Matlab.
Both R and Matlab give a one-sided P-value of .1018 at 9 df. Since .1018 >.05, at the .05
signiﬁcance level we fail to reject H0. There is insufﬁcient evidence to conclude the true mean
strength of this concrete is less than 100 MPa. As a consequence, the concrete should be used.
In Fig. 5.12a , R gives the computed value of the test statistic, t=/C01.3708, as well as the sample
mean, /C22x¼96:42 MPa, and a (one-sided) CI for μof (/C01, 101.2073). (See Exercises 46–47 from the
previous section for information on such bounds.) In Matlab, the signiﬁcance level of .05 is a required
b a
> x<-c(112.3,97.0,92.7,86.0,
102.0,99.2,95.8,103.5
,89.0,86.7)
> 
t.test(x,mu=100,alternative="less"
)
One Sample t-test
data:  x 
t = -1.3708, df = 9, p-value = 
0.1018
alternative hypothesis: true mean 
is less than 100 
95 percent confidence interval:
-Inf 101.2073
sample estimates:
mean of x 
96.42>> x=[112.3,97.0,92.7,86.0,
102.0,99.2,95.8,103.5,
89.0,86.7];
>> 
[H,P]=ttest(x,100,.05,'left'
)
H =
0
P =
0.1018
Fig. 5.12 Performing the hypothesis test of Example 5.19: ( a)R ;( b) Matlab5.4 Testing Hypotheses About a Population Mean 395
input; the ’left’ argument instructs Matlab to perform a lower-tailed test. As seen in Fig. 5.12b ,
Matlab then returns two items: the P-value, and also a bit denoted Hindicated whether to reject H0.
(TheH=0output tells the user not to reject H0at the speciﬁed αlevel.) ■
Calculations of power, as well as sample size required to achieve a prescribed power level, are
available through the samplesizepwr function in Matlab and the pwr package in R. The former is
part of the Matlab Statistics Toolbox; the latter is not part of the R base package and must be
downloaded and installed.
5.4.6 Exercises: Section 5.4(51–76)
51. For each of the following assertions, state whether it is a legitimate statistical hypothesis
and why:
(a) H:σ>100
(b) H:^P¼:45
(c) H:S/C20.20
(d) H:σ1/σ2<1
(e) H:/C22X/C0/C22Y¼5
(f)H:λ/C20.01, where λis the parameter of an exponential distribution used to model
component lifetime
52. For the following pairs of assertions, indicate which do not comply with our rules for setting up
hypotheses and why (the subscripts 1 and 2 differentiate between quantities for two different
populations or samples):
(a) H0:μ¼100, Ha:μ>100
(b) H0:σ¼20,Ha:σ/C2020
(c) H0:p6¼.25,Ha:p¼.25
(d) H0:μ1/C0μ2¼25,Ha:μ1/C0μ2>100
(e) H0:S12¼S22,Ha:S126¼S22
(f)H0:μ¼120, Ha:μ¼150
(g) H0:σ1/σ2¼1,Ha:σ1/σ26¼1
(h) H0:p1/C0p2¼/C0.1,Ha:p1/C0p2</C0.1
53. To determine whether the girder welds in a new performing arts center meet speciﬁcations, a
random sample of welds is selected, and tests are conducted on each weld in the sample. Weld
strength is measured as the force required to break the weld. Suppose the speciﬁcations state that
mean strength of welds should exceed 100 lb/in2; the inspection team decides to test H0:
μ¼100 versus Ha:μ>100. Explain why it might be preferable to use this Harather than
μ<100.
54. Let μdenote the true average radioactivity level (picocuries per liter). The value 5 pCi/L is
considered the dividing line between safe and unsafe water. Would you recommend testing H0:
μ¼5 versus Ha:μ>5o rH0:μ¼5 versus Ha:μ<5? Explain your reasoning. [ Hint: Think
about the consequences of a Type I and Type II error for each possibility.]
55. For which of the given P-values would the null hypothesis be rejected when performing a level
.05 test?
(a) .001
(b) .021
(c) .078396 5 The Basics of Statistical Inference
(d) .047
(e) .148
56. Pairs of P-values and signiﬁcance levels, α, are given. For each pair, state whether the observed
P-value would lead to rejection of H0at the given signiﬁcance level.
(a) P-value ¼.084, α¼.05
(b) P-value ¼.084, α¼.10
(c) P-value ¼.003, α¼.01
(d) P-value ¼.039, α¼.01
57. Give as much information as you can about the P-value of a ttest in each of the following
situations:
(a) Upper-tailed test, df ¼8,t¼2.0
(b) Lower-tailed test, df ¼11,t¼/C02.4
(c) Two-tailed test, df ¼15,t¼/C01.6
(d) Upper-tailed test, df ¼19,t¼/C0.4
(e) Upper-tailed test, df ¼5,t¼5.0
(f) Two-tailed test, df ¼40,t¼/C04.8
58. The paint used to make lines on roads must reﬂect enough light to be clearly visible at night. Let
μdenote the true average reﬂectometer reading for a new type of paint under consideration. A
test of H0:μ¼20 versus Ha:μ>20 will be based on a random sample of size nfrom a normal
population distribution. What conclusion is appropriate in each of the following situations?
(a) n¼15, test statistic value ¼3.2,α¼.05
(b) n¼9, test statistic value ¼1.8,α¼.01
(c) n¼24, test statistic value ¼/C0.2
59. Let μdenote the mean reaction time to a certain stimulus. For a large-sample ztest of H0:μ¼5
versus Ha:μ>5, ﬁnd the P-value associated with each of the given values of the ztest statistic.
(a) 1.42
(b) .90
(c) 1.96
(d) 2.48
(e)/C0.11
60. Newly purchased tires of a certain type are supposed to be ﬁlled to a pressure of 35 lb/in2. Let μ
denote the true average pressure. Find the P-value associated with each given zstatistic value for
testing H0:μ¼35 versus the alternative Ha:μ6¼35.
(a) 2.10
(b)/C01.75
(c)/C0.55
(d) 1.41
(e)/C05.3
61. A pen has been designed so that true average writing lifetime under controlled conditions
(involving the use of a writing machine) is at least 10 h. A random sample of 18 pens is selected,
the writing lifetime of each is determined, and a normal probability plot of the resulting data
supports the use of a one-sample ttest.
(a) What hypotheses should be tested if the investigators believe a priori that the design
speciﬁcation has been satisﬁed?
(b) What conclusion is appropriate if the hypotheses of part (a) are tested, t¼/C02.3, and
α¼.05?5.4 Testing Hypotheses About a Population Mean 397
(c) What conclusion is appropriate if the hypotheses of part (a) are tested, t¼/C01.8, and
α¼.01?
(d) What should be concluded if the hypotheses of part (a) are tested and t¼/C03.6?
62. Lightbulbs of a certain type are advertised as having an average lifetime of 750 h. The price of
these bulbs is very favorable, so a potential customer has decided to go ahead with a purchase
arrangement unless it can be conclusively demonstrated that the true average lifetime is smaller
than what is advertised. A random sample of 50 bulbs was selected and the lifetime of each bulb
determined. These 50 light bulbs had a sample mean lifetime of 738.44 h with a sample standard
deviation of 38.20 h. What conclusion would be appropriate for a signiﬁcance level of .05?
63. Automatic identiﬁcation of the boundaries of signiﬁcant structures within a medical image is an
area of ongoing research. The paper “Automatic Segmentation of Medical Images Using Image
Registration: Diagnostic and Simulation Applications” ( J. of Medical Engr. and Tech. , 2005: 53–
63) discussed a new technique for such identiﬁcation. A measure of the accuracy of the automatic
region is the average linear displacement (ALD). The paper gave the following ALD
observations for a sample of 49 kidneys (units of pixel dimensions).
1.38 0.44 1.09 0.75 0.66 1.28 0.51
0.39 0.70 0.46 0.54 0.83 0.58 0.64
1.30 0.57 0.43 0.62 1.00 1.05 0.82
1.10 0.65 0.99 0.56 0.56 0.64 0.45
0.82 1.06 0.41 0.58 0.66 0.54 0.83
0.59 0.51 1.04 0.85 0.45 0.52 0.58
1.11 0.34 1.25 0.38 1.44 1.28 0.51
(a) Is it plausible that ALD is at least approximately normally distributed? Must normality be
assumed prior to testing hypotheses about true average ALD? Explain.
(b) The authors commented that in most cases the ALD is better than or of the order of 1.0. Does
the data in fact provide strong evidence for concluding that true average ALD under these
circumstances is less than 1.0? Carry out an appropriate test of hypotheses.
64. A dynamic cone penetrometer (DCP) is used for measuring material resistance to penetration
(mm/blow) as a cone is driven into pavement or subgrade. Suppose that for a particular applica-
tion it is required that the true average DCP value for a certain type of pavement be less than 30.
The pavement will not be used unless there is conclusive evidence that the speciﬁcation has been
met. Test the appropriate hypotheses using the following data (“Probabilistic Model for the
Analysis of Dynamic Cone Penetrometer Test Values in Pavement Structure Evaluation,” J. of
Testing and Evaluation , 1999: 7–14):
14.1 14.5 15.5 16.0 16.0 16.7 16.9 17.1 17.5 17.8
17.8 18.1 18.2 18.3 18.3 19.0 19.2 19.4 20.0 20.0
20.8 20.8 21.0 21.5 23.5 27.5 27.5 28.0 28.3 30.0
30.0 31.6 31.7 31.7 32.5 33.5 33.9 35.0 35.0 35.0
36.7 40.0 40.0 41.3 41.7 47.5 50.0 51.0 51.8 54.4
55.0 57.0
65. The article “Uncertainty Estimation in Railway Track Life-Cycle Cost” ( J. of Rail and Rapid
Transit , 2009) presented the following data on time to repair (min) a rail break in the high rail on
a curved track of a certain railway line.
159 120 480 149 270 547 340 43 228 202 240 218398 5 The Basics of Statistical Inference
A normal probability plot of the data shows a reasonably linear pattern, so it is plausible that the
population distribution of repair time is at least approximately normal. The sample mean and
standard deviation are 249.7 and 145.1, respectively. Is there compelling evidence for concluding
that true average repair time exceeds 200 min? Carry out a test of hypotheses using a signiﬁcance
level of .05.
66. Have you ever been frustrated because you could not get a container of some sort to release the
last bit of its contents? The article “Shake, Rattle, and Squeeze: How Much Is Left in That
Container?” ( Consumer Reports , May 2009: 8) reported on an investigation of this issue for
various consumer products. Suppose ﬁve 6.0 oz tubes of toothpaste of a particular brand are
randomly selected and squeezed until no more toothpaste will come out. Then each tube is cut
open and the amount remaining is weighed, resulting in the following data (consistent with what
the cited article reported): .53, .65, .46, .50, .37. Does it appear that the true average amount left is
less than 10% of the advertised net contents?
(a) Check the validity of any assumptions necessary for testing the appropriate hypotheses.
(b) Carry out a test of the appropriate hypotheses using a signiﬁcance level of .05. Would your
conclusion change if a signiﬁcance level of .01 had been used?
(c) Describe in context Type I and II errors, and say which error might have been made in
reaching a conclusion.
67. A random sample of soil specimens was obtained, and the amount of organic matter (%) in the
soil was determined for each specimen, resulting in the accompanying data (from “Engineering
Properties of Soil,” Soil Science , 1998: 93–102).
1.10 5.09 0.97 1.59 4.60 0.32 0.55 1.45
0.14 4.47 1.20 3.50 5.02 4.67 5.22 2.69
3.98 3.17 3.03 2.21 0.69 4.47 3.31 1.17
0.76 1.17 1.57 2.62 1.66 2.05
The values of the sample mean and standard deviation are 2.481 and 1.616, respectively. Does
this data suggest that the true average percentage of organic matter in such soil is something other
than 3%? Carry out a test of the appropriate hypotheses at signiﬁcance level .10. Would your
conclusion be different if α¼.05 had been used? [ Note: A normal probability plot of the data
shows an acceptable pattern in light of the reasonably large sample size.]
68. Glycerol is a major by-product of ethanol fermentation in wine production and contributes to the
sweetness, body, and fullness of wines. The article “A Rapid and Simple Method for Simulta-
neous Determination of Glycerol, Fructose, and Glucose in Wine” ( American J. of Enology and
Viticulture , 2007: 279–283) includes the following observations on glycerol concentration
(mg/ml) for samples of standard-quality (uncertiﬁed) white wines: 2.67, 4.62, 4.14, 3.81, 3.83.
Suppose the desired concentration value is 4. Does the sample data suggest that true average
concentration is something other than the desired value? Carry out a test of appropriate
hypotheses using the one-sample ttest with a signiﬁcance level of .05.
69. Exercise 41 gave n¼26 observations on escape time (seconds) for oil workers in a simulated
exercise, from which the sample mean and sample standard deviation are 370.69 and 24.36,
respectively. Suppose the investigators had believed a priori that true average escape time would
be at most 6 min. Does the data contradict this prior belief? Assuming normality, test the
appropriate hypotheses using a signiﬁcance level of .05.
70. Minor surgery on horses under ﬁeld conditions requires a reliable short-term anesthetic produc-
ing good muscle relaxation, minimal cardiovascular and respiratory changes, and a quick, smooth
recovery with minimal aftereffects so that horses can be left unattended. The article “A Field
Trial of Ketamine Anesthesia in the Horse” ( Equine Vet. J. , 1984: 176–179) reports that for a5.4 Testing Hypotheses About a Population Mean 399
sample of n¼73 horses to which ketamine was administered under certain conditions, the
sample average lateral recumbency (lying-down) time was 18.86 min and the standard deviation
was 8.6 min. Does this data suggest that true average lateral recumbency time under these
conditions is less than 20 min? Test the appropriate hypotheses at level of signiﬁcance .10.
71. The recommended daily dietary allowance for zinc among males older than age 50 years is
15 mg/day. The article “Nutrient Intakes and Dietary Patterns of Older Americans: A National
Study” ( J. Gerontol. , 1992: M145–150) reports the following summary data on intake for a
sample of males age 65–74 years: n¼115, /C22x¼11:3, and s¼6.43. Does this data indicate that
average daily zinc intake in the population of all males age 65–74 falls below the recommended
allowance?
72. The industry standard for the amount of alcohol poured into many types of drinks (e.g., gin for a
gin and tonic, whiskey on the rocks) is 1.5 oz. Each individual in a sample of 8 bartenders with at
least 5 years of experience was asked to pour rum for a rum and coke into a short, wide (tumbler)
glass, resulting in the following data:
2.00 1.78 2.16 1.91 1.70 1.67 1.83 1.48
(Summary quantities agree with those given in the article “Bottoms Up! The Inﬂuence of
Elongation on Pouring and Consumption Volume,” J. Consumer Res. , 2003: 455–463.)
(a) Carry out a test of hypotheses to decide whether there is strong evidence for concluding that
the true average amount poured differs from the industry standard.
(b) Does the validity of the test you carried out in (a) depend on any assumptions about the
population distribution? If so, check the plausibility of such assumptions.
73. Before agreeing to purchase a large order of polyethylene sheaths for a particular type of high-
pressure oil-ﬁlled submarine power cable, a company wants to see conclusive evidence that the
true standard deviation of sheath thickness is less than .05 mm. What hypotheses should be tested,
and why? In this context, what are the Type I and Type II errors?
74. Many older homes have electrical systems that use fuses rather than circuit breakers. A manu-
facturer of 40-amp fuses wants to make sure that the mean amperage at which its fuses burn out is
in fact 40. If the mean amperage is lower than 40, customers will complain because the fuses
require replacement too often. If the mean amperage is higher than 40, the manufacturer might be
liable for damage to an electrical system due to fuse malfunction. To verify the amperage of the
fuses, a sample of fuses is to be selected and inspected. If a hypothesis test were to be performed
on the resulting data, what null and alternative hypotheses would be of interest to the manufac-
turer? Describe Type I and Type II errors in the context of this problem situation.
75. Water samples are taken from water used for cooling as it is being discharged from a power plant
into a river. It has been determined that as long as the mean temperature of the discharged water is
at most 150/C14F, there will be no negative effects on the river’s ecosystem. To investigate whether
the plant is in compliance with regulations that prohibit a mean discharge-water temperature
above 150/C14, 50 water samples will be taken at randomly selected times, and the temperature of
each sample recorded. The resulting data will be used to test the hypotheses H0:μ¼150/C14versus
Ha:μ>150/C14. In the context of this situation, describe Type I and Type II errors. Which type of
error would you consider more serious? Explain.
76. A regular type of laminate is currently being used by a manufacturer of circuit boards. A special
laminate has been developed to reduce warpage. The regular laminate will be used on one sample
of specimens and the special laminate on another sample, and the amount of warpage will then be
determined for each specimen. The manufacturer will then switch to the special laminate only if it
can be demonstrated that the true average amount of warpage for that laminate is less than for the
regular laminate. State the relevant hypotheses, and describe the Type I and Type II errors in the
context of this situation.400 5 The Basics of Statistical Inference
5.5 Inferences for a Population Proportion
The previous two sections illustrated the methods of conﬁdence intervals and hypothesis testing for
an unknown mean, μ. In this section, we will apply those same ideas to drawing inferences about an
unknown probability or population proportion.
Letpdenote the proportion of “successes” in a population, where success identiﬁes an individual
or object that has a speciﬁed property. Equivalently, pis the probability that a randomly selected
individual or object is a success. A random sample of nindividuals is to be selected, and Xdenotes the
number of successes in the sample. The natural estimator of pis^P¼X=n, the sample fraction of
successes. As derived in Sect. 2.4and discussed earlier in this chapter, E^P/C0/C1
¼p(unbiasedness) and
SD ^P/C0/C1
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1/C0pðÞ =np
; moreover, provided np/C2110 and n(1/C0p)/C2110, ^Phas approximately a
normal distribution.
5.5.1 Confidence Intervals for p
Since ^Pis approximately normal, standardizing ^Pby subtracting pand dividing by σ^Pimplies that, for
example,
P/C01:96<^P/C0pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1/C0pðÞ =np <1:96 !
/C25:95
A conﬁdence interval for presults from replacing each <by¼and solving the resulting quadratic
equation for p. After some tedious algebra, this gives the two roots
p¼^Pþ1:962=2n/C0/C1
/C61:96ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^P1/C0^P/C0/C1
=nþ1:962=4n2q
1þ1:962=n
These form the endpoints of an approximate 95% CI for p. The more general formula is given in
the following proposition.
ONE-PROPORTION ZINTERVAL
Let ^pbe the fraction of successes in a random sample of size n. Then a conﬁdence interval for
the true/population proportion phas endpoints
ep/C6z*ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1/C0^pðÞ =nþz*ðÞ2=4n2q
1þz*ðÞ2=nð5:3Þ
where z* is the standard normal critical value for the speciﬁed conﬁdence level (e.g., z*¼1.96
for 95% conﬁdence) and epis the adjusted sample proportion of successes deﬁned by
ep¼/C2^pþz*ðÞ2=2n/C3
=/C2
1þz*ðÞ2=n/C3
.
This is often referred to as the score conﬁdence interval forp.
If the sample size nis very large, then all the terms in Expression ( 5.3) of order 1/ nare negligible
compared to the others. Keeping only the dominant terms, Eq. ( 5.3) is approximated by5.5 Inferences for a Population Proportion 401
^p/C6z*/C1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1/C0^pðÞ
nr
ð5:4Þ
This approximate CI (Eq. 5.4) has the form ^p/C6z*/C1^σ^P, similar to the large-sample CI for μ
presented in Sect. 5.3, and is the one that for decades has appeared in introductory statistics textbooks.
It clearly has a much simpler and more appealing form than Eq. ( 5.3), so why bother with the score
interval at all?
Suppose we use z*¼1.96 in the traditional formula ( 5.4). Then our nominal conﬁdence level (the
one we think we’re buying by using that zcritical value) is approximately 95%. So before a sample is
selected, the probability that the random interval includes the actual value of p(i.e., the coverage
probability ) should be about .95. But it turns out that the actual coverage probability for this interval
can differ considerably from the nominal probability .95, particularly when pis not close to .5. This is,
generally speaking, a deﬁciency of the traditional interval—the actual conﬁdence level can be quite
different from the nominal level even for reasonably large sample sizes. Recent research has shown
that the score interval (Eq. 5.3) rectiﬁes this behavior—for virtually all sample sizes and values of p,
its actual conﬁdence level will be quite close to the nominal level speciﬁed by the choice of z*. This is
due largely to the fact that the interval (in particular, the midpoint ep) is shifted a bit toward .5
compared to the traditional interval. This is especially important when pis close to 0 or 1.
In addition, the score interval can be used with nearly all sample sizes and parameter values. It is
thus not necessary to check the conditions n^p/C2110 and n1/C0^pðÞ /C21 10 which would be required were
the traditional interval employed. So rather than asking when nis large enough for Eq. ( 5.4) to yield a
good approximation to Eq. ( 5.3), our recommendation is that the score CI should always be used
unless the sample size is extremely large (such as in simulations, where n¼10,000 or more is
typical). The slight additional tediousness of the computation is outweighed by the desirable
properties of the interval.
Example 5.20 A Gallup poll published June 28, 2013 reported that 41% of US adults surveyed felt
that the most important factor in choosing which college or university to attend should be the
percentage of graduates who are able to get a good job. (This was the most popular response; cost
of tuition was a close second.) The survey was based on a random sample of n¼1012 adults; we will
assume the number who gave the above response is x¼415, so that ^p¼415=1012¼:4101,
matching the survey. Let pdenote the proportion of allUS adults that feel this same way, for
which ^pis our point estimate. A conﬁdence interval for pwith a conﬁdence level of approximately
95% is
:4101þ1:962=2 1012ðÞ
1þ1:962=1012/C61:96ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:4101ðÞ :5899ðÞ =1012þ1:962=4/C110122/C0/C1q
1þ1:962=1012
¼:4103/C6:0302¼:3801 ;:4405 ðÞ
Hence, we are 95% conﬁdent that between 38 and 44% of all US adults feel that the percentage of
graduates that get good jobs is the most important factor when choosing a college or university. The
traditional interval is
:4101/C61:96ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:410ðÞ :590ðÞ
1012r
¼:4101/C6:0303¼:3798 ;:4404 ðÞ
These two intervals are practically identical because n¼1012 is so large. ■402 5 The Basics of Statistical Inference
Example 5.21 The article “Repeatability and Reproducibility for Pass/Fail Data” ( J. Testing Eval. ,
1997: 151–153) reported that in n¼48 trials in a particular laboratory, 16 resulted in ignition of a
particular type of substrate by a lighted cigarette. Let pdenote the long-run proportion of all such
trials that would result in ignition. A point estimate for pis^p¼16=48¼:333. A 95% conﬁdence
interval for pis
:333þ1:962=96
1þ1:962=48/C61:96ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:333ðÞ :667ðÞ =48þ1:962=4/C1482/C0/C1q
1þ1:962=48¼:346/C6:129¼:217;:475 ðÞ
So, the researchers can be 95% conﬁdent that between 21.7 and 47.5% of all trials under the same
conditions will result in ignition. This interval isn’t very precise—its width is nearly 26 percentage
points—as a consequence of the relatively small sample size. If the researchers wanted a narrower
interval, they would need to use a larger n(which, of course, requires more time and money).
The traditional CI formula ( 5.4) gives
:333/C61:96ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:333ðÞ :667ðÞ =48p
¼:333/C6:133¼:200;:466 ðÞ
These two intervals are somewhat different because n¼48 is not very large. ■
5.5.2 Hypothesis Testing for p
Analogous to hypothesis testing for a population mean μ, tests for pconcern deciding which of
two competing hypotheses about the value of pis correct. The null hypothesis will always be written
in the form
H0:p¼p0
where p0is the null value for the parameter p(i.e., the value claimed for pby the null hypothesis). The
alternative hypothesis has one of three forms, depending on context:
Ha:p>p0Ha:p<p0Ha:p6¼p0
Inferences about pare again based on the value of a sample proportion, ^P. When H0is true,
E^P/C0/C1
¼p0and SD ^P/C0/C1
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0 ðÞ =np
. Moreover, when nis large and H0is true, the test statistic
Z¼^P/C0p0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0 ðÞ =np
has approximately a standard normal distribution. The P-value of the hypothesis test is then
determined in an analogous manner to those of the one-sample ttest in Sect. 5.4, except that
calculation is made using the ztable rather than a tdistribution.
ONE-PROPORTION ZTEST
Consider testing the null hypothesis H0:p¼p0based on a random sample of size n. Let ^P
denote the proportion of “successes” in the sample. The test statistic is
Z¼^P/C0p0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0 ðÞ =np5.5 Inferences for a Population Proportion 403
Provided np0/C2110 and n(1/C0p0)/C2110,Zhas approximately a standard normal distribution
when H0is true. Let zdenote the calculated value of the test statistic. The calculation of the
P-value depends on the choice of Haas follows:
Alternative Hypothesis P-value
Ha:p>p0 1/C0Φ(z)
Ha:p<p0 Φ(z)
Ha:p6¼p0 2[1/C0Φ(|z|)]
Illustrations of these P-values are essentially identical to those in Fig. 5.10.
Example 5.22 Obesity is an increasing problem in America among all age groups. The Centers for
Disease Control and Prevention (CDCP) reported in 2012 that 35.7% of US adults are obese (a body
mass index exceeding 30; this index is a measure of weight relative to height). Physicians at a large
hospital in Los Angeles measured the body mass index of 122 randomly selected patients and found
that 38 of them should be classiﬁed as obese. Do the hospital’s data suggest that the true proportion of
adults served by this hospital who are obese is less than the national ﬁgure of 35.7%? Let’s carry out a
test of hypotheses using α¼.05.
The parameter of interest is p¼the proportion of all adults served by this hospital who are obese.
The competing hypotheses are
H0:p¼.357 (the hospital’s obesity rate matches the national rate)
Ha:p<.357 (the hospital’s obesity rate is less than the national rate)
Since np0¼122(.357) ¼43.6/C2110 and n(1/C0p0)¼122(1 /C0.357) ¼78.4/C2110, the
one-proportion ztest may be applied. With ^p¼38=122¼:311, the calculated value of the test
statistic is
z¼^p/C0p0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0 ðÞ =np ¼:311/C0:357ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:357 1 /C0:357ðÞ =122p ¼/C01:05
So, the observed sample proportion is about one standard deviation below what we would expect if
the null hypothesis is true .T h e P-value of the test is the probability of obtaining a test statistic value at
least that low:
P-value ¼PZ/C20/C01:05 ðÞ ¼ Φ/C01:05ðÞ ¼ :1469
Since the P-value of .1469 is greater than the signiﬁcance level .05, we fail to reject H0. On the
basis of the observed data, we cannot conclude that the obesity rate of the population served by this
hospital is less than the national rate of 35.7%. ■
As was the case for inferences on μ, it is desirable to calculate the power of a hypothesis test
concerning a population proportion p. The power of our one-sample ztest depends on how far the true
value of pis from the null value p0, the sample size, and the selected signiﬁcance level. The details of
such power calculations, which many software packages can perform automatically, are developed in
Exercises 96 and 97 of this section.
Inferences about pwhen nis small can be based directly on the binomial distribution. There are
also procedures available for making inferences about a difference p1/C0p2between two population404 5 The Basics of Statistical Inference
proportions (e.g., the proportion of all female versus male students that make the honor roll at your
school). Please consult the reference by Devore and Berk for more information.
5.5.3 Software for Inferences about p
Theprop.test function in R will calculate the traditional CI (Eq. 5.4) for a population proportion
and perform a one-proportion ztest upon request. Figure 5.13 shows output corresponding to
Examples 5.21 and 5.22. The 95% CI in Fig. 5.13a forpis roughly (.208, .485); the difference
between this interval and the traditional interval provided in Example 5.21 is due to rounding and an
adjustment made automatically in R called Yates’ continuity correction . The inputs to prop.test
in Fig. 5.13b include not only the raw data xandn, but also the null value of pand the direction of the
test. The resulting P-value, .1698, is close to the value of .1469 obtained in Example 5.22. Again, the
disparity comes from a combination of rounding and the continuity correction. Unfortunately, to the
authors’ knowledge, there are no one-proportion zintervals or ztests built into Matlab.
5.5.4 Exercises: Section 5.5(77–97)
77. In a sample of 1000 randomly selected consumers who had opportunities to send in a rebate claim
form after purchasing a product, 250 of these people said they never did so (“Rebates: Get What
You Deserve,” Consumer Reports , May 2009: 7). Reasons cited for their behavior included too
many steps in the process, amount too small, missed deadline, fear of being placed on a mailing
list, lost receipt, and doubts about receiving the money. Calculate and interpret a 95% conﬁdence
level for the true proportion of such consumers who never apply for a rebate.> prop.test(16,48)
        1-sample proportions test with continuity correction
data:  16 out of 48, null probability 0.5 
X-squared = 4.6875, df = 1, p-value = 0.03038
alternative hypothesis: true p is not equal to 0.5 
95 percent confidence interval:
 0.2080794 0.4851357 
sample estimates:
        p 
0.3333333 
> prop.test(38,122,p=.357,"less")
        1-sample proportions test with continuity correction
data:  38 out of 122, null probability 0.357 
X-squared = 0.9121, df = 1, p-value = 0.1698
alternative hypothesis: true p is less than 0.357 
95 percent confidence interval:
 0.0000000 0.3881457 
sample estimates:
        p 
0.3114754a
b
Fig. 5.13 Inferences on pin R: ( a) Example 5.21; ( b) Example 5.225.5 Inferences for a Population Proportion 405
78. A Wireless News article (July 6, 2008) found that 62% of people surveyed would use a Bluetooth
device while driving in order to comply with the law. The survey was based upon a random
sample of 600 cell phone users. Construct a 95% conﬁdence interval for the proportion of all cell
phone users who will use Bluetooth technology while driving.
79. The article “Limited Yield Estimation for Visual Defect Sources” ( IEEE Trans. Semicon. Manuf. ,
1997: 17–23) reported that, in a study of a particular wafer inspection process, 356 dies were
examined by an inspection probe and 201 of these passed the probe. Assuming a stable process,
calculate a 99% conﬁdence interval for the proportion of all dies that pass the probe.
80. The technology underlying hip replacements has changed as these operations have become more
popular (over 250,000 in the USA in 2008). Starting in 2003, highly durable ceramic hips were
marketed. Unfortunately, for too many patients the increased durability has been counterbalanced
by an increased incidence of squeaking. The May 11, 2008, issue of the New York Times reported
that in one study of 143 individuals who received ceramic hips between 2003 and 2005, 10 of the
hips developed squeaking. Calculate and interpret 95% conﬁdence interval for the true proportion
of such hips that develop squeaking.
81. The Pew Forum on Religion and Public Life reported on December 9, 2009, that in a survey of
2003 American adults, 25% said they believed in astrology. Calculate and interpret a conﬁdence
interval at the 99% conﬁdence level for the proportion of all adult Americans who believe in
astrology.
82. Reconsider the score CI (Eq. 5.3) for p, and focus on a conﬁdence level of 95%. Show that the
endpoints agree quite well with those of the traditional interval (Eq. 5.4) once two successes and
two failures have been appended to the sample, i.e., Eq. ( 5.4) based on ( x+2)S’s in ( n+4)
trials. [ Hint: 1.96 /C252.]
83. It is often important in planning studies to know in advance what sample size is required to
estimate an unknown proportion to within a certain margin of error.
(a) Suppose we wish to achieve a bound Bon the margin of error of a CI for p. By equating the
margin of error in the “very large n” CI Eq. ( 5.4)t oBand solving for n, show that the
required sample size is
n¼z*ðÞ2^p1/C0^pðÞ
B2
(b) A state legislator wishes to survey residents of her district to see what proportion of the
electorate is aware of her position on using state funds to pay for abortions. If the legislator
has strong reason to believe that at least 2/3 of the electorate know of her position, how large
a sample size would you recommend in order to estimate the true proportion to within /C6
5 percentage points? Assume 95% conﬁdence.
(c) What sample size is necessary if the 95% CI for pis to have width of at most .10 irrespective
of^p?[Hint: What value of ^pmakes the expression for nas large as possible?]
84. Write a function in Matlab or R to implement (Eq. 5.3). Your function should have three inputs:
the number of successes x, the sample size n, and the desired conﬁdence level. The output of the
function should be the endpoints of the CI.
85. Natural cork in wine bottles is subject to deterioration, and as a result wine in such bottles may
experience contamination. The article “Effects of Bottle Closure Type on Consumer Perceptions
of Wine Quality” ( Amer. J. of Enology and Viticulture , 2007: 182–191) reported that, in a tasting
of commercial chardonnays, 16 of 91 bottles were considered spoiled to some extent by cork-
associated characteristics. Does this data provide strong evidence for concluding that more than406 5 The Basics of Statistical Inference
15% of all such bottles are contaminated in this way? Carry out a test of hypotheses using a
signiﬁcance level of .10.
86. It is known that roughly 2/3 of all human beings have a dominant right foot or eye. Is there also
right-sided dominance in kissing behavior? The article “Human Behavior: Adult Persistence of
Head-Turning Asymmetry” ( Nature , 2003: 771) reported that in a random sample of 124 kissing
couples, both people in 80 of the couples tended to lean more to the right than to the left. Does the
result of the experiment suggest that the 2/3 ﬁgure is implausible for kissing behavior? State and
test the appropriate hypotheses.
87. The article referenced in Exercise 85 also reported that in a sample of 106 wine consumers,
22 (20.8%) thought that screw tops were an acceptable substitute for natural corks. Suppose a
particular winery decided to use screw tops for one of its wines unless there was strong evidence
to suggest that fewer than 25% of wine consumers found this acceptable.
(a) Using a signiﬁcance level of .10, what would you recommend to the winery?
(b) For the hypotheses tested in (a), describe in context what the Type I and II errors would be,
and say which type of error might have been committed.
88. With domestic sources of building supplies running low several years ago, roughly 60,000 homes
were built with imported Chinese drywall. According to the article “Report Links Chinese
Drywall to Home Problems” ( New York Times , November 24, 2009), federal investigators
identiﬁed a strong association between chemicals in the drywall and electrical problems, and
there is also strong evidence of respiratory difﬁculties due to the emission of hydrogen sulﬁde
gas. An extensive examination of 51 homes found that 41 had such problems. Suppose these
51 were randomly sampled from the population of all homes having Chinese drywall. Does the
data provide strong evidence for concluding that more than 50% of all homes with Chinese
drywall have electrical/environmental problems? Carry out a test of hypotheses using α¼.01.
89. A common characterization of obese individuals is that their body mass index is at least
30 [BMI ¼weight/(height)2when height is in meters and weight is in kilograms]. The article
“The Impact of Obesity on Illness Absence and Productivity in an Industrial Population of
Petrochemical Workers” ( Annals of Epidemiology , 2008: 8–14) reported that in a sample of
female workers, 262 had BMIs of less than 25, 159 had BMIs that were at least 25 but less than
30, and 120 had BMIs exceeding 30. Is there compelling evidence for concluding that more than
20% of the individuals in the sampled population are obese?
(a) State and test appropriate hypotheses using a signiﬁcance level of .05.
(b) Explain in the context of this scenario what constitutes Type I and II errors.
90. The article “Analysis of Reserve and Regular Bottlings: Why Pay for a Difference Only the
Critics Claim to Notice?” ( Chance , Summer 2005, pp. 9–15) reported on an experiment to
investigate whether wine tasters could distinguish between more expensive reserve wines and
their regular counterparts. Wine was presented to tasters in four containers labeled A, B, C,
and D, with two of these containing the reserve wine and the other two the regular wine. Each
taster randomly selected three of the containers, tasted the selected wines, and indicated which of
the three he/she believed was different from the other two. Of the n¼855 tasting trials,
346 resulted in correct distinctions (either the one reserve that differed from the two regular
wines or the one regular wine that differed from the two reserves). Does this provide compelling
evidence for concluding that tasters of this type have some ability to distinguish between reserve
and regular wines? State and test the relevant hypotheses. Are you particularly impressed with the
ability of tasters to distinguish between the two types of wine?
91. The article “Heavy Drinking and Polydrug Use Among College Students” ( J. of Drug Issues ,
2008: 445–466) stated that 51 of the 462 college students in a sample had a lifetime abstinence5.5 Inferences for a Population Proportion 407
from alcohol. Does this provide strong evidence for concluding that more than 10% of the
population sampled had completely abstained from alcohol use? Test the appropriate hypotheses.
[Note: The article used more advanced statistical methods to study the use of various drugs
among students characterized as light, moderate, and heavy drinkers.]
92. Scientists have recently become concerned about the safety of Teﬂon cookware and various food
containers because perﬂuorooctanoic acid (PFOA) is used in the manufacturing process. An
article in the July 27, 2005, New York Times reported that of 600 children tested, 96% had PFOA
in their blood. According to the FDA, 90% of all Americans have PFOA in their blood. Does the
data on PFOA incidence among children suggest that the percentage of all children who have
PFOA in their blood exceeds the FDA percentage for all Americans? Carry out an appropriate
test of hypotheses at the α¼.05 level.
93. A manufacturer of nickel–hydrogen batteries randomly selects 100 nickel plates for test cells,
cycles them a speciﬁed number of times, and determines that 14 of the plates have blistered. Does
this provide compelling evidence for concluding that more than 10% of all plates blister under
such circumstances? State and test the appropriate hypotheses using a signiﬁcance level of .05. In
reaching your conclusion, what type of error might you have committed?
94. A random sample of 150 recent donations at a blood bank reveals that 82 were type A blood.
Does this suggest that the actual percentage of type A donations differs from 40%, the percentage
of the population having type A blood? Carry out a test of the appropriate hypotheses using a
signiﬁcance level of .01. Would your conclusion have been different if a signiﬁcance level of .05
had been used?
95. The article “Statistical Evidence of Discrimination” ( J. Amer. Statist. Assoc. , 1982: 773–783)
discusses the court case Swain v. Alabama (1965), in which it was alleged that there was
discrimination against blacks in grand jury selection. Census data suggested that 25% of those
eligible for grand jury service were black, yet a random sample of 1050 people called to appear
for possible duty yielded only 177 blacks. Using a level .01 test, does this data argue strongly for
a conclusion of discrimination?
96. Consider testing hypotheses H0:p¼p0versus Ha:p<p0. Suppose that, in fact, the true value of
the parameter pisp0, where p0<p0(soHais true).
(a) Show that the expected value and variance of the test statistic Zin the one-proportion
ztest are
EZðÞ ¼p0/C0p0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0 ðÞ =np VarZðÞ ¼p01/C0p0ðÞ =n
p01/C0p0 ðÞ =n
(b) It can be shown that P-value /C20αiffZ/C20/C0zα, where /C0zαdenotes the αquantile of the
standard normal distribution (i.e., Φ(/C0zα)¼α). Show that the power of the lower-tailed
one-sample ztest when p¼p0is given by
Φp0/C0p0/C0zαﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0 ðÞ =np
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p01/C0p0ðÞ =np !
(c) A package-delivery service advertises that at least 90% of all packages brought to its ofﬁce
by 9 a.m. for delivery in the same city are delivered by noon that day. Let pdenote the true
proportion of such packages that are delivered as advertised and consider the null
hypotheses H0:p¼.9 versus the alternative Ha:p<.9. If only 80% of the packages are
delivered as advertised, how likely is it that a level .01 test based on n¼225 packages will
detect such a departure from H0?408 5 The Basics of Statistical Inference
97. Because of variability in the manufacturing process, the actual yielding point of a sample of mild
steel subjected to increasing stress will usually differ from the theoretical yielding point. Let
pdenote the true proportion of samples that yield before their theoretical yielding point. If on the
basis of a sample it can be concluded that more than 20% of all specimens yield before the
theoretical point, the production process will have to be modiﬁed.
(a) If 15 of 60 specimens yield before the theoretical point, what is the P-value when the
appropriate test is used, and what would you advise the company to do?
(b) If the true percentage of “early yields” is actually 50% (so that the theoretical point is the
median of the yield distribution) and a level .01 test is used, what is the probability that the
company concludes a modiﬁcation of the process is necessary? [ Hint: Refer back to the
previous exercise. Modify the expression in part (b) to accommodate an upper-tailed test.]
5.6 Bayesian Inference
Throughout this chapter, we have regarded parameters such as μ,σ,p, and λas having an unknown
but single, ﬁxed value. This is often referred to as the classical orfrequentist approach to statistical
inference. However, there is a different paradigm, called subjective orBayesian inference , in which
an unknown parameter is assigned a distribution of possible values, analogous to a probability
distribution. This distribution reﬂects all available information—past experience, intuition, common
sense—about the parameter prior to observing the data. For this reason, it is called the prior
distribution of the parameter.
DEFINITION
Aprior distribution for a parameter θ, denoted π(θ), is a probability distribution on the set of
possible values for θ. In particular, if the possible values of the parameter θform an interval I,
thenπ(θ) is a pdf that must satisfy
Z
IπθðÞdθ¼1
Similarly, if θis potentially any value in a discrete set D, then π(θ) is a pmf that must satisfy
X
θ2DπθðÞ ¼ 1
Example 5.23 Consider the parameter μ¼the mean GPA of all students at your university. Since
GPAs are always between 0.0 and 4.0, μmust also lie in this interval. But common sense tells you that
μis unlikely to be below 2.0, or very few people would graduate, and it would be likewise surprising
to ﬁnd μmuch above 3.5. This “prior belief” can be expressed mathematically as a prior distribution
forμon the interval I¼[0, 4]. If our best guess a priori is that μ/C252.5, then our prior distribution
π(μ) should be centered around 2.5. The variability of the prior distribution we select should reﬂect
how sure we feel about our initial information.
If we feel very sure that μis near 2.5, then we should select a prior distribution for μthat has less
variation around that value. On the other hand, if we are less certain, this can be reﬂected by a prior
distribution with much greater variability. Figure 5.14 illustrates these two cases.5.6 Bayesian Inference 409
5.6.1 The Posterior Distribution of a Parameter
The key to Bayesian inference is having a mathematically rigorous way to incorporate the actual
sample data. Suppose we observe values x1,...,xnfrom a distribution depending on the unknown
parameter θfor which we have selected some prior distribution. Then a Bayesian statistician wants to
“update” her or his belief about the distribution of θ, taking into account both prior belief and the
observed xis. This is achieved using a form of Bayes’ Theorem for random variables.
DEFINITION
Suppose X1,...,Xnhave joint pdf f(x1,...,xn;θ) and the unknown parameter θhas been
assigned a continuous prior distribution π(θ). Then the posterior distribution ofθ, given the
observations X1¼x1,...,Xn¼xn,i s
πθ/C12/C12x1,...,xn/C0/C1
¼πθðÞfx1;...;xn;θ ðÞZ1
/C01πθðÞfx1;...;xn;θ ðÞ dθð5:5Þ
The integral in the denominator of Eq. ( 5.5) insures that the posterior distribution is a valid
probability density for θ.
IfX1,...,Xnare discrete, the joint pdf is replaced by their joint pmf and integration by
summation.
Notice that constructing the posterior distribution of a parameter requires a speciﬁc probability
model f(x1,...,xn;θ) for the observed data. In Example 5.23, it would not be enough to simply
observe the GPAs of a random sample of nstudents; one must specify the underlying distribution,
with mean μ, from which those GPAs are drawn.
Example 5.24 Emissions of subatomic particles from a radiation source are often modeled as a
Poisson process. As we shall see in Chap. 7, this implies that the time between successive emissionsnarrower
prior
wider
priorπ(m)
m3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
01234
Fig. 5.14 Two prior distributions for a parameter: a more diffuse prior (less certainty) and a more concentrated prior
(more certainty) ■410 5 The Basics of Statistical Inference
follows an exponential distribution. In practice, the parameter λof this distribution is typically
unknown. If researchers believe a priori that the average time between emissions is about half a
second, so λ/C252, a prior distribution with a mean around 2 might be selected for λ. One example is
the following gamma distribution, which has mean (and variance) of 2:
πλðÞ ¼ λe/C0λ,λ>0
Notice the gamma distribution lies on the interval (0, 1), which is also the set of possible values
for the unknown parameter λ.
The times X1,...,X5between ﬁve particle emissions will be recorded; it is these variables that
have an exponential distribution with the unknown parameter λ(equivalently, mean 1/ λ). Because the
Xis are also independent, their joint pdf is
fx1;...;x5;λ ðÞ ¼ fx1;λðÞ /C1 /C1 /C1 fx5;λðÞ ¼ λe/C0λx1/C1/C1/C1λe/C0λxn¼λ5e/C0λΣxi
Applying Eq. ( 5.5) with these two components, the posterior distribution of λgiven the observed
data is
πλ/C12/C12x1,...,x5/C0/C1
¼πλðÞfx1;...;x5;λ ðÞZ1
/C01πλðÞfx1;...;x5;λ ðÞ dλ¼λe/C0λ/C1λ5e/C0λΣxi
Z1
0λe/C0λ/C1λ5e/C0λΣxidλ¼λ6e/C0λ1þΣxi ðÞ
Z1
0λ6e/C0λ1þΣxi ðÞdλ
Suppose the ﬁve observed inter-emission times are x1¼0.66, x2¼0.48, x3¼0.44, x4¼0.71,
x5¼0.56. The sum of these ﬁve times is ∑xi¼2.85, and so the posterior distribution simpliﬁes to
πλ/C12/C120:66, ...,0:56/C0/C1
¼λ6e/C03:85λ
Z1
0λ6e/C03:85λdλ¼3:857
6!λ6e/C03:85λ
The integral in the denominator was evaluated using the gamma integral formula ( 3.5) from
Chap. 3; as noted previously, the purpose of this integral is to guarantee that the posterior distribution
ofλis a valid probability density. As a function of λ, we recognize this as a gamma distribution with
parameters α¼7 and β¼1/3.85. The prior and posterior density curves of λappear in Fig. 5.15.
posteriorDensity
prior0.6
0.5
0.4
0.3
0.2
0.1
0
024λ
68 1 0
Fig. 5.15 Prior and posterior distribution of λfor Example 5.24 ■5.6 Bayesian Inference 411
Example 5.25 A 2010 National Science Foundation study found that 488 out of 939 surveyed adults
incorrectly believe that antibiotics kill viruses (they only kill bacteria). Let θdenote the proportion of
allUS adults that hold this mistaken view. Imagine that an NSF researcher, in advance of
administering the survey, believed (hoped?) the value of θwas roughly 1 in 3, but he was very
uncertain about this belief. Since any proportion must lie between 0 and 1, the beta family of
distributions from Sect. 3.5provides a natural source of priors for θ. One such beta distribution,
with an expected value of 1/3, is the Beta(2, 4) model whose pdf is
πθðÞ ¼ 20θ1/C0θðÞ30<θ<1
The data mentioned at the beginning of the example can be considered either a random sample of
size 939 from the Bernoulli distribution or, equivalently, a single observation from the binomial
distribution with n¼939. Let X¼the number of US adults in a random sample of 939 that believe
antibiotics kill viruses. Then X~ Bin(939, θ), and the pmf of Xisp(x;θ)¼939
x/C18/C19
θx1/C0θðÞ939/C0x.
Substituting the observed value x¼488, Eq. ( 5.5) gives the posterior distribution of θas
πθ/C12/C12X¼488/C0/C1
¼πθðÞp488 ;θðÞZ
πθðÞp488 ;θðÞ dθ¼20θ1/C0θðÞ3/C1939
488/C18/C19
θ4881/C0θðÞ451
Z1
020θ1/C0θðÞ3/C1939
488/C18/C19
θ4881/C0θðÞ451dθ
¼θ4891/C0θðÞ454
Z1
0θ4891/C0θðÞ454dθ¼c/C1θ4891/C0θðÞ4540<θ<1
Recall that the constant c, which equals the reciprocal of the integral in the denominator, serves to
insure that the posterior distribution π(θ|X¼488) integrates to 1. Rather than evaluate the integral,
we can simply recognize the expression θ489(1/C0θ)454as a standard beta distribution, speciﬁcally
with parameters α¼490 and β¼455, that’s just missing the constant of integration in front.
It follows that the posterior distribution of θgiven X¼488 must be Beta(490, 455); if we require
c, it can be copied directly from the beta pdf. (This trick comes in handy quite often in Bayesian
statistics: if we can recognize a posterior distribution as being the “kernel” of a particular probability
distribution, then it must necessarily be that distribution.)
The prior and posterior density curves for θare displayed in Fig. 5.16. While the prior distribution
is centered around 1/3 and exhibits a great deal of uncertainty (variability), the posterior distribution
ofθis centered much closer to the sample proportion of incorrect answers, 488/939 /C25.52, with
considerably less uncertainty.
0 0.25 0.50 0.75 0 0.2 0.4q qπ(q)π ( q|x)
0.6 0.8 1 1ab
Fig. 5.16 Density curves for the parameter θin Example 5.25: ( a) prior Beta(2, 4); ( b) posterior Beta(490, 455) ■412 5 The Basics of Statistical Inference
5.6.2 Inferences from the Posterior Distribution
Inferences about an unknown parameter can be obtained from its posterior distribution. The most
common Bayesian point estimate for a parameter θis the mean of its posterior distribution:
^θ¼Eθ/C12/C12x1,...,xn/C0/C1
An interval [ a,b] having posterior probability .95 gives a 95% credibility interval , the Bayesian
analogue of a 95% conﬁdence interval (but the interpretation is different). Typically one selects the
middle 95% of the posterior distribution, i.e., the endpoints of a 95% credibility interval are ordinarily
the .025 and .975 quantiles of the posterior distribution.
Example 5.26 (Example 5.24 continued) Given the observed values of X1,...,X5, we previously
found that the mean emission rate λhas a Gamma(7, 1/3.85) posterior distribution. Thus, the mean of
the posterior distribution of λis
^λ¼Eλ/C12/C120:66, ...,0:56/C0/C1
¼αβ¼71=3:85ðÞ ¼ 1:82
This isn’t too different from the researchers’ prior belief that λ/C252. A 95% credibility interval for
λrequires determining the .025 and .975 quantiles of the Gamma(7, 1/3.85) model; using statistical
software, η.025¼0.7310 and η.975¼3.3921. Under the Bayesian interpretation, having observed the
ﬁve aforementioned inter-emission times, there is a 95% posterior probability that λis between
0.7310 and 3.3921 emissions per second. ■
Example 5.27 (Example 5.25 continued) The posterior distribution of the parameter θ¼the
proportion of all US adults that incorrectly believe antibiotics kill viruses was a Beta(490, 455)
distribution. A point estimate of θis the mean of this distribution:
^θ¼Eθ/C12/C12X¼488/C0/C1
¼α
αþβ¼490
490þ455¼490
945¼:5185
Notice this is quite close to the traditional estimate x/n¼488/939 ¼.5197; in general, when nis
large the mean of the posterior distribution of a parameter will be quite similar to its more traditional,
frequentist estimate.
The .025 and .975 quantiles of this beta distribution are η.025¼.4866 and η.975¼.5503. So, after
observing the results of the NSF survey, there is a 95% posterior probability that θis between .4866
and .5503. ■
5.6.3 Further Comments on Bayesian Inference
In most cases, the role of the observed values in shaping the posterior distribution of a parameter θ
increases as the sample size nincreases. More precisely, it can be shown that under very general
conditions, as n!1 the mean of the posterior distribution will converge to the true value of θwhile
the variance of the posterior distribution of θconverges to zero:
Eθ/C12/C12X1,...,Xn/C0/C1
!θ Var θ/C12/C12X1,...,Xn/C0/C1
!0
The second property manifests itself in our two previous examples: the variability of the posterior
distribution of λbased on n¼5 observations was still rather substantial, while the posterior
distribution of θbased on a sample of size n¼939 was quite concentrated.5.6 Bayesian Inference 413
Since traditional estimators such as ^Pand /C22Xconverge to the true values of corresponding
parameters (e.g., porμ) by the Law of Large Numbers, it follows that Bayesian and frequentist
estimates will typically be quite close when nis large. This is true both for the point estimates and the
interval estimates. But when nis small—a common occurrence in Bayesian methodology—parame-
ter estimates can differ drastically between the two methods. This is especially true if the researcher’s
prior belief is very far from what’s actually true (e.g., believing a proportion is around 1/3 when it’s
really more than .5).
It should be emphasized that even if the conﬁdence interval is nearly the same as the credibility
interval for a parameter, they have different interpretations. To interpret the Bayesian credibility
interval, we say that there is a 95% probability that the parameter θis in the interval. However, for the
frequentist conﬁdence interval such a probability statement does not make sense: as we discussed in
Sect. 5.3, neither the parameter θnor the endpoints of the interval are considered random under the
classical, frequentist view.
In the examples of this section, prior distributions were chosen partially by matching the mean of a
distribution to someone’s a priori “best guess” about the value of the parameter. We also mentioned at
the beginning of the section that the variability of the prior distribution often reﬂects the strength of
that belief. In practice, there is a third consideration for choosing a prior distribution: the ability to
apply Eq. ( 5.5) in a simple fashion. Ideally, we would like to choose a prior distribution from a family
(gamma, beta, etc.) such that the posterior distribution is from that same family. When this happens
we say that the prior distribution is conjugate to the data distribution.
In Example 5.24, the prior π(λ) is the Gamma(2, 1) pdf; we determined, using Eq. ( 5.5), that the
posterior distribution was Gamma(7, 1/3.85). It can be shown in general (Exercise 104) that any
gamma distribution is conjugate to an exponential data distribution. Similarly, the prior and posterior
distributions of θin Example 5.25 were Beta(2, 4) and Beta(490, 455), respectively. Exercise
105 shows that any beta distribution is conjugate to a binomial (or Bernoulli) data distribution. If
data are normally distributed with known σ, then a normal prior for μresults in a normal posterior.
The case of unknown σis more complicated; see Section 14.4 of the reference by Devore & Berk.
5.6.4 Exercises: Section 5.6(98–106)
98. Nationwide, IQs have a normal distribution with mean 100 and standard deviation 15. Let X1,...,
Xnrepresent the IQs of a random sample of ﬁrst graders, which we assume also come from a
normal distribution having σ¼15 but possibly a different mean μ. Assign a N(110, 7.5) prior
distribution to μ.
(a) Find the posterior distribution of μ.
(b) Here are the actual IQ scores of a random sample of n¼18 ﬁrst graders:
113 108 140 113 115 146 136 107 108
119 132 127 118 108 103 103 122 111
Calculate a point estimate of μusing the posterior distribution.
(c) Calculate and interpret 95% credibility interval for μ.
(d) Calculate a one-sample z95% conﬁdence interval for μusing the 18 observations with
σ¼15, and compare with the credibility interval of (b).
99. The number of customers arriving during a 1-h period at an ice cream shop is modeled by a
Poisson distribution with unknown parameter μ. Based on past experience, the owner believes
that the average number of customers in 1 h is about 15.414 5 The Basics of Statistical Inference
(a) Assign a prior to μfrom the gamma family of distributions, such that the mean of the prior is
15 and the standard deviation is 5 (reﬂecting moderate uncertainty).
(b) The number of customers in ten randomly selected 1-h intervals is recorded:
16 9 11 13 17 17 8 15 14 16
Find the posterior distribution of μ.
(c) Find and interpret a 95% credibility interval for μ.
100. In a study of 70 restaurant bills, 40 of the 70 were paid using cash. Let pdenote the population
proportion paying cash.
(a) Assuming a beta prior distribution for pwith α¼2 and β¼2, obtain the posterior
distribution of p.
(b) Repeat (a) on with αandβpositive and close to 0.
(c) Calculate a 95% credibility interval for pusing (b). Is your interval compatible with
p¼.5?
(d) Calculate a 95% conﬁdence interval for pusing Eq. ( 5.3), and compare with the result of
(c).
(e) Compare the interpretations of the credibility interval and the conﬁdence interval.
(f) Based on the prior in (b), test the hypothesis p/C20.5 using the posterior distribution to ﬁnd
P(p/C20.5).
101. For the data of Example 5.25 assume a Beta(2, 4) prior distribution and assume that the
939 observations are a random sample from the Bernoulli distribution. Use Eq. ( 5.5) to derive
the posterior distribution, and compare your answer with the result of Example 5.25.
102. Laplace’s rule of succession says that if there have been nBernoulli trials and they have all been
successes, then the probability of a success on the next trial is ( n+1)/(n+2). For the
derivation, Laplace used a Beta(1, 1) prior for the parameter p.
(a) Show that, if a Beta(1, 1) prior is assigned to pand there are nsuccesses in ntrials, then the
posterior mean of pis (n+1)/(n+2).
(b) Explain (a) in terms of total successes and failures; that is, explain the result in terms of two
prior trials plus nlater trials.
(c) Laplace applied his rule of succession to compute the probability that the sun will rise
tomorrow using 5000 years, or n¼1,826,214 days of history in which the sun rose every
day. Is Laplace’s method equivalent to including two prior days when the sun rose once
and failed to rise once? Criticize the answer in terms of total successes and failures.
103. Suppose you have a random sample X1,X2,...,Xnfrom the Poisson distribution with mean μ.I f
the prior distribution for μhas a gamma distribution with parameters αandβ, show that the
posterior distribution is also gamma distributed. What are its parameters?
104. Suppose you have a random sample X1,X2,...,Xnfrom the exponential distribution with
parameter λ. If the prior distribution for λhas a gamma distribution with parameters αandβ,
show that the posterior distribution is also gamma distributed. What are its parameters?
105. Suppose X~ Bin( n,p), where the probability parameter pis unknown. If the prior distribution
forphas a beta distribution with parameters αandβ, show that the posterior distribution is also
beta distributed. What are its parameters?
106. Consider a random sample X1,X2,...,Xnfrom the normal distribution with mean 0 and variance
σ2¼1/τ. (The parameter τ¼1/σ2is called the precision of the normal distribution.) Assume a
gamma-distributed prior for τand show that the posterior distribution of τis also gamma. What
are its parameters?5.6 Bayesian Inference 415
5.7 Supplementary Exercises (107–138)
107. At time t¼0, there is one individual alive in a certain population. A pure birth process then
unfolds as follows. The time until the ﬁrst birth is exponentially distributed with parameter λ.
After the ﬁrst birth, there are two individuals alive. The time until the ﬁrst gives birth again is
exponential with parameter λ, and similarly for the second individual. Therefore, the time until
the next birth is the minimum of two exponential ( λ) variables, which is exponential with
parameter 2 λ. Similarly, once the second birth has occurred, there are three individuals alive, so
the time until the next birth is an exponential rv with parameter 3 λ, and so on (the memoryless
property of the exponential distribution is being used here). Suppose the process is observed
until the sixth birth has occurred and the successive birth times are 25.2, 41.7, 51.2, 55.5, 59.5,
61.8 (from which you should calculate the times between successive births). Derive the mle of λ.
[Hint: The likelihood is a product of exponential terms.]
108. When the sample standard deviation Sis based on a random sample from a normal population
distribution, it can be shown that
ESðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2=n/C01ðÞp
Γn=2ðÞ σ=Γn/C01ðÞ =2 ½/C138
Use this to obtain an unbiased estimator for σof the form cS. What is cwhen n¼20?
109. Each of nspecimens is to be weighed twice on the same scale. Let XiandYidenote the two
observed weights for the ith specimen. Suppose XiandYiare independent of each other, each
normally distributed with mean value μi(the true weight of specimen i) and variance σ2.
(a) Show that the mle of σ2is ^σ2¼PXi/C0Yi ðÞ2=4nðÞ [Hint:I f /C22z¼z1þz2 ðÞ =2, thenP/C0
zi/C0/C22z/C12¼z1/C0z2 ðÞ2=2.]
(b) Is the mle ^σ2an unbiased estimator of σ2? Find an unbiased estimator of σ2.[Hint: For any
rvZ,E(Z2)¼Var(Z)+[E(Z)]2. Apply this to Z¼Xi/C0Yi.]
110. The Principle of Unbiased Estimation has been criticized on the grounds that in some situations
the only unbiased estimator is patently ridiculous. Here is one such example. Suppose that the
number of major defects Xon a randomly selected vehicle has a Poisson distribution with
parameter μ. You are going to purchase two such vehicles and wish to estimate θ¼P(X1¼0,
X2¼0)¼e/C02μ, the probability that neither of these vehicles has any major defects. Your
estimate is based on observing the value of Xfor a single vehicle. Denote this estimator by
^θ¼gXðÞ. Write the equation implied by the condition of unbiasedness, E[g(X)]¼e/C02μ, cancel
e/C0μfrom both sides, then expand what remains on the right-hand side in an inﬁnite series, and
compare the two sides to determine g(X). If X¼200, what is the estimate? Does this seem
reasonable? What is the estimate if X¼199? Is this reasonable?
111. Let X, the payoff from playing a certain game, have pmf
px ;θðÞ ¼θ x¼/C01
1/C0θðÞ2θxx¼0, 1, 2, .../C26
(a) Verify that p(x;θ) is a legitimate pmf, and determine the expected payoff. [ Hint: Look back
at the properties of a geometric random variable discussed in Chap. 2.]
(b) Let X1,...,Xnbe the payoffs from nindependent games of this type. Determine the mle of
θ.[Hint: Let Ydenote the number of observations among the nthat equal /C01, and write the
likelihood as a single expression in terms of ∑xiandy.]
112. The reaction time (RT) to a stimulus is the interval of time commencing with stimulus
presentation and ending with the ﬁrst discernible movement of a certain type. The article416 5 The Basics of Statistical Inference
“Relationship of Reaction Time and Movement Time in a Gross Motor Skill” ( Percept. Motor
Skills , 1973: 453–454) reports that the sample average RT for 16 experienced swimmers to a
pistol start was .214 s and the sample standard deviation was .036 s. Making any necessary
assumptions, derive a 90% CI for true average RT for all experienced swimmers.
113. For each of 18 preserved cores from oil-wet carbonate reservoirs, the amount of residual gas
saturation after a solvent injection was measured at water ﬂood-out. Observations, in percentage
of pore volume, were
23.5 31.5 34.0 46.7 45.6 32.5
41.4 37.2 42.5 46.9 51.5 36.4
44.5 35.7 33.5 39.3 22.0 51.2
(See “Relative Permeability Studies of Gas-Water Flow Following Solvent Injection in Carbon-
ate Rocks,” Soc. Petrol. Eng. J. , 1976: 23–30.)
(a) Is it plausible that the sample was selected from a normal population distribution?
(b) Calculate a 98% CI for the true average amount of residual gas saturation.
114. Aphid infestation of fruit trees can be controlled either by spraying with pesticide or by
inundation with ladybugs. In a particular area, four different groves of fruit trees are selected
for experimentation. The ﬁrst three groves are sprayed with pesticides 1, 2, and 3, respectively,
and the fourth is treated with ladybugs, with the following results on yield:
Treatment ni(number of trees) /C22xi(bushels/tree) si
1 100 10.5 1.5
2 90 10.0 1.3
3 100 10.1 1.8
4 120 10.7 1.6
Letμi¼the true average yield (bushels/tree) after receiving the ith treatment. Then
θ¼1
3μ1þμ2þμ3 ðÞ /C0 μ4
measures the difference in true average yields between treatment with pesticides and treatment
with ladybugs. When n1,n2,n3, and n4are all large, the estimator ^θobtained by replacing each μi
by /C22Xiis approximately normal. Use this to derive a large-sample 100(1 /C0α)% CI for θ, and
compute the 95% interval for the given data.
115. It is important that face masks used by ﬁreﬁghters be able to withstand high temperatures
because ﬁreﬁghters commonly work in temperatures of 200–500/C14F. In a test of one type of
mask, 11 of 55 masks had lenses pop out at 250/C14. Construct a 90% CI for the true proportion of
masks of this type whose lenses would pop out at 250/C14.
116. A journal article reports that a sample of size 5 was used as a basis for calculating a 95% CI for
the true average natural frequency (Hz) of delaminated beams of a certain type. The resulting
interval was (229.764, 233.504). You decide that a conﬁdence level of 99% is more appropriate
than the 95% level used. What are the limits of the 99% interval? [ Hint: Use the center of the
interval and its width to determine /C22xands.]
117. Chronic exposure to asbestos ﬁber is a well-known health hazard. The article “The Acute
Effects of Chrysotile Asbestos Exposure on Lung Function” ( Envir. Res ., 1978: 360–372)
reports results of a study based on a sample of construction workers who had been exposed to
asbestos over a prolonged period. Among the data given in the article were the following
(ordered) values of pulmonary compliance (cm3/cm H 2O) for each of 16 subjects 8 months after5.7 Supplementary Exercises (107–138) 417
the exposure period (pulmonary compliance is a measure of lung elasticity, or how effectively
the lungs are able to inhale and exhale):
167.9 180.8 184.8 189.8 194.8 200.2
201.9 206.9 207.2 208.4 226.3 227.7
228.5 232.4 239.8 258.6
(a) Is it plausible that the population distribution is normal?
(b) Compute a 95% CI for the true average pulmonary compliance after such exposure.
118. A triathlon consisting of swimming, cycling, and running is one of the more strenuous amateur
sporting events. The article “Cardiovascular and Thermal Response of Triathlon Performance”
(Medicine and Science in Sports and Exercise , 1988: 385–389) reports on a research study
involving nine male triathletes. Maximum heart rate (beats/min) was recorded during perfor-
mance of each of the three events. For swimming, the sample mean and sample standard
deviation were 188.0 and 7.2, respectively. Assuming that the heart-rate distribution is (approx-
imately) normal, construct a 98% CI for true mean heart rate of triathletes while swimming.
119. An April 2009 survey of 2253 American adults conducted by the Pew Research Center’s
Internet & American Life Project revealed that 1262 of the respondents had at some point
used wireless means for online access.
(a) Calculate and interpret a 95% CI for the proportion of all American adults who at the time
of the survey had used wireless means for online access.
(b) What sample size is required if the desired width of the 95% CI is to be at most .04,
irrespective of the sample results? [ Hint: See Exercise 83.]
120. High concentration of the toxic element arsenic is all too common in groundwater. The article
“Evaluation of Treatment Systems for the Removal of Arsenic from Groundwater” ( Practice
Periodical of Hazardous ,Toxic ,and Radioactive Waste Mgmt. , 2005: 152–157) reported that
for a sample of n¼5 water specimens selected for treatment by coagulation, the sample mean
arsenic concentration was 24.3 mg/L, and the sample standard deviation was 4.1. The authors of
the cited article used t-based methods to analyze their data, so hopefully had reason to believe
that the distribution of arsenic concentration was normal.
(a) Calculate and interpret a 95% CI for true average arsenic concentration in all such water
specimens.
(b) Predict the arsenic concentration for a single water specimen in a way that conveys
information about precision and reliability. (See Exercise 49.)
121. Let θ1andθ2denote the mean weights for animals of two different species. An investigator
wishes to estimate the ratio θ1/θ2. Unfortunately the species are extremely rare, so the estimate
will be based on ﬁnding a single animal of each species. Let Xidenote the weight of the species
ianimal ( i¼1, 2), assumed to be normally distributed with mean θiand standard deviation 1.
(a) What is the distribution of the variable θ2X1/C0θ1X2 ðÞ =ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
θ2
1þθ2
2q
? Show that this variable
depends on θ1andθ2only through θ1/θ2(divide numerator and denominator by θ2).
(b) Since the variable in (a) is normally distributed, we have
P/C01:96<θ2X1/C0θ1X2 ðÞ =ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
θ2
1þθ2
2q
<1:96/C18/C19
¼:95
Now replace <by¼and solve for θ1/θ2. Then show that a conﬁdence interval results if
x12+x22/C211.962, whereas if this inequality is not satisﬁed, the resulting conﬁdence set is
the complement of an interval.418 5 The Basics of Statistical Inference
122. Let X1,X2,...,Xnbe a random sample from a uniform distribution on the interval [0, θ].
Then if Y¼max( Xi), the techniques of Sect. 4.9show that Yhas density function
fyðÞ ¼n
θnyn/C010/C20y/C20θ
0 otherwise(
(a) Use f(y) to verify that
Pθ/C1α=2ðÞ1=n/C20Y/C20θ/C11/C0α=2 ðÞ1=nhi
¼1/C0α
and use this to derive a 100(1 /C0α)% CI for θ.
(b) Verify that P(θ/C1α1/n/C20Y/C20θ)¼1/C0α, and derive a 100(1 /C0α)% CI for θbased on
this probability statement.
(c) Which of the two intervals derived in (a) and (b) is shorter? If your waiting time for a
morning bus is uniformly distributed and observed waiting times are x1¼4.2,x2¼3.5,
x3¼1.7,x4¼1.2, and x5¼2.4, obtain a 95% CI for θby using the shorter of the two
intervals.
123. Consider 95% CIs for two different parameters θ1andθ2, and let Ai(i¼1, 2) denote the event
that the value of θiis included in the random interval that results in the CI. Thus P(Ai)¼.95.
(a) Suppose that the data on which the CI for θ1is based is independent of the data used to
obtain the CI for θ2(e.g., we might have θ1¼μ, the population mean height for American
females, and θ2¼p, the proportion of all Kodak digital cameras that don’t need warranty
service). What can be said about the simultaneous (i.e., joint) conﬁdence level for the two
intervals? That is, how conﬁdent can we be that the ﬁrst interval contains the value of θ1
and that the second contains the value of θ2?[Hint: Consider P(A1\A2).]
(b) Now suppose the data for the ﬁrst CI is not independent of that for the second one. What
now can be said about the simultaneous conﬁdence level for both intervals? [ Hint:
Consider P(A10[A20), the probability that at least one interval fails to include the value
of what it is estimating. Now use the fact that P(A10[A20)/C20P(A10)+P(A20) [why?] to show
that the probability that both random intervals include what they are estimating is at
least .90. The generalization of the bound on P(A10[A20) to the probability of a k-fold
union is one version of the Bonferroni inequality.]
(c) What can be said about the simultaneous conﬁdence level in (b) if the conﬁdence level for
each interval separately is 100(1 /C0α)%? What can be said about the simultaneous
conﬁdence level if a 100(1 /C0α)% CI is computed separately for each of kparameters
θ1,...,θk?
124. Let X1,...,Xnbe a random sample from a continuous probability distribution having median η
(so that P(Xi/C20η)¼P(Xi/C21η)¼.5). Let Y1andYnbe the smallest and largest order statistic
for the sample (i.e., Y1¼min(Xi) and Yn¼max( Xi)).
(a) Show that
PY 1/C20η/C20Yn ðÞ ¼ 1/C01
2/C18/C19n/C01
so that ( Y1,Yn) is a 100(1 /C0α)% conﬁdence interval for ηwith α¼(1/2)n/C01.[Hint: Use
the same arguments employed in Sect. 4.9to derive the cdfs of Y1andYn.]5.7 Supplementary Exercises (107–138) 419
(b) For each of six normal male infants, the amount of the amino acid alanine (mg/100 mL)
was determined while the infants were on an isoleucine-free diet, resulting in the following
data:
2.84 3.54 2.80 1.44 2.94 2.70
Compute a 97% CI for the true median amount of alanine for infants on such a diet (“The
Essential Amino Acid Requirements of Infants,” Amer. J. of Nutrition , 1964: 322–330).
(c) Let Y2andYn/C01denote the second-smallest and second-largest of the Xis, respectively.
What is the conﬁdence level of the interval ( Y2,Yn/C01) for η?
125. One method for straightening wire before coiling it to make a spring is called “roller straight-
ening.” The article “The Effect of Roller and Spinner Wire Straightening on Coiling Perfor-
mance and Wire Properties” ( Springs , 1987: 27–28) reports on the tensile properties of wire.
Suppose a sample of 16 wires is selected and each is tested to determine tensile strength
(N/mm2). The resulting sample mean and standard deviation are 2160 and 30, respectively.
(a) The mean tensile strength for springs made using spinner straightening is 2150 N/mm2.
What hypotheses should be tested to determine whether the mean tensile strength for the
roller method exceeds 2150?
(b) Assuming that the tensile strength distribution is approximately normal, what test statistic
would you use to test the hypotheses in part (a)?
(c) What is the value of the test statistic for this data?
(d) What is the P-value for the value of the test statistic computed in part (c)?
(e) For a level .05 test, what conclusion would you reach?
126. A new method for measuring phosphorus levels in soil is described in the article “A Rapid
Method to Determine Total Phosphorus in Soils” ( Soil Sci. Amer. J. , 1988: 1301–1304).
Suppose a sample of 11 soil specimens, each with a true phosphorus content of 548 mg/kg, is
analyzed using the new method. The resulting sample mean and standard deviation for phos-
phorus level are 587 and 10, respectively.
(a) Is there evidence that the mean phosphorus level reported by the new method differs
signiﬁcantly from the true value of 548 mg/kg? Use α¼.05.
(b) What assumptions must you make for the test in part (a) to be appropriate?
127. The article “Orchard Floor Management Utilizing Soil-Applied Coal Dust for Frost Protection”
(Agric. Forest Meteorol. , 1988: 71–82) reports the following values for soil heat ﬂux of eight
plots covered with coal dust.
34.7 35.4 34.7 37.7 32.5 28.0 18.4 24.9
The mean soil heat ﬂux for plots covered only with grass is 29.0. Assuming that the heat-ﬂux
distribution is approximately normal, does the data suggest that the coal dust is effective in
increasing the mean heat ﬂux over that for grass? Test the appropriate hypotheses using
α¼.05.
128. The article “Caffeine Knowledge, Attitudes, and Consumption in Adult Women” ( J. Nutrit. Ed. ,
1992: 179–184) reports the following summary data on daily caffeine consumption for a sample
of adult women: n¼47, /C22x¼215mg, s¼235 mg, and range ¼5–1176.
(a) Does it appear plausible that the population distribution of daily caffeine consumption is
normal? Is it necessary to assume a normal population distribution to test hypotheses about
the value of the population mean consumption? Explain your reasoning.
(b) Suppose it had previously been believed that mean consumption was at most 200 mg. Does
the given data contradict this prior belief? Test the appropriate hypotheses at signiﬁcance
level .10.420 5 The Basics of Statistical Inference
129. The accompanying observations on residual ﬂame time (seconds) for strips of treated children’s
nightwear were given in the article “An Introduction to Some Precision and Accuracy of
Measurement Problems” ( J. Test. Eval. , 1982: 132–140). Suppose a true average ﬂame time
of at most 9.75 had been mandated. Does the data suggest that this condition has not been met?
Carry out an appropriate test after ﬁrst investigating the plausibility of assumptions that underlie
your method of inference.
9.85 9.93 9.75 9.77 9.67 9.87 9.67
9.94 9.85 9.75 9.83 9.92 9.74 9.99
9.88 9.95 9.95 9.93 9.92 9.89
130. The incidence of a certain type of chromosome defect in the US adult male population is
believed to be 1 in 75. A random sample of 800 individuals in US penal institutions reveals
16 who have such defects. Can it be concluded that the incidence rate of this defect among
prisoners differs from the presumed rate for the entire adult male population?
(a) State and test the relevant hypotheses using α¼.05.
(b) What type of error might you have made in reaching a conclusion?
131. In an investigation of the toxin produced by a certain poisonous snake, a researcher prepared
26 different vials, each containing 1 g of the toxin, and then determined the amount of antitoxin
needed to neutralize the toxin. The sample average amount of antitoxin necessary was found to
be 1.89 mg, and the sample standard deviation was .42. Previous research had indicated that the
true average neutralizing amount was 1.75 mg/g of toxin. Does the new data contradict the value
suggested by prior research? Test the relevant hypotheses. Does the validity of your analysis
depend on any assumptions about the population distribution of neutralizing amount? Explain.
132. The sample average unrestrained compressive strength for 45 specimens of a particular type of
brick was computed to be 3107 psi, and the sample standard deviation was 188. The distribution
of unrestrained compressive strength may be somewhat skewed. Does the data strongly indicate
that the true average unrestrained compressive strength is less than the design value of 3200?
Test using α¼.001.
133. To test the ability of auto mechanics to identify simple engine problems, an automobile with a
single such problem was taken in turn to 72 different car repair facilities. Only 42 of the
72 mechanics who worked on the car correctly identiﬁed the problem. Does this strongly
indicate that the true proportion of mechanics who could identify this problem is less than
.75? Test the appropriate hypotheses.
134. The December 30, 2009, the New York Times reported that in a survey of 948 American adults
who said they were at least somewhat interested in college football, 597 said the Bowl
Championship System should be replaced by a playoff similar to that used in college basketball
(in fact, a playoff system replaced the BCS starting with the 2014 season). Does this provide
compelling evidence for concluding that a majority of all such individuals favored replacing the
BCS with a playoff at that time? Test the appropriate hypotheses.
135. An article in the November 11, 2005, issue of the San Luis Obispo Tribune reported that
researchers making random purchases at California Wal-Mart stores found scanners coming up
with the wrong price 8.3% of the time. Suppose this was based on 200 purchases. The National
Institute for Standards and Technology says that in the long run at most two out of every
100 items should have incorrectly scanned prices. Carry out an appropriate hypothesis test to
decide whether the NIST benchmark is not satisﬁed. [ Caution : Are the conditions for a
one-proportion ztest met? If not, what distribution can be used instead?]
136. Annual holdings turnover for a mutual fund is the percentage of a fund’s assets that are sold
during a particular year. Generally speaking, a fund with a low value of turnover is more stable5.7 Supplementary Exercises (107–138) 421
and risk averse, whereas a high value of turnover indicates a substantial amount of buying and
selling in an attempt to take advantage of short-term market ﬂuctuations. Here are values of
turnover for a sample of 20 large-cap blended funds extracted from Morningstar.com:
1.03 1.23 1.10 1.64 1.30 1.27 1.25 0.78 1.05 0.64
0.94 2.86 1.05 0.75 0.09 0.79 1.61 1.26 0.93 0.84
(a) Would you use the one-sample ttest to decide whether there is compelling evidence for
concluding that the population mean turnover is less than 100%? Explain.
(b) A normal probability plot of the 20 ln(turnover) values shows a very pronounced linear
pattern, suggesting it is reasonable to assume that the turnover distribution is lognormal.
Recall that Xhas a lognormal distribution if ln( X) is normally distributed with mean value
μand standard deviation σ. Because μis also the median of the ln( X) distribution, eμis the
median of the Xdistribution. Use this information to decide whether there is compelling
evidence for concluding that the median of the turnover population distribution is less than
100%.
137. When X1,X2,...,Xnare independent Poisson variables, each with parameter μ, and nis large,
the sample mean /C22Xhas approximately a normal distribution with E/C22XðÞ ¼ μand Var /C22XðÞ ¼ μ=n.
This implies that
Z¼/C22X/C0μﬃﬃﬃﬃﬃﬃﬃﬃ
μ=np
has approximately a standard normal distribution. For testing H0:μ¼μ0, we can replace μby
μ0in the equation for Zto obtain a test statistic. This statistic is actually preferred to the large-
sample statistic with denominator S=ﬃﬃﬃnp(when the Xis are Poisson) because it is tailored
explicitly to the Poisson assumption. If the number of requests for consulting received by a
certain statistician during a 5-day work week has a Poisson distribution and the total number of
consulting requests during a 36-week period is 160, does this suggest that the true average
number of weekly requests exceeds 4.0? Test using α¼.02.
138. When the population distribution is normal and nis large, the sample standard deviation Shas
approximately a normal distribution with E(S)/C25σand Var( S)/C25σ2/(2n). We already know
that in this case, for any n,/C22Xis normal with E/C22XðÞ ¼ μand Var /C22XðÞ ¼ σ2=n.
(a) Assuming that the underlying distribution is normal, what is an approximately unbiased
estimator of the 99th percentile θ¼μ+ 2.33 σ?
(b) When the Xis are normal, it can be shown that /C22XandSare independent rvs (one measures
location whereas the other measures spread). Use this to compute Var ^θ/C0/C1
andσ^θfor the
estimator ^θof part (a). What is the estimated standard error ^σ^θ?
(c) Write a test statistic formula for testing H0:θ¼θ0that has approximately a standard
normal distribution when H0is true. If soil pH is normally distributed in a certain region
and 64 soil samples yield /C22x¼6:33,s¼.16, does this provide strong evidence for
concluding that at most 99% of all possible samples would have a pH of less than 6.75?
Test using α¼.01.422 5 The Basics of Statistical Inference
Markov Chains6
This chapter explores the properties of a broadly applicable probability model called a Markov chain ,
named after Russian mathematician A. A. Markov (1856–1922). Markov observed that many real-
world phenomena can be modeled as a sequence of “transitions” from one “state” to another, with
each transition having some associated uncertainty. For example, a taxi driver might “transition”
between several towns (or zones within a large city); each time he drops off a passenger, he can’t be
certain where his next fare will want to go. Similarly, a gambler might think of her winnings as
transitioning from one “state”—really, a dollar amount—to another; with each round of the game she
plays, she cannot be certain whether that dollar amount will go up or down (though, obviously, she
hopes it goes up!). The same could be said for modeling the daily closing prices of a stock: each new
day, there is uncertainty about whether that stock will “transition” to a higher or lower value, and this
uncertainty could be modeled using the tools of probability.
In all of these examples, aside from the probability model for how transitions occur, one extra
piece of information is critical: the current “state” (where the taxi driver is, how much money the
gambler has). After all, if the gambler is making $5 wagers, how much money she might have after
the next game depends on how much she has now—if she currently holds $45 in chips, then at the end
of the upcoming round she can only have $40 or $50 on an even bet. The model structure proposed by
Markov applies to situations where only knowledge of the current state, and the nature of transitions,
is necessary—we don’t care how our gambler arrived at $45 in chips, only that that’s how much she
currently possesses.
Section 6.1introduces basic notation for Markov chains and provides a rigorous deﬁnition of the
property alluded to in the previous paragraph. In Sects. 6.2and6.3we explain how the use of matrix
notation can facilitate Markov chain computations. Section 6.4focuses on a special class of Markov
chains, so-called regular chains , which have a rather exceptional property embodied in the Steady-
State Theorem. Section 6.5considers a different class of Markov chains, those with one or more
“inescapable” states, such as a gambler going broke. Finally, Sect. 6.6discusses the simulation of
Markov chains using software.
6.1 Terminology and Basic Properties
Markov chains provide a model for sequential information that allows future outcomes to depend on
previous ones, albeit in a very speciﬁc way (the deﬁning Markov property ). Researchers in numerous
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_6423
ﬁelds employ Markov chains to model the phenomena they study. Recent examples include
predicting changes in electricity demand; modeling the motion of sperm whales off the Galapagos
Islands; Chinese citizens changing their cell phone service; keeping track of inpatient bed usage at
hospitals; monitoring patterns in Web browser histories to deploy better-targeted advertising; the
evolution of drought conditions over time; and the dynamics of capital assets.
This ﬁrst section introduces the basic vocabulary and notation of Markov chains. We begin with
the following classic (if slightly artiﬁcial) example, which will serve as a thread throughout the
chapter.
Example 6.1 A city has three different taxi zones, numbered 1, 2, and 3. A taxi driver operates his
cab in all three zones. The probability that his next passenger has a destination in a particular one of
these zones depends on where the passenger is picked up. Speciﬁcally, whenever the taxi driver is in
zone 1, the probability his next passenger is going to zone 1 is .3, to zone 2 is .2, and to zone 3 is .5.
Starting in zone 2, the probability his next passenger is going to zone 1 is .1, to zone 2 is .8, and to
zone 3 is .1. Finally, whenever he is in zone 3, the probability his next passenger is going to zone 1 is
.4, to zone 2 is .4, and to zone 3 is .2. These probabilities are encapsulated in the state diagram in
Fig.6.1.
In every such state diagram, the sum of the probabilities on branches exiting any state must equal 1.
For example, in Fig. 6.1the probabilities exiting state 2 (i.e., zone 2) are .1, .8, and .1. We include in
this calculation the probability .8 indicated by a “loop” in the state diagram, which simply means that
the taxi driver has .8 probability of staying in zone 2 once he has dropped off a fare in zone 2.
Deﬁne X0to be the zone in which the taxi driver starts and Xn(n/C211) to be the zone where he
drops off his nth fare. Since X0,X1,X2,...“occur” in sequence, they are often referred to as a chain .
More precisely, this particular sequence is a ﬁnite -state,discrete -time,time-homogeneous Markov
chain . Each of these terms will be explained shortly. ■
In Example 6.1, each of the Xnforn/C210 assumes the value 1, 2, or 3 according to the destination
zone. The zones collectively constitute the states of our chain, and so the state space is {zone 1, zone
2, zone 3}, although we will often drop the state names and just use the integers {1, 2, 3}. States can
be identiﬁed with physical locations, levels (such as high/medium/low), dollar amounts, or just about
anything else. We’ll sometimes refer to the Xnas random variables, even though they are not
necessarily numerical (which goes against the deﬁnition from Chap. 2). The random variable X0is
called the initial state of the chain. A discrete -space chain is one for which the number of possible
states is ﬁnite or countably inﬁnite. If there are ﬁnitely many possible states, we have a ﬁnite -state
chain..3
.2.4.4 .5.1.2
.12 1
3.8 Fig. 6.1 State diagram for
Example 6.1424 6 Markov Chains
Since time was indexed by the discrete listing n¼0, 1, 2, ..., the sequence of zones the taxi driver
visited in Example 6.1 is called a discrete -time chain. Section 7.7gives an overview of continuous -
time chains, often indexed as { Xt:t2[0,1)}, which are useful for modeling behavior continuously
in time rather than just at discrete time points (e.g., tracking over time the number of people looking at
a particular Web site). The taxi driver chain is also time-homogeneous , in that the speciﬁed
probabilities do not change over time. One could imagine a different, more complicated model
where the probabilities speciﬁed in Example 6.1 apply during morning hours but not in the evening,
so that the probability of taking a fare from zone 1 to zone 3 is .5 for n¼1 (beginning of the work
day) but is .1, say, for n¼20 (end of his shift). See Exercises 78 and 79 for examples of nonhomo-
geneous Markov chains.
Example 6.2 This is a simple version of the famous Gambler ’sR u i n problem, which we previously
considered in Exercise 145 of Chap. 1. Allan and Beth play a succession of independent games for $1
each. Suppose Allan starts with $2 and Beth with $1, and the chance of Allan winning $1 is pon each
game. Ties are not allowed, so the chance of Beth winning $1 on any particular game is 1 /C0p. They
compete until one of the two players goes broke (has $0).
Forn¼0, 1, 2, ..., deﬁne Xn¼the amount of money Allan has after ngames. The initial state has
been speciﬁed as X0¼$2; Allan’s successive holdings X0,X1,X2,...form our chain. The state space
forXnis {$0, $1, $2, $3} or just {0, 1, 2, 3}, so we again have a ﬁnite-state chain. The state space and
the speciﬁed probabilities are illustrated by the state diagram in Fig. 6.2. Notice we have included two
“loops” with probability 1 at $0 and $3—these reﬂect the constraint that the game stops once Allan
reaches one of these dollar amounts. That is, once Allan is “at” $3, he will stay at $3, and the same goes
for $0. Also, it will always be understood that if no arrow points from state ito state jin such a diagram,
then the probability of moving from state iimmediately into state j(i.e., in one time step) is zero.
Example 6.3 A random walk . Imagine a marker initially placed at 0 on the number line. A fair coin is
ﬂipped repeatedly; each head moves the marker one integer to the right, while each tail moves it one
integer to the left. Let X0¼0, the initial state, and Xn¼the marker’s position after ncoin ﬂips for
n/C211. Each member of the chain can only take on a ﬁnite set of values: X1is either +1 or /C01,X2is
one of /C02, 0, or 2, and so on. However, the collection of allpossible states across alltime indices
comprises the entire set of integers: { ...,/C03,/C02,/C01, 0, 1, 2, 3, ...}. Thus, this so-called “random
walk” is an inﬁnite-state (though still discrete-state) chain; it is partially illustrated in Fig. 6.3.$0 $1
1−p 1−ppp1 1
$2 $3
Fig. 6.2 State diagram for Example 6.2 ■
−2.5
.5 .5.5
.5.5
.5.5
−1 012
Fig. 6.3 State diagram for Example 6.3 ■6.1 Terminology and Basic Properties 425
6.1.1 The Markov Property
All of the preceding examples have an important feature known as the Markov property . Loosely
speaking, it says that in order to know where the chain will go next (say, Xn+1), it sufﬁces to know
where the chain is now (the value of Xn). In particular, once the current state is speciﬁed, the path that
brought the chain to that state is irrelevant. Consider, for example, the random walk of Example 6.3: if
for any particular nwe have Xn¼4, then we know Xn+1¼3 or 5 with probability .5 each. It does not
matter whether the chain arrived at 4 quickly (0 !1!2!3!4) or by a more circuitous route;
the probability distribution of the next state in the chain is the same. This notion is formalized in the
following deﬁnition.
DEFINITION
LetX0,X1,X2,...be a sequence of random variables (a chain) on some discrete state space. The
sequence is said to have the Markov property if, for any time index nand any set of (not
necessarily distinct) states s0,s1,...,sn,sn+1,
PX nþ1¼snþ1/C12/C12X0¼s0,X1¼s1,...,Xn¼sn/C0/C1
¼PX nþ1¼snþ1/C12/C12Xn¼sn/C0/C1
ð6:1Þ
Such a sequence { Xn:n¼0, 1, 2, ...} is called a Markov chain .
The conditional probabilities speciﬁed in Eq. ( 6.1) are called the one-step transition probabilities
of the chain, or sometimes just transition probabilities . These are precisely the probabilities speciﬁed
in Examples 6.1–6.3. It’s critical to recognize that these are conditional probabilities: they specify the
likelihood of the next member of the chain Xn+1being in any particular state, given the current state of
the chain Xn.
Example 6.4 (Example 6.1 continued) The sequence of successive zones visited by our taxi driver is
characterized by nine one-step transition probabilities. For example, it is stated that the driver
“transitions” from zone 1 to zone 3 with probability .5, which means that for any time index n,
PX nþ1¼3/C12/C12Xn¼1/C0/C1
¼:5
This probability does not depend on the value of n, because the chain is time-homogeneous.
Instead of writing P(Xn+1¼3jXn¼1)¼.5, we will sometimes abbreviate with P(1!3)¼.5 to
emphasize the idea of transitioning from one state to another. Thus, the complete set of one-step
transition probabilities for the taxi driver is
P1!1 ðÞ ¼ :3 P1!2 ðÞ ¼ :2 P1!3 ðÞ ¼ :5
P2!1 ðÞ ¼ :1 P2!2 ðÞ ¼ :8 P2!3 ðÞ ¼ :1
P3!1 ðÞ ¼ :4 P3!2 ðÞ ¼ :4 P3!3 ðÞ ¼ :2 ■
Example 6.5 (Example 6.2 continued) The changing fortunes of Allan are governed by six
(non-zero) transition probabilities:426 6 Markov Chains
P1!0 ðÞ ¼ 1/C0pP 1!2 ðÞ ¼ pP 2!1 ðÞ ¼ 1/C0pP 2!3 ðÞ ¼ p
P0!0 ðÞ ¼ 1P3!3 ðÞ ¼ 1
The last two probabilities above correspond to termination of the sequence of games. From a
mathematical (if not practical) perspective, they communicate the idea that the chain marches on even
when gameplay has ended (e.g., 2 !3!3!3...). That is, the conditional probability
P(3!3)¼P(Xn+1¼3|Xn¼3)¼1 indicates that if Allan has all $3 at stake after ngames, he
will retain his $3 while some imaginary future gameplay continues (the ( n+ 1)st game, the ( n+ 2)nd
game, etc.). This convention eliminates the need to “stop” the Markov chain at some particular time
point n. We’ll elaborate much more on this in Sect. 6.5.
In addition, there are ten one-step transition probabilities that equal zero; for example, according to
the rules of Gambler’s Ruin, P(1!3)¼0, and P(3!x)¼0 for x2{0, 1, 2}. In general, a ﬁnite-
state Markov chain with sstates is speciﬁed by s2one-step transition probabilities, although it is quite
common for many (if not most) of these to be zero. ■
Example 6.6 Markov chains are often used to model changing weather conditions; research litera-
ture in both meteorology and climate science is rife with Markov chain applications. The article “To
Ski or Not to Ski: Estimating Transition Matrices to Predict Tomorrow’s Snowfall Using Real Data”
(J. of Statistics Educ ., vol. 18, no. 3, 2010) provides data for several US cities on the daily transitions
between “snow days,” deﬁned by a snow depth of at least 50 mm, and “green days” (snow depth <
50 mm). Let Xnrepresent the snow status, either Sfor snow or Gfor green, on the nth recorded day.
For New York City, the following one-step transition probabilities are provided:
PG!G ðÞ ¼ :964 PG!S ðÞ ¼ :036 PS!G ðÞ ¼ :224 PS!S ðÞ ¼ :776
If today is a “green day” in New York, then there is a 96.4% chance that tomorrow’s snow depth
will also be below 50 mm, based on the available weather data (which, incidentally, stretches back to
the year 1912 for New York). On the other hand, as the author notes, “the presence of a signiﬁcant
snow depth (accumulation) on the current day in Central Park (New York) has an approximately 1 in
5 chance of melting before the next day.” ■
Not all sequences of random variables possess the Markov property. In econometrics (statistical
methodology applied to economic scenarios), for example, most models for the closing price Xn+1of a
stock on the ( n+ 1)st day of trading incorporate not only the previous day’s closing price Xnbut also
information from many previous days (the data Xn/C01,Xn/C02, and so on). The likelihood that Xn+1will
be $5 higher than Xnmay depend on the stock’s behavior over all of last week, not just where it closed
on day n.
That said, in some instances a model that includes more than a one-time-step dependence can be
modiﬁed by reconﬁguring the state space in such a way that it satisﬁes the Markov property. This
expansion of states is illustrated in the next example.
Example 6.7 The weather model presented in Example 6.6 satisﬁes the Markov property; in
particular, it assumes that one can model tomorrow’s weather based on today’s conditions without
incorporating any previous information. A more realistic model might assume that tomorrow’s snow
depth depends on today’s andyesterday’s weather. Suppose, for example, that tomorrow will be a
snow day with probability .8 if both yesterday and today were snow days; with probability .6 if today6.1 Terminology and Basic Properties 427
was a snow day but yesterday was a green day; with probability .3 if it was green today and snowy
yesterday; and with probability .1 if both previous days were green.
Once again let Xn¼the “state” of the weather on day n:Gfor green day, Sfor snow day. Then the
sequence X0,X1,X2,.... of weather states does notsatisfy the Markov property, because the
conditional distribution of Xn+1given all previous weather information depends on both Xnand
Xn/C01(the previous two days’ weather conditions). So, let’s make the following modiﬁcation: deﬁne
Ynto be the ordered pair
Yn¼(day nweather, day n+ 1 weather) ¼(Xn,Xn+1)
So, for example, if snow depth was /C2150 mm on day 4 but <50 mm on day 5, then Y4¼(S,G).
The weather on day 6 depends on these previous 2 days, but they are now both contained in a single
“variable,” Y4. In other words, Y5can be modeled entirely by knowing Y4:Y5’s ﬁrst entry, X5, matches
the second entry of Y4, and the probability distribution of the second entry of Y5(i.e., X6)i s
determined by the rules given at the beginning of this example.
With this modiﬁcation, the sequence Y0,Y1,Y2,...forms a Markov chain. The state space of this
chain is not { S,G}, but rather {( G,G), (G,S), (S,G), (S,S)}. The earlier weather rules can be
expressed as one-step transition probabilities for this chain:
PS;SðÞ ! S;SðÞ ðÞ ¼ :8PS;GðÞ ! G;SðÞ ðÞ ¼ :3
PG ;SðÞ ! S;SðÞ ðÞ ¼ :6PG ;GðÞ ! G;SðÞ ðÞ ¼ :1
Four other transition probabilities can be found by considering the complements of the given
transition events. The ﬁnal eight transition probabilities (with four states, there are 42¼16 total
one-step transition probabilities) are all 0, e.g., P((S,G)!(S,S))¼0, because if Yn¼(S,G) then it
was “green” on day n+1( Xn+1¼G), meaning the ﬁrst entry of Yn+1must also be G. ■
The remainder of this chapter will focus almost exclusively on ﬁnite-state, discrete-time, time-
homogeneous chains; these are the most commonly encountered models in practice. The case of
inﬁnite-state chains, including the random walk of Example 6.3, is considered in several more
advanced texts; see, for example, the book Introduction to Probability Models by Ross listed in the
references.
6.1.2 Exercises: Section 6.1(1–10)
1. The article “Markov Chain Models of Negotiators’ Communication” ( Encyclopedia of Peace
Psychology 2012: 608-612) describes the following set-up for the back and forth dialogue
between two negotiators. If at any stage a negotiator engages in a cooperative strategy,
the other negotiator will respond with a cooperative strategy with probability .6. Otherwise, the
response is described as a competitive strategy. Similarly, there is probability .7 that a competi-
tive strategy offered at any stage of the negotiations will be met by another competitive strategy.
LetXn¼the strategy employed at the nth stage of the negotiation. Identify the state space for the
chain, specify its one-step transition probabilities, and draw the corresponding state diagram.
2. Imagine mballs being exchanged between two adjacent chambers (left and right) according to the
following rules. At each time step, one of the mballs is randomly selected and moved to
the opposite chamber, i.e., if the selected ball is currently in the right chamber, it will be
moved to the left one, and vice versa. Let Xn¼the number of balls in the left chamber after428 6 Markov Chains
thenth exchange. (This is called an Ehrenfest chain , a model often used to describe the
movement of gas molecules.)
(a) Identify the state space of this chain.
(b) Suppose m¼3. Specify the one-step transition probabilities for this chain. [ Hint: It might
be helpful to draw the two chambers and the possible positions of the three balls.]
(c) Draw the state diagram corresponding to (b).
(d) Generalize the probabilities in (b) to the case of mballs.
3. A certain machine used in a manufacturing process can be in one of three states: fully operational
(“full”), partially operational (“part”), or broken. If the machine is fully operational today, there’s
a .7 probability it will be fully operational again tomorrow, a .2 chance it will be partially
operational tomorrow, and otherwise tomorrow it will be broken. If the machine is partially
operational today, there is a .6 probability it will continue to be partially operational tomorrow
and otherwise it will be broken (because the machine is never repaired in its partially operational
state). Finally, if the machine is broken today, there is a .8 probability it will be repaired to fully
operational status tomorrow; otherwise, it remains broken. Let Xn¼the state of the machine on
dayn.
(a) Identify the state space of this chain.
(b) Determine the complete set of one-step transition probabilities, and draw the corresponding
state diagram.
4. Michelle will ﬂip a coin until she gets heads four times in a row. Deﬁne X0¼0 and, for n/C211,
Xn¼the number of heads in the current streak of heads after the nth ﬂip.
(a) If the ﬁrst seven ﬂips result in the sequence HTHHHTH , determine the values of X1,X2,...,
X7.[Hint: Each time Michelle ﬂips tails, the streak is reset to 0.]
(b) Is this an example of a Markov chain? Explain why or why not.
(c) Identify the state space of the chain. Treat reaching four heads in a row in the same manner
that the $3 state was treated in the Gambler’s Ruin scenario of Example 6.2.
(d) Assume P(H)¼pfor this particular coin. Determine the one-step transition probabilities of
this chain, and draw the corresponding state diagram.
5. A single cell has probability pof dividing into two cells and probability 1 /C0pof dying without
dividing. Once two new cells have been created, each has the same probability pof splitting in
two, independent of the other. In this fashion, cells continue to divide, either indeﬁnitely or until
all cells are dead (extinction of the cell line). Let Xn¼the number of cells in the nth generation,
with X0¼1 to reﬂect the initial, single cell.
(a) What are the possible numerical values of X1, and what are their probabilities?
(b) What are the possible numerical values of X2?
(c) Determine the one-step transition probabilities for this chain. That is, given there are xcells
in the nth generation ( Xn¼x), determine the conditional probability distribution of Xn+1.
[Note: This is an example of a branching process , commonly known as a Galton -Watson process .
See Exercise 163 at the end of Chap. 4for information on determining the probability of eventual
extinction.]
6. Imagine a set of stacked ﬁles, such as papers on your desk. Occasionally, you will need to retrieve
one of these ﬁles, which you will ﬁnd by “sequential search”: looking at the ﬁrst paper in the
stack, then the second, and so on until you ﬁnd the document you require. A sensible sequential
search algorithm is to place the most recently retrieved ﬁle at the top of the stack, the idea being
that ﬁles accessed more often will “rise to the top” and thus require less searching in the long run.
For simplicity’s sake, imagine such a scenario with just three ﬁles, labeled A,B,C.6.1 Terminology and Basic Properties 429
(a) Let Xnrepresent the sequence of the entire stack after the nth search. For example, if the ﬁles
are initially stacked Aon top of Bon top of C, then X0¼ABC. Determine the state space for
this chain.
(b) If X0¼ABC, list all possible states for X1.[Hint: One of the three ﬁles will be selected and
rise to the front of the stack. Is every arrangement listed in (a) possible, starting from ABC?]
(c) Suppose that, at any given time, there is probability pAthat ﬁle Amust be retrieved, pBthat
ﬁleBmust be retrieved, and similarly for pC(¼1/C0pA/C0pB). Determine all of the
non-zero one-step transition probabilities.
7. Social scientists have used Markov chains to study “social mobility,” the movement of people
between social classes, for more than a century. In a typical such model, states are deﬁned as
social classes, e.g., lower class, middle class, and upper class. The time index nrefers to a familial
generation, so if Xnrepresents a man’s social class, then Xn/C01is his father’s social class, Xn/C02his
grandfather’s, and so on.
(a) In this context, what would it mean for Xnto be a Markov chain? In particular, would that
imply that a grandfather’s social class has no bearing on his grandson’s? Explain.
(b) What would it mean for this chain to be time-homogeneous? Does that seem realistic?
Explain why or why not.
8. The article “Markov Chain Models for Delinquency: Transition Matrix Estimation and
Forecasting” ( Appl. Stochastic Models Bus. Ind ., 2011: 267-279) classiﬁes loan status into four
categories: current (payments are up-to-date), delinquent (payments are behind but still being
made), loss (payments have stopped permanently), and paid (the loan has been paid off). Let
Xn¼the status of a particular loan in its nth month, and assume (as the authors do) that Xnis a
Markov chain.
(a) Suppose that, for one particular loan type, P(delinquent !current) ¼.1 and P(current !
delinquent) ¼.3. Interpret these probabilities.
(b) According to the deﬁnitions of the “loss” and “paid” states, what are P(loss!loss) and
P(paid !paid)? [ Hint: Refer back to Example 6.2.]
(c) Draw the state diagram for this Markov chain.
(d) What would it mean for this Markov chain to be time-homogeneous? Does that seem
realistic? Explain.
9. The article cited in Exercise 1 also suggests a more complex negotiation model, wherein the
strategy employed at the nth stage (cooperative or competitive) is predicted not only by the
immediately preceding action but also the one before it. So, negotiator A’s next strategy is
determined not only by negotiator B’s most recent move, but also by A’s choice just before that.
Again, let Xn¼the negotiating strategy used at the nth stage.
(a) Is Xna Markov chain? Explain.
(b) How could you modify this example to create a Markov chain? What additional information
would you need to completely specify this chain? [ Hint: See Example 6.7.]
10. Let X0,X1,X2,...be a sequence of independent discrete rvs taking values in some common state
space.
(a) Show that Xnsatisﬁes the Markov property. (That is, all sequences of independent rvs on a
common state space are trivially discrete-space Markov chains.)
(b) What additional condition(s), if any, must be satisﬁed for Xnto be a time-homogeneous
Markov chain?430 6 Markov Chains
6.2 The Transition Matrix and the Chapman–Kolmogorov Equations
Section 6.1introduced the notion of a Markov chain and its characteristic one-step transition
probabilities. In this section, we will develop a systematic way to determine the probability that a
chain moves from one state to another in twosteps (or three or four ...) by considering all the
intermediate paths the chain may have taken. Such calculations are facilitated by aggregating the
transition probabilities into a matrix.
6.2.1 The Transition Matrix
The one-step transition probabilities for the taxi driver example were displayed in Example 6.4 as a
3/C23 array. It would be more efﬁcient to simply specify the probabilities themselves in that same
format, with the understanding that the probability in the ith row and jth column indicates the
transition probability P(i!j), the chance the taxi driver takes his next fare to zone jgiven that he
picks up the fare in zone i. Such a representation will be critical to understanding how various
multistep transition probabilities are calculated.
DEFINITION
LetX0,X1,X2,...be a ﬁnite-state, time-homogeneous Markov chain, and index the states of the
chain by the positive integers 1, 2, ...,s. The ( one-step)transition matrix of the Markov chain
is the s/C2smatrix Pwhose ( i,j)th entry is given by
pij¼Pi!jðÞ ¼ PX nþ1¼j/C12/C12Xn¼i/C0/C1
fori¼1,...,sandj¼1,...,s.
Example 6.8 (Example 6.4 continued) The one-step transition matrix for our taxi driver example is
P¼:3:2:5
:1:8:1
:4:4:22
43
5
which is identical in format to the display in Example 6.4. The entries are interpreted as the preceding
deﬁnition suggests, e.g., the upper left entry (ﬁrst row, ﬁrst column) of the matrix is
p11¼P1!1 ðÞ ¼ PX nþ1¼1/C12/C12Xn¼1/C0/C1
¼:3,
i.e., the conditional probability that his next fare is dropped off somewhere in zone 1 given that the
taxi is currently in zone 1. ■
Example 6.9 (Example 6.5 continued) For the Gambler’s Ruin scenario with a total available
fortune of $3, rather than label the four possible states as 1, 2, 3, 4, it’s more natural to use state
labels 0, 1, 2, and 3 corresponding to Allan’s fortune at any particular time. The transition
probabilities speciﬁed previously may be written as the following 4 /C24 matrix:6.2 The Transition Matrix and the Chapman–Kolmogorov Equations 431
P¼0
1
2
310 0 0
1/C0p 0 p0
01 /C0p0p
00 0 12
6643
775
The labels along the left-hand side of the matrix indicate the ordering of the states for the purpose
of creating this matrix; they are not, strictly speaking, a part of P. For example, P(Xn+1¼1jXn¼2)¼
P(Allan loses the next game) ¼1/C0p, while P(Xn+1¼3jXn¼0)¼0. ■
Example 6.10 (Example 6.6 continued) The snow depth model has only two states, S(snowy day)
and G(“green” day). The one-step transition probabilities given for New York City can be
summarized by the following 2 /C22 matrix:
P¼G
S:964 :036
:224 :776/C20/C21
■
Notice that the entries of every row in all of the preceding transition matrices sums to 1. This will
always be the case: given that the chain is currently in some state i, it has to go somewhere in its next
step (even if that entails remaining in state i). That is, for any state iand any time index n, we must
have
Xs
j¼1pij¼Xs
j¼1Pi!jðÞ ¼Xs
j¼1PX nþ1¼j/C12/C12Xn¼i/C0/C1
¼1
6.2.2 Computation of Multistep Transition Probabilities
We now turn to the determination of multistep transition probabilities. Given that a Markov chain is
currently in state i, what is the probability it will be in state jtwo steps later (i.e., after two
transitions)? Three steps later? We begin with the following deﬁnition.
DEFINITION
LetX0,X1,X2,...be a time-homogeneous Markov chain. For any positive integer k, the k-step
transition probabilities are deﬁned by
PkðÞi!jðÞ ¼ PX nþk¼j/C12/C12Xn¼i/C0/C1
ð6:2Þ
where iandjrange across the states of the chain (typically 1, ...,s). For k¼1, we will typically
revert to the previous notation: P(1)(i!j)¼P(i!j).
The superscript ( k) in Expression ( 6.2) does not indicate taking the kth power; it is simply notation
representing “in ksteps.” The next example illustrates how these k-step transition probabilities can be
calculated, and how they can be represented compactly in terms of powers of the one-step transition
matrix.
Example 6.11 (Example 6.8 continued) Suppose our taxi driver just dropped off a fare in zone 3, so
that that is his current state. What is the probability that his second fare, counting from now, takes him432 6 Markov Chains
to zone 1? That is, we wish to determine P(Xn+2¼1jXn¼3)¼P(2)(3!1). The calculation
method is suggested by Fig. 6.4. Consider all the possible destinations of the ( n+ 1)st fare, i.e., all
the intermediate steps the taxi driver could take from zone 3 to zone 1, and then employ the Law of
Total Probability (applied here to conditional probabilities).
The partitioning events in the Law of Total Probability are the possible states at time n+1 :
P2ðÞ3!1 ðÞ ¼ P/C0
Xnþ2¼1/C12/C12Xn¼3/C1
¼PX nþ1¼1/C12/C12Xn¼3/C0/C1
P/C0
Xnþ2¼1/C12/C12Xn¼3,Xnþ1¼1/C1
þPX nþ1¼2/C12/C12Xn¼3/C0/C1
P/C0
Xnþ2¼1/C12/C12Xn¼3,Xnþ1¼2/C1
þPX nþ1¼3/C12/C12Xn¼3/C0/C1
P/C0
Xnþ2¼1/C12/C12Xn¼3,Xnþ1¼3/C1
By the Markov property, P(Xn+2¼1jXn¼3,Xn+1¼1)¼P(Xn+2¼1jXn+1¼1), and the other
two probabilities involving conditioning on XnandXn+1simplify analogously. Thus,
P2ðÞ3!1 ðÞ ¼ P/C0
Xnþ1¼1/C12/C12Xn¼3/C1
P/C0
Xnþ2¼1/C12/C12Xnþ1¼1/C1
þP/C0
Xnþ1¼2/C12/C12Xn¼3/C1
P/C0
Xnþ2¼1/C12/C12Xnþ1¼2/C1
þPX nþ1¼3/C12/C12Xn¼3/C0/C1
P/C0
Xnþ2¼1/C12/C12Xnþ1¼3/C1
¼P3!1 ðÞ P/C0
1!1/C1
þP/C0
3!2/C1
P/C0
2!1/C1
þP/C0
3!3/C1
P/C0
3!1/C1
¼:4ðÞ/C0
:3/C1
þ/C0
:4/C1/C0
:1/C1
þ/C0
:2/C1/C0
:4/C1
¼:24
For later reference, the last expression could be written in terms of the elements of the transition
matrix P; speciﬁcally, it’s p31p11+p32p21+p33p31.
Similarly, the conditional probability that his second fare wants to be dropped off in zone 2 is
computed by
P2ðÞ3!2 ðÞ ¼ P/C0
3!1/C1
P/C0
1!2/C1
þP/C0
3!2/C1
P/C0
2!2/C1
þP/C0
3!3/C1
P/C0
3!2/C1
¼X3
j¼1P3!j ðÞ Pj!2ðÞ ¼X3
j¼1p3jpj2
¼:4ðÞ/C0
:2/C1
þ/C0
:4/C1/C0
:8/C1
þ/C0
:2/C1/C0
:4/C1
¼:48
Finally, the probability the taxi driver ﬁnds himself back in zone 3 after two fares is1
1 2.4
.4
.4 .2.3
.1
33time n time n+1 time n+2 Fig. 6.4 Transitioning
from state 3 to state 1 in
two time steps6.2 The Transition Matrix and the Chapman–Kolmogorov Equations 433
P2ðÞ3!3 ðÞ ¼X3
j¼1p3jpj3¼:4ðÞ:5ðÞ þ :4ðÞ:1ðÞ þ :2ðÞ:2ðÞ ¼ :28
This makes sense, since the taxi driver must arrive in one of the three zones at time n+ 2, and
1/C0(.24 + .48) ¼.28. ■
The sums of products of matrix entries that appear repeatedly in the preceding example should
look familiar: they are the same manner of computation that arises when one matrix is multiplied by
another (or, here, a matrix is multiplied by itself). Indeed, consider what happens if we multiply the
one-step transition matrix Pfrom Example 6.8 by itself:
P2¼PP¼:3:2:5
:1:8:1
:4:4:22
643
75:3:2:5
:1:8:1
:4:4:22
643
75¼:31 :42 :27
:15 :70 :15
:24 :48 :282
643
75
The entries in the bottom row—.24, .48, .28—are precisely the two-step transition probabilities
computed in Example 6.11. Speciﬁcally, the (3, 1) entry of P2isP(2)(3!1)¼.24, the (3, 2) entry of
P2isP(2)(3!2)¼.48, and the (3, 3) entry of P2isP(2)(3!3)¼.28. It should come as no surprise
that the other six entries of P2follow the same pattern: the ( i,j) entry of P2is equal to P(2)(i!j).
Hence, we can obtain all nine two-step transition probabilities with a single matrix computation
(which, of course, can be facilitated by Matlab or other matrix-capable software).
The foregoing result can be generalized to an arbitrary ﬁxed number of steps: to ﬁnd the three -step
transition probabilities, for example, one only needs to compute the matrix P3. It is not necessary to
consider explicitly the many different paths by which the Markov chain could transition from state ito
state jin three steps and add up all the corresponding probabilities (this is, secretly, what the threefold
matrix multiplication does). The most general result is often referred to as the set of Chapman –
Kolmogorov Equations .
CHAPMAN–KOLMOGOROV EQUATIONS
If a Markov chain has one-step transition matrix P, then the k-step transition probabilities are
the entries of the matrix Pk. Speciﬁcally,
PkðÞi!jðÞ ¼ thei;jðÞ entry of Pk
Example 6.12 (Example 6.11 continued) Back to our intrepid taxi driver: if he just dropped off a fare
in zone 2, what is the probability that he will be in zone 1 two fares later? That is, we wish to
determine the two-step transition probability P(Xn+2¼1jXn¼2)¼P(2)(2!1). According to the
Chapman–Kolmogorov Equations, this is simply the (2, 1) entry of the foregoing matrix P2:
P2ðÞ2!1 ðÞ ¼ :15
Now consider a longer-term question: If the taxi driver starts the day in zone 3 and transports ten
fares before lunch, what is the probability he ends up “back home” (i.e., in zone 3) for lunch? The goal
is to ﬁnd P(X10¼3jX0¼3)¼P(10)(3!3), which could involve summing up a terrifying number
of intermediate travel options (19,683 of them, to be precise!). But the Chapman–Kolmogorov
Equations, coupled with computer software, makes light work of the problem. With the aid of
Matlab, the tenth power of Pis found to be434 6 Markov Chains
P10¼:2004 :5993 :2004
:1998 :6004 :1998
:2002 :5996 :20022
643
75
The desired probability is just the (3, 3) entry of this 10-step transition matrix: P(10)(3!3)
¼.2002. ■
Example 6.13 The report “Research and Application by Markov Chain Operators in the Mobile
Phone Market” (Second International Conference on Artiﬁcial Intelligence, Management Science and
Electronic Commerce (AIMSEC), 2011) details an analysis of customer loyalty and movement
between China’s three major cell phone service providers: (1) China Telecom, (2) China Unicom,
and (3) China Mobile. A “transition” in this setting refers to an opportunity for a customer to renew
his or her contract with a current provider or else switch to one of the other two companies. The report
includes the following one-step transition matrix, with the companies numbered as above:
P¼:84 :06 :10
:08 :82 :10
:10 :04 :862
643
75
The entries along the main diagonal indicate customer loyalty, e.g., 84% of China Telecom
customers stick with that company when their contract expires.
Suppose a customer is currently with China Unicom. What is the probability she will be with the
same service provider three contracts from now? In other words, what is P(3)(2!2)? According to
the Chapman–Kolmogorov Equations, we need the (2, 2) entry of P3. That matrix is computed to be
P3¼:6310 :1352 :2338
:1920 :5742 :2338
:2267 :1006 :67272
643
75
from which we may extract P(3)(2!2)¼.5742.
It’s important to distinguish this probability from the answer to a more restrictive question: what’s
the chance she stays with China Unicom for allof her next three cell phone contracts? This
probability can be represented as P(Xn+1¼2\Xn+2¼2\Xn+3¼2jXn¼2) or, less formally,
asP(2!2!2!2). Applying the Markov property gives [ P(2!2)]3¼p223¼(.82)3¼.5514.
This probability is slightly lower than P(3)(2!2)¼.5742, since the latter accounts for the possi-
bility that the customer switches companies at some intermediate stage(s) but ends up back with
China Unicom three contracts later. ■
Example 6.14 (Example 6.9 continued) Suppose in our earlier Gambler’s Ruin example that
p¼.55; that is, Allan has a 55% chance of winning any particular $1 game. The one- and
two-step transition matrices are as follows:6.2 The Transition Matrix and the Chapman–Kolmogorov Equations 435
P¼0
1
2
31000
:45 0 :55 0
0:45 0 :55
00012
666643
77775P2¼0
1
2
31000
:45 :2475 0 :3025
:2025 0 :2475 :55
00012
666643
77775
As before, Allan starts with $2. Looking at the $2 row (i.e., the third row) of P2, there is a .2025
probability he has gone broke after two games. This is easy to compute by hand: since he could only
lose $2 in two games by losing twice, the chance is (.45)2¼.2025. The chance that he is back to
where he started after two games (i.e., X2¼$2) is the ($2, $2) entry of P2: .2475. This also could
have occurred in just one way: $2 !$1!$2, for which the two-step transition probability is (.45)
(.55)¼.2475. Notice that the ($2, $1) entry of P2is 0, i.e., P(2)($2!$1)¼0. Since exactly $1
exchanges hands at the end of each game, it’s impossible for Allan to transition from $2 to $1 in
exactly two steps. Finally, observe that the ($2, $3) entry of both matrices is .55, so P($2!$3)¼
P(2)($2!$3)¼.55. That’s because the game ends when Allan has all $3 at stake, which he could
achieve in one step with probability p¼.55. Having done so, he will “stay at $3” in the imaginary
second game/step, i.e., from a mathematical perspective, the observed sequence of the Markov chain
steps X0,X1, and X2is $2!$3!$3, with the second transition occurring with probability 1.
A natural concern from Allan’s perspective is the likelihood that he will eventually win. One way
to estimate that probability is to look at the chance Allan has arrived at the $3 “state” after some large
number of steps. (This works because once he has $3, he will always remain at $3.) Matlab can easily
calculate high powers of small matrices; we requested P75:
P75¼1000
:5980 0 :0000 :4020
:2691 :0000 0 :7309
00012
666643
77775
The two entries that read .0000 indicate that the probability is not strictly 0, but rather is 0 to four
decimal places. From this matrix, we have that
PAllan eventually has $3/C12/C12X0¼$2/C0/C1
/C25PAllan has $3 after 75 steps/C12/C12X0¼$2/C0/C1
¼P75ðÞ$2!$3 ðÞ ¼ :7309
Had Allan started with just $1, he would have a roughly .4020 chance of eventually winning all the
money.
In Sect. 6.5, we will present an exact method for determining the probability that Allan eventually
wins (or loses) his competition with Beth. ■
6.2.3 Exercises: Section 6.2(11–22)
11. The authors of the article “The Fate of Priority Areas for Conservation in Protected Areas: A
Fine-Scale Markov Chain Approach” ( Envir. Mgmnt ., 2011: 263–278) postulated the following
model for landscape changes in the forest regions of Italy. Each “pixel” on a map is classiﬁed as
forested ( F) or non-forested ( NF). For any speciﬁc pixel, Xnrepresents its status nyears after
2000 (so X1corresponds to 2001, X2to 2002, and so on). Their analysis showed that a pixel has a436 6 Markov Chains
90% chance of being forested next year if it is forested this year and an 11% chance of being
forested next year if it non-forested this year; moreover, data in the twenty-ﬁrst century are
consistent with the assumptions of a Markov chain.
(a) Construct the one-step transition matrix for this chain, with states 1 ¼Fand 2 ¼NF.
(b) If a map pixel was forested in the year 2000, what is the probability it was still forested in
2002? 2013?
(c) If a map pixel was non-forested in the year 2000, what is the probability it was still
non-forested in 2002? 2013?
(d) The article’s authors use this model to project forested status for several Italian regions in
the years 2050 and 2100. Comment on the assumptions required for these projections to be
valid.
12. A large automobile insurance company classiﬁes its customers into four risk categories (1 being
the lowest risk, aka best/safest drivers, 4 being the worst/highest risk; premiums are assigned
accordingly). Each year, upon renewal of a customer’s insurance policy, the risk category may
change depending on the number of accidents in the previous year. Actuarial data suggest the
following: category 1 customers stay in category 1 with probability .9 and move to categories
2, 3, 4 with probabilities .07, .02, and .01, respectively. Category 2 customers shift to category
1 (based on having no accidents last year) with probability .8 and rise to risk categories 3 and
4 with probabilities .15 and .05, respectively. Similarly, category 3 customers transition to 2 and
4 with probabilities .7 and .3, while category 4 customers stay in that risk category with
probability .4 and move to category 3 otherwise.
(a) Let Xndenote a customer’s risk category for his/her nth year with the insurance company.
Construct the one-step transition matrix for this Markov chain.
(b) If a customer starts in category 1, what is the probability she falls into risk category 2 ﬁve
years later?
(c) If a customer is currently in risk category 4, determine the probability he will be a category
1 driver in kyears, for k¼1, 2, 3, 4, 5, 6.
(d) What is the probability that a driver currently in category 1 remains in that category for
each of the next 5 years?
13. The article cited in Example 6.6 also gives the following one-step transition matrix, with the
same deﬁnitions of states, for Willow City, ND:
P¼G
S:933 :067
:012 :988/C20/C21
(a) Contrast Willow City with New York City: where is snow more likely to stay on the ground
for an extended time period? Explain.
(b) If today is a snowy day in Willow City, what is the probability it will also be a snowy day
there 2 days from now? three days from now?
(c) If today is a snowy day in Willow City, what is the probability it will continue to be snowy
for the next 4 days in a row?
14. I (author Carlton) have a six-room house whose conﬁguration is depicted in the accompanying
diagram. When my sister and her family visit, I often play hide-and-seek with my young
nephew, Lucas. Consider the following situation: Lucas counts to ten in Room 1, while I run
and hide in Room 6. Lucas’ “strategy,” as much as he has one, is such that standing in any room
of the house, he is equally likely to next visit any of the adjacent rooms, regardless of where he6.2 The Transition Matrix and the Chapman–Kolmogorov Equations 437
has searched previously. (The exception, of course, is if he enters Room 6, in which case he
discovers me and the round of hide-and-seek is over.)
1236
5
4
(a) Let Xn¼thenth room Lucas visits (with X0¼1, his starting point). Construct the
one-step transition matrix for the corresponding Markov chain.
(b) What is the probability that his third room-to-room transition will take him into Room 2?
(c) What is the fewest number of time steps (i.e., room transitions) required for Lucas to ﬁnd
me?
(d) What is the probability that, after 12 time steps, he still hasn’t found me?
15. Refer back to Exercise 1 in the previous section. Consider two negotiators, A and B, who
employ strategies according to the Markov chain model described.
(a) Construct the one-step transition matrix for the Markov chain Xn¼strategy employed at
thenth stage of a negotiation, assuming the states are (1) cooperative and (2) competitive.
(b) If negotiator A employs a cooperative strategy at some stage, what is the probability she
uses a competitive strategy the next time? [Don’t forget that A’s turns are two time steps
apart, since B counter-negotiates in between.]
(c) Now introduce a third state, (3) end of the negotiation. Assume that a Markov chain model
with the following one-step transition matrix applies:
P¼:6:2:2
:3:4:3
0012
643
75
Given that the initial strategy presented is cooperative, what is the probability the
negotiations end within three time steps?
(d) Refer back to (c). Given that the initial strategy presented is competitive, what is the
probability the negotiations end within three time steps?
16. Sarah, a statistician at a large Midwestern polling agency, owns four umbrellas. Initially, two of
them are at her home and two are at her ofﬁce. Each morning, she takes an umbrella with her to
work (assuming she has any at home) if and only if it’s currently raining, which happens on 20%
of mornings. Each evening, she takes an umbrella from work to home (again, assuming any are
available) if and only if it’s raining when she leaves work, which happens on 30% of all
evenings. Assume weather conditions, including morning and evening on the same day, are
independent (in the Midwest, that’s not unrealistic). Let Xn¼the number of umbrellas Sarah
has at home at the end of her nth work day (i.e., once she’s back at home).
(a) Identify the state space for this chain.
(b) Assume Sarah has two umbrellas at home tonight. By considering all possible weather
conditions tomorrow morning and tomorrow evening, determine the one-step transition
probabilities for the number of umbrellas she’ll have at home tomorrow night.438 6 Markov Chains
(c) Repeat the logic of (b) to determine the complete one-step transition matrix for this chain.
Be careful when considering the two extreme cases!
17. Refer back to the previous exercise.
(a) Given that Sarah has two umbrellas at home (and two at work) as of Sunday night, what is
the probability she’ll have exactly two umbrellas at home the following Friday night?
What is the probability she’ll have at least two umbrellas at home the following Friday
night?
(b) Given that Sarah has two umbrellas at home Sunday night, what are the chances she won’t
have an umbrella to take with her to work the following Thursday morning when a surprise
thunderstorm moves through the area?
(c) Assume again that Sarah has two umbrellas at home at the start of the week. Determine the
expected number of umbrellas she has at home at the end of Monday and at the end of
Tuesday. [ Hint:Xnis a discrete rv; if X0¼2, then the probability distribution of Xnappears
in the corresponding row of Pn.]
18. A box always contains three marbles, each of which is green or yellow. At regular intervals, one
marble is selected at random from the box and removed, while another is put in its place
according to the following rules: a green marble is replaced by a yellow marble with probability
.3 (and otherwise by another green marble), while a yellow marble is equally likely to be
replaced by either color. Let Xn¼the number of green marbles in the box after the nth swap.
(a) What are the possible values of Xn?
(b) Construct the one-step transition matrix for this Markov chain.
(c) If all three marbles currently in the box are green, what is the probability the same will be
true three swaps from now?
(d) If all three marbles currently in the box are green, what is the probability that the fourth
marble selected from the box will be green? [ Hint: Use part (c). Be careful not to confuse the
color of the marble selected on the fourth swap with the color of the one that replaces it!]
19. A Markov chain model for customer visits to an auto repair shop is described in the article
“Customer Lifetime Value Prediction by a Markov Chain Based Data Mining Model: Applica-
tion to an Auto Repair and Maintenance Company in Taiwan” ( Scientia Iranica , 2012:
849-855). Customers make between 0 and 4 visits to the repair shop each year; for any customer
that made exactly ivisits last year, the number of visits s/he will make next year follows a
Poisson distribution with parameter μi. (The event “4 visits” is really “4 or more visits,” so the
probability of 4 visits next year is calculated as 1 /C0∑x¼03p(x;μi) from the appropriate
Poisson pmf.) Parameter values cited in the article, which were estimated from real data, appear
in the accompanying table.
i 01234
μi 1.938696 1.513721 1.909567 2.437809 3.445738
(a) Construct the one-step transition matrix for the chain Xn¼number of repair shop visits by
a randomly selected customer in the nth observed year.
(b) If a customer made two visits last year, what is the probability that s/he makes two visits
next year and two visits the year after that?
(c) If a customer made no visits last year, what is the probability s/he makes a total of exactly
two visits in the next 2 years?
20. The four vans in a university’s vanpool are maintained at night by a single mechanic, who can
service one van per night (assuming any of them need repairs). Suppose that there is a 10%
chance that a van working today will need service by tonight, independent of the status of the6.2 The Transition Matrix and the Chapman–Kolmogorov Equations 439
other vans. We wish to model Xn¼the number of vans unavailable for service at the beginning
of the nth day.
(a) Suppose all four vans were operational as of this morning. Find the probability that exactly
jof them will be unusable tomorrow morning for j¼0, 1, 2, 3. [ Hint: The number of
unusable vans for tomorrow will be 1 less than the number that break down today, unless
that’s 0, because the mechanic can ﬁx only one van per night. What is the probability
distribution of Y¼the number of vans that break down today, assuming all 4 worked this
morning?]
(b) Suppose three vans were operational as of this morning, and one was broken. Find the
probabilities P(1!j) for this chain. [ Hint: Assume the broken van will be ﬁxed tonight.
Then the number of unavailable vans tomorrow morning is the number that break down
today, out of the three currently functioning.]
(c) Use reasoning similar to that of (a) and (b) to determine the complete one-step transition
matrix for this Markov chain.
21. Refer back to the previous exercise.
(a) If all four vans were operational as of Monday morning, what is the probability exactly three
vans will be usable Wednesday morning? Thursday morning? Friday morning?
(b) A backlog occurs whenever Xn/C211, indicating that some vans will be temporarily out of
commission because the mechanic could not get to them the previous night. Assuming there
was no backlog as of Monday morning, what is the probability a backlog exists Tuesday
morning? Answer the same question for Wednesday, Thursday, and Friday mornings.
(c) How do the probabilities in (b) change if there was a backlog of 1 van as of Monday
morning?
22. Consider a Markov chain with state space {1, 2, ...,s}. Show that, for any positive integers mand
nand any states iandj,
PmþnðÞ
ij¼Xs
k¼1PmðÞ
ikPnðÞ
kj
This is an alternative version of the Chapman–Kolmogorov Equations. [ Hint: Write the left-hand
side as P(Xm+n¼jjX0¼i), and consider all the possible states after mtransitions.]
6.3 Specifying an Initial Distribution
Thus far, every probability we have considered in this chapter (i.e., all the one-, two-, and higher-step
transition probabilities) has been conditional. For example the entries of any one-step transition
matrix indicate P(Xn+1¼jjXn¼i). In this section, we brieﬂy explore unconditional probabilities,
which result from specifying a distribution for the rv X0, the initial state of the chain. We will consider
two cases: modeling the initial state X0as a random variable, and treating X0as having a ﬁxed/known
value.
Example 6.15 (Example 6.11 continued) The never-ending saga of the taxi driver continues!
Imagine this poor fellow sleeps in his taxi, so from his perspective each new day starts in a “random”
zone. Speciﬁcally, suppose for now that he has a 20% chance of waking up in zone 1, a 50% chance of440 6 Markov Chains
waking up in zone 2, and a 30% chance of waking up in zone 3. That is, we have assigned the
following initial distribution to the Markov chain:
i 123
P(X0¼i).2 .5 .3(6.3)
Notice that, unlike the conditional probabilities that comprise the transition matrix of the Markov
chain, this initial distribution (6.3) speciﬁes the unconditional (aka marginal) distribution for the rv
X0. In what follows, we will sometimes refer to the bottom row of (6.3) as the “initial probability
vector” of X0.
Now consider the rv X1, the destination of the taxi driver’s ﬁrst fare. The probability his ﬁrst fare
wants to go somewhere in zone 3 can be determined via the Law of Total Probability:
PX 1¼3 ðÞ ¼ P/C0
X0¼1/C1
P/C0
X1¼3/C12/C12X0¼1/C1
þP/C0
X0¼2/C1
P/C0
X1¼3/C12/C12X0¼2/C1
þP/C0
X0¼3/C1
P/C0
X1¼3/C12/C12X0¼3/C1
¼X3
i¼1PX 0¼i ðÞ Pi!3ðÞ ½/C138 ¼X3
i¼1PX 0¼i ðÞ pi3 ½/C138
¼:2ðÞ/C0
:5/C1
þ/C0
:5/C1/C0
:1/C1
þ/C0
:3/C1/C0
:2/C1
¼:21
As indicated in the intermediate step, this unconditional probability can be computed by taking the
product of the initial probability vector [.2 .5 .3], regarded as a 1 /C23 matrix, with the third column of
the one-step transition matrix P. Similarly, the (unconditional) probability that his ﬁrst fare wants to
be dropped off in zone 2 is
PX 1¼2 ðÞ ¼X3
i¼1PX 0¼i ðÞ Pi!2ðÞ ½/C138 ¼X3
i¼1PX 0¼i ðÞ pi2 ½/C138
¼:2ðÞ:2ðÞ þ :5ðÞ:8ðÞ þ :3ðÞ:4ðÞ ¼ :56
The foregoing computation is the product of the initial probability vector with the second column
ofP. Finally, the probability that the ﬁrst fare is taken to zone 1 equals .23, which can be computed
either as a similar product or by observing that 1 /C0(.21 + .56) ¼.23. All together, the unconditional
pmf of the rv X1is
i 123
P(X1¼i).23 .56 .21
Clearly, the most efﬁcient way to determine the distribution of X1is to compute all three products
simultaneously through matrix multiplication. If we multiply the transition matrix Pon the left by a
1/C23 row vector containing the initial probabilities for X0, we obtain
:2:5:3 ½/C138 P¼:2:5:3 ½/C138:3:2:5
:1:8:1
:4:4:22
43
5¼:23 :56 :21 ½/C138
■6.3 Specifying an Initial Distribution 441
The method illustrated in the preceding example can be generalized to ﬁnd the unconditional
distribution of the state Xnin the chain after any number of transitions n, starting with a speciﬁed
initial distribution for X0.
THEOREM
LetX0,X1,...,Xn,...be a Markov chain with state space {1, ...,s} and one-step transition
matrix P. Let v0¼[v01...v0s]b ea1 /C2svector specifying the initial distribution of the chain,
i.e.,v0k¼P(X0¼k) for k¼1,...,s.I fv1denotes the vector of marginal (i.e., unconditional)
probabilities associated with X1, then
v1¼v0P
More generally, if vndenotes the 1 /C2svector of marginal probabilities for Xn, then
vn¼v0Pn
Proof The formula v1¼v0Pcan be established using the same computational approach displayed in
Example 6.15. Now consider v2, the vector of unconditional probabilities for X2. By the same
reasoning as in Example 6.15, we have
v2¼v1P
The substitution v1¼v0Pthen yields v2¼(v0P)P¼v0P2. Continuing by induction, we have for
general nthatvn¼vn/C01P¼(v0Pn/C01)P¼v0Pn, as claimed. ■
With the aid of software such as Matlab, the unconditional distributions of future states of the
Markov chain can be computed very quickly once the initial distribution is speciﬁed. For example, as
a continuation of Example 6.15, the probability vector for X2, the destination of the driver’s second
fare, is given by
v2¼v1P¼:23 :56 :21 ½/C138:3:2:5
:1:8:1
:4:4:22
43
5¼:209 :578 :213 ½/C138
or, equivalently,
v2¼v0P2¼:2:5:3 ½/C138:3:2:5
:1:8:1
:4:4:22
43
52
¼:209 :578 :213 ½/C138
That is, assuming that the initial distribution speciﬁed in Example 6.15 is correct, the taxi driver
has a 20.9% chance of taking his second fare to zone 1, a 57.8% chance of taking him/her to zone
2, and a 21.3% chance of being in zone 3 after two fares.
Example 6.16 As you probably learned in high school biology, Austro-Hungarian scientist Gregor
Mendel studied the inheritance of characteristics within plant species, particularly peas. Suppose one
particular pea plant can either be green or yellow, which is determined by a single gene with green
(G) dominant over yellow ( g). That is, the genetic material determining a plant’s color (its “geno-
type”) can be one of three pairings— GG,Gg,o rgg—depending on which types were passed on by442 6 Markov Chains
the parent plants. To say that green is “dominant” over yellow means that the plant’s visible color—
its “phenotype”—will be green unless that gene is completely absent from the plant (so plants with
GGorGggenotype appear green, while only ggplants are yellow).
Consider cross-breeding with a yellow plant, whose genotype is therefore known to be gg.
Mendel’s laws of genetic recombination can be expressed by the following transition matrix, where
Xnis the genotype of an nth-generation plant resulting from cross-breeding with a ggplant:
P¼GG
Gg
gg01 0
0:5:5
00 12
43
5
For example, crossing GG/C2ggyields Ggwith probability 1, while Gg/C2ggresults in Ggorgg
with probability .5 each.
Suppose our initial population of plants (to be cross-bred with the pure yellow specimens) has the
following genotype distribution: 70% GG, 20% Ggand 10% gg. The initial probability vector
associated with this “0th generation” is v0¼[.7 .2 .1]. The probabilities associated with the ﬁrst
generation of cross-bred plants is v1¼v0P¼[0 .8 .2], meaning that 80% of ﬁrst-generation plants
are expected be Ggand the remaining 20% gg. Notice that GGplants cannot exist past the ﬁrst
generation, since cross-breeding with ggplants makes such a recombination impossible.
Similarly, the second-generation probabilities are given by v2¼v1P¼v0P2¼[0 .4 .6], so that
within two generations ggplants should be the majority (60% ggcompared to 40% Gg). As cross-
breeding with pure ggplants continues, that genotype will increase in relative proportion (80% in
generation 3, 90% in generation 4), until eventually the dominant Gallele dies out. ■
6.3.1 A Fixed Initial State
The case is which the initial state X0is ﬁxed or known rather than random can be handled by forming
a “degenerate” initial probability distribution.
Example 6.17 (Example 6.15 continued) Suppose that our taxi driver lives in zone 3 and always
goes home at night, which means that he starts each new day in zone 3. Starting with certainty in zone
3 means that P(X0¼3)¼1, while P(X0¼1)¼P(X0¼2)¼0. Written as a pmf, the distribution
ofX0is
i 123
P(X0¼i)001
Equivalently, the probability vector for X0isv0¼[0 0 1]. From the original description of the
Markov chain (Example 6.1), the initial state being zone 3 implies that X1¼1 with probability .4,
X1¼2 with probability .4, and X1¼3 with probability .2. This same result can be obtained by
applying the theorem of this section:
v1¼v0P¼001½/C138:3:2:5
:1:8:1
:4:4:22
43
5¼:4:4:2 ½/C138
Notice that left-multiplying Pby the vector [0 0 1] simply extracts the third row of P. Similarly, the
pmf of X5, the destination of the ﬁfth passenger, is given by6.3 Specifying an Initial Distribution 443
v5¼v0P5¼001½/C138:3:2:5
:1:8:1
:4:4:22
643
755
¼001½/C138:2115 :5767 :2118
:1938 :6125 :1938
:2073 :5858 :20702
643
75
¼:2073 :5858 :2070 ½/C138
The matrix P5was computed by Matlab. The row vector v5is simply the third row of P5, because
the chain begins in zone 3 with probability 1. So, starting the day at home in zone 3, the taxi
driver ﬁnds himself in zone 1, 2, or 3 after ﬁve fares with probabilities .2073, .5858, and .2070,
respectively. ■
Example 6.18 (Example 6.14 continued) As before, we can use a high power of the one-step
transition matrix, say P75, to approximate the long-term behavior of our Gambler’s Ruin Markov
chain. Suppose as before that p¼.55 and Allan’s initial stake is $2. We can express the latter as
v0¼[0 0 1 0]; recall that the states, in order, are $0, $1, $2, $3. Then the probability distribution of
X75is
v75¼v0P75¼[ 0010 ] P75¼the third (i.e., $2) row of P75¼[.2691 .0000 0 .7309]
If Allan begins the competition with $2 (and Beth with $1), there is a 73.09% chance he will end up
with all the money within 75 games, and a 26.91% chance he will end up broke after 75 games. As
discussed previously, the competition will almost certainly end long before a 75th game, but for
purposes of forecasting long-run behavior we imagine that when either player goes broke, game-play
continues but no further money is exchanged.
Suppose instead that Allan’s initial stake is just $1, while Beth starts with $2. Then Allan’s initial
“distribution” is speciﬁed by v0¼[0 1 0 0], meaning P(X0¼$1)¼1 while P(X0¼$0, $2, $3) ¼0.
After 75 plays, we now have
v75¼v0P75¼[ 0100 ] P75¼the second (i.e., $1) row of P75¼[.5980 0 .0000 .4020]
Starting with $1, Allan has a 40.2% chance of winning the competition (i.e., ending up with $3)
and a 59.8% chance of being “ruined.” ■
6.3.2 Exercises: Section 6.3(23–30)
23. Refer back to Exercise 1 of this chapter. Suppose that Negotiator A goes ﬁrst and that 75% of the
time she begins negotiations with a cooperative strategy. (Consider this to be time index 0.)
(a) Determine the (unconditional) probability that Negotiator B’s ﬁrst strategy will also be
cooperative.
(b) Determine the (unconditional) probability that Negotiator B’s second strategy will be
cooperative. [ Hint: Which time index corresponds to his second move?]
24. Refer back to the Ehrenfest chain model described in Exercise 2 with m¼3 balls. The possible
states of the chain Xn¼number of balls in the left chamber after the nth exchange are {0, 1, 2, 3}.
(a) Suppose that all four possible initial states are equally likely. Determine the probability
distributions of X1andX2.
(b) Suppose instead that each of the three balls is initially equally likely to be placed in the left
or right chamber. In this situation, what is the initial distribution of the chain?444 6 Markov Chains
(c) Using the initial distribution speciﬁed in (b), determine the unconditional distributions of X1
andX2. What do you notice?
25. Information bits (0s and 1s) in a binary communication system travel through a long series of
relays. At each relay, a “bit-switching” error might occur. Suppose that at each relay, there is a
4% chance of a 0 bit being switched to a 1 bit and a 5% chance of a 1 becoming a 0. Let X0¼a
bit’s initial parity (0 or 1), and let Xn¼the bit’s parity after traversing the nth relay.
(a) Construct the one-step transition matrix for this chain. [ Hint: There are only two states,
0 and 1.]
(b) Suppose the input stream to this relay system consists of 80% 0s and 20% 1s. Determine the
proportions of 0s and 1s exiting the ﬁrst relay.
(c) Under the same conditions as (b), determine the proportions of 0s and 1s exiting the ﬁfth
relay.
26. Refer to the genetic recombination scenario of Example 6.16. Suppose that plants will now be
cross-bred with known hybrids (i.e., those with genotype Gg). Mendel’s laws imply the following
transition matrix for such breeding:
P¼GG
Gg
gg:5:50
:25 :5:25
0:5:52
43
5
Again assume the initial population genotype distribution of plants to be cross-bred with these
hybrids is 70% GG,2 0 % Gg, and 10% gg.
(a) Determine the genotype distribution of the ﬁrst generation of plants resulting from this
cross-breeding experiment.
(b) Determine the genotype distributions of the second, third, and fourth generations.
27. Refer to the weather scenario described in Example 6.6 and Example 6.10. Suppose today’s
weather forecast for New York City gives a 20% chance of experiencing a snowy day.
(a) Let X0denote today’s weather condition. Express the information provided as an initial
probability vector for X0.
(b) Determine the (unconditional) likelihoods of a snowy day and a green day tomorrow, using
the one-step transition probabilities speciﬁed in Example 6.6.
(c) Based on today’s forecast and the transition probabilities, what is the chance New York City
will experience a “green day” 1 week (7 days) from now?
28. The article “Option Valuation Under a Multivariate Markov Chain Model” (Third International
Joint Conference on Computational Science and Optimization, 2010) includes information on the
dynamic movement of certain assets between three states: (1) up, (2) middle, and (3) down. For a
particular class of assets, the following one-step transition probabilities were estimated from
available data:
P¼:4069 :3536 :2395
:3995 :5588 :0417
:5642 :0470 :38882
43
5
Suppose that the initial valuation of this asset class found that 31.4% of such assets were in the
“up” dynamic state, 40.5% were “middle,” and the remainder were “down.”
(a) What is the initial probability vector for this chain?
(b) Determine the unconditional probability distribution of X1, the asset dynamic state one time
step after the initial valuation.
(c) Determine the unconditional probability distribution of X2, the asset dynamic state two time
steps after the initial valuation.6.3 Specifying an Initial Distribution 445
29. Refer back to Exercise 23, and now suppose that Negotiator A always opens talks with a
competitive strategy.
(a) What is the probability vector for X0, Negotiator A’s initial strategy?
(b) Without performing any matrix computation, determine the distribution of X1, Negotiator
B’s ﬁrst strategy choice.
(c) What is the probability Negotiator A’s second strategy is cooperative? competitive?
30. Transitions between sleep stages are described in the article “Multinomial Logistic Estimation of
Markov-Chain Models for Modeling Sleep Architecture in Primary Insomnia Patients”
(J. Pharmacokinet. Pharmacodyn ., 2010:137–155). The following one-step transition
probabilities for the ﬁve stages awake (AW), stage 1 sleep (ST1), stage 2 sleep (ST2), slow-
wave sleep (SWS), and rapid-eye movement sleep (REM) were obtained from a graph in the
article:
P¼AW
ST1
ST2
SWS
REM:90 :09 :01 :00 :00
:21 :40 :34 :02 :03
:02 :02 :84 :09 :03
:02 :02 :22 :72 :02
:04 :04 :05 :00 :872
666643
77775
The time index of the Markov chain corresponds to half-hour intervals (i.e., n¼1 is 30 min after
the beginning of the study, n¼2 is 60 min in, etc.). Initially, all patients in the study were awake.
(a) Let v0denote the probability vector for X0, the initial state of a patient in the sleep study.
Determine v0.
(b) Without performing any matrix computations, determine the distribution of patients’ sleep
states 30 min (one time interval) into the study.
(c) Determine the distribution of patients’ sleep states 4 h into the study. [ Hint: What time index
corresponds to the 4-h mark?]
6.4 Regular Markov Chains and the Steady-State Theorem
In previous sections, we have alluded to the long-term behavior of certain Markov chains. In some
cases, such as Gambler’s Ruin, we anticipate that the chain will eventually reach, and remain in, one
of several “absorbing” states (we’ll discuss these in Sect. 6.5). Our taxi driver, in contrast, should
continually move around, but perhaps something can be said about how much time he will spend in
each of the three zones over the course of many, many fares. It turns out that the taxi driver example
belongs to a special class of Markov chains, called regular chains , for which the long-run behavior
“stabilizes” in some sense and can be determined analytically.
6.4.1 Regular Chains
DEFINITION
A ﬁnite-state Markov chain with one-step transition matrix Pis said to be a regular chain if
there exists a positive integer nsuch that all of the entries of the matrix Pnare positive.
In other words, for a regular Markov chain there is some positive integer nsuch that every
state can be reached from every state (including itself) in exactly nsteps.446 6 Markov Chains
It’s straightforward to show that if all the entries of Pnare positive, then so are all of the entries
ofPn+1,Pn+2, and so on (Exercise 37). Our taxi driver example is a regular chain, since all nine entries
ofPitself are positive. The next example shows that a regular chain may have some one-step
transition probabilities equal to zero.
Example 6.19 Internet users’ browser histories can be modeled as Markov chains, where the “states”
are different Web sites (or classes of Web sites) and transitions occur when users move from one Web
site to another. The article “Evaluating Variable-Length Markov Chain Models for Analysis of User
Web Navigation Sessions” ( IEEE Trans. Knowl. Data Engr . 2007: 441-452) discusses increasingly
complex models of this type. Suppose for simplicity that Web sites are grouped into ﬁve categories:
(1) social media, (2) e-mail, (3) news and sports, (4) online retailers, and (5) other (use your
imagination). Consider a Markov chain model for users’ transitions between these ﬁve categories
whose state diagram is depicted in Fig. 6.5.
Notice that, according to this model, not every state can access all ﬁve states in one step, because
many one-step transition probabilities are zero. The one-step transition matrix Pof this Markov chain
is as follows:
P¼0:30 :70
:2:1:60 :1
00 :2:4:4
:70 :1:20
00 :5:502
666643
77775
Eleven of the twenty-ﬁve entries in Pare zero. However, consider several higher powers of this
matrix:
P2¼:55 :03 :25 :14 :03
:02 :07 :23 :43 :25
:28 0 :28 :36 :08
:14 :21 :04 :57 :04
:35 0 :15 :30 :202
666643
77775,P3¼:104 :168 :097 :528 :103
:315 :013 :256 :317 :099
:252 :084 :132 :420 :112
:441 :063 :211 :248 :037
:210 :105 :160 :465 :0602
666643
77775
Since every entry of P3is positive, by deﬁnition we have a regular Markov chain. Every state can
reach every state (including itself) in exactly three moves. ■41
.3
.2
.2
.2.7
.1.1 .7
.4
.4.5
.5 .65.1
32Fig. 6.5 State diagram for
Example 6.196.4 Regular Markov Chains and the Steady-State Theorem 447
In contrast, Gambler’s Ruin is nota regular Markov chain. It is not possible for Allan to go from $2
to $1 in an even number of moves, so the ($2, $1) entry of Pnis zero whenever nis even. Similarly,
Allan cannot go from $2 back to $2 in an odd number of steps, so the ($2, $2) entry of Pnequals zero
for every odd exponent n. Thus, there exists no positive integer nfor which all sixteen entries of Pnare
positive. (In fact, six other entries of Pnmust always be 0: P(n)(0!j)¼0 for states j6¼0 and
P(n)(3!j)¼0 for j6¼3, since both $0 and $3 are “absorbing” states.) Another non-regular Markov
chain, one that does not have any absorbing states, is given in the following example.
Example 6.20 Unlike our taxi driver, bus drivers follow a well-deﬁned route. Consider a bus route
from campus (state 1), to the nearby student housing complex (state 2), to downtown (state 3), and
then back to campus. The associated Markov chain cycles endlessly: 1 !2!3!1!2!3
!1.... Figure 6.6shows the corresponding state diagram.
The one-step transition matrix for this chain is
P¼010
001
1002
43
5
Direct computation shows that
P2¼001
100
0102
43
5and P3¼100
010
0012
43
5¼I,
where Idenotes the 3 /C23 identity matrix. Hence, P4¼P3P¼IP¼P;P5¼P3P2¼IP2¼P2;
P6¼P3P3¼II¼I; and so on. That is, the n-step transition matrix Pnequals one of P,P2,o rIfor
every positive integer n, and all three of these contain some zero entries. Therefore, this is nota
regular Markov chain. ■
6.4.2 The Steady-State Theorem
What’s so special about regular chains? The transition matrices of regular Markov chains exhibit a
rather interesting property. Consider a very high power of the transition matrix for our taxi driver,
computed with the aid of Matlab:11
1 12
3Fig. 6.6 State diagram for
Example 6.20448 6 Markov Chains
P¼:3:2:5
:1:8:1
:4:4:22
43
5)P100¼:2000 :6000 :2000
:2000 :6000 :2000
:2000 :6000 :20002
43
5
Notice that every row of P100is identical: roughly, each one is [.2 .6 .2]. What’s more, raising Pto
even higher powers yields the same matrix to several decimal places. That is, P101,P102, and so on are
all roughly equal to P100. Something similar occurs for the regular Markov chain of Example 6.19:
P¼0:30 :70
:2:1:60 :1
00 :2:4:4
:70 :1:20
00 :5:502
666643
77775)P100¼:2844 :0948 :1659 :3791 :0758
:2844 :0948 :1659 :3791 :0758
:2844 :0948 :1659 :3791 :0758
:2844 :0948 :1659 :3791 :0758
:2844 :0948 :1659 :3791 :07582
666643
77775
Again, every row of P100is the same, and replacing 100 by an even higher power gives the same
result (i.e., to several decimal places P100¼P101¼P102¼...). These are two examples of the
central result in the theory of Markov chains, the so-called Steady -State Theorem .
STEADY-STATE THEOREM
LetPbe the one-step transition matrix of a ﬁnite-state, regular Markov chain. Then the matrix
limit
Π¼lim
n!1Pnð6:4Þ
exists. Moreover, the rows of the limiting matrix Πare identical, with all positive entries.
The proof of the Steady-State Theorem is beyond the scope of this book; interested readers may
consult the text by Karlin and Taylor listed in the references.
If we let π¼[π1/C1/C1/C1πs] denote each of the identical rows of the limiting matrix Πin Eq. (6.4), πis
called the steady -state distribution of the Markov chain. Thus, for the taxi driver example, the
steady-state distribution is π¼[.2 .6 .2], while the steady-state distribution for the Web browsing
Markov chain in Example 6.19 is π¼[.2844 .0948 .1659 .3791 .0758].
A Markov chain does not have to be regular for the limit of Pnto exist as n!1 . For example,
computing progressively larger powers of the one-step transition matrix for the Gambler’s Ruin
scenario of Example 6.14 shows that, for large n,
Pn/C25P75¼1000
:5980 0 :0000 :4020
:2691 :0000 0 :7309
00012
6643
775
That is, the limit of Pnexists and is, at least to four decimal places, equal to the matrix displayed
above. However, unlike in the case of a regular Markov chain, the rows of this limiting matrix are not
identical and the matrix includes several zeros. We will consider in more detail Markov chains of this
type in the next section.
The transition matrix of a “periodic” Markov chain, such as the one in Example 6.20, does not
have a limit. This is not surprising, since periodic functions in general do not have long-run limits but
rather cycle through their possible values.6.4 Regular Markov Chains and the Steady-State Theorem 449
6.4.3 Interpreting the Steady-State Distribution
The steady-state distribution πof a regular Markov chain can be interpreted in several ways. We
present four different interpretations here; veriﬁcations of the second and fourth statements can be
found in the Karlin and Taylor text.
1. If the “current” state of the Markov chain is observed after a large number of transitions, there is an
approximate probability πjof the chain being in state j. That is, for large n,P(Xn¼j)/C25πj.
Moreover, this holds regardless of the initial distribution of the chain (i.e., the unconditional
distribution of the initial state X0).
The ﬁrst sentence is essentially the deﬁnition of πstemming from the Steady-State Theorem.
2. The long-run proportion of time the Markov chain visits the jth state is πj.
To be more precise, for any state jletNj(n) denote the number of times the chain visits state jin
its ﬁrst ntransitions; that is,
NjnðÞ ¼ #1/C20k/C20n:Xk¼j fg
Then it can be shown that Nj(n)/n, the proportion of time the Markov chain spends in state
jamong the ﬁrst ntransitions, converges in probability to πj.
3. If we assign πto be the initial distribution of X0, then the distribution of Xnis also πfor any
subsequent number of transitions n. For this reason, πis customarily referred to as the stationary
distribution of the Markov chain.
To prove Statement 3, ﬁrst let Πdenote the matrix in Eq. (6.4), each of whose rows is π. Now
write Pn+1¼PnPand take the limit of both sides as n!1 :
Pnþ1¼PnP)lim
n!1Pnþ1¼lim
n!1PnP½/C138 ¼ lim
n!1Pnhi
P)Π¼ΠP
Each side of the last equation is an s/C2smatrix; equating the top rows of these two matrices,
we have π¼πP. (You could just as well equate any other row, since all the rows of Πare the
same.)
Now, assign the steady-state distribution to X0:v0¼π. Then the (unconditional) distribution of
X1, using the results of Sect. 6.3,i sv1¼πP, which we have established equals π. Continuing by
induction, we have for any nthat the unconditional distribution of Xnisvn¼vn/C01P¼πP¼π,
completing the proof.
4. The expected number of transitions required to return to the jth state, beginning in the jth state, is
equal to 1/ πj. This is called the mean recurrence time for state j.
Compare this result to the mean of a geometric rv from Chap. 2: the expected number of trials
(replications) required to ﬁrst observe an event whose probability is pequals 1/ p. The difference is
that the geometric model assumes the trials are independent, while a Markov chain model assumes
that successive states of the chain are dependent (as speciﬁed by the Markov property). But if we
think of “return to the jth state” as our event of interest, then Statement 1 implies that (at least for
large n) the probability of this event is roughly πj, and so it seems reasonable that the average
number of tries/steps it will take to achieve this event will be 1/ πj.
Example 6.21 The steady-state distribution for the taxi driver example is given by the 1 /C23 vector
π¼[.2 .6 .2]. For now, this relies on the computation of P100above; shortly, we will present a
derivation of this vector that does not require raising Pto a high power. From the preceding
descriptions, we conclude all of the following:450 6 Markov Chains
1. Regardless of where the taxi driver starts his day, for large nthere is about a 20% chance his nth
fare will be dropped off in zone 1, a 60% chance that that fare will go to zone 2, and a 20% chance
for zone 3.
2. In the long run, the taxi driver drops off about 20% of his fares in zone 1, about 60% in zone 2, and
about 20% in zone 3.
3. Suppose the taxi driver sleeps in his cab, thus waking up each day in a “random” zone, and we
assign to X0(his point of origin tomorrow, say) the initial distribution v0¼π¼[.2 .6 .2]. The
unconditional distribution of X1, the destination of tomorrow’s ﬁrst fare, is
v1¼v0P¼:2:6:2 ½/C138:3:2:5
:1:8:1
:4:4:22
43
5
By direct computation, the ﬁrst entry of v1is (.2)(.3) + (.6)(.1) + (.2)(.4) ¼.2; the second entry is
(.2)(.2) + (.6)(.8) + (.2)(.4) ¼.6; and the last is .2. That is, v1¼[.2 .6 .2] ¼π, and so X1has the
same distribution as X0. The same will hold for X2,X3, and so on.
4. If the driver starts from his home in zone 3, then on the average the number of fares he handles
until he is brought back to zone 3 is given by 1/ π3¼1/(.2) ¼5. That is, the mean recurrence time
for state 3 (zone 3) is ﬁve transitions. ■
6.4.4 Efficient Computation of Steady-State Probabilities
The preceding examples of regular Markov chains and the resulting steady-state distributions may
suggest that one determines πby computing a high power of the transition matrix P, preferably with
software, and then extracting any row of the resulting matrix (all of which will be the same, according
to the Steady-State Theorem). Fortunately there is a more direct technique for determining π. The
method was hinted at in the proof of Statement 3 above: the steady-state distribution πsatisﬁes the
matrix equation πP¼π. In fact, something stronger is true.
THEOREM
LetPbe the one-step transition matrix of a regular Markov chain on the state space {1, ...,s}.
The steady-state distribution of the Markov chain is the unique solution π¼[π1/C1/C1/C1πs] to the
system of equations formed by
πP¼πand π1þ/C1/C1/C1þ πs¼1 ð6:5Þ
Proof Statement 3 above and the fact that πis a probability vector (because it’s the limit of
probability vectors) ensures that πitself satisﬁes Eq. ( 6.5). We must show that any other vector
satisfying both equations in Eq. ( 6.5) is, in fact, π. To that end, let wbe any 1 /C2svector satisfying the
two conditions wP¼wand∑wi¼1. Similar to earlier derivations, we have wP2¼(wP)P¼wP
¼wand, by induction, wPn¼wfor any positive integer n. Taking the limit of both sides as n!1 ,
the Steady-State Theorem implies that wΠ¼w.6.4 Regular Markov Chains and the Steady-State Theorem 451
Now expand wΠ:
wΠ¼w1/C1/C1/C1ws ½/C138π1/C1/C1/C1 πs
⋮⋮⋮
π1/C1/C1/C1 πs2
643
75¼/C2/C0
w1π1þ/C1/C1/C1þ wsπ1/C1
/C1/C1/C1/C0
w1πsþ/C1/C1/C1þ wsπs/C1/C3
¼/C2
ΣwiðÞ π1/C1/C1/C1/C0
Σwi/C1
πs/C3
¼/C0
Σwi/C1/C2
π1/C1/C1/C1πs/C3
¼/C0
Σwi/C1
π
Since∑wi¼1 by assumption, we have wΠ¼π. It was established above that wΠ¼w, and so
we conclude that w¼π, as originally claimed. ■
Example 6.22 Consider again the Markov chain model for snowy days ( S) and non-snowy or
“green” days ( G) in New York City, begun in Example 6.6. The one-step transition matrix was
given by
P¼G
S:964 :036
:224 :776/C20/C21
Since all the entries of Pare positive, this is a regular Markov chain. The preceding theorem can be
used to determine the steady-state probabilities π¼[π1π2]. The equations in Eq. ( 6.5), written out
long-hand, are
:964π1þ:224π2¼π1
:036π1þ:776π2¼π2
π1þπ2 ¼1
Substituting π2¼1/C0π1into the ﬁrst equation gives .964 π1+ .224(1 /C0π1)¼π1; solving for π1
produces π1¼.224/.260 ¼.8615 and then π2¼1/C0.8615 ¼.1385. For the season to which this
model applies, in the long run New York City has at least 50 mm of snow on 86.15% of days and less
than 50 mm on the other 13.85% of days.
It’s important to note that the top two equations alone, i.e., those provided by the relationship
πP¼π, do not uniquely determine the value of the vector π. The ﬁrst equation is equivalent to
.224π2¼.036π1(subtract .964 π1from both sides), but so is the second equation (subtract .776 π2from
both sides). The ﬁnal equation, requiring the entries of πto sum to 1, is necessary to obtain a unique
solution. ■
Expression ( 6.5) may be reexpressed as a single matrix equation. Taking a transpose,
πP¼π)PTπT¼πT¼IπT)PT/C0I/C0/C1
πT¼0,
where 0is an s/C21 vector of zeros. The requirement π1+/C1/C1/C1+πs¼1 can be rendered in matrix
form as [1 /C1/C1/C11]πT¼[1], and so the system of Eq. ( 6.5) can be expressed with the augmented matrix
T11 1
0
0−PIð6:6Þ
Example 6.23 (Example 6.21 continued) To analytically determine the steady-state distribution of
our taxi driver example, ﬁrst construct the matrix PT/C0I:452 6 Markov Chains
PT/C0I¼:3:1:4
:2:8:4
:5:1:22
43
5/C0100
010
0012
43
5¼/C0:7 :1 :4
:2/C0:2 :4
:5 :1/C0:82
43
5
Second, form the augmented matrix indicated in Expression ( 6.6), and then ﬁnally use Gauss-
Jordan elimination to ﬁnd its reduced row echelon form (e.g., with the rref command in Matlab):
111 1 1 0 0 . 2
.1 .4 0 0 1 0 .6
.2 .4 0 0 0 1 .2
.5 .1 0 0 0 0 0.7-
.2-
.8-RREF
From the right-hand matrix, we infer that π1¼.2,π2¼.6, and π3¼.2. This matches our earlier
deduction from the matrix P100. ■
Example 6.24 For the Internet browser scenario of Example 6.19, the steady-state distribution can
be determined as follows:
RREF
T1 1 1 1 1 1 1 0 0 0 0 .2840
1 1 1 1 .2 0 .7 0 0 0 1 0 0 0 .0948
0 .3 .9 0 0 0 0 0 0 1 0 0 .1659
0 .6 .1 .5 0 0 0 0 1 0 .3791
0 .7 0 .4 .5 0 0 0 0 0 1 .0758
0 . 1 . 4 0 0 00000 0-
-=.8-
.8-
1--PI
That is, π1¼.2840, π2¼.0948, and so on; these match the results suggested earlier by considering
P100. In the long run, about 28.40% percent of Web pages visited by Internet users under consideration
are social media sites, 9.48% are for checking e-mail, 16.59% are news and sports Web sites, etc. Also,
when a user ﬁnishes checking her or his e-mail online, the average number of Web sites visited until
s/he checks e-mail again is 1/ π2¼1/.0948 ¼10.55 (including the second login to e-mail). ■
6.4.5 Irreducible and Periodic Chains
The existence of a stationary distribution is not unique to regular Markov chains.
DEFINITION
Letiandjbe two (not necessarily distinct) states of a Markov chain. State jisaccessible from
state i(or, equivalently, icanaccess j)i fP(n)(i!j)>0 for some integer n/C210.1A Markov
chain is irreducible if every state is accessible from every other state.
1Forn¼0, the symbol P(0)(i!j) is interpreted as the probability of going from itojin zero steps, and so necessarily
P(0)(i!i)¼1 for all iandP(0)(i!j)¼0 for i6¼j. In particular, this means every state iis, by deﬁnition, accessible
from itself.6.4 Regular Markov Chains and the Steady-State Theorem 453
It should be clear that every regular chain is irreducible (do you see why?). However, the reverse is
not true: an irreducible Markov chain need not be a regular chain. Consider the cyclic chain of
Example 6.20: the bus can access any of the three locations it visits (campus, housing, downtown)
from any other location, so the chain is irreducible. However, as discussed earlier in this section, the
chain is deﬁnitely not regular. The Ehrenfest chain model developed in Exercise 2 is another example
of an irreducible but not regular chain; see Exercise 43 at the end of this section.
It can be shown that any ﬁnite -state,irreducible Markov chain has a stationary distribution .T h a t
is, if Pis the transition matrix of an irreducible chain, there exists a row vector πsuch that πP¼π;
moreover, there is a unique such vector satisfying the additional constraint ∑πi¼1. For example, the
cyclic bus route chain of Example 6.20 has stationary distribution π¼[1/3 1/3 1/3], as seen by the
computation
πP¼1=31 =31 =3 ½/C138010
001
1002
43
5¼1=31 =31 =3 ½/C138 ¼ π
So, if the bus is equally likely to be at any of its three locations right now, it is also equally likely to
be at any of those three places after the next transition (the “stationary” interpretation of π). This is
true even though the chain is not regular, so the Steady-State Theorem does not apply.
If an s-state Markov chain is irreducible but not regular, then every state can access every other
state but there exists no integer nfor which all s2probabilities P(n)(i!j) are positive. The only way
this can occur is if the chain exhibits some sort of “periodic” behavior, e.g., when one group of states
can access some states only in an even number of steps and others only in an odd number of steps.
Formally, the period of a state iis deﬁned as the greatest common divisor of all positive integers
nsuch that P(n)(i!i)>0; if that gcd equals 1, then state iis called aperiodic . All three states in the
cyclic chain above have period 3, because for every state the period is gcd(3, 6, 9, ...)¼3. It can be
shown that every state in an irreducible chain has the same period; the chain is called aperiodic if that
common period is 1 and is called periodic otherwise.
As noted previously, for any regular Markov chain there exists an integer nsuch that all the entries
ofPn,Pn+1,Pn+2, and so on are positive. Since the gcd of the set { n,n+1 , n+2 , ...}i s1 ,i t
immediately follows that every regular Markov chain is aperiodic. The following theorem
characterizes regularity for ﬁnite-state chains.
THEOREM
A ﬁnite-state Markov chain is regular if, and only if, it is both irreducible and aperiodic.
The “only if” direction of the theorem is established in the earlier paragraphs of this sub-section.
The converse statement, that all irreducible and aperiodic ﬁnite-state chains are regular, can be
proved using a result called the Frobenius coin -exchange theorem (we will not present the
proof here).
6.4.6 Exercises: Section 6.4(31–43)
31. Refer back to Mendel’s plant breeding experiments in Example 6.16 and Exercise 26.454 6 Markov Chains
(a) Do the genotypes formed by successive cross-breeding with pure recessive plants gg,a si n
Example 6.16, form a regular Markov chain?
(b) Do the genotypes formed by successive cross-breeding with hybrid plans Gg,a si n
Exercise 26, form a regular Markov chain?
32. Refer back to Exercise 2. Assume m¼3 balls are being exchanged between the two chambers.
Is the Markov chain Xn¼number of balls in the left chamber a regular chain?
33. Refer back to Example 6.13 regarding cell phone contracts in China.
(a) Determine the steady-state probabilities of this chain.
(b) In the long run, what proportion of Chinese cell phone users will have contracts with China
Mobile?
(c) A certain cell phone customer currently has a contract with China Telecom. On the
average, how many contract changes will s/he make before signing with China Telecom
again?
34. The article “Markov Chain Model for Performance Analysis of Transmitter Power Control in
Wireless MAC Protocol” (Twenty-ﬁrst International Conference on Advanced Networking and
Applications, 2007) describes a Markov chain model for the state of a communication channel
using a particular “slotted non-persistent” (SNP) protocol. The channel’s possible states are
(1) idle, (2) successful transmission, and (3) collision. For particular values of the authors’
proposed four-parameter model, the following transition matrix results:
P¼:50 :40 :10
:02 :98 0
:12 0 :882
43
5
(a) Verify that Pis the transition matrix of a regular Markov chain.
(b) Determine the steady-state probabilities for this channel.
(c) What proportion of time is this channel idle, in the long run?
(d) What is the average number of time steps between successive collisions?
35. Refer back to Exercise 3.
(a) Construct the one-step transition matrix Pof this chain.
(b) Show that Xn¼the machine’s state (full, part, broken) on the nth day is a regular Markov
chain.
(c) Determine the steady-state probabilities for this chain.
(d) On what proportion of days is the machine fully operational?
(e) What is the average number of days between breakdowns?
36. Refer back to Exercise 6, and assume three ﬁles A,B,Care to be repeatedly requested. Suppose
that 60% of requests are for ﬁle A, 10% for ﬁle B, and 30% for C. Let Xn¼the stacked order of
the ﬁles (e.g., ABC) after the nth request.
(a) Construct the transition matrix Pfor this chain. (The one-step transition probabilities were
established in Exercise 6(c).)
(b) Determine the steady-state probability for the stack ABC.
(c) Show that, in general, the steady-state probability for ABC is given by
πABC¼pA/C1pB
pBþpC
where pA¼P(ﬁleAis requested) and pBandpCare deﬁned similarly. (The other ﬁve steady-
state probabilities can be deduced by changing the subscripts appropriately.)6.4 Regular Markov Chains and the Steady-State Theorem 455
37. Let Pbe the one-step transition matrix of a Markov chain. Show that if all the entries of Pnare
positive for some positive integer n, then so are all the entries of Pn+1,Pn+2, and so on. [ Hint:
Write Pn+1¼P/C1Pnand consider how the ( i,j)th entry of Pn+1is obtained.]
38. Refer back to Exercise 19.
(a) Consider a new customer. By deﬁnition, s/he made no visits to the repair shop last year.
What is his/her expected number of visits this year?
(b) Now suppose a car owner has been a customer of this repair shop for many years. What is
the expected number of shop visits s/he will make next year?
39. Consider a Markov chain with just two states, 0 and 1, with one-step transition probabilities
α¼P(0!1) and β¼P(1!0).
(a) Assuming 0 <α<1 and 0 <β<1. Determine the steady-state probabilities of states
0 and 1 in terms of αandβ.
(b) What happens if αand/or βequals 0 or 1?
40.Occupational prestige describes how particular jobs are regarded by society and is often used by
sociologists to study class. The article “Social Mobility in the United States as a Markov
Process” ( J. for Economic Educators , v. 8 no. 1 (2008): 15-37) investigates the occupational
prestige of fathers and sons. Data provided in the article can be used to derive the following
transition matrix for occupational prestige classiﬁed as low (L), medium (M), or high (H):
P¼L
M
H:5288 :2096 :2616
:3688 :2530 :3782
:2312 :1738 :59502
43
5
(a) Which occupational prestige “state” is the most likely to self-replicate (i.e., father and son
are in the same category)? Which is the least likely?
(b) Determine the steady-state distribution of this Markov chain.
(c) Interpret the distribution in (b), assuming the model speciﬁed by the matrix is valid across
many generations.
[Note: The authors actually used 11 categories of occupational prestige; we have collapsed these
into three categories for simplicity.]
41. The two ends of a wireless communication system can each be inactive (0) or active (1).
Suppose the two nodes act independently, each as a Markov chain with the transition
probabilities speciﬁed in Exercise 39. Let Xn¼the “combined” state of the two relays at the
nth time step. The state space for this chain is {00, 01, 10, 11}, e.g., state 01 corresponds to an
inactive transmitter with an active receiver. (Performance analysis of such systems is described
in “Energy-Efﬁcient Markov Chain-Based Duty Cycling Schemes for Greener Wireless Sensor
Networks,” ACM J. on Emerging Tech. in Computing Systems (2012):1-32.)
(a) Determine the transition matrix for this chain. [ Hint: Use independence to uncouple the
two states, e.g., P(00!10)¼P(0!1)/C1P(0!0).]
(b) Determine the steady state distribution of this chain.
(c) As the authors note, “a connection is feasible only when both wireless nodes are active.”
What proportion of time is a connection feasible under this model?
42. A particular gene has three expressions: AA,Aa, and aa. When two individuals mate, one half of
each parent’s gene is contributed to the offspring (and each half is equally likely to be donated).
For example, an AAmother can only donate Awhile an Aafather is equally likely to donate Aor
a, resulting in a child that is either AAorAa. Suppose that the population proportions of AA,Aa,
andaaindividuals are p,q, and r, respectively (so p+q+r¼1). Consider the offspring of a456 6 Markov Chains
randomly selected individual; speciﬁcally, let Xn¼the gene expression of the oldest child in his
or her nth generation of descendants (whom we assume will have at least one offspring).
(a) Assume the nth-generation individual’s mate is selected at random from the genetic
population described above. Show the following: P(Xn+1¼AAjXn¼AA)¼p+q/2,
P(Xn+1¼AajXn¼AA)¼q/2 + r, and P(Xn+1¼aajXn¼AA)¼0. [Hint: Apply the
Law of Total Probability.]
(b) Using the same method as in (a), determine the other one-step transition probabilities and
construct the transition matrix Pof this chain.
(c) Verify that Xnis a regular Markov chain.
(d) Suppose there exists some α2[0, 1] such that p¼α2,q¼2α(1/C0α), and r¼(1/C0α)2.
(In this context, α¼P(Aallele).) Show that π¼[pqr ] is the stationary distribution of
this chain. (This fact is called the Hardy-Weinberg law ; it establishes that the rules of
genetic recombination result in a long-run stable distribution of genotypes.)
43. Refer back to the Ehrenfest chain model of Exercises 2 and 24. Once again assume that m¼3
balls are being exchanged between the two chambers.
(a) Explain why this is an irreducible chain, but not a regular chain.
(b) Explain why each state has period equal to 2.
(c) Show that the vector [1/8 3/8 3/8 1/8] is a stationary distribution for this chain. (Thus, even
though the chain is not regular and the transition matrix Pdoes not have a limit, there still
exists a stationary distribution due to irreducibility.)
6.5 Markov Chains with Absorbing States
The Gambler’s Ruin scenario, begun in Example 6.2, has the feature that the chain “terminates” when
it reaches either of two states ($0 or $3 in our version of the competition). As we’ve noted previously,
it’s mathematically advantageous to imagine that the Markov chain actually continues in these cases,
just never leaving the state 0 or 3; one such sample path is
2!1!2!1!0!0!0!0!...
In this section, we ﬁrst investigate states from which a Markov chain can never exit and the time it
takes to arrive in one of those states.
DEFINITION
A state jof a Markov chain is called an absorbing state if
Pj!jðÞ ¼ 1:
Equivalently, jis an absorbing state if the ( j,j)th entry of the one-step transition matrix of
the chain is 1.
The states 0 and 3 are both absorbing states in our Gambler’s Ruin example. In contrast, the taxi
driver example has no absorbing states. The next example shows that some care must be taken in
identifying absorbing states.6.5 Markov Chains with Absorbing States 457
Example 6.25 Anyone who has applied for a bank loan knows that the process of eventual approval
(or rejection) involves many steps and, occasionally, a lot of complex negotiation. Figure 6.7
illustrates the possible route of a set of loan documents from (1) document initiation to (6) ﬁnal
approval or rejection. The intermediate steps (2)–(5) represent various exchanges between
underwriters, loan ofﬁcers, and the like. In this particular chain, two such individuals (at states
3 and 5) have the authority to make a ﬁnal decision, though the agent at state 3 may elect to return the
documents for further discussion.
The one-step transition matrix of this chain is
P¼0:50 :500
00 :5:500
0:5000 :5
00 :50 :50
000001
0000012
66666643
7777775
Although the number 1 appears twice in P, only state 6 is an absorbing state of this chain. Indeed,
p66¼P(6!6)¼1; however, state 5 is notan absorbing state because p55¼P(5!5)¼0.
Rather, the ﬁfth row of Pindicates that if the chain ever enters state 5, it will necessarily pass in
the next transition into state 6 (where, as it happens, it will be “absorbed”). ■
To be clear, a Markov chain may have no absorbing states (the taxi driver), a single absorbing state
(Example 6.25), or multiple absorbing states (Gambler’s Ruin).
6.5.1 Time to Absorption
When a Markov chain has one or more absorbing states, it is natural to ask how long it will take to
reach an absorbing state. Of course, the answer depends on where (i.e., in which state) the Markov
chain begins. For any non-absorbing state i, deﬁne a random variable Tiby
Ti¼number of transitions until the Markov chain reaches an absorbing state, starting in state i
This rv Tiis called the time to absorption from state i; the possible values of Tiare 1, 2, 3, 4, ....
As we shall now illustrate, the distribution of Tican be approximated from the k-step transition
matrices Pkfork¼1, 2, 3, .... For simplicity’s sake, consider ﬁrst a Markov chain with a single
absorbing state, which we will call a. Then the ( i,a)th entry of Pis the probability of transitioning
directly from state iinto the absorbing state a, which is therefore also the probability that Tiequals 1:.5.5 .5
.5
.5.5
.5.5
2
41
11
563Fig. 6.7 State diagram for
Example 6.25458 6 Markov Chains
Pi!aðÞ ¼ PT i¼1 ðÞ
Since Tiis always a positive integer, this also equals P(Ti/C201), a fact which will prove important
shortly. Now consider the ( i,a)th entry of P2, which represents P(2)(i!a). There are two ways the
Markov chain could transition from itoain two steps:
i!any non-absorbing state !aT i¼2 ðÞ ,o r
i!a!aT i¼1 ðÞ
Therefore, the two-step probability P(2)(i!a) does not represent the chance Tiequals 2, but
rather the chance that Tiisat most 2. That is,
P2ðÞi!aðÞ ¼ PT i/C202 ðÞ :
Following the same pattern, the k-step transition probability P(k)(i!a) is equal to P(Ti/C20k) for
any positive integer k.
If the Markov chain has two absorbing states a1anda2, say, then the chance of being absorbed
from state iin one step is simply the sum P(i!a1)+P(i!a2), since those two events are mutually
exclusive (you can only arrive in one state). Similarly, the probability P(Ti/C202) is determined by
adding P(2)(i!a1)+P(2)(i!a2), and so on. The general result is stated in the following theorem.
THEOREM
Consider a ﬁnite-state Markov chain, and let Adenote the (non-empty) set of absorbing states.
For any state i=2A, deﬁne Ti¼the number of transitions, starting in state i, until the chain
arrives in some absorbing state. Then the cdf of Tiis given by
FTikðÞ ¼ PT i/C20k ðÞ ¼X
a2APkðÞi!aðÞ k¼1, 2, 3, ...
In the special case of a single absorbing state, a, this simpliﬁes to
FTikðÞ ¼ PT i/C20k ðÞ ¼ PkðÞi!aðÞ
The probability distribution of Ti(i.e., the pmf of the rv Ti) can then be determined from the cdf.
Example 6.26 (Example 6.25 continued) Let’s consider the rv T1, the absorption time from state
1 (i.e., the number of steps from loan document initiation to the bank’s ﬁnal decision). From the
one-step transition matrix P, we know that
FT11ðÞ ¼ PT 1/C201 ðÞ ¼ PT 1¼1 ðÞ ¼ P1!6 ðÞ ¼ p16¼0:
The (1,6) entry of P2is also zero, so FT12ðÞ ¼ PT 1/C202 ðÞ ¼ P2ðÞ1!6 ðÞ ¼ 0. Software was used
to obtain the matrices P3,...,P12, resulting in the following values for the (1,6) entry.
k 1 2 3 456 7 8 9 1 01 11 2
FT1kðÞ 0 0 .5 .6875 .75 .8594 .8984 .9336 .9570 .9707 .9810 .98736.5 Markov Chains with Absorbing States 459
The accompanying table is, of course, an incomplete description of the cdf of T1, since this process
could theoretically be continued indeﬁnitely. Next, because the rv T1is integer-valued, its pmf is
easily determined from the cdf:
PT 1¼2 ðÞ ¼ PT 1/C202 ðÞ /C0 PT 1/C201 ðÞ ¼ FT12ðÞ /C0 FT11ðÞ ¼ 0/C00¼0
PT 1¼3 ðÞ ¼ PT 1/C203 ðÞ /C0 PT 1/C202 ðÞ ¼ FT13ðÞ /C0 FT12ðÞ ¼ :5/C00¼:5
PT 1¼4 ðÞ ¼ PT 1/C204 ðÞ /C0 PT 1/C203 ðÞ ¼ FT14ðÞ /C0 FT13ðÞ ¼ :6875/C0:5¼:1875
The ﬁrst 12 probabilities in the pmf of T1are as follows (their sum is .9873):
k 1 2 3 456789 1 0 1 1 1 2
pT1kðÞ 0 0 .5 .1875 .0625 .1094 .0390 .0352 .0234 .0137 .0103 .0063
This incomplete pmf is graphed in Fig. 6.8. Notice that T1must be at least 3, which is consistent
with the state diagram in Fig. 6.7: it takes at least three steps to get from state 1 to state 6 (one of
1!2!3!6, 1!4!3!6, or 1 !4!5!6).
A call to the bank determines that the documents are in the hands of the underwriter indicated by
state 4. So, let’s now consider the rv T4¼time to absorption (completion of the process) starting
from state 4. Based on the state diagram, it seems reasonable to anticipate that it will typically take
less time to reach state 6 starting from state 4 than it did when the chain began in state 1. Reading off
the (4, 6) entries of P,P2,...,P12yields the cdf values in the accompanying table; subtraction as
before then gives the corresponding pmf values.
k 1 2 3 456789 1 0 1 1 1 2
FT4kðÞ 0 .75 .75 .8125 .9063 .9219 .9531 .9688 .9785 .9863 .9907 .9939
pT4kðÞ 0 .75 0 .0625 .0938 .0156 .0312 .0157 .0097 .0078 .0044 .0032
Notice that, starting in state 4, the chain is quite likely to be absorbed into state 6 in exactly two
steps (either 4 !5!6o r4 !3!6, with probabilities .5 and .25, respectively), and that it is
impossible to move from 4 to 6 in exactly three steps. ■1211109876543210.5p(t1)
0.4
0.3
0.2
0.1
0.0 t1Fig. 6.8 The (incomplete)
pmf of T1from Example
6.26460 6 Markov Chains
Example 6.27 In the Gambler’s Ruin scenario with p¼.55, how many games will Allan and Beth
play against each other before one player goes broke? Recall that the transition matrix Pis set from
Allan’s perspective, and that he begins with $2. Thus, the rv of interest is T2, the number of transitions
(aka games), starting from Allan having $2, until the competition ends because Allan either has $0 or
$3. The one- and two-step transition matrices of this chain appear in Example 6.14. Hence
PT 2/C201 ðÞ ¼ P2!0 ðÞ þ P2!3 ðÞ ¼ 0þ:55¼:55
PT 2/C202 ðÞ ¼ P2ðÞ2!0 ðÞ þ P2ðÞ2!3 ðÞ ¼ :2025þ:55¼:7525
In general, the cumulative probability P(T2/C20k) can be determined by adding the (2,0) and (2,3)
entries of the k-step transition matrix Pk. These values were determined with the aid of software for
k¼1 through 10 and are summarized in the accompanying table.
k 1 23456789 1 0
FT2kðÞ .55 .7525 .8886 .9387 .9724 .9848 .9932 .9962 .9983 .9991
pT2kðÞ .55 .2025 .1361 .0501 .0337 .0124 .0084 .0030 .0021 .0008
It’s important to notice that T2indicates the number of steps required to enter some absorbing state
(here, either $0 or $3), not the number of steps to enter a particular such state. ■
6.5.2 Mean Time to Absorption
With Ti¼time to absorption starting from state i, the expected value of Tiis called the mean time to
absorption (mtta )from state i:
μi¼E(Ti)¼expected number of transitions until the Markov chain reaches an absorbing state,
starting in state i
For each of the preceding examples, the incomplete pmf can be used to approximate the mtta from
state i. Consider the Markov chain in Example 6.26:
μ1¼ET 1ðÞ ¼X1
k¼1k/C1pT1kðÞ /C25X12
k¼1k/C1pT1kðÞ
¼10ðÞ þ 2/C0
0/C1
þ3/C0
:5/C1
þ4/C0
:1875/C1
þ/C1/C1/C1þ 11/C0
:0103/C1
þ12/C0
:0063/C1
¼4:31
To a hopefully reasonable approximation, on average the chain requires 4.31 transitions, starting in
state 1, to be absorbed into state 6. Similarly, the mean time to absorption from state 4 is
approximately
μ4¼X1
k¼1k/C1pT4kðÞ /C25X12
k¼1k/C1pT4kðÞ
¼10ðÞ þ 2:75ð Þþ/C1/C1/C1þ 11:0044ðÞ þ 12:0032ðÞ ¼ 2:91
For the Gambler’s Ruin competition with p¼.55 and Allan’s initial stake at $2, the pmf displayed
in Example 6.27 gives6.5 Markov Chains with Absorbing States 461
μ2/C251:55ðÞ þ 2:2025ð Þþ/C1/C1/C1þ 10:0008ðÞ ¼ 1:92
That is, if Allan starts with $2 and p¼.55, the expected length of the Gambler’s Ruin competition
is approximated to be 1.92 games.
In all such approximations, two things should be clear. First, the estimated means are smaller than
the correct values, since the sums used are truncated versions of the correct summations and every
term is nonnegative. So, in the Gambler’s Ruin scenario, μ2>1.92. Second, the more terms we
include in the truncated sum, the closer the approximation will be to the correct mean time to
absorption from that state. Of course, additional terms require overcoming the practical hurdle of
computing successively higher powers of the matrix P. With software, one could in practice use this
method to get a very good approximation to the mtta.
Exercise 56 presents a different approximation method that always yields a better approximation
to the mean time to absorption; moreover, it relies directly on the cdf values and thus does not require
computing differences to form the pmf. But this is still an approximation; what we would really like is
an explicit method for determining the exact mean time to absorption from various states in the chain.
The following theorem provides such a result.
MTTA THEOREM
Suppose a ﬁnite-state Markov chain with one-step transition matrix Phasrnon-absorbing
states (and at least one absorbing state). Suppose further that there exists a path from every
non-absorbing state into some absorbing state.
LetQbe the r/C2rsub-matrix of Pcorresponding to the non-absorbing states of the chain.
Then the mean times to absorption from these states are given by the matrix formula
μ¼I/C0QðÞ/C011,
where μi¼mtta from the ith state in the Qsub-matrix, μ¼[μ1...μr]T,Iis the r/C2ridentity
matrix, and 1¼[1...1]T.
This theorem not only provides the exact mean times to absorption (as opposed to the earlier
approximations) but also computes allof them simultaneously. A proof of the MTTA Theorem will
be presented shortly, but ﬁrst we illustrate its use with our two ongoing examples.
Example 6.28 For the bank loan Markov chain in Example 6.25, state 6 is the only absorbing
state, so there are r¼5 non-absorbing states. The sub-matrix corresponding to these non-absorbing
states is
Q¼1
2
3
4
50:50 :50
00 :5:50
0:5000
00 :50 :5
000002
666643
77775
This can be obtained by “crossing out” the row and column of Pcorresponding to absorbing state 6.
Letμi¼E(Ti) be the mean time to absorption from state ifori¼1, 2, 3, 4, 5. Then, according to the
MTTA Theorem,462 6 Markov Chains
μ¼I/C0QðÞ/C011
¼1/C0:50 /C0:50
01 /C0:5/C0:50
0/C0:5100
00 /C0:51 /C0:5
000012
666666643
77777775/C011
1
1
1
12
666666643
77777775
¼1111 :5
01 :61 :2 :8:4
0 :81 :6 :4:2
0 :4 :81 :2:6
0000 12
666666643
777777751
1
1
1
12
666666643
77777775¼4:5
4:0
3:0
3:0
1:02
666666643
77777775
The inverse of I/C0Qwas determined using software.
So, for example, the mean time to absorption from state 1 is μ1¼4.5 transitions, slightly larger
than our earlier approximation of 4.31. On the average, it takes 4.5 steps to arrive at a loan decision
starting from the time the loan documents are initiated. Similarly, the earlier estimate μ4/C252.91 was a
little off from the correct answer of μ4¼3. The last entry of the vector μis obvious from the design
of the chain: since state 5 transitions immediately into state 6 with certainty, T5is identically equal to
1, and so its mean is 1. ■
Example 6.29 Consider once again our Gambler’s Ruin scenario, this time with an arbitrary
probability pthat Allan triumphs over Beth in any one game. The only two non-absorbing states
are $1 and $2, so the required sub-matrix Qconsists of the “center four” entries of the original 4 /C24
transition matrix:
P¼0
1
2
310 0 0
1/C0p 0 p0
01 /C0p0p
00 0 12
6643
775)Q¼1
20 p
1/C0p0/C20/C21
There is a simple inverse formula for a 2 /C22 matrix:
ab
cd/C20/C21/C01
¼1
ad/C0bcd/C0b
/C0ca/C20/C21
ð6:7Þ
Applying Eq. ( 6.7) and the MTTA Theorem,
μ¼I/C0QðÞ/C011
¼1 /C0p
/C01þp1"#/C011
1"#
¼1
1ðÞ1ðÞ /C0/C0 pðÞ /C0 1þp ðÞ1 p
1/C0p1"#
1
1"#
¼1
1/C0pþp21þp
2/C0p"#6.5 Markov Chains with Absorbing States 463
Hence
μ1¼1þp
1/C0pþp2μ2¼2/C0p
1/C0pþp2
Since we have always started Allan with $2, let’s explore μ2further. If p¼1, so Allan cannot lose,
then μ2¼(2/C01)/(1/C01+12)¼1. This is logical, since Allan would automatically transition from
$2 to $3 in 1 step/game and the competition would be over. Similarly, substituting p¼0 into this
expression gives μ2¼2, reﬂecting the fact that if Allan cannot win games then the chain must
necessarily proceed along the path 2 !1!0, a total of two transitions. For p¼.55, the numerical
case illustrated earlier, we have
μ2¼2/C0:55
1/C0:55þ:552¼1:45
:7525¼580
301¼1:92691
which is quite close to our previous approximation of 1.92.
For what value of pis the competition expected to take the longest? Using calculus, one can ﬁnd
the maximum of μ2with respect to p, which turns out to occur at p¼2/C0ﬃﬃﬃ
3p
/C25:268. If Allan begins
with $2 and has a .268 probability of winning each game, the expected duration of the competition is
maximized, speciﬁcally with μ2¼1þ2=ﬃﬃﬃ
3p
/C252:155 games. ■
Proof of the MTTA Theorem For notational ease, let 1, 2, ...,rbe the non-absorbing states of the
chain. Also, let Adenote the set of absorbing states (which, if the Markov chain has stotal states,
could be enumerated as r+1 , ...,s). Starting in any non-absorbing state i, consider the ﬁrst transition
of the chain. If the chain transitions into any member of A, then it has been “absorbed” in one step and
soTi¼1. On the other hand, if the chain transitions into any non-absorbing state j(including back
intoiitself), then the expected number of steps to absorption is 1 + E(Tj), where the 1 accounts for the
step just taken and Tjrepresents the time to absorption starting from the newstate j. Apply the Law of
Total Expectation:
ET iðÞ ¼ 1/C1Pi!AðÞ þXr
j¼11þET j/C0/C1 /C1
/C1P/C0
i!j/C0/C1
¼Pi!AðÞ þXr
j¼1Pi!jðÞ þXr
j¼1ET j/C0/C1
/C1Pi!jðÞ
Since the state space of the Markov chain is A[{1, 2, ...,r}, the ﬁrst two terms in the expression
above must sum to 1. Thus, we have μi¼1+Σj¼1rμjP(i!j) for i¼1, 2, ...r. Stacking these
equations and rewriting slightly, we have
μ1¼P1!1 ðÞ μ1þ/C1/C1/C1þ P/C0
1!r/C1
μrþ1
μ2¼P2!1 ðÞ μ1þ/C1/C1/C1þ P/C0
2!r/C1
μrþ1
⋮⋮
μr¼Pr!1 ðÞ μ1þ/C1/C1/C1þ P/C0
r!r/C1
μrþ1
This stack can be written more compactly as μ¼Qμ+1. Solving for μyields the desired
result. ■464 6 Markov Chains
The MTTA Theorem requires that every non-absorbing state can reach (at least) one absorbing
state. That is, the set of absorbing states must be accessible from every non-absorbing state. What
would happen if this were not the case?
Example 6.30 In the Markov chain depicted in Fig. 6.9, 4 is an absorbing state, but it is only
accessible from state 3. It is clear that the chain will eventually be absorbed into state 4 if X0¼3 and
will never be absorbed into state 4 if X0¼1 or 2. So, where does the MTTA Theorem break down?
The one-step transition matrix Pfor this chain, the resulting sub-matrix Qfor the non-absorbing
states, and the matrix I/C0Qrequired for calculating mean times to absorption are
P¼1
2
3
4:5:50 0
:4:60 0
00 :7:3
00012
6643
775Q¼1
2
3:5:50
:4:60
00 :72
43
5 I/C0Q¼:5/C0:50
/C0:4:40
00 :32
43
5
The matrix I/C0Qis not invertible; this can be seen by noting that the ﬁrst and second rows are
multiples of each other, or by computing the determinant and discovering that det( I/C0Q)¼0.
Because ( I/C0Q)/C01does not exist, the formula from the MTTA Theorem cannot be applied.
Recall that the cdf of T1can be determined from the appropriate entries of the k-step transition
matrices; speciﬁcally, since the only absorbing state of this chain is state 4,
FT1kðÞ ¼ PT 1/C20k ðÞ ¼ PkðÞ1!4 ðÞ ¼ the 1 ;4ðÞ entry of Pk
The (1, 4) entry of the matrix Pabove is 0, so P(T1/C201)¼0. But since state 4 is not accessible
from state 1, the (1, 4) entry of every transition matrix Pkis 0. Thus, P(T1/C20k)¼0 for all positive
integers kandpT1kðÞ ¼ 0/C00¼0 for all k. Since the probabilities associated with T1sum to zero and
not 1, T1is not actually a valid rv (and so, in particular, has no mean). ■
In general, when the set of absorbing states is not accessible from every non-absorbing state, the
matrix I/C0Qwill be singular (i.e., not invertible). If a subset of the non-absorbing states can access
the absorbing states (that’s true for state 3 in Example 6.30), we can apply the MTTA Theorem if we
deﬁne Qto be the sub-matrix of Pcorresponding to those states that can access the absorbing states.
6.5.3 Mean First Passage Times
We now brieﬂy turn our attention back to regular Markov chains. In Sect. 6.4, we saw that one
interpretation of the probability πifrom the Steady-State Theorem is that 1/ πirepresents the expected
number of transitions necessary for the chain to return to state igiven that it starts there—the mean
recurrence time for state i. With a clever use of the MTTA Theorem, we can also determine the
expected number of transitions required for the chain to transition from a state ito adifferent state j—
themean ﬁrst passage time from itoj.1.5 .5
.4.3.6
.721 4 3Fig. 6.9 State diagram for
Example 6.306.5 Markov Chains with Absorbing States 465
Example 6.31 (Example 6.23 continued) For the ubiquitous taxi driver example, it was found that
the steady-state probability for zone 3 is π3¼.2 and, thus, the expected number of fares until the
driver returns to zone 3 is 1/ π3¼1/.2¼5.
But suppose the taxi driver just dropped off a fare in zone 1 (or zone 2). He wonders how long it
will take him to get back home to zone 3 for lunch. More precisely, he wishes to know the expected
number of fares required to reach zone 3, starting from some other state (i.e., different than zone 3).
The trick to answering the taxi driver’s question—i.e., to determine the mean ﬁrst passage time for
zone 3 when beginning in zone 1 or zone 2—is to pretend that zone 3 is an absorbing state, and then
invoke the MTTA Theorem. Modify the original one-step transition matrix Pof the Markov chain so
that zone 3 is absorbing state, and label the new matrix eP:
P¼:3:2:5
:1:8:1
:4:4:22
43
5) eP¼:3:2:5
:1:8:1
0012
43
5
Now proceed as before: the sub-matrix for the non-absorbing states, which in ePare zone 1 and zone
2, is
Q¼:3:2
:1:8/C20/C21
,
from which
μ¼I/C0QðÞ/C011¼:7/C0:2
/C0:1:2/C20/C21/C011
1/C20/C21
¼/C1/C1/C1¼3:33
6:67/C20/C21
Thus, starting in zone 1, the average number of trips required for the taxi driver to get home to zone
3 is 3.33, while it takes twice that long on the average if he’s starting from zone 2. ■
6.5.4 Probabilities of Eventual Absorption
As discussed in Example 6.29 in the context of Gambler’s Ruin, when a Markov chain has multiple
absorbing states one can only speak of the mean time to absorption into the setof absorbing states, not
any particular absorbing state (e.g., not time to $0 separate from time to $3). We can, however,
ask about the probability of eventual absorption into state $0, as opposed to eventual absorption into
state $3.
DEFINITION
Letabe an absorbing state of a Markov chain and let ibe a non-absorbing state. The
probability of eventual absorption into afrom state i, denoted π(i!a), is deﬁned by
πi!aðÞ ¼ lim
n!1PnðÞi!aðÞ
That is, π(i!a) is deﬁned to be the limit of the ( i,a) entry of Pnasn!1 . This is consistent with
our previous efforts to determine the probability of eventual absorption by examining P75orP100.
But rather than approximate these probabilities by taking a high power of P, we now present an explicit
method for determining them.466 6 Markov Chains
Before illustrating the method for determining π(i!a), a few observations are in order. First, if
state ais not accessible from state i, then P(n)(i!a)¼0 for all nand the limit is also zero, i.e.,
π(i!a)¼0 when icannot access a. This occurred in Example 6.30, with state 4 not being
accessible from states 1 or 2.
Second, if the Markov chain has a single absorbing state a, then π(i!a)¼1 for every state ithat
can access a. That is, a chain with an accessible absorbing state will always eventually be absorbed.
This would be the case, for instance, in Example 6.25: it is a sure thing that the chain will eventually
arrive at (and stay in) state 6, irrespective of where the chain begins. So, the interesting cases of
determining π(i!a) are for Markov chains with multiple absorbing states, such as Gambler’s Ruin.
Third, suppose we extended the preceding deﬁnition to non-absorbing states. That is, what can be
said about
lim
n!1PnðÞi!jðÞ
when jisnotan absorbing state? If the Markov chain has any absorbing states (and assuming at least
one of these is accessible from i), then the chain will eventually get absorbed and so P(n)(i!j)!0.
If we have a regular Markov chain—which, in particular, means there are no absorbing states—then
the Steady State Theorem tells us P(n)(i!j)!πj, a steady-state probability that is independent of i.
For other cases, such as the cyclic chain of Example 6.20, the limit of P(n)(i!j) may not exist at all.
On to the calculation: as in the proof of the MTTA Theorem, rearrange the states so that the
non-absorbing states of the Markov chain are 1, 2, ...,rand the absorbing states are r+1 , ...,s. Then
the one-step transition matrix Pcan be partitioned as follows:
1
r
s=1r+QR
P
OIð6:8Þ
Expression ( 6.8) is sometimes called the canonical form of a Markov chain. In Eq. ( 6.8),Qis the
r/C2rsub-matrix for the non-absorbing states, as before. The matrix Oin the lower left of Eq. ( 6.8)
consists entirely of zeros, since that quadrant of Pindicates the probabilities of transitioning from an
absorbing state ( r+1 , ...,s)t oa non-absorbing state (1, ...,r). Similarly, Iis the ( s/C0r)/C2(s/C0r)
identity matrix, since its diagonal entries correspond to P(a!a) for the absorbing states and its
off-diagonal entries to impossible events (transitions from one absorbing state to another). The
“remainder” matrix Rindicates the transition probabilities from the non-absorbing states into the
absorbing states and can have (fairly) arbitrary entries.
The probabilities of eventual absorption into every absorbing state from every non-absorbing state
are provided by the following theorem.6.5 Markov Chains with Absorbing States 467
THEOREM
Consider a Markov chain with non-absorbing states 1, ...,rand absorbing states r+1 , ...,s.
Deﬁne sub-matrices QandRof the one-step transition matrix Pas in Eq. ( 6.8). Suppose further
than every absorbing state is accessible from every non-absorbing state. Then the probabilities
of eventual absorption are given by
Π¼I/C0QðÞ/C01R,
where Iis the r/C2ridentity matrix and Πis an r/C2(s/C0r) matrix whose entries are the
probabilities π(i!a) for i¼1,...,randa¼r+1 , ...s.
Some guidance for the proof of this theorem can be found in Exercise 57.
Example 6.32 (Example 6.29 continued) To apply the previous theorem to our Gambler’s Ruin
example, we need to reorder the states, so that non-absorbing states $1 and $2 come ﬁrst while
absorbing states $0 and $3 come last. The canonical form of P, along with the relevant sub-matrices
QandR, are
01 0
21 0 0
0010
00011
0
3pp
pp−
−=P , 0
10p
p=−Q , 10
0p
p−=R
Applying the previous theorem, along with the inverse formula ( 6.7) for a 2 /C22 matrix,
Π¼I/C0QðÞ/C01R¼1 /C0p
/C01þp1"#/C011/C0p0
0 p"#
¼1
1/C0pþp21 p
1/C0p1"#
1/C0p0
0 p"#
¼1/C0p
1/C0pþp2p2
1/C0pþp2
1/C02pþp2
1/C0pþp2p
1/C0pþp22
66643
7775
Reading off the entries of the matrix Π, we have
π$1!$0 ðÞ ¼1/C0p
1/C0pþp2π/C0
$1!$3/C1
¼p2
1/C0pþp2
π$2!$0 ðÞ ¼1/C02pþp2
1/C0pþp2π/C0
$2!$3/C1
¼p
1/C0pþp2
In particular, if Allan starts with $2, the probability he will eventually win the competition is
π($2!$3)¼p/(1/C0p+p2). As a check, this probability equals zero when p¼0 (Allan never
wins games) and equals one when p¼1 (Allan always wins games). If p¼.55, as in several of the
previous examples in this chapter,
π$2!$3 ðÞ ¼:55
1/C0:55þ:552¼:55
:7525¼220
301/C25:7309468 6 Markov Chains
Notice that this is, to four decimal places, the probability we approximated by computing P75with
software and thereby obtaining P(75)($2!$3). ■
The matrices RandΠin Example 6.32 are square, but this is not necessarily the case in other
scenarios. In general, Qis an r/C2rmatrix (hence, square), but the dimensions of both RandΠare
r/C2(s/C0r).
6.5.5 Exercises: Section 6.5(44–58)
44. Explain why a Markov chain with one or more absorbing states cannot be a regular chain.
45. A local community college offers a three-semester athletics training (AT) program. Suppose
that at the end of each semester, 75% of students successfully move on to the next semester
(or to graduation from the third semester) and 25% are required to repeat the most recent
semester.
(a) Construct a transition matrix to represent this scenario. The four states are (1) ﬁrst
semester, (2) second semester, (3) third semester, (4) graduate.
(b) What is the probability a student graduates the program within three semesters? Four
semesters? Five semesters?
(c) What is the average number of semesters required to graduate from this AT program?
(d) According to this model, what is the probability of eventual graduation? Does that seem
realistic?
46. Refer back to the previous exercise. Now suppose that at the end of each semester, 75% of
students successfully move on to the next semester (or to graduation from the third semester),
15% ﬂunk out of the program, and 10% repeat the most recent semester.
(a) Construct a transition matrix to represent this updated situation by adding a ﬁfth state,
(5) ﬂunk out. [ Hint: Two of the ﬁve states are absorbing.]
(b) What is the probability a student exits the program, either by graduating or ﬂunking out,
within three semesters? Four semesters? Five semesters?
(c) What is the average number of semesters students spend in this program before exiting
(again, either by graduating or ﬂunking out)?
(d) What proportion of students that enter the program eventually graduate? What proportion
eventually ﬂunk out?
(e) Given that a student has passed the ﬁrst two semesters (and, so, is currently in her third-
semester courses), what is the probability she will eventually graduate?
47. The article “Utilization of Two Web-Based Continuing Education Courses Evaluated by
Markov Chain Model” ( J. Am. Med. Inform. Assoc . 2012: 489-494) compared students’ ﬂow
between pages of an online course for two different Web layouts in two different health
professions classes. In the ﬁrst week of the classes, students could visit (1) the homepage,
(2) the syllabus, (3) the introduction, and (4) chapter 1 of the course content. Each student was
tracked until s/he either reached chapter 1 or exited without reading chapter 1 (call the latter
state 5). For one version of the Web content in one class, the following transition matrix was
estimated:6.5 Markov Chains with Absorbing States 469
P¼01000
:21 0 :33 :05 :41
:09 :15 0 :67 :09
00010
000012
666643
77775
When students log into the course, they are always forced to begin on the homepage.
(a) Identify the absorbing state(s) of this chain.
(b) Let T1¼the number of transitions students take, starting from the homepage, until the
either arrive at chapter 1 or exit early. Determine P(T1/C20k) for k¼1, 2, ..., 10.
(c) Use (b) to approximate the pmf of T1, and then approximate the mean time to absorption
starting from the class homepage.
(d) Determine the (true) mean time to absorption starting from the homepage.
(e) What proportion of students eventually got to chapter 1 in the ﬁrst week? What proportion
exited the course without visiting chapter 1?
48. Refer back to the previous exercise. After some content redesign, the same Web-based health
professions course was run a second time. The ﬁrst-week transition probabilities for the revised
course were as follows:
P¼01000
:15 0 :43 :06 :36
:09 :16 0 :66 :09
00010
000012
666643
77775
(a) How did the redesign affect the average amount of time students spent in the course
(at least as measured by the number of Web page visits within a session)?
(b) Did the redesign improve the chances that students would get to the chapter 1 content
before exiting the system?
49. In Exercise 4, we introduced a game in which Michelle will ﬂip a coin until she gets heads four
times in a row. Deﬁne X0¼0 and, for n/C211,Xn¼the number of heads in the current streak of
heads after the nth ﬂip.
(a) Construct the one-step transition matrix Pfor this chain, on the state space {0, 1, 2, 3, 4}.
What is special about state 4?
(b) Let T0denote the total number of coin ﬂips required by Michelle to achieve four heads in a
row. Construct the cdf of T0,P(T0/C20k), for k¼1, 2, ..., 15. [ Hint: The cdf values for
k¼1, 2, 3 should be obvious.]
(c) Michelle will win a prize if she can get four heads in a row within 10 coin ﬂips. What is the
probability she wins the prize?
(d) Use (b) to construct an incomplete pmf of T0. Then use this incomplete pmf to approximate
both the mean and standard deviation of T0.
(e) What is the (exact) expected number of coin ﬂips required for Michelle to get four heads in
a row?
50. Refer back to Exercise 8. The article referenced in that exercise provides the following
transition matrix for the states (1) current, (2) delinquent, (3) loss, and (4) paid, for a certain
class of loans:470 6 Markov Chains
P¼:95 :04 0 :01
:15 :75 :07 :03
0010
00012
6643
775
(a) Identify the absorbing state(s).
(b) Determine the mean time to absorption for a loan customer who is current on payments,
and for a customer who is delinquent.
(c) If a loan customer is current on payments, what is the probability s/he will eventually pay
off the loan? What is the probability the loan company will suffer a loss on this account?
(d) Answer (c) for customers who are delinquent on their loans.
51. Refer back to Exercise 15(c). Calculate and interpret the mean times to absorption for this chain.
For which opening strategy, cooperative or competitive, is the negotiation process longer on the
average?
52. Refer back to Exercise 14. Assuming Lucas begins searching for his uncle in room 1 and his
uncle is hiding in room 6, what is the expected number of rooms Lucas will visit in order to
“win” this round of hide-and-seek?
53. Modify the Gambler’s Ruin example of this section to a $4 total stake. That is, Allan may start
with x0¼$1, $2, or $3, and Beth has $(4 /C0x0) initially. As usual, let pdenote the probability
Allan wins any single game.
(a) Construct the one-step transition matrix.
(b) Determine the mean times to absorption for each of Allan’s possible starting values, as
functions of p.
(c) Determine the probability Allan eventually wins, starting with $1 or $2 or $3, as functions
ofp.
54. Refer back to the Ehrenfest chain model introduced in Exercise 2. Suppose there are m¼3 balls
being exchanged between the two chambers. If the left chamber is currently empty, what is the
expected number of exchanges until it is full (i.e., all 3 balls are on the left side)?
55. Refer back to Exercise 40. If a man has a low-prestige occupation, what is the expected number
of generations required for him to have an offspring with a high-prestige occupation?
56. Exercise 48 of Chap. 2established the following formula for the mean of a rv Xwhose possible
values are positive integers:
EXðÞ ¼ 1þX1
x¼11/C0FxðÞ ½/C138 ,
where F(x) is the cdf of X. Hence, if the values F(1),F(2), ...,F(x*) are known for some integer
x*, the mean of Xcan be approximated by 1 + ∑x¼1x*[1/C0F(x)].
(a) Refer back to Example 6.26. Use the given cdf values and the above expression with
x*¼12 to approximate E(T1), the mean time to absorption starting in state 1. How does
this compare to the pmf-based approximation in the example? How does it compare to the
exact answer, 4.5?
(b) Repeat part (a), starting in state 4 of the bank loan Markov chain.
(c) Will this method always under-approximate the true mean of the rv, or can you tell?
Explain.
[Note: It can be shown that this “cdf method” of approximating the mean will always produce a
higher value than the truncated sum of x/C1p(x).]6.5 Markov Chains with Absorbing States 471
57. This exercise outlines a proof of the formula Π¼(I/C0Q)/C01Rfor the probabilities of eventual
absorption. You will want to refer back to Eq. ( 6.8), as well as the proof of the MTTA Theorem.
(a) Starting in a non-absorbing state i, the chain will eventually be absorbed into absorbing
state aif either (1) the chain transitions immediately into a, or (2) the chain transitions into
any non-absorbing state and then eventually is absorbed into state a. Use this to explain
why
πi!aðÞ ¼ Pi!aðÞ þX
j2A0Pi!jðÞ πj!aðÞ ,
where A0denotes the set of non-absorbing states of the chain.
(b) The equation in (a) holds for all i2A0and all a2A. Show that this collection of equations
can be rewritten in matrix form as Π¼R+QΠ, and then solve for Π. (You may assume
the matrix I/C0Qis invertible.)
58. The matrix ( I/C0Q)/C01arises in several contexts in this section. This exercise provides an
interpretation of its entries. Consider a Markov chain with at least one absorbing state, and
assume that every non-absorbing state can access at least one absorbing state. As before, Aand
A0will denote the sets of absorbing and non-absorbing states, respectively.
(a) Consider any two non-absorbing states iandj. Let μijdenote the expected number of visits
to state j, starting in state i, before the chain is absorbed. (When j¼i, the initial state is
counted as one visit.) Mimicking the proof of the MTTA Theorem, show that
μii¼X
a2A1/C1Pi!aðÞ þX
k2A01þμki ðÞ /C1 Pi!kðÞ
¼1þX
k2A0μki/C1Pi!kðÞ
(b) Using similar reasoning, show that for i6¼j,
μij¼X
k2A0μkjPi!kðÞ
(c) Let Mbe the square matrix whose ( i,j)th entry is μij. Combine (a) and (b) to establish the
equation M¼I+QM, and solve for M.
6.6 Simulation of Markov chains
A typical Markov chain simulation requires two elements: the one-step transition matrix, P, and an
indication of the initial state X0(either as a ﬁxed state value or as a rv with a probability distribution).
The actual simulation of a single realization of the chain X0,X1,X2,...then amounts to repeated
selections from the transitional probability distributions speciﬁed by elements of P. Simulation of
Markov chains allow us to conﬁrm theoretical results and, more importantly, determine properties of
Markov chains that are not covered by the theorems of this chapter or other theoretical results.
The main step in any Markov chain simulation is to simulate a value for the next step, Xn+1, based
on the transition probabilities coming out of the current step Xn. Let’s start with the initial state X0.
Suppose for one particular run of the simulation, X0has been assigned the state i, either because that’s
the ﬁxed initial state or because a single draw from some initial distribution v0yielded i. Conditional472 6 Markov Chains
onX0¼i, the distribution of X1is determined by the transition probabilities P(i!j)f o rj¼1, 2, 3, ...,
which appear in the ith row of P. Thus, one needs to extract the ith row of Pand use it as the basis for a
single discrete simulation. If the result of this simulation is X1¼j, then the jth row of Pcan be
accessed to simulate X2, and so on.
Example 6.33 Let’s simulate a typical day in the life of our taxi driver. Although a real taxi driver
does not have the same number of fares each day, for purposes of this ﬁrst simulation we’ll assume
that he takes exactly 25 fares in 1 day.
Suppose ﬁrst that the driver begins each day in a random zone X0, as in Example 6.15, speciﬁcally
with the initial distribution p(1)¼.2,p(2)¼.5,p(3)¼.3. We begin by simulating a single value
from this initial distribution. Once that is determined, our program should then simulate a single value
ofX1using the row of Pcorresponding to the value of X0, then do the same for X2based on the
simulated value of X1, and so on. Figure 6.10 shows Matlab and R code for such a simulation.
In Matlab, P(current,:) calls for the row of Pspeciﬁed by the numerical index current ;
the code P[current,] performs the same task in R. The output of both of these programs is a
vector, X, containing the sequence of states for the Markov chain (beginning with X0). For example,
one run of the above program in R yielded the following output:
>X
[ 1 ]21312222111131331322223313
The randomly selected initial state was X0¼2, followed by X1¼1,X2¼3,..., and ﬁnally
X25¼3. (The symbol [1] at the left is not the initial state; this is just R’s way of denoting the
beginning of X.) If we weren’t interested in the initial state of the chain, the code could easily be
modiﬁed not to store X0, in which case the indices of the output vector would match the time indices
of the Markov chain (i.e., the subscripts on X1,X2,...,X25).
To make X0a ﬁxed initial state instead of a true random variable, one need simply replace the two
lines of code specifying the initial probability vector and the ﬁrst random selection. In the Matlab
code, the third and fourth lines could be replaced by the statement X¼3;to ﬁx the taxi driver’s initial
state as zone 3. A similar comment applies to the R code. And, again, one could choose whether or not
to store the initial state as part of the output vector. ■
It is important not to confuse the number of transitions of the chain with the number of runs of the
simulation . In Example 6.33, both programs simulate the chain through 25 transitions, but only aP=[.3 .2 .5; .1 .8 .1; .4 .4 .2];
states=[1 2 3];
v0=[.2 .5 .3];
X=randsample(states,1,true,v0);
current=X;
fori=1:25
nextstate=
randsample(states,1,true,P(current,:));
X=[X nextstate];
current=nextstate;
endP <- matrix(c(.3,.2,.5,.1,.8,.1,
.4,.4,.2),nrow=3,ncol=3,byrow=TRUE)
states <- c(1,2,3)
v0 <- c(.2,.5,.3)
X <- sample(states,1,TRUE,v0)
current <- X
for (i in 1:25){
nextstate <-
sample(states,1,TRUE,P[current,])
X <- c(X,nextstate)
current <- nextstate
}ab
Fig. 6.10 Code for Example 6.33: ( a) Matlab; ( b)R6.6 Simulation of Markov chains 473
single run. If it’s our desire to keep track of the chain’s behavior across many different runs,
analogous to the simulations described at the ends of Chaps. 1–4, then we must add an additional
layer of code, typically in the form of a surrounding “for” or “while” loop.
Example 6.34 As an illustration of the Steady-State Theorem, consider the model for Web users’
browser histories discussed in Example 6.19 (refer back to that example to see the one-step transition
matrix). Let’s simulate the distribution of X75, the Web site category of a user’s 75th visited page. For
variety’s sake, suppose users are equally likely to start surﬁng the Web in any one of the ﬁve Web site
categories; recall that the initial distribution of a regular chain will not affect its long-run behavior.
The programs displayed in Fig. 6.11 perform 10,000 runs of simulating this Markov chain up through
X75. Purely to save space, the code to create Phas been suppressed in Fig. 6.11, but it is very similar to
what appears in Fig. 6.10.
In the fourth line of code, we have employed a shortcut version of the randsample andsample
functions in Matlab and R, respectively, to randomly and uniformly select a single random integer
from 1 to 5 (this is the initial state). Both programs store the state of the Markov chain after
75 transitions in the vector named X75 for each of 10,000 runs. (Notice that the intermediate states
X1,...,X74are not permanently stored.)
The 10,000 simulated values of X75from one execution of the Matlab program are summarized in
the accompanying table, along with the steady-state probabilities for this chain determined in
Sect. 6.4.
j 12345
# of times 2822 1004 1599 3816 759
^PX 75¼j ðÞ .2822 .1004 .1599 .3816 .0759
πj .2840 .0948 .1659 .3791 .0758
The estimated and theoretical steady-state probabilities are quite similar. Remember that these two
rows of probabilities should differ slightly for tworeasons: ﬁrst, this is only a simulation of 10,000
values of the rv X75, so there is natural simulation error; second, the steady-state probabilities indicate
the behavior of Xnasn!1 , and we don’t expect the rv X75to have exactly this distribution
(although it should be close).
Section 6.6introduced the notions of time to absorption and mean time to absorption for Markov
chains with one or more absorbing states. We can also use simulation to explore properties of time-to-
absorption variables and ﬁrst-passage times.P=not shown ;
X75=zeros(10000,1);
fori=1:10000
current=randsample(5,1);
forj=1:75
nextstate=
randsample(1:5,1,true,P(current,:));
current=nextstate;
end
X75(i)=current;
endP <- not shown
X75 <- NULL
for (i in 1:10000){
current <- sample(5,1)
for (j in 1:75){
nextstate <-
sample(1:5,1,TRUE,P[current,])
current <- nextstate
}
X75[i] <- current
}ab
Fig. 6.11 Code for Example 6.34: ( a) Matlab; ( b)R ■474 6 Markov Chains
Example 6.35 Consider again the bank loan application process described in Example 6.25, with
lone absorbing state 6 (ultimate acceptance or rejection of the application), and the random variable
T1¼time to absorption from state 1 (document initiation). To simulate the distribution of T1, one
begins the chain in state 1 and continues to simulate its transitions until it arrives in state 6. The
simulation program now must keep track of how many transitions occur, rather than just where the
chain ends up. Figure 6.12 shows Matlab and R code for this purpose; again, to save space, the code
for entering the matrix Pis not included.
The simulated distribution of T1from one execution of the R code in Fig. 6.12b appears in
Fig. 6.13. These particular 10,000 simulated values had a sample mean of 4.508 and a sample
standard deviation of 2.276. Notice the sample mean is quite close to the theoretical expectation,
E(T1)¼4.5, determined in Sect. 6.5.
Clearly, the sample mean of the simulated T1values is a better estimate of E(T1) than the approach
utilizing the truncated pmf presented in the previous section. Of course, neither is strictly necessary
since the mean of T1can be found explicitly using the MTTA Theorem. The new information
provided by the simulation is a measure of the variability ofT1: we estimate the standard deviation
ofT1to be 2.276, whereas no simple matrix formula exists for its theoretical standard deviation. ■
The preceding examples employed simulations primarily to conﬁrm theoretical results established
in earlier sections. (Or, perhaps better put, our earlier theoretical results validate the simulations!) The
ﬁnal two examples of this section indicate situations where we must rely on simulation methods to
approximate values of desired quantities.P=not shown ;
T1=zeros(10000,1);
fori=1:10000
current=1;
steps=0;
whilecurrent~=6
steps=steps+1;
nextstate=
randsample(1:6,1,true,P(current,:));
current=nextstate;
end
T1(i)=steps;
endP <- not shown
T1 <- NULL
for (i in 1:10000){
current <- 1
steps <- 0
while (current!=6){
steps <- steps+1
nextstate <-
sample(1:6,1,TRUE,P[current,])
current <- nextstate
}
T1[i] <- steps
}ab
Fig. 6.12 Code for Example 6.35: ( a) Matlab; ( b)R6.6 Simulation of Markov chains 475
Example 6.36 Refer back to Example 6.13, which described Chinese cell phone users’ transitions
between three major carriers. Suppose users may renew or change contracts annually, and that annual
plans for the three carriers cost the following (in $US): 550 for China Telecom, 600 for China
Unicom, and 525 for China Mobile. Assume that, because of governmental regulations, these prices
will remain the same for the next 10 years. If last year the market shares of the three carriers were .4,
.2, and .4, respectively and all contracts are about to come up for renewal, what is the average amount
a Chinese cell phone customer will pay over the next decade?
We will employ a Markov chain simulation to model the behavior of customers’ carrier choices for
10 consecutive years. Critically, we must keep track of how much money a customer spends each
year—that is, our three states now have associated quantitative values. (This is a common instance
where simulation proves useful.) Let Y¼the total cost of ten 1-year calling plans for a Chinese cell
phone customer. Figure 6.14 shows code for simulating Yusing the techniques of this section.
An initial state x0is ﬁrst determined using the speciﬁed initial probability distribution (here,
v0¼[.4 .2 .4]). Then, ten steps of the Markov chain are simulated; each of these states is temporarily
held in nextstate . The vector AnnualCost stores the cost of a 1-year calling plan by calling the
appropriate element of the Prices vector. Once a 10-year chain has been simulated, those annual
costs are summed and stored as a simulated value of Y.7000
6000
5000
4000
3000FrequencyHistogram of T1
51 0 1 5
T1202000
1000
0Fig. 6.13 Simulation
distribution of T1in
Example 6.35476 6 Markov Chains
A histogram of the 10,000 simulated Yvalues appears in Fig. 6.15. Notice that the distribution of
Yhas three spikes, at $5250, $5500, and $6000. These correspond to customers who keep the same
carrier all 10 years; the large probabilities along the main diagonal of the transition matrix indicate
reasonably strong customer loyalty. For this particular run, the simulated values of Yhad a sample
mean and standard deviation of $5503.40 and $199.32, respectively, from which we can be 95%
conﬁdent, using the methods of Sect. 5.3, that μYis between $5499.49 and $5507.31.
Example 6.37 Our taxi driver now makes one last appearance (hopefully to applause). He starts each
morning at home in zone 3. Methods from Sects. 6.4and6.5allow us to determine the expected
number of fares required for him to return home, or to reach one of the other two zones. But how long
does it take him, in the typical day, to visit all three zones ? Let
Tall¼number of transitions required to visit every state at least once (not counting the initial state, X0)P=[.84 .06 .1;.08 .82 .1;.1 .04 .86];
Prices=[550 600 525];
Y=zeros(10000,1);
fori=1:10000
v0=[.4 .2 .4];
AnnualCost=zeros(10,1);
x0=randsample(1:3,1,true,v0);
current=x0;
forn=1:10
nextstate=
randsample(1:3,1,true,P(current,:));
AnnualCost(n)=
Prices(nextstate);
current=nextstate;
end
Y(i)=sum(AnnualCost);
endP <- matrix(c(.84,.06,.1,.08,.82,.1, 
.1,.04,.86),nrow=3,ncol=3,byrow=TRUE)
Prices <- c(550,600,525)
Y <- NULL
for (i in 1:10000){
v0 <- c(.4,.2,.4)
AnnualCost <- NULL
x0 <- sample(1:3,1,TRUE,v0)
current <- x0
for (n in 1:10){
nextstate <-
sample(1:3,1,TRUE,P[current,])
AnnualCost[n] <-
Prices[nextstate];
current <- nextstate
}
Y[i]=sum(AnnualCost)
}ab
Fig. 6.14 Code for Example 6.36: ( a) Matlab; ( b)R
5200 5300 5400 5500 5600 5700 5800 5900 6000 6100y 020040060080010001200Frequency
Fig. 6.15 Histogram of values of Yin Example 6.36 ■6.6 Simulation of Markov chains 477
To simulate Tall, our program must keep track of which states have been visited thus far. Once all
states/zones have been visited, the numerical value of Tallfor that simulation run can be recorded.
Figure 6.16 shows appropriate code.
In both programs, a vector called visits keeps a record of which states the chain visits within
that particular run. When chain jis visited ( j¼1, 2, 3), the jth entry of visits switches from 0 to
1. Once all three entries of visits equal 1, as detected by its sum, the while loop terminates and the
temporary count of transitions ( Talltemp ) is stored in Tall . The result of the program is 10,000
simulated values of the rv Tall, stored in the vector Tall .
Figure 6.17 displays a histogram of the 10,000 values resulting from running the Matlab program
in Fig. 6.16a. The sample mean and standard deviation of these 10,000 values were /C22x¼8:1674 and
s¼5.8423. Hence, we estimate the average number of fares required for the taxi driver to visit all
three zones to be 8.1674, with an estimated standard error of s=ﬃﬃﬃnp¼5:8423 =ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ10, 000p¼0:058423.
Using the techniques of Chap. 5, we can say with 95% conﬁdence that μall, the true mean of Tall, lies in
the intervalP =[.3 .2 .5;.1 .8 .1;.4,.4,.2];
Tall=zeros(10000,1);
fori=1:10000
current=3;
visits=[0 0 0];
Talltemp=0;
while(sum(visits)<3)
nextstate=randsample(1:3,1,
true,P(current,:));
visits(nextstate)=1;
current=nextstate;
Talltemp=Talltemp+1;
end
Tall(i)=Talltemp;
endP <- matrix(c(.3,.2,.5,.1,.8,.1, 
.4,.4,.2),nrow=3,ncol=3,byrow=TRUE)
Tall <- NULL
for (i in 1:10000){
current <- 3
visits <- c(0,0,0)
Talltemp <- 0
while (sum(visits)<3){
nextstate <-
sample(1:3,1,TRUE,P[current,])
visits[nextstate] <- 1
current <- nextstate
Talltemp <- Talltemp+1
}
Tall[i] <- Talltemp
}ab
Fig. 6.16 Code for Example 6.37: ( a) Matlab; ( b)R
0 1 02 03 04 05 06 0Tall 050010001500200025003000350040004500Frequency Fig. 6.17 Simulation
distribution of the rv Tallin
Example 6.37478 6 Markov Chains
/C22x/C61:96sﬃﬃﬃnp¼8:1674/C61:96 0:058423ðÞ ¼ 8:053;8:282 ðÞ
Among the 10,000 simulated values of Tall, 4204 were at most 5 (so, 3 or 4 or 5). Hence, the
estimated probability that the taxi driver visits all three zones within his ﬁrst ﬁve fares is
^p¼^PT all/C205 ðÞ ¼4204
10, 000¼:4204
The estimated standard error of this estimate is given byﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
^p1/C0^pðÞ =np
¼:0049. Hence we are
95% conﬁdent that the true probability P(Tall/C205) lies in .4204 /C61.96(.0049) ¼(.4108, .4300). ■
6.6.1 Exercises: Section 6.6(59–66)
59. Refer back to Exercise 3. Suppose this machine produces 150 units on days when it is fully
operational, 75 units per day when partially operational, and 0 units when broken. Consider a
month with 20 work days, and assume the machine ended the previous month fully operational.
(a) Write a simulation of the rv Y¼the number of units produced by this machine in
20 work days.
(b) Create a histogram of simulated values of Yfor at least 10,000 simulation runs.
(c) Construct a 95% conﬁdence interval for the mean number of units produced by this machine
across 20 work days.
(d) Construct a 95% conﬁdence interval for the probability that the machine produces at least
2000 units in such a month.
60. Four friends A, B, C, and D are notorious for sharing rumors amongst themselves. Being very
gossipy but not particularly bright, each friend is equally likely to share a rumor with any of the
other three friends, even if that friend has already heard it. (For example, if friend B most recently
heard the rumor, each of friends A, C, and D is equally likely to hear it next, regardless of how B
came to hear the rumor!) Let Xn¼thenth person in this foursome to hear a particular rumor.
(a) Construct the one-step transition matrix for this Markov chain.
(b) Friend A has just overheard a particularly nasty rumor about a classmate and is eager to
share it with the other three friends. Let Tequal the number of times the rumor is repeated
within the foursome until all of them have heard the rumor. Write a program to simulate T,
and use your program to estimate E(T).
61. A state lottery ofﬁcial has proposed the following system for a new game. In the ﬁrst week of a
new year, a $10 million prize is available. If nobody gets the winning lottery numbers correct and
wins the prize that week, the value doubles to $20 million for the second week; otherwise, the
prize for the second week is also $10 million. Each week, the prize value doubles if nobody wins
it and returns to $10 million otherwise. Suppose that there is a 40% chance that someone in the
state wins the lottery prize each week, irrespective of the current value of the prize. Let Xn¼the
value of the lottery prize in the nth week of the year.
(a) Determine the one-step transition probabilities for this chain. [ Hint: Given the value of Xn,
Xn+1can only be one of two possible dollar amounts.]
(b) Let Mbe the maximum value the lottery prize achieves over the course of a 52-week year.
Simulate at least 10,000 values of the rv M, and report the sample mean and SD of these
simulated values. [ Hint: Given the large state space of this Markov chain, don’t attempt to
construct the transition matrix. Instead, code the probabilities in (a) directly.]6.6 Simulation of Markov chains 479
(c) Let Ybe the total amount paid out by the lottery in a 52-week year. Simulate at least 10,000
values of the rv Y, and report a 95% conﬁdence interval for E(Y).
(d) Repeat (c), but now assume the probability of a winner is .7 each week rather than .4. Should
the lottery commission make it easier or harder for someone to win each week? Explain.
62. Write a Markov chain simulation program with the following speciﬁcations. The inputs should be
the transition matrix P, an initial state x0, and the number of steps n. The output should be a single
realization of X1,X2,...,Xn, as either a row vector or a column vector.
63. Refer back to Exercise 12. Suppose that the typical annual premium for a category 1 (safest)
customer is $500; for category 2, $600; for category 3, $1000; and for category 4 (riskiest driver),
$1500.
(a) Use a Markov chain simulation to estimate the distribution of the rv Y1¼total premium
paid by a customer over 10 years with the insurance company, assuming s/he starts in
category 1. Create a histogram of values for Y1, and construct a 95% conﬁdence interval for
E(Y1).
(b) Repeat (a) assuming instead that the customer starts as a category 3 driver.
64. Write a simulation program for Gambler’s Ruin. The inputs should be a¼Allan’s initial stake,
b¼Beth’s initial stake (so a+bis the total stake), p¼the probability Allan defeats Beth in any
single game, and N¼the number of tournaments to be simulated. The program should output
twoN-by-1 vectors: one recording the number of games played for each of the Nruns, and one
indicating who won each time. Use your program to determine (a) the average tournament length
and (b) the probability Allan eventually wins for the settings a¼b¼$5 and p¼.4. Give 95%
conﬁdence intervals for both answers.
65. Example 6.3 describes a (one-dimensional) random walk. This is sometimes called a simple
random walk .
(a) Write a program to simulate the ﬁrst 100 steps of a random walk starting at X0¼0. [Hint:I f
Xn¼s, then Xn+1¼s/C61 with probability 1/2 each.]
(b) Run your program in (a) 10,000 times, and use the results to estimate the probability that a
random walk returns to its origin at any time within the ﬁrst 100 steps.
(c) Let R0¼the number of returns to the origin in the ﬁrst 100 steps of the random walk, not
counting its initial state. Use your simulation to (1) create a histogram of simulated values of
R0and (2) construct a 95% conﬁdence interval for E(R0).
66. A two-dimensional random walk is a model for movement along the integer lattice in the xy-
plane, i.e., points ( x,y) where xandyare both integers. The “walk” begins at X0¼(0, 0). At each
time step, a move is made one unit left or right (probability 1/2 each) and, independently, one unit
up or down (also equally likely). If Xn¼the (x,y)-coordinates of the chain after nsteps, then Xn
is a Markov chain.
(a) Write a program to simulate the ﬁrst 100 steps of a two-dimensional random walk. [ Hint:
Thex- and y-coordinates of a two-dimensional random walk are each simple random walks.
Since they are independent, the x- and y-movements can be simulated separately.]
(b) Use your program in (a) to estimate the probability that a two-dimensional random walk
returns to its origin within the ﬁrst 100 steps. Use at least 10,000 runs.
(c) Use your program in (a) to estimate E(R0), where R0¼the number of times the walk returns
to (0, 0) in the ﬁrst 100 steps.480 6 Markov Chains
6.7 Supplementary Exercises (67–82)
67. A hamster is placed into the six-chambered circular habitat shown in the accompanying ﬁgure.
Sitting in any chamber, the hamster is equally likely to next visit either of the two adjacent
chambers. Let Xn¼thenth chamber visited by the hamster.
1 2
3
4 56
(a) Construct the one-step transition matrix for this Markov chain.
(b) Is this a regular Markov chain?
(c) Intuitively, what should the stationary probabilities of this chain be? Verify these are indeed
its stationary probabilities.
(d) Given that the hamster is currently in chamber 3, what is the expected number of transitions
it will make until it returns to chamber 3?
(e) Given that the hamster is currently in chamber 3, what is the expected number of transitions
it will make until it arrives in chamber 6?
68. Teenager Mike wants to borrow the car. He can ask either parent for permission to take the car. If
he asks his mom, there is a 20% chance she will say “yes,” a 30% chance she will say “no,” and a
50% chance she will say, “ask your father.” Similarly, the chances of hearing “yes”/“no”/“ask
your mother” from his dad are .1, .2, and .7, respectively. Imagine Mike’s efforts can be modeled
as a Markov chain with states (1) talk to Mom, (2) talk to Dad, (3) get the car (“yes”), (4) strike
out (“no”). Assume that once either parent has said “yes” or “no,” Mike’s begging is done.
(a) Construct the one-step transition matrix for this Markov chain.
(b) Identify the absorbing state(s) of the chain.
(c) Determine the mean times to absorption.
(d) Determine the probability that Mike will eventually get the car if (1) he asks Mom ﬁrst and
(2) he asks Dad ﬁrst. Whom should he ask ﬁrst?
69. Refer back to Exercise 14. Suppose Lucas starts in room 1 and proceeds as described in that
exercise; however, his mean-spirited uncle has snuck out of the house entirely, leaving Lucas to
search interminably. So, in particular, if Lucas enters room 6 of the house, his next visit will
necessarily be to room 5. (This really happened one summer!)
(a) Determine the transition matrix for this chain.
(b) Verify that this Markov chain is regular.
(c) Determine the steady-state probabilities of this chain.
(d) What proportion of time in the long run does Lucas spend in room 2?
(e) What is the average number of room transitions between Lucas’ visits to room 1?
70. Refer back to Exercises 20 and 21.
(a) Suppose all four vans were operational as of Monday morning. What is the expected
backlog—that is, the expected number of vans needing repair—as of Friday evening?
(b) Suppose instead that two of the four vans were down for repairs Monday morning. Now
what is the expected backlog as of Friday evening?6.7 Supplementary Exercises (67–82) 481
71. Five Mercedes E550 vehicles are shipped to a local dealership. The dealer sells one E550 in any
week with probability .3 and otherwise sells none in that week. When all E550s in stock have
been sold, the dealer requests a new shipment of ﬁve such cars, and it takes 1 week for that
delivery to occur. Let Xn¼the number of Mercedes E550s at this dealership nweeks after the
initial delivery of ﬁve cars.
(a) Construct the transition matrix for this chain. [ Hint: The states are 0, 1, 2, 3, 4, 5.]
(b) Determine the steady-state probabilities for this chain.
(c) On the average, how many weeks separate successive orders of ﬁve E550s?
72. Refer back to the previous exercise. Let m¼the number of Mercedes E550s delivered to the
dealership at one time (both initially and subsequently), and let p¼the probability an E550 is
sold in any particular week ( m¼5 and p¼.3 in the previous exercise). Determine the steady-
state probabilities for this chain and then the average number of weeks between vehicle orders.
73. Sports teams can have long streaks of winning (or losing) seasons, but occasionally a team’s
fortunes change quickly. Suppose that each team in the population of all college football teams
can be classiﬁed as (1) weak, (2) medium, or (3) strong, and that the following one-step transition
probabilities apply to the Markov chain Xn¼a team’s strength nseasons from now:
P¼:8:20
:2:6:2
:1:2:72
43
5
(a) If a college football team is weak this season, what is the minimum number of seasons
required for it to become strong?
(b) If a team is strong this season, what is the probability it will also be strong four seasons from
now?
(c) What is the average number of seasons that must pass for a weak team to become a strong
team?
(d) What is the average number of seasons that must pass for a strong team to become a weak
team?
74. Jay and Carol enjoy playing tennis against each other. Suppose we begin watching them when
they are at deuce . This means the next person to win a point earns advantage . If that same person
scores the next point, then s/he wins the game; otherwise, the game returns to deuce.
(a) Construct a transition matrix to describe the status of the game after npoints have been
scored (starting at deuce). [ Hint: There are ﬁve states: (1) Jay wins, (2) advantage Jay,
(3) deuce, (4) advantage Carol, (5) Carol wins.]
(b) Suppose Carol is somewhat better than Jay and has a 60% chance of winning any particular
point. Determine (1) the probability Carol eventually wins and (2) the expected number of
points to be played, starting at deuce. [ Hint: This should bear surprising similarity to a game
played earlier in the chapter by Allan and Beth!]
75. The authors of the article “Pavement Performance Modeling Using Markov Chain” ( Proc.
ISEUSAM , 2012: 619–627) developed a system for classifying pavement segments into ﬁve
categories: (1) Very good, (2) Good, (3) Fair, (4) Bad, and (5) Very bad. Analysis of pavement
samples led to the construction of the following transition matrix for the Markov chain Xn¼
pavement condition nyears from now:482 6 Markov Chains
:958 :042 0 0 0
0 :625 :375 0 0
00 :797 :203 0
000 :766 :234
000012
666643
77775
Notice that a pavement segment either maintains its condition or goes down by one category each
year.
(a) The evaluation of one particular stretch of road led to the following initial probability vector
(what the authors call a “condition matrix”): [.3307 .2677 .2205 .1260 .0551]. Use the
Markov chain model to determine the condition matrix of this same road section 1, 2, and
3 years from now.
(b) “Very bad” road segments require repairs before they are again usable; the authors’ model
applies to unrepaired road. What is the average time (number of years) that a very good road
can be used before it degrades into very bad condition? Make the same determination for
good, fair, and bad roads.
(c) Suppose one road segment is randomly selected from the area to which the condition matrix
in (a) applies. What is the expected amount of time until this road segment becomes very
bad? [ Hint: Use the results of part (b).]
76. A constructive memory agent (CMA) is an autonomous software unit that uses its interactions not
only to change its data (“memory”) but also its fundamental indexing systems for that data
(“structure”). The article “Understanding Behaviors of a Constructive Memory Agent: A Markov
Chain Analysis” ( Knowledge -Based Systems , 2009: 610–621) describes a study of one such CMA
as it moved between nine different stages of learning. (Stage 1 is sensation and perception; later
stages add on other behaviors such as hypothesizing, neural network activation, and validation.
Consult the article for details.) The accompanying state diagram mirrors the one given in the
article for the authors’ ﬁrst experiment.
1
6 2
3
4
5 78.45
.45
.47.43
.07.121
1
111
91.03
.88.03.076.7 Supplementary Exercises (67–82) 483
(a) Construct the transition matrix for this chain.
(b) What are the absorbing states of the chain?
(c) All CMA processes begin in stage 1. What is the mean time to absorption for such a process?
Here, “time” refers to the number of transitions from one learning stage to another. [ Note:I n
this particular experiment, absorbing states correspond to any instance of so-called “induc-
tive” learning.]
(d) Starting in stage 1, what is the probability a CMA will end the experiment in state
8 (constructive learning plus inductive learning)?
77. The authors of the article “Stationarity of the Transition Probabilities in the Markov Chain
Formulation of Owner Payments on Projects” ( ANZIAM J ., v. 53, 2012: C69-C89) studied
payment delays in road construction in Australia. States for any payment were deﬁned as follows:
kweeks late for k¼0, 1, 2, 3; paid (pd), an absorbing state; and “to be resolved” (tbr), meaning
the payment was at least 1 month late, which the authors treated as another absorbing state. For
one particular project, the following QandRmatrices were given for the canonical form of the
one-step transition matrix:
Q¼0
1
2
301 0 0
00 :959 0
00 0 :897
00 0 02
6643
775R¼pd tbr
0
1
2
300
:041 0
:013 0
:804 :1962
6643
775
(a) Construct the complete 6 /C26 transition matrix Pfor this Markov chain.
(b) Draw the state diagram of this Markov chain.
(c) Determine the mean time to absorption for payment that is about to come due (i.e., one that
is presently 0 weeks late).
(d) What is the probability a payment is eventually made, as opposed to being classiﬁed as “to
be resolved”?
(e) Consider the two probabilities P(0!1) and P(3!pd). What is odd about each of these
values? (The authors of the article offer no explanation for the irregularity of these two
particular probabilities.)
78. In a nonhomogeneous Markov chain, the conditional distribution of Xn+1depends on both the
previous state Xnand the current time index n. As an example, consider the following method for
randomly assigning subjects one at a time to either of two treatment groups, A or B. If npatients
have been assigned a group so far, and aof them have been assigned to treatment A, the
probability the next patient is assigned to treatment group A is
Pnþ1ðÞ st patient assigned to A/C12/C12aout of first ninA/C0/C1
¼n/C0aþ1
nþ2
Hence, the ﬁrst patient is assigned to A with probability (0 /C00 + 1)/(0 + 2) ¼1/2; if the ﬁrst
patient was assigned to A, then the second patient is also assigned to A with probability
(1/C01 + 1)/(1 + 2) ¼1/3. This assignment protocol ensures that each next patient is more
likely to be assigned to the smaller group. Let Xn¼the number of patients in treatment group
A after ntotal patients have been assigned ( X0¼0). To simplify matters, assume there are only
4 patients in total to be randomly assigned.
(a) Let P1denote the transition matrix from n¼0t on¼1. Assume the state space of the chain
is {0, 1, 2, 3, 4}. Construct P1.[Hint: Since X0¼0, only the ﬁrst row of P1is really relevant.484 6 Markov Chains
To make this a valid transition matrix, treat the “impossible” states 1, 2, 3, and 4 as
absorbing states.]
(b) Construct P2, the transition matrix from n¼1t o n¼2. Use the same hint as above for
states 2, 3, and 4, which are impossible at time n¼1.
(c) Following the pattern of (a) and (b), construct the matrices P3andP4.
(d) For a nonhomogeneous chain, the multistep transition probabilities can be calculated by
multiplying the aforementioned matrices from left to right, e.g., the 4-step transition matrix
for this chain is P1P2P3P4. Calculate this matrix, and then use its ﬁrst row to determine the
likelihoods of 0, 1, 2, 3, and 4 patients being randomly assigned to treatment group A using
this method.
[Note: Random assignment strategies of this type were originally investigated in the article
“Forcing a Sequential Experiment to be Balanced,” Biometrika (1971): 403-417.]
79. A communication channel consists of ﬁve relays through which all messages must pass. Suppose
that bit switching errors of either kind (0 to 1, or 1 to 0) occur with probability .02 at the ﬁrst
relay. The corresponding probabilities for the other four relays are .03, .02, .01, and .01,
respectively. If we deﬁne Xn¼the parity of a bit after traversing the nth relay, then X0,X1,
...,X5forms a nonhomogeneous Markov chain.
(a) Determine the one-step transition matrices P1,P2,P3,P4, and P5.
(b) What is the probability that a 0 bit entering the communication relay system also exits as a
0 bit? [ Hint: Refer back to the previous exercise for information on nonhomogeneous
Markov chains.]
80. Consider the two-state Markov chain described in Exercise 39, whose one-step transition matrix
is given by
0
11/C0αα
β 1/C0β/C20/C21
for some 0 <α,β<1. Use mathematical induction to show that the k-step transition
probabilities are given by
PkðÞ0!0 ðÞ ¼ δkþ/C0
1/C0π/C1/C0
1/C0δk/C1
PkðÞ0!1 ðÞ ¼ π/C0
1/C0δk/C1
PkðÞ1!0 ðÞ ¼/C0
1/C0π/C1/C0
1/C0δk/C1
PkðÞ1!1 ðÞ ¼ δkþπ/C0
1/C0δk/C1
where π¼α=ðαþβÞandδ¼1/C0α/C0β.[Note: Applications of these multistep probabilities
are discussed in “Epigenetic Inheritance and the Missing Heritability Problem,” Genetics , July
2009: 845-850.]
81. A 2012 report (“A Markov Chain Model of Land Use Change in the Twin Cities, 1958-2005,”
available online) provided a detailed analysis from maps of Minneapolis-St. Paul, MN over the
past half-century. The Twin Cities area was divided into 610,988 “cells,” and each cell was
classiﬁed into one of ten categories: (1) airports, (2) commercial, (3) highway, (4) industrial,
(5) parks, (6) public, (7) railroads, (8) residential, (9) vacant, (10) water. The report’s authors
found that Xn¼classiﬁcation of a randomly selected cell was well modeled by a time-
homogeneous Markov chain when a time increment of about 8 years is employed. The
accompanying matrix shows the one-step transition probabilities from 1997 ( n) to 2005
(n+ 1); rows and columns are in the same order as the sequence of states described above.6.7 Supplementary Exercises (67–82) 485
:7388 :0010 :0068 :0010 :0325 :0131 :0000 :0055 :1984 :0029
:0001 :8186 :0201 :0560 :0045 :0227 :0002 :0413 :0350 :0015
:0004 :0107 :9544 :0054 :0058 :0031 :0002 :0094 :0105 :0001
:0004 :0710 :0099 :8371 :0082 :0086 :0011 :0106 :0517 :0014
:0022 :0036 :0031 :0025 :9128 :0062 :0002 :0116 :0364 :0214
:0001 :0193 :0100 :0384 :0569 :7364 :0004 :0223 :1091 :0071
:0000 :0065 :0142 :0201 :0110 :0032 :9139 :0168 :0130 :0013
:0000 :0024 :0024 :0009 :0041 :0023 :0002 :9634 :0230 :0013
:0004 :0141 :0099 :0156 :0513 :0057 :0002 :0988 :7920 :0120
:0001 :0010 :0003 :0014 :0136 :0001 :0000 :0055 :0096 :96842
6666666666666643
777777777777775
(a) In 2005, the distribution of cell categories (out of the 610,988 total cells) was as follows:
[4047 20,296 16,635 24,503 74,251 18,820 1505 195,934 200,837 54,160]
The order of the counts matches the category order above, e.g., 4047 cells were part of
airports and 54,160 cells were located on water. Use the transition probabilities to predict
the land use distribution of the Twin Cities region in 2013.
(b) Determine the predicted land use distribution for the years 2021 and 2029 (remember, each
time step of the Markov chain is 8 years). Then determine the percent change from 1995 to
2029 in each of the ten categories (similar computations were made in the cited report).
(c) Though it’s unlikely that land use evolution will remain the same forever, imagine that the
one-step probabilities can be applied in perpetuity. What is the projected long-run land use
distribution in Minneapolis-St. Paul?
82. In the article “Reaching a Consensus” ( J. Amer. Stat. Assoc ., 1974: 118-121), Morris DeGroot
considers the following situation: sstatisticians must reach an agreement about an unknown
population distribution, F. (The same method, he argues, could be applied to opinions about the
numerical value of a parameter, as well as many nonstatistical scenarios.) Let F10,...,Fs0
represent their initial opinions. Each statistician then revises his belief about Fas follows: the ith
individual assigns a “weight” pijto the opinion of the jth statistician ( j¼1,...s), where pij/C210
andpi1+/C1/C1/C1+pis¼1. He then updates his own belief about Fto
Fi1¼pi1F10þ/C1/C1/C1þ pisFs0
This updating is performed simultaneously by all sstatisticians (so, i¼1, 2, ...,s).
(a) Let F0¼(F10,...,Fs0)T, and let Pbe the s/C2smatrix with entries pij. Show that the vector
of updated opinions F1¼(F11,...,Fs1)Tis given by F1¼PF0.
(b) DeGroot assumes that updates to the statisticians’ beliefs continue iteratively, but that the
weights do not change over time (so, Premains the same). Let Fndenote the vector of
opinions after nupdates. Show that Fn¼PnF0.
(c) The group is said to reach a consensus if the limit of Fnexists as n!1 andeach entry of
that limit vector is the same (so all individuals’ opinions converge toward the same belief).
What would be a sufﬁcient condition on the weights in Pfor the group to reach a consensus?
(d) DeGroot speciﬁcally considers four possible weight matrices:486 6 Markov Chains
PA¼1
21
2
1
43
42
6643
775PB¼1
21
20
1
43
40
1
31
31
32
66666643
7777775PC¼10
01"#
PD¼1
21
200
1
21
200
001
21
2
001
21
22
666666666643
77777777775
Discuss what each one indicates about the statisticians’ views on each other, and determine for
which matrices the group ultimately reaches a consensus. If a consensus is reached, write out the
consensus “answer” as a linear combination of F10,...,Fs0.6.7 Supplementary Exercises (67–82) 487
Random Processes7
In Chap. 1, we introduced the concept of a random event: a collection of one or more outcomes
resulting from a random experiment (e.g., a randomly selected device works for 1000 h, or a
randomly selected person has brown hair). In Chaps. 2–4, we studied random variables: numerical
values resulting from random experiments (the number of ﬂaws on a randomly selected wafer, the
number of wins in 5 games of chance, the mass of a randomly selected object). In this chapter, we
look at random processes , also called stochastic processes (“stochastic” is a synonym for “random”):
time-dependent functions resulting from random phenomena.
For example, consider modeling the number of people logged into a particular server over the
course of the day. Since the exact times at which individuals log in are generally unpredictable, we
might reasonably apply a model which treats logins as “random.” In particular, at any speciﬁc, ﬁxed
point in time—say, noon—we can model the number of people logged in by an appropriate (discrete)
random variable. The new concept in Chap. 7is to model the evolution of that random count over time.
This gives us two dimensions of interest: the random variable itself (here, the count of logins) and time.
Among the most common applications of random processes in engineering is that of random noise ,
a term for the disparity between what a received signal should “ideally” look like and what actually
arrives at the receiver. Our ability to accurately model this distortion or noise will enable us to ﬁlter
out some (hopefully large) proportion of that noise, thereby recovering a cleaner signal.
In Sect. 7.1, we look at classiﬁcations of random processes according to whether the variable
dimension and/or the time dimension are modeled as discrete or continuous. In Sect. 7.2, we connect
previous ideas of mean, standard deviation, and so on to this new world of random processes. Section 7.3
introduces the concept of a stationary random process and the special class of wide-sense stationary
processes ; these will be the backbone of signal processing in Chap. 8(available online). Sections 7.4–
7.7consider several speciﬁc classes of random processes: discrete-time, Poisson, Gaussian, and
continuous-time Markov.
7.1 Types of Random Processes
In Chap. 2, we deﬁned a random variable as a rule that associates a number with each outcome in the
sample space of some experiment. For example, we may associate with each outcome of the
experiment of rolling two dice an integer Xbetween 2 and 12 indicating the sum of the two
up-facing sides. Any single realization of this experiment results in a speciﬁc number, a sample
value of X. We deﬁne random processes analogously.
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_7489
DEFINITION
For a given sample space Sof some experiment, a random process is any rule that associates a
time-dependent function with each outcome in S. Any such function that may result is a sample
function of the random process. The collection of all possible sample functions is called the
ensemble of the random process.
Figure 7.1illustrates this deﬁnition. Analogous to our notation for random variables, we will
denote a (continuous-time) random process by X(t), while the lower-case x(t) indicates a particular
sample function.
Example 7.1 Some communication systems use phase-shift keying to transmit information.
A quaternary phase-shift keying (QPSK) system can transmit four distinct symbols (often used to
encode two bits at a time: 00, 01, 10, 11). The four symbols are distinguished by varying the phase at
which they are transmitted; speciﬁcally, for k¼1, 2, 3, 4, the kth symbol is transmitted for Tseconds
with the wave
xktðÞ¼ cos 2 πf0tþπ=4þkπ=2 ðÞ ,0/C20t/C20T ð7:1Þ
for some predetermined frequency f0. If we consider the transmission of a single randomly selected
symbol, we may let X(t) denote the corresponding transmitted wave. Each function xk(t) in Expression
(7.1) is a sample function; the set of these four functions comprises the ensemble of X(t) and is
displayed in Fig. 7.2for 0/C20t/C204.tFig. 7.1 A random
process
−101
tx(t)
Fig. 7.2 Ensemble of the QPSK process in Example 7.1 ■490 7 Random Processes
Example 7.2 Imagine the ﬂuctuation in the value of Apple Inc. stock (symbol: AAPL) during the
next 8-h trading day, measuring time from the opening bell on Wall Street. Since that ﬂuctuation
cannot be predicted precisely, we may reasonably model the stock’s value by an appropriate random
process X(t); the ensemble of X(t) would be subject to the constraint X(0)¼yesterday’s closing
value. Two examples of possible sample functions appear in Fig. 7.3, where we have assumed a
previous day’s closing value of $580. The ensemble of X(t) consists of all possible paths that the price
of Apple stock could hypothetically take tomorrow, starting at $580 per share. Economists and
statisticians use a variety of time series models to forecast the behaviors of such random processes.
Example 7.3 Consider modeling the number of people N(t) logged in to a speciﬁc server at time
t(perhaps measured from midnight). Since logins and logouts are unpredictable, we might reasonably
apply a random process model to N(t). Figure 7.4shows one possible sample function; notice that,
since our variable is integer-valued, the function “jumps” rather than varying continuously. In this
context, the ensemble of N(t) consists of all nonnegative integer-valued functions n(t) that might
hypothetically arise from successive logins and logouts.0 1 2 3 4 5 6 78540550560570580590600610620x(t)
t
Fig. 7.3 Two sample functions for a stock price’s ﬂuctuation ■7.1 Types of Random Processes 491
Example 7.4 A dust particle lands on the surface of the water in a glass. For simplicity’s sake,
consider observing the motion of the particle only in the vertical direction (relative to our orientation)
as time progresses. If we deﬁne the particle’s initial position as 0, then we have a random process
Y(t)¼the vertical position of the particle tseconds after landing on the water.
Figure 7.5a shows one possible sample function for this particle motion, while Fig. 7.5b shows
100 different sample functions and thus approximates the ensemble of Y(t). Notice that, as tincreases,
the particle has greater potential to be farther away from its origin, the line y¼0, since the particle
has had more time to move. However, the particle will naturally “wiggle,” and so a typical sample
function will return to its origin multiple times, rather than “ﬂying off” away from 0.
This is an example of Brownian motion , a model physicists regularly use for the seemingly random
motion of electrons and various microscopic particles. Brownian motion, in turn, is an example of a
Gaussian process ; we will study Gaussian processes (in particular, Brownian motion) in Sect. 7.6.0 0.5 1 1.5 2 2.5 3 3.5 40123
tx(t)
Fig. 7.4 A sample function for the random process of Example 7.3 ■
0 1 2 3 4 5 6 7 8 91 0−8−7−6−5−4−3−2−10a b
0y(t) y(t)
tt
Fig. 7.5 Brownian motion: ( a) a single sample function, ( b) 100 sample functions ■492 7 Random Processes
7.1.1 Classification of Processes
As mentioned in the introduction to this chapter, we can classify random processes according to
whether the variable and time dimensions are modeled as discrete or continuous. We call X(t)a
discrete-space process if its set of possible values at any time tis ﬁnite or countably inﬁnite.
Otherwise, X(t)i sa continuous-space process.
Example 7.1 and Example 7.2 illustrate continuous-space processes, since the variables
(height of the sinusoid, value of the stock) may take on a continuum of values. In contrast, we have
a discrete-space process in Example 7.3, since the only possible values of N(t) are the countable set
{0, 1, 2, ...}. These classiﬁcations are consistent with our usage of the terms discrete andcontinuous
in Chaps. 2and3.
The difference between discrete- and continuous-space processes is less important than
distinguishing how we model time. All of our above examples are continuous-time processes ,
because time is measured on a continuous scale, typically [0, 1) or [0, T] for some ﬁxed T.I n
contrast, imagine recording the value of Apple stock at the end of each day (or the number of people
logged into a server at the end of each hour). Treating the variable as random, we would have a
sequence X1,X2,X3,..., where Xdenotes the value of the variable and the index ncorresponds to the
nth instance of measuring the process. The listing X1,X2,..., or more simply Xn,i sa discrete-time
random process , also called a random sequence. We already saw a special type of random
sequence, Markov chains, in Chap. 6; we will consider general discrete-time processes more carefully
in Sect. 7.4. Throughout the rest of this chapter as well as Chap. 8(available online), the term
“random process” will always refer to a continuous-time process unless indicated otherwise.
7.1.2 Random Processes Regarded as Random Variables
In most of the ﬁgures in this section, you will notice we have displayed time, t, on the horizontal axis,
while the “random” behavior is illustrated in the vertical direction. You may ﬁnd it helpful to think of
these as the “time direction” and “random direction,” respectively. To model a random process, we
must truly understand its behavior in the “random direction.” Toward that understanding, consider
Fig. 7.5b, which shows the ensemble of a Brownian motion process. Fix a time point—say, t¼1.
Looking in the vertical direction, we have a collection of “heights” corresponding to the numerical
values of the many sample functions y(t) displayed in the ﬁgure evaluated at t¼1. These many
values of y(1) form a probability distribution in the vertical direction: they show possible values of Y
(1), and the underlying random experiment that generated these sample functions determines the
relative likelihoods of those values. It is in that sense that the vertical axis of our graphs is the
“random direction.”
More simply (and perhaps more usefully) put, we make the following observation: At any ﬁxed
time point t0, the ensemble of a random process X(t) forms a probability distribution; that is, X(t0)i sa
random variable .
Example 7.5 An intended signal may have the form v0+acos(ω0t+θ0), but amplitude variation
may occur (due to natural current or voltage variation). We can deﬁne a random process by
XtðÞ¼ v0þAcosω0tþθ0 ðÞ
where Ais a random variable whose distribution describes the amplitude variation. Figure 7.6
illustrates part of the ensemble of X(t) when the model for amplitude variation is a uniform7.1 Types of Random Processes 493
distribution on [ /C01, 1], for the speciﬁcations v0¼0,ω0¼2π, and θ0¼0. That is, X(t)¼Acos(2πt)
with A~ Unif[ /C01, 1].
At time t¼0 (the far left edge of the graph), we may write X(0)¼Acos(2π0)¼Acos(0) ¼A.
Since A~ Unif[ /C01, 1] and X(0)¼A, clearly X(0) ~ Unif[ /C01, 1]. That matches what our eyes see in
the graph at t¼0: the values in the vertical direction seem “evenly” distributed on [ /C01, 1]. This same
distribution can be seen at t¼0.5, 1, 1.5, 2, ....
In contrast, X(1/3) ¼Acos(2π/3)¼/C0.5A~ Unif[ /C0.5,.5]. This, too, is visible in the graph: at
t¼1/3/C25.33, the vertical expanse of the graph is not from /C01 to 1 but rather from /C0.5 to .5.
Finally, at t¼1.75 we have X(1.75) ¼Acos(7π/2)¼A(0)/C170, i.e., X(1.75) equals 0 with prob-
ability 1 (i.e., for every member of the ensemble). We see in the graph that all functions of the form
x(t)¼acos(2πt) indeed equal 0 at t¼1.75 (as well as at t¼0.25, 0.75, and 1.25). ■
Example 7.6 (Example 7.4 continued) A Brownian motion process Y(t) is partially characterized by
the fact that, at any time t,Y(t) has a Gaussian (i.e., normal) distribution with mean 0 and variance αt,
for some constant α. In Fig. 7.5, we used the parameter α¼1 to generate the graph. Thus, in Fig. 7.5b
the probability distribution displayed in the vertical direction at time t¼1 is Gaussian with a mean
of 0 and a variance of (1)(1) ¼1, i.e., a standard normal distribution. In contrast, looking at
time t¼9,Y(9) is also Gaussian with mean zero, but with standard deviation equal toﬃﬃﬃﬃαtp¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1ðÞ9ðÞp
¼3, i.e., Y(9) ~ N(0, 3). The increase in the variability of the ensemble as tincreases
is apparent in Fig. 7.5b. The Gaussian nature of the model is reﬂected by the fact that we see a greater
concentration of values nearer the y¼0 line and a sparser set of values far from that midline. ■
In the previous two examples, we have focused on the probability distribution of X(t) at a single
ﬁxed time point, t. In fact, a random process is characterized by its simultaneous behavior at alltime
points. To be precise, a random process X(t) is characterized only if we know the joint distribution of
X(t1),...,X(tr) for all sets of time points t1< ...<trandr¼1, 2, 3, 4, .... The “joint behavior” of a
random process, particularly at two points in time, will be explored in depth in the next section.
7.1.3 Exercises: Section 7.1(1–10)
1. Classify each of the following processes as discrete-time or continuous-time, and discrete-space
or continuous-space.-10
121ab
-101x(t) x(t)
t
12t
Fig. 7.6 The ensemble of X(t)¼Acos(2πt): (a) three sample functions; ( b) hundreds of sample functions494 7 Random Processes
(a) The temperature in downtown Chicago throughout a day
(b) The number of customers in line at a certain store throughout the day
(c) The high temperature in downtown Chicago for each day in a year
(d) The total number of customers served each day at a certain store
2. Classify each of the following processes as discrete-time or continuous-time, and discrete-space
or continuous-space.
(a) The baud rate of a modem, recorded every 60 s
(b) The number of people logged into Facebook throughout the day
(c) The operational state, denoted 1 or 0, of a certain machine recorded at the end of each hour
(d) The noise (in dB) in an audio signal measured throughout transmission
3. For each of the processes in Exercise 1, sketch two possible sample functions.
4. For each of the processes in Exercise 2, sketch two possible sample functions.
5. Consider the server login scenario of Example 7.3. Assuming N(0)¼0, sketch sample functions
forN(t) in each of the following cases:
(a) The login rate exceeds the logout rate.
(b) The logout rate exceeds the login rate.
(c) The login and logout rates are equal.
6.Correlated bit noise . Let Xnbe a sequence of random bits (0s and 1s) constructed as follows:
X0¼0 or 1 with probability .5 each. For integers n/C211,Xn¼Xn/C01with probability .9 and
Xn¼1/C0Xn/C01with probability .1.
(a) Write out and sketch two examples of possible sample functions of Xnforn¼0, ..., 10.
(b) Which sample function is more likely to be observed: 01100101010, or 00011110000?
Explain.
(c) Find the distribution of Xnat time n¼1.
7.Binary phase-shift keying (BPSK) is a simpliﬁed version of the QPSK system described in
Example 7.1. One version of the system transmits the bit b, 0 or 1, with the waveform
xbtðÞ¼ cos 2 πf0tþπþbπ ðÞ 0/C20t/C20T
for suitable choices of frequency f0and time duration T. For purposes of this example, assume
f0¼1 and T¼1.
(a) Sketch the ensemble of this process.
(b) Can the two bits be distinguished at time t¼0.25 s? Why or why not?
(c) Suppose random bit noise with p(0)¼.8 and p(1)¼.2 is transmitted via BPSK, and call
the resulting random process X(t). Find the probability distributions of X(0) and of X(.5).
8. Consider a random process X(t) deﬁned by X(t)¼Acos(πt)+Bsin(πt), where AandBare iid
N(5, 2) rvs.
(a) Graph a sample function of X(t).
(b) Find the probability distributions of X(1/4) and of X(1/2).
(c) Find the joint pdf of X(1/4) and X(1/2). [ Hint: Refer back to Sect. 4.7.]
9. A gambler plays roulette conservatively: she bets on black every time, which gives her probabil-
ity 18/38 of winning on each spin. Deﬁne a random sequence Xn¼the number of wins she has
after the nth spin for n¼1, 2, 3, ....
(a) Is Xna discrete-space or continuous-space sequence?
(b) Sketch two possible sample functions (sequences) for n¼1, ..., 10.
(c) What is the probability distribution of Xnfor ﬁxed n?7.1 Types of Random Processes 495
10. Refer to Example 7.2. Suppose Apple stock has value $580 at time t¼0, that the stock’s value
increases an average of 25 cents per day, and that the variation around that increasing trend can be
described by a Brownian motion process with parameter α¼20 (see Example 7.6).
(a) Write an expression for X(t), starting at time t¼0, in terms of tand the process Y(t) from
Example 7.6.
(b) Sketch two sample functions of X(t).
(c) Find the probability distribution of Apple’s stock at the end of 1 week of trading ( t¼5) and
at the end of 2 weeks’ trading ( t¼10).
7.2 Properties of the Ensemble: Mean and Autocorrelation Functions
In the previous section, we introduced the notion of a random process X(t). We emphasized that, for a
ﬁxed time value t,X(t) is a random variable possessing some probability distribution. Moreover, if we
look at two ﬁxed time points tands, the two random variables X(t) and X(s) are usually not
independent, and we can attempt to describe their joint probability distribution. In this section, we
explore these ideas further.
7.2.1 Mean and Variance Functions
At any particular time t, the random variable X(t) has a probability distribution and thus has both a
mean value and variance. Since X(t) for ﬁxed tis a random variable, we should be able to calculate its
mean using the techniques of Chaps. 2and3. Such a mean value exists for every time t, and the mean
might not be the same at every time t, i.e., the mean of X(t) may vary with t. Thus, considering all
values of tgives a mean function . Similar comments apply to the variance and standard deviation of t.
DEFINITION
Themean function of a random process X(t) is given by
μXtðÞ¼ EX tðÞ½/C138 ,
where E[X(t)] is the expected value of the random variable X(t) for the ﬁxed time point t.
Similarly, we deﬁne the variance function ofX(t)b y
σ2
XtðÞ¼ VarXtðÞðÞ ¼ EX tðÞ/C0 μXtðÞ ðÞ2hi
¼EX2tðÞ/C2/C3
/C0μXtðÞ½/C1382
and the standard deviation function ofX(t)b y σXtðÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarXtðÞðÞp
.
Notice that the mean, variance, and standard deviation functions are nonrandom functions of the
time variable t, just as the mean, variance, and standard deviation of a random variable are numbers
and not random quantities. It’s vital to keep in mind that the mean function of a random process is
taking an average with respect to the ensemble (i.e., in the “random direction”) and not with respect
to time.496 7 Random Processes
Example 7.7 Reconsider Example 7.5 from Sect. 7.1, where the random process X(t) was deﬁned by
the equation X(t)¼v0+Acos(ω0t+θ0). To ﬁnd the mean and variance functions of X(t), we apply
the properties of expected value and variance established in earlier chapters. Remembering that time
tis ﬁxed, the entire term cos( ω0t+θ0) may be treated as a constant, from which we obtain
μXtðÞ¼ EX tðÞ½/C138 ¼ Ev 0þAcos ω0tþθ0 ðÞ ½/C138 ¼ v0þEAðÞ /C1 cosω0tþθ0 ðÞ
σ2
XtðÞ¼ VarXtðÞðÞ ¼ Varv0þAcos ω0tþθ0 ðÞ ðÞ ¼ VarAðÞ /C1 cos2ω0tþθ0 ðÞ
Remember that we must square a multiplicative constant for variance.
In the case where A~ Unif[ /C01, 1] illustrated in Fig. 7.6, we have E(A)¼(/C01 + 1)/2 ¼0. Thus
the mean function of X(t)i sμX(t)¼v0+ 0cos( ω0t+θ0)¼v0. We can see this in Fig. 7.6: at any
ﬁxed time point t, the average of the values in the vertical (“random”) direction is clearly zero, the
value of v0for that graph. If we imagine vertically averaging these functions, we would arrive at the
constant function f(t)¼0, as claimed.
In this same case, the variance of Ais given by Var( A)¼(1/C0(/C01))2/12¼1/3, whence
σ2
XtðÞ¼1
3cos2ω0tþθ0 ðÞ
Thus, the variability of X(t) in the vertical direction increases and decreases in a periodic manner
as we vary t. This, too, can be seen in Fig. 7.6: the vertical spread varies with t, and this variability is
the largest at t¼0, 0.5, 1, and so on, when the cosine function is maximal. The standard deviation
function of X(t)i s
σXtðÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
3cos2ω0tþθ0 ðÞr
¼1ﬃﬃﬃ
3p cosω0tþθ0 ðÞjj
The absolute value ensures that our standard deviation is always nonnegative. ■
Example 7.8 In the previous example, we modeled amplitude variation with a uniform distribution.
However, this is not a realistic model for most observed amplitude variation. Engineers frequently
model amplitude variation Afrom a signal with a Rayleigh distribution . One example of a Rayleigh
pdf is given by
fAaðÞ ¼ae/C0a2=2a>0
0 otherwise/C26
ð7:2Þ
The graph of Eq. ( 7.2) appears in Fig. 7.7a, illustrating that a small amplitude is more likely than a
large one. Notice that this model only provides positive values for the amplitude. Figure 7.7b shows
the ensemble of X(t)¼Acos(2πt) when Ahas the pdf speciﬁed in Eq. ( 7.2).7.2 Properties of the Ensemble: Mean and Autocorrelation Functions 497
It’s clear from the graph that the mean function is not zero; rather, it appears to be itself a sinusoid.
(See if you can estimate the amplitude of the mean function by looking at t¼0 on the graph.)
Borrowing from Example 7.7, it’s still true that X(t) has mean and variance functions given by
μX(t)¼v0+E(A)cos( ω0t+θ0) and σX2(t)¼Var(A)cos2(ω0t+θ0), respectively. Using calculus, it
can be shown the pdf in Eq. ( 7.2) has expected valueﬃﬃﬃﬃﬃﬃﬃﬃ
π=2p
/C251:253; hence, the mean function of the
random process displayed in Fig. 7.7b isμX(t)/C251.253cos( t), which is indeed a sinusoid. ■
Example 7.9 Signal plus noise . A deterministic (i.e., nonrandom) signal s(t) incurs noise during
transmission, in which case the received message may have the form Y(t)¼s(t)+N(t). The term N(t)
is called the “noise component” of the received signal, and a variety of models can be used to describe
such noise. Figure 7.8below illustrates (part of) the ensemble of
YtðÞ¼ 3 cos 2 πtþπ=2 ðÞ þ NtðÞ,
where N(t)i sGaussian noise with mean 0 and standard deviation 1 (that is, at each ﬁxed time point t,
N(t) is standard normal).
Let’s ﬁrst determine the probability distribution of Y(t) at both t¼0.25 and t¼2. With s(t)¼
3cos(2 πt+π/2),Y(0.25) ¼s(0.25) + N(0.25) ¼/C03+N(0.25). Since N(0.25) has a Gaussian distri-
bution with mean 0 and variance 1, it follows that Y(0.25) is also Gaussian, but with a mean of /C03 and
standard deviation 1. Similarly, Y(2)¼s(2) + N(2)¼0+N(2)¼N(2), so Y(2) is standard normal.
We can visualize both of these distributions by looking vertically in Fig. 7.8.−3−2−10
12ab
3
2
1
afA(a)x(t)
t
Fig. 7.7 (a) a Rayleigh pdf; ( b) the resulting ensemble of X(t)¼Acos(2πt)
−6−4−20246y(t)
t
0.5 1 1.5 2 2.5 3Fig. 7.8 The ensemble for
Example 7.9498 7 Random Processes
To ﬁnd the mean and variance functions of Y(t), note that s(t) is to be treated as a constant with
respect to the ensemble. We ﬁnd that
μYtðÞ¼ EY tðÞ½/C138 ¼ EstðÞþ NtðÞ ½/C138 ¼ stðÞþ EN tðÞ½/C138 ¼ stðÞþ 0¼stðÞ
That is, the mean function of this random process is just the original signal, s(t); this is the sinusoid
that “carves down the middle” of Fig. 7.8. Finally, since s(t) is an additive constant in the expression
forY(t),σY2(t)¼Var(Y(t))¼Var(s(t)+N(t))¼Var(N(t))¼1. The amount of variability around the
signal is the same at every point tin the process.
Notice that the distribution of the noise component, N(t), isN(0, 1) at every point t. But be careful:
saying a process has the same distribution at every point tis very different from saying N(t)i sa
constant! ■
Example 7.10 Signal plus noise, round two. Let’s modify the previous example by specifying that
the spread of the noise component N(t) varies with time; speciﬁcally, suppose that N(t) is Gaussian
with mean 0 and variance t. The ensemble of the resulting random process Y(t) appears in Fig. 7.9.
The mean function of Y(t) is still s(t); however, following the derivation in Example 7.9, we ﬁnd
Var(Y(t))¼Var(N(t))¼t,s oσYtðÞ¼ﬃﬃtp.
7.2.2 Autocovariance and Autocorrelation Functions
The mean and variance describe the distribution of a single random variable. In the context of a
random process, the mean and variance functions contain information about the behavior of the
ensemble at each single point in time. But it should be clear that for two different times tands, the
random variables X(t) and X(s) will typically be related. A complete statistical analysis of a random
process should include an exploration of that relationship. To that end, we now extend the notion of
covariance from Chap. 4to a random process.−8−6−4−20246810
tx(t)
12
Fig. 7.9 The ensemble for Example 7.10 ■7.2 Properties of the Ensemble: Mean and Autocorrelation Functions 499
DEFINITION
Theautocovariance function of a random process X(t) is deﬁned by
CXXt;sðÞ ¼ Cov XtðÞ,XsðÞ ðÞ ¼ EX tðÞ/C0 μXtðÞ ðÞ XsðÞ /C0 μXsðÞ ðÞ ½/C138
Notice that the autocovariance function is a nonrandom function of twotime points, tands.
We can interpret the autocovariance function of X(t) much as Cov( X, Y) was interpreted back in
Chap. 4. When CXX(t, s)>0, above-average values of X(t) tend to be associated with above-average
values of X(s). That is, when X(t) is above its mean function at time t, it also tends to be above its mean
function at time s(and vice versa). If CXX(t, s)<0, then above-average values of the random process
at time tare associated with below -average values at time s(and vice versa).
Properties of the autocovariance function follow directly from the properties previously derived
for covariance. We provide a partial listing here.
PROPOSITION
LetCXX(t, s) denote the autocovariance function of a random process X(t).
1.CXXt;sðÞ ¼ CXXs;tðÞ
2.CXXt;sðÞ ¼ EX tðÞXsðÞ ½/C138 /C0 μXtðÞμXsðÞ
3.σ2
XtðÞ¼ VarXtðÞðÞ ¼ Cov XtðÞ,XtðÞ ðÞ ¼ CXXt;tðÞ ¼ EX2tðÞ/C2/C3
/C0μ2
XtðÞ
In the engineering literature, E[X(t)X(s)] in property 2 is called the autocorrelation function of
X(t) and is denoted RXX(t, s). Although it will be vital to our study of signal processing in Chap. 8
(available online), don’t confuse this with the correlation coefﬁcient from Chap. 4; in particular, the
sign of RXX(t, s) does not indicate the direction of the association between X(t) and X(s), and the
magnitude of RXX(t, s) is not bounded by 1.
Example 7.11 Let’s ﬁnd the autocovariance function of the random process X(t)¼Acos(2πt) from
Examples 7.5 and 7.7. We will illustrate two methods here. Since we already have the mean function
ofX(t), we can calculate the autocorrelation function and then apply property 2 from the preceding
proposition:
RXXt;sðÞ ¼ E/C2
XðtÞXðsÞ/C3
¼E/C2
Acosð2πtÞAcosð2πsÞ/C3
¼E/C2
A2cosð2πtÞcosð2πsÞ/C3
¼EA2/C0/C1
cosð2πtÞcosð2πsÞ)
CXXt;sðÞ ¼ E/C2
XðtÞXðsÞ/C3
/C0μXðtÞμXðsÞ
¼EA2/C0/C1
cosð2πtÞcosð2πsÞ/C0EðAÞcosð2πtÞ/C1EðAÞcosð2πsÞ
¼/C2
EA2/C0/C1
/C0EAðÞðÞ2/C3
cosð2πtÞcosð2πsÞ
¼VarAðÞcosð2πtÞcosð2πsÞ
Alternatively, we can manipulate the covariance expression directly by applying its distributive
properties from Chap. 4:500 7 Random Processes
CXXt;sðÞ ¼ CovðXðtÞ,XðsÞÞ ¼ CovðAcosð2πtÞ,Acosð2πsÞÞ
¼Cov A;AðÞ cosð2πtÞcosð2πsÞ
¼VarAðÞcosð2πtÞcosð2πsÞ
As a check, substituting s¼tgives CXX(t, t)¼Var(A)cos2(2πt), which matches the expression for
σX2(t) we found in Example 7.7 (with ω0¼2πandθ0¼0). ■
Example 7.12 Let’s now consider a sinusoid with phase variation , rather than amplitude variation.
Deﬁne a random process X(t)b y
XtðÞ¼ A0cosω0tþΘ ðÞ ð 7:3Þ
where the phase shift Θis a rv, uniformly distributed on the interval ( /C0π,π]. The amplitude A0and
fundamental frequency ω06¼0 are constants. Figure 7.10 shows several sample functions for this
random process with A0¼1 and ω0¼2π.
Until now, we have managed to compute means and variances without any calculus; however,
because the random process X(t) deﬁned by Eq. ( 7.3)i sa nonlinear function (cosine) of a random
variable, we must rely on calculus here. Speciﬁcally, we apply the Law of the Unconscious Statisti-
cian, presented in Chap. 3:
μXtðÞ¼ E/C2
XtðÞ/C3
¼E/C2
A0cosðω0tþΘÞ/C3
¼ð1
/C01A0cosω0tþθ ðÞ fΘθðÞdθ¼ðπ
/C0πA0cosω0tþθ ðÞ1
π/C0/C0πðÞdθ
¼A0
2πðπ
/C0πcosω0tþθ ðÞ dθ¼A0
2π0ðÞ ¼ 0
The last integral equals zero because, as a function of θ, it represents the integration of a cosine
through one period. A mean function identically equal to zero coincides with what we see in Fig. 7.10.
Since the mean function is zero, the autocovariance and autocorrelation functions will be identical.
Calculation of these functions requires a trig identity:−101x(t)
tFig. 7.10 Sample
functions for the phase-
variation process in
Example 7.127.2 Properties of the Ensemble: Mean and Autocorrelation Functions 501
CXXt;sðÞ ¼ E/C2
XtðÞXðsÞ/C3
/C0ð0Þð0Þ¼E/C2
XðtÞXðsÞ/C3
¼E/C2
A0cosω0tþΘ ðÞ /C1 A0cosω0sþΘ ðÞ/C3
¼A2
0E/C2
cosðω0tþΘÞcosðω0sþΘÞ/C3
¼A2
0E1
2cosω0tþΘþω0sþΘ ðÞ þ cosω0tþΘ/C0ω0sþΘ ½/C138 ðÞ fg/C20/C21
¼A2
0
2E/C2
cosω0tþω0sþ2Θ ðÞ þ cosðω0t/C0ω0sÞ/C3
¼A2
0
2Eh
cosω0tþω0sþ2Θ ðÞi
þA2
0
2cosðω0t/C0ω0sÞ
¼A2
0
2/C10þA2
0
2cosω0t/C0ω0s ðÞ ¼A2
0
2cosðω0t/C0ω0sÞ
The last expected value equals zero because it corresponds to a integral of a cosine through two
periods. Finally, the variance function is given by
σ2
XtðÞ¼ CXXt;tðÞ ¼A2
0
2cosω0t/C0ω0t ðÞ ¼A2
0
2cos 0ðÞ ¼A2
0
2
Notice that the variance function of X(t) is a constant (same spread for all t), which agrees with
Fig.7.10. ■
7.2.3 The Joint Distribution of Two Random Processes
Some applications involve the consideration of two random processes X(t) and Y(t). We may then be
concerned not only with their individual distributions but also their joint behavior. This is especially
true when Y(t) is the result of some action taken on X(t), such as passing the random signal X(t)
through an appropriate ﬁlter. To quantify their relationship, we deﬁne the cross-covariance function
ofX(t) with Y(t)byCXY(t, s)¼Cov( X(t),Y(s)) and the cross-correlation function of X(t) with Y(t)
byRXY(t, s)¼E[X(t)Y(s)]. These two functions are, not surprisingly, connected by the formula
CXY(t, s)¼RXY(t, s)/C0μX(t)μY(s).
DEFINITION
Two random processes X(t) and Y(t) areindependent if, for all ﬁxed tands, the random variables
X(t) and Y(s) are independent rvs as deﬁned in Chap. 4.X(t) and Y(t) are uncorrelated if, for all
tands,CXY(t, s)¼0. Finally, X(t) and Y(t) are orthogonal ifRXY(t, s)¼0 for all tands.
Notice in these deﬁnitions that properties must hold for all times tands. For example, the
independence of X(t) and Y(t) requires that the random variables X(2) and Y(10) be independent, as
must X(2) and Y(2) be, and so on. A similar comment applies to being uncorrelated or orthogonal.
As in Chap. 4, independence is a stronger condition than zero correlation:
XtðÞandYtðÞindependent )XtðÞandYtðÞuncorrelated,
but the converse is false. If X(t) and Y(t) are uncorrelated, it follows from the deﬁnition of covariance
that E[X(t)Y(s)]¼E[X(t)]E[Y(s)] for all tands. Thus, being uncorrelated does not imply being
orthogonal (nor vice versa); however, if either random process has mean identically equal to zero,
then the properties of being uncorrelated and orthogonal are equivalent.502 7 Random Processes
7.2.4 Exercises: Section 7.2(11–24)
11. Consider the QPSK system described in Example 7.1 as a model for random noise. Suppose the
four possible symbols to be transmitted are equally likely to occur, i.e., we have a random process
XtðÞ¼ cos 2πf0tþπ=4þKπ=2 ðÞ
where Kis 0, 1, 2, or 3 with probability .25 each.
(a) Find the mean function of X(t). Simplify as much as possible.
(b) Find the variance function of X(t). Are your answers consistent with Fig. 7.2?
12. Show that CXX(t, s)¼E[X(t)X(s)]/C0μX(t)μX(s).
13. Consider the random process X(t)¼v0+Acos(ω0t+θ0) from Example 7.7. Find the
autocovariance function and autocorrelation function of X(t).
14. Let X(t)¼At+B, where AandBare independent random variables with A~ Unif[0, 6] and
B~ Unif[ /C010, 10].
(a) Describe the ensemble of X(t).
(b) Determine the mean function of X(t).
(c) Determine the autocovariance function of X(t).
(d) Determine the autocorrelation function of X(t).
(e) Determine the variance function of X(t).
15. Let N(t) be a Gaussian noise process as in Example 7.9, with mean 0 for all tand autocovariance
function CNN(t, s)¼e/C0|s/C0t|.
(a) Verify that N(t) has variance 1 for all t.
(b) If N(t)>0, would you predict that N(s)>0o rN(s)<0? Explain.
(c) Determine the correlation coefﬁcient ρofN(10) and N(12).
(d) Determine the probability distribution of N(12)/C0N(10).
16. Let N(t) be the Gaussian noise process of Example 7.10, with mean function 0 and variance
function t.
(a) Calculate P(N(1)>.5) and P(N(4)>.5).
(b) Could the autocovariance function of N(t)b ee/C0|s/C0t|? Why or why not?
(c) Suppose the autocovariance function of N(t) is min( t, s), i.e., CNN(t, s)¼tfort/C20sandsfor
t>s. Find the correlation coefﬁcient between N(t) and N(s). [Hint: consider the two cases
t/C20sandt>s.]
(d) Determine the probability distribution of N(s)/C0N(t).
17. Consider the phase-variation random process ( 7.3) with A0¼1 and ω0¼2π.
(a) Use the results of Example 7.12 to show that, for ﬁxed t,X(t) does nothave a uniform
distribution. [ Hint: What is the interval of possible values for X(t)? If X(t) were uniform,
what would its variance be?]
(b) Use the transformation method of Sect. 3.7to show that the rv Y¼X(0) has an arcsine
distribution :
fYyðÞ ¼1
π1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0y2p /C01<y<1
[Note : It can be shown that X(t) has this same distribution for all t.]
18. Let A(t) be a random process, and deﬁne an “amplitude modulated” version of A(t)b y X(t)¼
A(t)cos( ω0t+Θ), where Θ~ Unif( /C0π,π] and is independent of A(t), and ω0is a constant.
(a) Determine the mean function of X(t).
(b) Determine the autocorrelation function of X(t).7.2 Properties of the Ensemble: Mean and Autocorrelation Functions 503
(c) Determine the cross-correlation of A(t) and X(t).
[Hint: Use the results of Example 7.12.]
19. Consider a “signal plus noise” process where both components are random: X(t)¼S(t)+N(t).
Assume S(t) and N(t) are uncorrelated random processes. Determine each of the following
functions in terms of the mean, autocorrelation, etc. of S(t) and N(t).
(a) The mean function of X(t).
(b) The autocorrelation function of X(t).
(c) The autocovariance function of X(t).
(d) The variance function of X(t).
20. Consider the random process X(t)¼S(t)+N(t) from the previous exercise. Find the cross-
correlation between the signal component S(t) and the overall process X(t).
21. Consider two random processes X(t) and Y(t).
(a) Show that if X(t) and Y(t) are uncorrelated random processes, then E[X(t)Y(s)]¼μX(t)μY(s).
(b) Show that if X(t) and Y(t) are uncorrelated random processes and X(t) has mean function
equal to zero, then X(t) and Y(t) are orthogonal.
22. Let A(t) and B(t) be iid processes, i.e., A(t) and B(t) are independent processes with the
same mean function μ(t), autocovariance function C(t, s), etc. Deﬁne a pair of new random
processes by
XtðÞ¼ AðtÞþBðtÞ
YtðÞ¼ AðtÞ/C0BðtÞ
(a) Find the mean functions of X(t) and Y(t).
(b) Find the autocovariance functions of X(t) and Y(t).
(c) Find the cross-covariance function CXY(t, s).
23. Let Θbe a uniformly distributed rv on ( /C0π,π]. Deﬁne a pair of random processes X(t)¼
cos(ω0t+Θ) and Y(t)¼sin(ω0t+Θ).
(a) Find the cross-correlation and cross-covariance of X(t) and Y(t).
(b) Are X(t) and Y(t) orthogonal random processes? Uncorrelated random processes? Indepen-
dent random processes?
24. Let RXY(t, s) be the cross-correlation function of X(t) with Y(t), and deﬁne the cross-correlation of
Y(t) with X(t)b yRYX(t, s). Show that RYX(t, s)¼RXY(s, t). Show that a similar relationship holds
for cross-covariance.
7.3 Stationary and Wide-Sense Stationary Processes
When modeling certain random processes, particularly those representing noise, it facilitates the
analysis if the statistical properties of the process remain the same across time. This turns out to be
true for some, though certainly not all, models. We will make this notion more precise shortly, but
ﬁrst let’s revisit three of the examples from the previous section. The relevant graphs are presented in
Fig. 7.11 below. Figure 7.11a shows the ensemble of the phase-variation random process from
Example 7.12. Notice that the probability distribution of X(t)—remember, that’s the distribution in
the vertical direction—appears to be the same at each time point t. Figure 7.11b shows just the noise
component, N(t), from Example 7.9. Again, we see roughly the same ensemble behavior at every time504 7 Random Processes
t, suggesting the process’ statistical properties do not change over time. In contrast, consider the noise
component N(t) from Example 7.10, displayed here in Fig. 7.11c . While the mean of N(t) is constant,
its variance clearly increases with t; this model does notpossess the property of interest.
We now formalize the notion of stable behavior over time.
DEFINITION
A random process X(t)i s(strict-sense) stationary if all of its statistical properties are invariant
with respect to time. More precisely, X(t) is stationary if, for any time points t1,...,trand any
value τ, the joint distribution of X(t1),...,X(tr) is the same as the joint distribution of X(t1+τ),
...,X(tr+τ).
This deﬁnition requires that the statistical behavior of X(t) remain the same if we “translate” the
random process τtime units. In particular, it requires that X(t1) and X(t1+τ) have the same
distribution for all t1andτ; it follows that X(t) must have the same mean, standard deviation, etc.
at all times t. This corresponds to what we see in Fig. 7.11a, b , but not c. Notice, however, that the
deﬁnition requires more: since the joint distribution of X(t1) and X(t2) must be translation-invariant, it
follows that the autocovariance function of X(t) must be translation-invariant as well. We certainly
cannot determine this from a visual inspection of the ensemble.−10ab
c1
−101
−101t tx(t) x(t)
x(t)
t
Fig. 7.11 Three ensembles: ( a) the phase-variation process from Example 7.12; ( b) the noise component from
Example 7.9; ( c) the noise component from Example 7.107.3 Stationary and Wide-Sense Stationary Processes 505
In fact, it is rarely practical to determine whether a particular random process model is strict-sense
stationary, since it requires an unlimited number of comparisons (joint distributions of rvariables at
all time-points and for all possible r). Fortunately, a weaker version of stationarity sufﬁces for the
purposes of many analyses.
DEFINITION
A random process X(t)i swide-sense stationary (WSS) if the following two conditions hold:
1. The mean function of X(t),μX(t), is a constant.
2. The autocovariance function of X(t),CXX(t,s), depends only on s/C0t.
We interpret condition 2 as follows: the degree of association between X(t) and X(s), as measured
by covariance, depends on how far apart the two times sandtare, but not where those times are
located on an absolute scale. So, for example, the covariance between X(3) and X(10) is the same as
the covariance between X(23) and X(30) when condition 2 is satisﬁed (since, in both cases,
s/C0t¼7).
Condition 2 of this deﬁnition can be stated more cleanly if we re-parameterize the second time
variable. Let’s write s¼t+τ, so that τrepresents the difference between the two times sandt. Then
wide-sense stationarity requires that the autocovariance function CXX(t,t+τ) depend only on τ(and
not on t). In fact, with this notation, we can deﬁne a wide-sense stationary process to be one such that
both μX(t) and CXX(t,t+τ) are independent of t.
Before looking at some examples, we note that the deﬁning conditions can be restated in terms of
the autocorrelation function, RXX: a random process X(t) is WSS iff (1) μX(t) is a constant and
(2)RXX(t,t+τ) depends only on τ.
Example 7.13 Is the amplitude-variation random process, X(t)¼Acos(2πt), wide-sense stationary?
The graphs in Figs. 7.6and7.7clearly indicate not. Indeed, in Example 7.11 we found the autocovariance
of this random process to be CXX(t, s)¼Var(A)cos(2 πt)cos(2 πs), which depends separately on tands,n o t
just their difference. Therefore, the amplitude-variation random process is not WSS. ■
Example 7.14 Is the phase-variation random process X(t)¼A0cos(ω0t+Θ) from Example 7.12
wide-sense stationary? Using the results of Example 7.12, we can check the two required conditions:
1.μX(t)¼0, a constant. Thus the ﬁrst condition is satisﬁed.
2.CXXt;sðÞ ¼A2
0
2cosω0t/C0ω0s ðÞ ,s o
CXXt,tþτ ðÞ ¼A2
0
2cosω0t/C0ω0tþτðÞ ðÞ ¼A2
0
2cos/C0ω0τðÞ ¼A2
0
2cosω0τðÞ :
Since CXX(t,t+τ) depends only on τand not on t, the second condition is met. Therefore, X(t)i s
indeed wide-sense stationary. ■
Example 7.15 LetAandBbe iid mean-zero random variables, and deﬁne a random process by
XtðÞ¼ Acosω0tðÞ þ Bsinω0tðÞ ð 7:4Þ
for some frequency ω0.I sX(t) wide-sense stationary? The mean function is506 7 Random Processes
μXtðÞ¼ EAcosω0tðÞ þ Bsinω0tðÞ ½/C138 ¼ EA½/C138cosω0tðÞ þ EB½/C138sinω0tðÞ
¼0cos ω0tðÞ þ 0sin ω0tðÞ ¼ 0
Since the mean of X(t) is a constant, the ﬁrst condition is met. Next, let’s consider the
autocovariance function. Using the distributive properties of covariance,
CXXt;sðÞ ¼ CovðAcosðω0tÞþBsinðω0tÞ,Acosðω0sÞþBsinðω0sÞÞ
¼CovðAcosω0tðÞ ,Acosðω0sÞÞ þ CovðAcosðω0tÞ,Bsinðω0sÞÞ
þCovðBsinω0tðÞ ,Acosðω0sÞÞ þ CovðBsinðω0tÞ,Bsinðω0sÞÞ
¼Cov A;AðÞ cosðω0tÞcosðω0sÞþCovðA,BÞcosðω0tÞsinðω0sÞ
þCov B;AðÞ sinðω0tÞcosðω0sÞþCovðB,BÞsinðω0tÞsinðω0sÞ
Since AandBare independent, Cov( A, B)¼Cov( B, A)¼0; since they’re identically distributed,
Cov( A, A)¼Cov( B, B)¼σ2, the common variance of AandB. Using a trig identity, we arrive at
CXXt;sðÞ ¼ σ2cosω0tðÞ cosω0sðÞ þ σ2sinω0tðÞ sinω0sðÞ ¼ σ2cosω0t/C0ω0s ðÞ ¼ σ2cosω0t/C0s½/C138ðÞ
Since this depends only on the difference in the two times tands, the second condition is met.
(In fact, we may simplify the last expression further, to σ2cos(ω0[t/C0(t+τ)])¼σ2cos(/C0ω0τ)¼
σ2cos(ω0τ).) Therefore, yes,X(t) in Expression ( 7.4) is wide-sense stationary. ■
Example 7.16 (Example 7.15 continued) In the previous example, we proved that any random
process of the form ( 7.4) is WSS, provided AandBare iid with mean zero.
As a curious example, suppose AandBare independent and each is equally likely to be +1 or /C01;
this particular distribution has mean 0 and variance 1. Since AandBare iid with mean 0, the random
process X(t) in Eq. ( 7.4) is WSS, with mean 0, autocovariance function CXX(t, t + τ)¼σ2cos(ω0τ)
¼cos(ω0τ), and thus variance σX2(t)¼CXX(t, t)¼1. However, since AandBeach can only take on
the two values /C61, the entire ensemble of X(t) consists of just four functions:
XtðÞ¼/C6 cosω0tðÞ /C6 sinω0tðÞ
This ensemble appears in Fig. 7.12; its appearance does not match earlier pictures of WSS
processes. But it is nonetheless true that the mean of the vertical coordinates at any time-point tis
0, the variance is 1, and the covariance between any two time points τunits apart is cos( ω0τ).
−1.5−1−0.500.511.5
1 2 3 4 5 6tx(t) Fig. 7.12 The ensemble
ofX(t) in Example 7.167.3 Stationary and Wide-Sense Stationary Processes 507
In part, the lesson here is that we cannot rely on a visualization of a random process to determine
whether it’s wide-sense stationary—despite appearances, this really is a WSS process. That said, it’s
clear that the probability distribution ofX(t) is not the same for all t, e.g., the possible values of the
process are { /C01, 0, 1} at some t-coordinates and /C01=ﬃﬃﬃ
2p
,1=ﬃﬃﬃ
2p/C8/C9
at others. So, while this random
process is wide -sense stationary, it is certainly not strict- sense stationary. ■
In Chap. 8(available online), we will study relationships between the behavior of the input X(t)
to a ﬁlter and the resulting output Y(t); we will often require that X(t) be WSS. Two random processes
X(t) and Y(t) are called jointly wide-sense stationary if (1) X(t) is WSS, (2) Y(t) is WSS, and (3) the
cross-covariance function CXY(t, t + τ) does not depend on t. (Equivalently, X(t) and Y(t) are jointly
WSS if they are both WSS processes and RXY(t, t + τ) is independent of t.)
7.3.1 Properties of Wide-Sense Stationary Processes
By deﬁnition, a WSS random process has a constant mean (function). In fact, such a process also has
constant variance, because σX2(t)¼CXX(t,t) wide-sense stationarity requires that this covariance not
depend on t. With that and our previous discussions in mind, we adopt the following notational
conventions for WSS processes.
NOTATION
Suppose X(t) is a wide-sense stationary process. Then we denote its statistical functions as
follows:
μX¼E/C2
XtðÞ/C3
σ2
X¼VarðXtðÞ Þ
CXXτðÞ ¼ CovðXðtÞ,XðtþτÞÞ ¼ RXXðτÞ/C0μ2
X
RXXτðÞ ¼ E/C2
XðtÞXðtþτÞ/C3
¼CXXðτÞþμ2
X
Next we present some important properties of these functions.
PROPOSITION
LetX(t) be a wide-sense stationary process with autocovariance function CXX(τ) and autocor-
relation function RXX(τ).
Properties of CXX(τ):
1.CXX0ðÞ ¼ EX2tðÞ/C2/C3
/C0μ2
X¼σ2
X¼VarXtðÞðÞ :
2.CXX(/C0τ)¼CXX(τ); that is, the autocovariance function is symmetric in τ.
3.jCXX(τ)j/C20CXX(0) for every τ; that is, the autocovariance function achieves its largest value
atτ¼0.
4. If X(t) is periodic, so is CXX(τ), and with the same period.
5. If X(t) is ergodic1and has no periodic component, then CXX(τ)!0a s| τ|!1 .
1Loosely speaking, a random process is ergodic if its time and ensemble properties “match.” We will deﬁne ergodicity
more carefully later in this section; for now, you may assume the processes referenced in this section are ergodic unless
noted otherwise.508 7 Random Processes
Properties of RXX(τ):
1.RXX(0)¼E[X2(t)], called the mean square value ofX(t).
2.RXX(/C0τ)¼RXX(τ); that is, the autocorrelation function is symmetric in τ.
3.jRXX(τ)j/C20RXX(0) for every τ; that is, the autocorrelation function achieves its largest value
atτ¼0.
4. If X(t) is periodic, so is RXX(τ), and with the same period.
5. If X(t) is ergodic and has no periodic component, then RXX(τ)!μX2asjτj!1 .
Proof We begin with the properties of CXX(τ). Property 1 follows from the covariance shortcut
formula. To prove Property 2, recall from Chap. 4that covariance is symmetric in its arguments:
CXXτðÞ¼ Cov XtðÞ,XtþτðÞ ðÞ ¼ Cov XtþτðÞ ,XtðÞ ðÞ
Because X(t) is WSS, the right-most expression depends only on the difference in the two times;
speciﬁcally, Cov( X(t+τ),X(t))¼CXX(t/C0[t+τ])¼CXX(/C0τ). This establishes the result.
Property 3 is left as an exercise (see Exercise 38). We note that Property 3 makes intuitive sense:
since covariance measures the association between two variables, and τrepresents the time distance
between these two variables, covariance should be largest when that time difference is as small as
possible. That is, the behaviors of X(t) and X(t+τ) should be more closely related when τis small
than when τis large.
Properties 1–3 for the autocorrelation function follow automatically, since RXX(τ) and CXX(τ)o n l y
differ by the constant μX2.
Toward proving property 4, suppose X(t) is periodic with period d,s oX(t)¼X(t+d) for all t.
Then, for any τ,
RXXτþdðÞ ¼ E/C2
XðtÞXðtþτþdÞ/C3
¼E/C2
XtðÞXðtþτÞ/C3
because XðtþτþdÞ¼XðtþτÞ
¼RXXτðÞ
which shows that RXX(τ) is also periodic with period d. The analogous property holds for
autocovariance, because subtracting μX2to get CXXfrom RXXdoes not affect periodicity.
A formal proof of Property 5 is beyond the scope of this book; however, the paragraph above
regarding Property 3 should give some intuition for why covariance should vanish as | τ|!1 .S o m e
further information about “ergodicity” appears at the end of this section. ■
It’s important to note that while every autocovariance and autocorrelation function for WSS
processes satisfy the properties listed in this proposition, these properties do not completely charac-
terize such functions. That is to say, there exist functions that satisfy properties 1–5 but are notvalid
autocovariance/autocorrelation functions. We’ll explore this further in Chap. 8(available online),
when we connect autocorrelation and autocovariance functions to the power spectrum of a random
signal. (For a preview, see Exercise 40.)
Example 7.17 Suppose X(t) is a wide-sense stationary random process with autocorrelation function
RXXτðÞ ¼ 100þ16
1þτ27.3 Stationary and Wide-Sense Stationary Processes 509
Let’s determine as much as we can about the other statistical properties of X(t). First, the mean
square value of X(t)i sRXX(0)¼100 + 16 ¼116 (Property 1). Next, X(t) clearly has no periodic
component; otherwise, RXX(τ) would also (Property 4). Thus we may apply Property 5:
μ2
X¼lim
jτj!1RXXτðÞ ¼ lim
jτj!1100þ16
1þτ2/C20/C21
¼100þ0¼100
from which μXeither equals +10 or /C010; notice that we cannot determine which is correct from
RXX(τ). We can, however, determine the autocovariance function:
CXXτðÞ ¼ RXXτðÞ /C0 μ2
X¼100þ16
1þτ2/C0100¼16
1þτ2
Notice this autocovariance function goes to 0 as | τ|!1 , as guaranteed by Property 5. Finally, the
variance of this random process is given by σX2¼CXX(0)¼16, and the standard deviation is
σX¼4. ■
Example 7.18 Partitioning a random process. Suppose X1(t) and X2(t) are independent, zero-mean,
WSS random processes with autocorrelation functions R11(τ)¼2000tri(10,000 τ) and R22(τ)¼
650cos(40,000 πτ), respectively.2
Deﬁne a new random process by X(t)¼X1(t)+X2(t) + 40. The mean function of X(t)i s
μXtðÞ¼ EX tðÞ½/C138 ¼ EX 1tðÞþ X2tðÞþ 40 ½/C138 ¼ EX 1tðÞ½/C138 þ EX 2tðÞ½/C138 þ 40¼0þ0þ40¼40
Determining the autocorrelation function requires some signiﬁcant algebraic work:
RXXt;sðÞ ¼ E/C2
XðtÞXðsÞ/C3
¼E/C2
ðX1ðtÞþX2ðtÞþ40ÞðX1ðsÞþX2ðsÞþ40Þ/C3
¼E/C2
X1tðÞX1ðsÞ/C3
þE/C2
X1ðtÞX2ðsÞ/C3
þE/C2
40X1ðtÞ/C3
þE/C2
X2ðtÞX1ðsÞ/C3
þE/C2
X2tðÞX2ðsÞ/C3
þE/C2
40X2ðtÞ/C3
þE/C2
40X1ðsÞ/C3
þE/C2
40X2ðsÞ/C3
þE/C2
1600/C3
ð7:5Þ
Four of the terms in Expression ( 7.5) may be simpliﬁed by removing the constant 40, e.g.,
E[40X1(t)]¼40E[X1(t)]¼40(0) ¼0, since X1(t) is a mean-zero process. The other three similar
terms are also 0. Using the independence assumption, we can rewrite the second term in Eq. ( 7.5)a s
E[X1(t)X2(s)]¼E[X1(t)]E[X2(s)]¼(0)(0) ¼0. The last term in the middle line of Eq. ( 7.5) is 0 for
the same reason. In fact, only three terms do not vanish:
RXXt;sðÞ ¼ E/C2
X1ðtÞX1ðsÞ/C3
þE/C2
X2ðtÞX2ðsÞ/C3
þE/C2
1600/C3
¼R11t;sðÞ þ R22ðt,sÞþ1600
¼R11τðÞ þ R22ðτÞþ1600 because X1ðtÞandX2ðtÞare WSS
¼2000tri 10,000 τ ðÞ þ 650cos ð40,000 πτÞþ1600 ðτ¼s/C0tÞ
That is, the autocorrelation function of X(t) equals the sum of the autocorrelations of X1(t) and
X2(t), plus the square of the constant term (402¼1600). Since the mean of X(t) is a constant, 40, and
the autocorrelation function of X(t) depends only on τ, the random process X(t) is indeed wide-sense
stationary.
2Readers not familiar with the triangular or “tri” function should consult Appendix B.510 7 Random Processes
Finally, we can easily ﬁnd the autocovariance function (which, since X(t)i sW S S ,o n l yd e p e n d so n τ):
CXXτðÞ¼ RXXτðÞ /C0 μ2
X¼RXXτðÞ /C0 402¼2000tri 10,000 τ ðÞ þ 650cos 40,000 πτ ðÞ
Notice that, since X1(t) and X2(t) have mean zero, the two terms in CXX(τ) are, in fact, their
respective autocovariance functions.
Let’s examine this example further. The random process X(t) consists of three components: X1(t),
which is not periodic (since R11(τ) isn’t); X2(t), which is periodic; and a constant. It’s often the case
that we can partition a random process in this manner:
XtðÞ¼ aperiodic componentsfg þperiodic componentsfg þconstantfg
Any or all of these three elements may be present, and the ﬁrst two components may themselves be
sums of other parts, e.g., the sum of several sinusoids with different periods may comprise the
“periodic components” piece. In engineering language, the constant term is called the dc offset :i f
X(t) represents a current waveform, the constant term is the direct current in X(t), while the other two
components comprise the alternating current (ac) of X(t).
Now look at RXX(τ), which also consists of three parts: an aperiodic part (also called the dissipative
component, since this is the term that goes to 0 as | τ|!1 ), a periodic part, and the square of the dc
offset. In general, the autocorrelation function of a WSS process can be decomposed into
RXXτðÞ ¼ dissipative componentsfg þperiodic componentsfg þconstantfg ð7:6Þ
The constant term in Eq. ( 7.6) is called the dc power offset , since it reﬂects the power that results
if the dc offset were to pass through a 1- Ωresistance (viz., P¼I2R¼402(1)¼1600). Finally, the
autocovariance function of X(t) includes only the ﬁrst two parts of Eq. ( 7.6), dissipative and periodic
components, and not the dc power offset. In a sense, CXX(τ) tells us something about the ac power in our
random current waveform. We will explore these ideas much further in Chap. 8(available online). ■
7.3.2 Ergodic Processes
As we’ll see in this chapter and the next, it is desirable to understand the statistical properties of a
random process—mean, variance, and so on. But because these are properties of the ensemble, a
problem arises: in practice, we generally only observe a single realization of the process, and so we
must somehow reconstruct the ensemble properties from this one signal. Thankfully, many stationary
random processes have the feature that their time and ensemble properties match (e.g., the time
average of a single realization equals the ensemble mean). A process with this feature is called
ergodic .
To give some intuition for this concept, imagine you have a large collection of identical dice. If
you did not know how dice behave, you would have two options: roll all of the dice once, or roll one
die many times. These should give us roughly the same information, but the ﬁrst method illustrates
ensemble properties (many realizations at one point in time) while the second gives time properties
(one sampled die observed again and again over time). This process is ergodic. The beneﬁt of
ergodicity here is that we can learn about the behavior of all dice by purchasing (and repeatedly
rolling) just a single die.
To make the property of ergodicity more precise, we need to introduce the time-dimension
analogues of mean, autocorrelation, etc. For a ﬁxed value T>0, the average of a function x(t)
over the interval [ /C0T, T] is deﬁned in calculus by7.3 Stationary and Wide-Sense Stationary Processes 511
xtðÞhiT¼1
2TðT
/C0TxtðÞdt
By allowing Tto approach inﬁnity, we may deﬁne the time average of a function x(t)b y
xtðÞhi ¼lim
T!1xtðÞhiT¼lim
T!11
2TðT
/C0TxtðÞdt
If the function x(t) is periodic, the time average deﬁned above is equal to the average of x(t) across one
period.
The time average of a random process X(t) is deﬁned by replacing x(t) in the above expression with
X(t). The result is a quantity that clearly does not depend on time (since we have integrated dt) but
which may vary across different members of the ensemble (i.e., the time average hX(t)iis still a
random quantity).
DEFINITION
A random process X(t)i smean ergodic if its time average and ensemble average are the same,
i.e., if
XtðÞhi ¼ EX tðÞ½/C138
in the mean square sense.3
Since the time average of X(t) does not depend on t, a necessary condition for mean ergodicity is
forE[X(t)] to be a constant. Generally speaking, a random process must be stationary in order to be
ergodic, but this is not always sufﬁcient.
Example 7.19 LetX(t)¼A0cos(ω0t+Θ), where Θ~ Unif( /C0π,π] and ω06¼0. We found in
Example 7.12 that E[X(t)]¼0. Now consider the time average:
XtðÞhiT¼A0cosω0tþΘ ðÞ hiT
¼1
2TðT
/C0TA0cosω0tþΘ ðÞ dt¼A0
2ω0Tsinω0TþΘ ðÞ /C0 sin/C0ω0TþΘ ðÞ ½/C138
Since the term in brackets is bounded no matter the value of Θ, the factor of Tin the denominator
implies that hX(t)iT!0a s T!1 , whence hX(t)i¼0. Therefore, by deﬁnition, X(t) is mean
ergodic. ■
Example 7.20 Consider a random dc signal, X(t)¼X. That is, any particular sample function of the
random process is some constant x, but that constant varies from realization to realization. This is
trivially a stationary process; in particular, E[X(t)]¼E[X]¼μX, a constant. However, the time
average of Xon [/C0T, T] is just X,s ohX(t)i¼X, which is not a constant but rather a random variable.
Therefore, hX(t)i6¼E[X(t)], and X(t) is not mean ergodic. (To be precise, the time and ensemble
averages would be equal in the mean-square sense if we had E[(X/C0μX)2]¼0, i.e., if Xhad zero
variance.)
3Ar v Yequals a constant cin the mean square sense ifE[(Y/C0c)2]¼0. This is equivalent to requiring that E(Y)¼c
and Var( Y)¼0. In the deﬁnition above, Y¼hX(t)iis the rv and c¼E[X(t)] is the constant.512 7 Random Processes
This matches with intuition: if the level of the dc signal Xvaries across different realizations of the
process, then a single realization, X(t)¼x, cannot tell us anything about the statistical behavior of the
signal. ■
The preceding example indicates the most common situation wherein a random process is
stationary but not ergodic: some element of the process is random but not time-varying. See Example
7.21 below, as well as Exercise 36, for other such examples.
Similar to the deﬁnition of time average, we may deﬁne the time autocorrelation of a random
process by
XtðÞXtþτðÞ hi ¼lim
T!1XtðÞXtþτðÞ hiT¼lim
T!11
2TðT
/C0TXtðÞXtþτðÞ dt
X(t) is said to be autocorrelation ergodic ifhX(t)X(t+τ)i¼E[X(t)X(t+τ)] in the mean square
sense. Since the time autocorrelation does not depend on t, a random process can only be autocorre-
lation ergodic if its autocorrelation function RXX(t,t+τ) is also free of t(the second condition for
wide-sense stationarity).
Example 7.21 (Example 7.19 continued) It follows from Examples 7.12 and 7.14 that the random
process X(t)¼A0cos(ω0t+Θ) has autocorrelation function RXX(τ)¼(A02/2)cos( ω0τ). Applying an
appropriate trig identity, we can also ﬁnd its time autocorrelation:
XtðÞXtþτðÞ hiT¼1
2TðT
/C0TA0cosω0tðÞ /C1 A0cosω0tþτðÞðÞ dt
¼A2
0
2TðT
/C0T1
2cosω0τðÞ þ cosω02tþτ ðÞðÞ ½/C138 dt
¼A2
0
2T/C11
2cosω0τðÞ/C2
T/C0ð /C0 TÞ/C3
þA2
0
4TðT
/C0Tcosω02tþτ ðÞðÞ dt
¼A2
0
2cosω0τðÞ þA2
0
8ω0T/C2
cosðω0ð2TþτÞÞ /C0 cosðω0ð/C02TþτÞÞ/C3
Taking the limit as T!1 , the second term above goes to zero, and we have hX(t)X(t+τ)i¼
(A02/2)cos( ω0τ), the same as the autocorrelation function of X(t). Hence, X(t) is autocorrelation
ergodic.
However, suppose we replace the constant amplitude A0with a random amplitude A, i.e., a random
variable not varying with time. Then calculations similar to those in Example 7.12 and above show
that
RXXτðÞ¼EA2/C2/C3
2cosω0τðÞ while XtðÞXtþτðÞ hi ¼A2
2cosω0τðÞ
These are notequal—that is, now X(t)i snotautocorrelation ergodic—unless it happens that
E[A2]¼A2in the mean square sense, which is true iff Var( A)¼0. ■7.3 Stationary and Wide-Sense Stationary Processes 513
7.3.3 Exercises: Section 7.3(25–40)
25. Deﬁne a random process X(t)¼V+A0cos(ω0t+Θ), where VandΘare independent random
variables; Θ~ Unif( /C0π,π]; and Vhas mean μVand variance σV2. (That is, X(t) models a signal
with both phase and dc variation.)
(a) Find the mean function of X(t).
(b) Find the autocorrelation function of X(t).
(c) Is X(t) wide-sense stationary?
26. Deﬁne a random process X(t)¼Acos(ω0t+Θ), where AandΘare independent random
variables; Θ~ Unif( /C0π,π]; and Ahas mean μAand variance σA2. (That is, X(t) models a signal
with both phase and amplitude variation.)
(a) Find the mean function of X(t).
(b) Find the autocorrelation function of X(t).
(c) Is X(t) wide-sense stationary?
27. Determine whether each of the following functions could potentially be the autocovariance
function of a WSS random process.
(a) cos( τ)
(b) sin( τ)
(c) 1 =ð1þτ2Þ
(d) e/C0|τ|
28. Determine whether each of the following functions could potentially be the autocovariance
function of a WSS random process.
(a) e/C0|τ+1|
(b) τ2
(c) tri( τ), deﬁned by tri( τ)¼1/C0|τ| for | τ|/C201 and 0 otherwise
(d) sinc( τ), deﬁned by sinc(0) ¼1 and sinc( τ)¼sin(πτ)/(πτ) for τ6¼0
29. Let AandBbe iid rvs, and deﬁne a random process by X(t)¼Acos(ω0t)+Bsin(ω0t). IsX(t)
necessarily WSS? Why or why not?
30. Deﬁne X(t)¼At + B , where AandBare independent, A~ Unif[ /C03, 3], and B~ Unif[ /C010, 10].
(a) Find the mean function of X(t).
(b) On the basis of (a), can you determine whether X(t) is WSS? If so, what is your
determination?
(c) Find the variance function of X(t).
(d) On the basis of (c), can you determine whether X(t) is WSS? If so, what is your
determination?
31. Let A(t) and B(t) be jointly wide-sense stationary random processes, and deﬁne a new process by
X(t)¼A(t)+B(t). Find the mean and autocovariance functions of X(t). IsX(t) WSS?
32. Let A(t) and B(t) be jointly wide-sense stationary random processes, and deﬁne a pair of new
processes by
XtðÞ¼ AðtÞþBðtÞ
YtðÞ¼ AðtÞ/C0BðtÞ
AreX(t) and Y(t) jointly wide-sense stationary?
33. A WSS process Y(t) has mean /C07 and autocovariance function CYY(τ)¼50cos(100 πτ)+
8cos(600 πτ).
(a) Does Y(t) have any periodic components? How can you tell?
(b) Find Cov( Y(0),Y(0.01)).514 7 Random Processes
(c) Find the autocorrelation function of Y(t).
(d) Find the mean square value of Y(t).
(e) Find the variance of Y(t).
34. A wide-sense stationary process X(t) has autocorrelation function RXX(τ)¼60 + 125 e/C0|τ|/100.
(a) Does X(t) have any periodic components? How can you tell?
(b) Find the mean square value of X(t).
(c) Find the mean of X(t), if possible.
(d) Find the autocovariance function of X(t).
(e) Find Cov( X(10), X(15)).
(f) Find the standard deviation of X(t).
35. Consider the random process X(t)¼Acos(ω0t)+Bsin(ω0t), where AandBare iid random
variables with mean zero. In Example 7.15, we showed that X(t) is wide-sense stationary. Is X(t)
mean ergodic?
36. Let X(t)¼A/C1Y(t), where Ais a random variable and Y(t) is an ergodic, WSS random process
independent of A.
(a) Find the mean and autocorrelation of X(t) in terms of the properties of AandY(t). Is X(t)
WSS?
(b) Show that the autocovariance function of X(t) is given by CXX(τ)¼E(A2)CYY(τ)+σA2μY2.
(c) Find the time average of X(t). IsX(t) mean ergodic?
(d) Assume Y(t) has no periodic component, so its autocovariance function goes to 0 as
|τ|!1 . Does the same hold true for the autocovariance function of X(t)? Why is this
not a violation of property 5 of WSS processes?
37. Recall from Chap. 4that the correlation coefﬁcient of two random variables XandYis given by
ρ(X, Y)¼Cov( X, Y)/σXσY. For a WSS random process X(t), ﬁnd the correlation coefﬁcient of X(t)
andX(t+τ) in terms of the autocovariance function CXX(τ).
38. Let X(t) be a WSS random process.
(a) Prove that the autocovariance function CXX(τ) satisﬁes jCXX(τ)j/C20CXX(0). [ Hint: Use the
previous exercise.]
(b) Prove that the autocovariance function RXX(τ) satisﬁes jRXX(τ)j/C20RXX(0).
39. Let X(t) and Y(t) be jointly wide-sense stationary. Show that RXY(τ)¼RYX(/C0τ) and
CXY(τ)¼CYX(/C0τ).
40. Let X(t) be a WSS random process.
(a) Show that for any constants a1,...,anand any time points t1,...,tn,
Vara1Xt1ðÞ þ /C1 /C1 /C1 þ anXtnðÞ ðÞ ¼Xn
j¼1Xn
k¼1ajakCXXtj/C0tk/C0/C1
(b) Explain why a valid autocovariance function must be positive semi-deﬁnite , i.e., CXX(τ)
must satisfyXn
j¼1Xn
k¼1ajakCXXtj/C0tk/C0/C1
/C210 for all constants a1,...,anand times t1,...,tn.
(c) Now consider the “rectangular function” deﬁned by
rect τðÞ¼1/C12/C12τ/C12/C12/C201=2
0 otherwise(
Show that rect( τ) satisﬁes the properties of an autocovariance function expressed in the main
proposition of this section.7.3 Stationary and Wide-Sense Stationary Processes 515
(d) By considering n¼3,a1¼a3¼1,a2¼/C01,t1¼/C0.3,t2¼0,t3¼.3, show that rect( τ)
is not positive semi-deﬁnite and, hence, cannot be the autocovariance function of any WSS
random process.
[Note : It can be shown that positive semi-deﬁniteness completely characterizes the collection of all
valid autocovariance functions. That is, any valid autocovariance function is automatically positive
semi-deﬁnite, as shown in part (b), and for any positive semi-deﬁnite function Cthere exists a WSS
random process whose autocovariance function is C(τ).]
7.4 Discrete-Time Random Processes
Much of what we have discussed in the previous two sections applies equally to discrete-time random
processes (aka random sequences). After reviewing deﬁnitions and properties for the discrete-time
case, we introduce a few speciﬁc examples of important discrete-time models.
Recall from Sect. 7.1that a random sequence is simply a list of random variables X1,X2, and so on;
we write Xnfor the general term. (We may also deﬁne a sequence indexed by the entire set of integers:
...,X/C02,X/C01,X0,X1,X2,....) The subscript takes the place of the time index tfrom earlier. The
sequence can also be written as X[1],X[2], ...orX[n] to mirror the continuous-time notation; the
square brackets will remind us that we’re working on a discrete-time scale.
Example 7.22 Let’s return to the value of Apple Inc. stock, but now consider only recording the
closing price at the end of each trading day (starting, say, January 2 of next year). If we deﬁne Xnto be
the closing price of Apple stock on the nth recorded day, then we can model X1,X2,...as a random
sequence. Figure 7.13 shows one possible sample function of this random sequence, assuming the
closing price just prior to our day 1 was $580. In effect, we have converted a continuous-time process
(analog) into a discrete-time process (digital) by sampling the process from Example 7.2 at
designated times.
0 5 10 15 20 25 30 35 40 45 50570580590600610620630640
tx(t)
Fig. 7.13 A sample function of the random sequence of Example 7.22 ■516 7 Random Processes
For any random sequence, we can deﬁne several statistical functions for times n¼1, 2, 3, ...as
follows:
 Mean function: μX[n]¼E[Xn]
 Variance function: σX2[n]¼Var(Xn)
 Standard deviation function: σXn½/C138 ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VarXnðÞp
 Autocovariance function: CXX[n, m]¼Cov( Xn,Xm)
 Autocorrelation function RXX[n, m]¼E[XnXm]
The relationships between these functions established in the continuous-time case still hold, e.g.,
σX2[n]¼CXX[n,n], and CXX[n, m]¼RXX[n, m]/C0μX[n]μX[m].
A random sequence is (strict-sense) stationary if the joint distribution of X[n1],...,X[nr] equals
the joint distribution of X[n1+k],...,X[nr+k] for any indices n1,...,nrand any integer k. A random
sequence is wide-sense stationary if (1) μX[n] is a constant, μX, and (2) CXX[n, m] depends only on the
difference m/C0n; if we call this difference k, we may then denote the autocovariance function as
CXX[k]. As was true for continuous-time processes, we may make an equivalent deﬁnition regarding
the mean and autocorrelation functions.
Example 7.23 Any Markov chain from Chap. 6is an example of a random sequence, provided the
states for the chain are truly quantitative (e.g., counts or dollar amounts and not indicators for
locations). Figure 7.14 shows two sample functions for the Gambler’s Ruin chain used repeatedly
in Chap. 6, assuming Allan’s initial stake is $3, Beth’s is $2, and p¼.5; the connecting line segments
are just to help distinguish the two iterations. For each nonnegative index n,Xnequals the amount of
money held by Allan after ngames have been played.
The mean function μX[n] is the mean value of Allan’s fortune after ngames have been played. For
example, with an initial stake of $3 ( X0¼3),X1equals $2 or $4 with probability .5 each, so
μX[1]¼$2(.5) + $4(.5) ¼$3. By considering the outcomes of the ﬁrst two games, we ﬁnd X2to
be $1, $3, or $5 with probabilities .25, .5, and .25, respectively; this gives μX[2]¼$3 as well. In fact,
it turns out that μX[n]¼$3 for all nunder the speciﬁed conditions, even though the probability0 1 2 3 4 5 6 7 8 9 10012345xn
nFig. 7.14 Two sample
functions of a Gambler’s
Ruin random sequence7.4 Discrete-Time Random Processes 517
distribution of Xnchanges with n. (In particular, the distribution of Xnconverges to p(0)¼.4 and
p(5)¼.6, i.e., Allan’s long run chance of winning all $5 at stake is 60%.)
Similarly, we can compute the variance function of Xn; using the distributions described in
the previous paragraph, it’s straightforward to calculate that σX2[0]¼0,σX2[1]¼1,σX2[2]¼2, and
σX2[n]!6a s n!1 . That the variance function is not constant is sufﬁcient to show that the
Gambler’s Ruin Markov chain is nota WSS random sequence. ■
7.4.1 Special Discrete Sequences
Perhaps the simplest type of random sequence is the Bernoulli random sequence : letX1,X2,...be
iid, with each Xnfollowing a Bernoulli( p) distribution. A sample function of a Bernoulli random
sequence with p¼.6 appears in Fig. 7.15. By grace of the variables being iid, a Bernoulli random
sequence is trivially (strict-sense) stationary; in particular, we have
μXn½/C138 ¼ EX n½/C138 ¼ p,σ2
Xn½/C138 ¼ VarXnðÞ ¼ p1/C0pðÞ , and
CXXn;m½/C138 ¼ Cov Xn;Xm ðÞ ¼p1/C0pðÞ m¼n
0 m6¼n/C26
A more general iid sequence will also be strict-sense stationary, but the formulas for the mean and
variance will, of course, depend on the underlying common distribution of the Xn.
From an iid sequence, we can construct a much more interesting random sequence as follows:
deﬁne S1¼X1,S2¼X1+X2, and so on, so that Sn¼Sn–1+Xn¼Pn
i¼1Xifor all n/C212. The
resulting sequence of partial sums S1,S2,S3,...is called a random walk . For example, from
Chap. 2we know the sum of iid Bernoulli rvs is binomial; hence, if Xnrepresents a Bernoulli random
sequence, then the corresponding random walk Snhas a Bin( n, p) distribution for each n.
We can use the properties of iid sums to derive some general properties of random walks.0 2 4 6 8 10 12 14 16 18 2001xn
nFig. 7.15 A sample
function of a Bernoulli
random sequence518 7 Random Processes
PROPOSITION
LetX1,X2,...be an iid sequence with common mean μXand common variance σX2. Let Snbe the
associated random walk, i.e., Sn¼X1+/C1/C1/C1+Xnfor every n.T h e n
1.μS[n]¼E[Sn]¼nμX
2.σ2
Sn½/C138 ¼ VarSnðÞ ¼ nσ2
X
3.CSSn;m½/C138 ¼ min n;mðÞ σ2
X
Proof The proofs of properties 1 and 2 were given in advance of the Central Limit Theorem
discussion in Sect. 4.5. To prove property 3, assume m>nand proceed as follows:
CSSn;m½/C138 ¼ CovðSn,SmÞ¼CovðX1þ/C1/C1/C1þ Xn,X1þ/C1/C1/C1þ XmÞ
¼Cov X1þ/C1/C1/C1þ Xn,X1þ/C1/C1/C1þ XnþXnþ1þ/C1/C1/C1þ Xm ðÞ
¼Cov X1þ/C1/C1/C1þ Xn,X1þ/C1/C1/C1þ Xn ðÞ þ CovðX1þ/C1/C1/C1þ Xn,Xnþ1þ/C1/C1/C1þ XmÞ
¼Cov Sn;SnðÞ þ CovðX1þ/C1/C1/C1þ Xn,Xnþ1þ/C1/C1/C1þ XmÞ
In the third line, we have used the distributive property of covariance. The ﬁrst term, Cov( Sn,Sn), is
simply Var( Sn) (the covariance of Snwith itself). In the second term, the Xis(i¼1t on) in the ﬁrst
argument are independent of the Xjs(j¼n+1 t o m) in the second argument; therefore, that
covariance equals zero. Thus, we have CSS[n,m]¼Var(Sn)¼nσX2.
This holds for m>n;i fm<n, the same argument yields CSS[n,m]¼mσX2. Therefore, for
general nandmwe may write CSS[n,m]¼min(n,m)σX2. ■
Example 7.24 LetX1,X2,...be iid, with each Xibeing +1 or /C01 with equal probability. The
resulting random walk Snis called the simple symmetric random walk in one dimension . Since each
“step” Xihas mean 0 and variance 1, it follows that μS[n]¼n(0)¼0 and σS2[n]¼n(1)¼n. Several
sample functions of Snare shown in Fig. 7.16; the connecting line segments are just to help
distinguish different iterations. Notice in Fig. 7.16b that the variability in Snincreases with n, but
not linearly; this corresponds to the fact that SD SnðÞ ¼ﬃﬃﬃnp. We also see, especially for larger n, that
values of the ensemble of Snare more concentrated near 0 and sparser at the edges. This is a
consequence of the Central Limit Theorem: since the Xis are iid, their sum Snbecomes increasingly
normal as nincreases. (In fact, this is true for any random walk.)
−101ab
−101xn xn
nn
Fig. 7.16 Simple symmetric random walks: ( a) the ﬁrst 30 steps for three sample functions; ( b) the ﬁrst 200 steps for
100 sample functions ■7.4 Discrete-Time Random Processes 519
7.4.2 Exercises: Section 7.4(41–52)
41. Let Tndenote the high temperature (/C14F) in Sacramento, CA on the nth day of the year. Consider
the following model:
Tn¼75þ25sin2π
365n/C0150ðÞ/C18/C19
þ4εn,
where the εns are a sequence of iid N(0, 1) rvs.
(a) Determine the probability that the high temperature on February 28 exceeds 60/C14F.
(b) Find E[Tn].
(c) Find CTT[n, m].
(d) Is Tna WSS random sequence? Should it be?
42. The output of a certain ampliﬁer, sampled every second, has the form
Xn½/C138 ¼ A0sinω0nðÞ þ Zn½/C138,
where the noise component Z[n] is a sequence of iid N(0,σ) rvs for some σ>0.
(a) Find the mean function of X[n].
(b) Find the autocovariance function of X[n].
(c) Is X[n] wide-sense stationary?
43. A gambler plays roulette, betting $5 on black every time (so, she has probability 18/38 of winning
on any particular spin). The gambler receives $5 for each win and gives up $5 for each loss.
(a) Deﬁne a random sequence Sn¼the number of games this gambler has won after nspins.
Find the mean, variance, autocovariance, and autocorrelation function of Sn.
(b) Deﬁne a random sequence Yn¼the amount of money this gambler has won after nspins.
Find the mean, variance, autocovariance, and autocorrelation function of Yn.
(c) What is the probability the gambler is “ahead” after 10 spins (i.e., Y10>0)?
44. Gravel is being loaded onto rail cars by a dump truck for long-distance transport. Let Xnequal the
amount of gravel (in tons) emptied onto the rail car by the nth dump truck run, and assume the Xn
are iid Unif[15, 17].
(a) Deﬁne Sn¼X1+/C1/C1/C1+Xn. Interpret Snin this context.
(b) Find the mean, variance, autocorrelation, and autocovariance functions of Sn.
(c) Use the Central Limit Theorem to approximate both the distribution of S6andP(S6/C21100),
the chance the dump truck will be able to ﬁll a 100-ton rail car in 6 runs.
45. Let X(t) be a WSS random process. For some ﬁxed Ts>0 deﬁne X[n]¼X(nTs), so that X[n]i sa
“sampled” version of X(t). Show that the random sequence X[n] is also WSS.
46. A subsample of a random sequence X[n] is obtained by observing every kth element of
the sequence, for some integer k>1. The resulting random sequence, Y[n], is given by
Y[n]¼X[kn].
(a) Find the mean and autocorrelation functions of Y[n] in terms of those of X[n].
(b) If X[n] is WSS, is the subsample Y[n] also WSS?
47. Let Xnbe a WSS random sequence, and deﬁne a simple moving average sequence Ynby
Yn¼XnþXn/C01
2
(a) Find the mean function of Yn.
(b) Find the autocovariance function of Yn.520 7 Random Processes
(c) Is Ynwide-sense stationary?
(d) Find the variance function of Yn.
48. Let Xnbe a sequence of iid random variables, and consider a new random sequence Yngiven by
Yn¼1
2Xnþ1
4Xn/C01þ1
8Xn/C02
IsYnWSS?
49. Let the random sequence ...,X/C02,X/C01,X0,X1,X2,...be iid, with mean μand variance σ2. Deﬁne
aﬁrst-order autoregressive sequence Ynby
Yn¼αYn/C01þXn,
where | α|<1.
(a) Show that, for N>0,Yn¼αNYn/C0N+PN/C01
i¼0αiXn/C0i.
(b) Let N!1 in (a) to conclude that Yn¼P1
i¼0αiXn/C0i.
(c) Find the mean function of Yn.
(d) Find the autocovariance function of Yn.
(e) Is Ynwide-sense stationary?
(f) Find the correlation coefﬁcient ρ(Yn,Yn+k).
50.Correlated bit noise . Let Xnbe a sequence of random bits (0s and 1s) constructed as follows:
X0¼0 or 1 with probability .5 each. For integers n/C211,Xn¼Xn/C01with probability .9 and
Xn¼1/C0Xn/C01with probability .1. (In the language of Chap. 6, this is a Markov chain with a
symmetric transition matrix.)
(a) Find the pmf of X1, and argue that this is also the pmf of Xnfor all n/C211.
(b) Is Xna WSS sequence?
(c) Find the mean and variance functions of Xn.
(d) It can be shown, using techniques from Chap. 6, that for k/C210
PX nþk¼1/C12/C12Xn¼1/C0/C1
¼1þ:8k
2
Use this to ﬁnd RXX[k] and CXX[k].
51. Let X[n] be a random sequence whose time index nranges across all integers ...,/C02,/C01, 0, 1,
2, .... Similar to the continuous-time case, the time average ofX[n] over { /C0N,...,0 , ...,N}i s
deﬁned by
Xn½/C138hiN¼1
2Nþ1XN
n¼/C0NXn½/C138,
which is just the arithmetic mean of X[/C0N],...,X[N]. The (overall) time average of X[n] is then
deﬁned as a limit: hX[n]i¼limN!1hX[n]iN. Assume X[n] is a WSS random sequence with
mean μXand autocovariance function CXX[k].
(a) Show that, for all integers N/C210,E[hX[n]iN]¼μX.
(b) Show that Var Xn½/C138hiN/C0/C1
¼1
2Nþ1X2N
k¼/C02NCXXk½/C1381/C0kjj
2Nþ1/C18/C19
.[Hint: Use the relationship
Var(Y)¼Cov( Y, Y) and the distributive property of covariance to create a double sum (with
indices mandn, say). Then make the change of variable k¼m/C0nand rearrange the terms
to create a single sum.]7.4 Discrete-Time Random Processes 521
52. Refer back to the previous exercise. A WSS random sequence X[n] is called mean ergodic if its
time average hX[n]iNconverges to μXasN!1 , in the sense that
lim
n!1EX n ½/C138hiN/C0μX/C0/C12hi
¼0
(a) Use the previous exercise to show that X[n] is mean ergodic if1
2Nþ1X2N
k¼/C02NCXXk½/C138 ! 0a s
N!1 .
(b) Let X[n] be WSS with autocovariance function CXX[k]¼α/C1ρ|k|for some α>0 and
|ρ|<1. Show that X[n] is mean ergodic.
7.5 Poisson Processes
In Sect. 2.5we indicated that, under rather general conditions, the Poisson distribution furnishes a
probability model for the number of events of some sort (logins to a server, arrivals of radioactive
pulses, ﬂaws on the surface of a wafer, etc.) that occur in some ﬁxed interval of time or region of
space. We now present a more formal development of conditions that lead to the Poisson distribution
in such contexts, and then explore properties of this event process.
DEFINITION
Consider the experiment of observing randomly occurring events of some type continuously
over time. Deﬁne X(0)¼0, and deﬁne X(t) for t>0 to be the number of events that occur in
the time interval (0, t].X(t) is called a Poisson (counting) process if it satisﬁes the following
two conditions:
1. The numbers of events occurring in nonoverlapping time intervals are independent.
2. There exists a parameter λ>0, called the rate of the process, such that the number of events
occurring in any interval of length τhas a Poisson distribution with mean λτ.
Later in this section, we present an alternative deﬁnition of a Poisson counting process which does
not explicitly assume that the event count follows a Poisson distribution.
Condition 1 states that a Poisson counting process X(t) has independent increments :i f(s1,t1] and
(s2,t2] are two time intervals with t1/C20s2, so that the intervals do not overlap, then the number of
events that occur in the ﬁrst interval is independent of the number of events that occur in the second
interval. That is, the “increment” X(t1)/C0X(s1) is independent of X(t2)/C0X(s2).
Condition 2 states that for any t>0 and τ>0, the number of events in the interval ( t, t + τ] has a
Poisson distribution with mean λτ. Since this count is represented by X(t+τ)/C0X(t), we may write
X(t+τ)/C0X(t) ~ Poisson( λτ). Since the distribution of this “increment” depends only on τand not t,
we say that a Poisson counting process has stationary increments . By substituting t¼0 and
τ¼tinto this expression, we have that X(t) ~ Poisson( λt), so that at each time tthe process itself
has a Poisson distribution. It follows that
μXtðÞ¼ λtand σ2
XtðÞ¼ λtsince mean and variance are equal for PoissonðÞ
A graph of a sample function of X(t) appears in Fig. 7.17. It is clear both from Fig. 7.17 and the
formulas above that a Poisson counting process is notstationary.522 7 Random Processes
Let us now derive the autocovariance function of X(t). We will rely on a clever trick; namely, for
t<swe will split the interval (0, s] into the smaller intervals (0, t] and ( t,s]. A similar “trick” was
used for a random walk in the previous section. Begin as follows:
CXXt;sðÞ ¼ Cov XtðÞ,XsðÞ ðÞ ¼ Cov XtðÞ,XtðÞþ XsðÞ /C0 XtðÞ ½/C138 ðÞ
¼Cov XtðÞ,XtðÞ ðÞ þ Cov XtðÞ,XsðÞ /C0 XtðÞ ðÞ
In the second argument of the covariance function, we have separated X(s), the count of the
number of events in (0, s], into two pieces: X(t), the number of events in (0, t], and X(s)/C0X(t), which
represents the number of events in the interval ( t,s]. Now, we simplify: the ﬁrst term, Cov( X(t),X(t)),
is simply Var( X(t)); the second term, thanks to Condition 1, represents the covariance of two
independent counts (since the intervals (0, t] and ( t,s] don’t overlap). Thus, the second term is
zero, and we have CXX(t,s)¼Var(X(t)) + 0 ¼λtfort<s.
Ifs<t, we can use the same argument to ﬁnd the covariance equals λs; therefore, the general
expression for the autocovariance function of X(t)i s
CXXt;sðÞ ¼ λ/C1min t;sðÞ
Example 7.25 Queries to a certain data warehouse occur randomly throughout the day. On average,
0.8 queries arrive per second during regular business hours. Assume a Poisson process model is
applicable here.
First, consider the number of queries in the ﬁrst ﬁve seconds, X(5). The rv X(5) is Poisson
with mean λt¼0.8(5) ¼4. Thus, for example, the chance of exactly three queries in the ﬁrst ﬁve
seconds is
PX 5ðÞ ¼ 3 ðÞ ¼e/C0443
3!¼:195
Next, let’s ﬁnd the probability of exactly 1 query in the ﬁrst second and exactly 2 queries in the
four seconds thereafter, which requires the independent increments property: the number of queries in
the ﬁrst second and the number of queries in the four seconds thereafter are independent. More
formally, X(1) and X(5)/C0X(1) are independent Poisson random variables with means 0.8(1) ¼0.8
and 0.8(4) ¼3.2, respectively. Hence,0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5024681012
tx(t) Fig. 7.17 A sample
function of a Poisson
(counting) process7.5 Poisson Processes 523
PX 1ðÞ ¼ 1\X5ðÞ /C0 X1ðÞ ¼ 2 ðÞ ¼ PX 1ðÞ ¼ 1 ðÞ /C1 PX 5ðÞ /C0 X1ðÞ ¼ 2 ðÞ
¼e/C00:80:81
1!/C1e/C03:23:22
2!¼:075
Finally, consider the random variables X(10) and X(30). These two rvs are notindependent: the
time intervals (0, 10] and (0, 30] overlap. In fact, it should be obvious that X(30) depends upon X(10),
since X(30) counts the number of queries in the ﬁrst 10 s, X(10), plus the number of additional queries
in the 20 s thereafter. Intuition suggests that the two random variables are positively correlated, and
we verify this now.
The mean of X(10) is 0.8(10) ¼8; since X(10) is Poisson, its standard deviation is thenﬃﬃﬃ
8p
.
Similarly, E[X(30)] ¼0.8(30) ¼24 and SD X30ðÞðÞ ¼ﬃﬃﬃﬃﬃ
24p
. We can ﬁnd the covariance of X(10) and
X(30) through the autocovariance function above:
Cov X10ðÞ,X30ðÞ ðÞ ¼ CXX10;30ðÞ ¼ λ/C1min 10 ;30ðÞ ¼ 0:81 0ðÞ ¼ 8
Finally, the correlation coefﬁcient of X(10) and X(30) is
Corr X10ðÞ,X30ðÞ ðÞ ¼Cov X10ðÞ,X30ðÞ ðÞ
SDX10ðÞðÞ SDX30ðÞðÞ¼8ﬃﬃﬃ
8pﬃﬃﬃﬃﬃ
24p¼1ﬃﬃﬃ
3p/C25:577
We’re not surprised to ﬁnd a moderate, positive relationship between these two variables. As you
might guess, the correlation coefﬁcient will be largest when the two time intervals (here, (0, 10] and
(0, 30]) overlap the most; if the time intervals of two increments only overlap to a very small degree,
the resulting correlation coefﬁcient will likewise be small (but still positive). ■
7.5.1 Relation to Exponential and Gamma Distributions
Because the events in a Poisson process occur “at random,” there is a second type of random variable
we may wish to model: the time between events . Consider Fig. 7.18, which illustrates a Poisson
process: the symbols along the time axis indicate the occurrences of events. Along the time axis, we
have indicated several random variables: T1¼the time until the ﬁrst event occurs, measured from
t¼0;T2¼the time between the ﬁrst and second events; T3¼the time between the second and third
events; and so on. These random variables T1,T2,...are called the interarrival times of the process.
Unlike the Poisson count of events, which is discrete, each of these random time lengths is a
continuous random variable. Thanks to the following theorem, their probability distribution is known.
t
T1 T2 T3 T4 T5 ……XX X X X
Fig. 7.18 Interarrival times in a Poisson process524 7 Random Processes
THEOREM
Suppose events occur in accordance with the conditions of a Poisson counting process. Deﬁne
T1¼the time until the ﬁrst event occurs and, for n/C212,Tn¼the time between the occurrence
of the ( n/C01)th and nth events. Also, deﬁne Yn¼the time until the nth event occurs, starting at
t¼0. Then
1.T1,T2,...are independent exponential random variables with parameter λ(mean 1/ λ) and
2.Ynis a gamma random variable with parameters α¼nand β¼1/λ(aka the Erlang
distribution).
Proof Since the time intervals spanned by T1,T2, etc. do not overlap, the Tnare independent by
condition 1 of a Poisson process (independent increments). To ﬁnd the distribution of T1, start with its
cdf: for t>0,
FT1tðÞ¼ PðT1/C20tÞ¼1/C0PðT1>tÞ
¼1/C0Pðno events occur in the time interval 0, t/C3/C0/C1
¼1/C0PðXtðÞ¼ 0Þ where XðtÞ/C24Poisson ðλtÞ
¼1/C0e/C0λtλtðÞ0
0!¼1/C0e/C0λt)
fT1tðÞ¼d
dtFT1ðtÞ¼d
dt/C2
1/C0e/C0λt/C3
¼λe/C0λt
This is the exponential( λ) pdf, as claimed. The distribution of T2is also exponential( λ) because,
thanks to Condition 1 of the deﬁnition, we may “restart the clock” when the ﬁrst event occurs and
derive the pdf of T2in the exact same manner as above. Propagating this idea forward, we have that
Tn~ exponential( λ) for all n.
As for Yn, notice we may write Yn¼T1+/C1/C1/C1+Tn, which implies Ynis the sum of niid
exponential( λ) rvs. Exercise 65 in Sect. 4.3showed, using moment-generating functions, that the
sum of niid exponential( λ) rvs has a gamma( n,1 /λ) distribution. ■
Exercise 72 offers a direct proof of Statement 2 of the preceding theorem, without relying on
moment-generating functions or Statement 1.
Example 7.26 Consider again the database queries described in Example 7.25. Rather than investi-
gate the number of queries in preset time intervals, let’s look at the random arrival times themselves.
The average time between successive queries can actually be deduced without the preceding theorem:
if queries arrive at 0.8 queries/second on average, then the mean time between queries is clearly just
the reciprocal: 1/0.8 ¼1.25 s. If we let T¼the time between queries, we now know that
T/C24exponential(0.8), whence E(T)¼1/λ¼1/0.8 ¼1.25 s, and SD( T)¼1/λ¼1.25 s as well
(remember that an exponential random variable has identical mean and standard deviation).
Next, let Y50¼time to the 50th query, starting at the beginning of regular business hours. The
preceding theorem tells us Y50~ gamma(50, 1/0.8), so E(Y50)¼50(1/0.8) ¼62.5. We expect the
50th query to arrive 62.5 s into regular business hours. The arrival time of the 50th query has a
standard deviation of SD Y50ðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃ
αβ2p
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
50 1=0:8ðÞ2q
¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
78:125p
¼8:84s.7.5 Poisson Processes 525
If 50 or more queries arrive within the ﬁrst minute, system users will experience a signiﬁcant
backlog in subsequent minutes because of processing time. What is the probability this happens?
A backlog occurs iff Y50/C201 min ¼60 s. The probability of this event, evaluated using software, is
PY 50/C2060 ðÞ ¼ð60
01
50/C01 ðÞ !1=0:8ðÞ50x50/C01e/C00:8xdx¼/C1/C1/C1¼ :4054
Alternatively, return to the original Poisson process: a backlog occurs iff X(60), the number of
queries in the ﬁrst 60 s, is 50 or more. Since X(60) has a Poisson distribution with mean 60(0.8) ¼48,
PX 60ðÞ/C2150 ðÞ ¼1/C0PX 60ðÞ/C2049 ðÞ ¼1/C0X49
x¼0e/C04848x
x!¼1/C0:5946¼:4054 ■
We have described a Poisson process as modeling the count of events that occur “at random”
across time. This notion can actually be made more precise: in a Poisson process, given that an event
has occurred by time t 0, it is equally likely to have occurred anywhere in the interval (0,t0]. To see
this, suppose we know that exactly one event has occurred by time t0,s oX(t0)¼1. Conditional on
that knowledge, let’s ﬁnd the distribution of the random variable T1¼arrival time of this event.
Begin with the (conditional) cdf: for t/C20t0,
PðT1/C20t/C12/C12Xt0ðÞ ¼ 1Þ¼PT 1/C20t\Xt0ðÞ ¼ 1 ðÞ
PXt 0ðÞ ¼ 1 ðÞ
¼Pð1 event occurred in ð0,t/C3
and none in t,t0/C3/C0/C1
PXt 0ðÞ ¼ 1 ðÞ
¼PXtðÞ¼ 1\Xt0ðÞ /C0 XtðÞ¼ 0 ðÞ
PXt 0ðÞ ¼ 1 ðÞ
¼e/C0λtλtðÞ1
1!e/C0λt0/C0tðÞλt0/C0t ðÞðÞ0
0!
e/C0λt0λt0ðÞ1
1!¼/C1/C1/C1¼t
t0
Differentiating with respect to t, we ﬁnd the conditional distribution of T, given X(t0)¼1, is 1/ t0,
the uniform pdf on (0, t0].
Generalizing this argument, conditional on X(t0)¼n(i.e., on exactly nevents occurring in (0, t0])
each of the nevent occurrence times is uniformly distributed on (0, t0]. Moreover, the ntimes are
independent of one another. In light of this uniform distribution property, it is fair to say that for a
Poisson process, events really do occur “at random.”
7.5.2 Combining and Decomposing Poisson Processes
In Sect. 4.3, we showed that the Poisson distribution is additive , i.e., the sum of two independent
Poisson rvs is again Poisson distributed. This result immediately generalizes to Poisson counting
processes.526 7 Random Processes
PROPOSITION
LetX1(t) and X2(t) be independent Poisson processes with rate parameters λ1andλ2, respec-
tively. Deﬁne a new random process by X(t)¼X1(t)+X2(t). Then X(t) is also a Poisson
process, with rate parameter λ1+λ2. (This theorem can be extended to the sum of
kindependent Poisson processes for k>2 as well.)
Example 7.27 Two roads feed into the northbound lanes on the Anderson Street Bridge. During rush
hour, the number of vehicles arriving from the ﬁrst road can be modeled by a Poisson process with a
rate parameter of 10 per minute, while arrivals from the second road form an independent Poisson
process with rate 8 cars per minute. If we let X(t) denote the total number of cars entering the
northbound lanes, then X(t) is also a Poisson process, with rate parameter 10 + 8 ¼18 vehicles per
minute. Hence, the probability that a total of more than 100 vehicles will arrive via the two feeder
roads in the ﬁrst 5 min of rush hour is given by
PX 5ðÞ>100 ðÞ ¼ 1/C0PX 5ðÞ /C20 100 ðÞ ¼ 1/C0X100
x¼0e/C018 5ðÞ18 5ðÞ½/C138x
x!¼1/C0:865¼:135
This calculation is much simpler than considering all the possible ways the two individual Poisson
processes could total more than 100 (e.g., 55 vehicles on the ﬁrst road and 48 on the second road, and
so on). ■
The foregoing proposition and example show that we can combine separate Poisson processes into
a single Poisson process. Interestingly, we can also do the reverse: if we can categorize the events of a
Poisson process (e.g., arrivals of people separated into women’s arrivals and men’s arrivals), then we
candecompose the overall process into two “smaller” processes. We make this more precise in the
next proposition.
PROPOSITION
Suppose events occur according to the conditions of a Poisson process, and that each event can
be classiﬁed as either Type 1 or Type 2. Suppose that each event is Type 1 with probability p,
independent of the types of all other events and independent of the number of events that have
occurred. Deﬁne two random processes: X1(t)¼number of Type 1 events up to time t, and
X2(t)¼number of Type 2 events up to time t. Then
1.X1(t) is a Poisson process with rate parameter pλ;
2.X2(t) is a Poisson process with rate parameter (1 /C0p)λ; and
3.X1(t) and X2(t) are independent.
Proof We will derive the joint distribution of X1(t) and X2(t), i.e., P(X1(t)¼xandX2(t)¼y), for
arbitrary nonnegative integers xandy. The event { X1(t)¼xandX2(t)¼y} is equivalent to the event
XtðÞ¼ xþyand exactly xof these xþyevents are Type 1 fg ¼A\B
where X(t) denotes the overall Poisson process. The second part of this event follows a binomial
model: we have a ﬁxed number of trials ( x+y ), each with two basic outcomes (Type 1 or Type 2),
plus independent trials and constant probability by assumption. Combining that with the known
Poisson distribution of X(t) and the Multiplication Rule PðA\BÞ¼PðAÞPðB/C12/C12AÞgives7.5 Poisson Processes 527
P(X(t)¼x+y and exactly xof these x+yevents are Type 1)
¼e/C0λtλtðÞxþy
xþyðÞ !/C1xþy
x !
px1/C0pðÞy¼e/C0λtλtðÞxþypx1/C0pðÞy
x!y!
¼e/C0pþ1/C0pðÞ½/C138 λtλtðÞxþypx1/C0pðÞy
x!y!¼e/C0pλtpλtðÞx
x!e/C01/C0pðÞ λt1/C0pðÞ λt ðÞy
y!
We recognize these two functions as the pmfs of a Poisson( pλt) distribution and a Poisson
((1/C0p)λt) distribution, respectively. Moreover, since the joint pmf of X1(t) and X2(t) separates
into the product of individual pmfs, X1(t) and X2(t) are independent. ■
Example 7.28 At a certain large hospital, patients enter the emergency room at a mean rate of 15 per
hour. Suppose 20% of patients arrive in critical condition, i.e., they require immediate treatment.
Assume patient arrivals meet the conditions of a Poisson process.
Let’s ﬁrst ﬁnd the probability that more than 50 patients arrive in the next 4 h. Let X(t) denote the
Poisson process of patient arrivals (regardless of condition). Then X(4) has a Poisson distribution with
mean μ¼λt¼15(4) ¼60, so
PX 4ðÞ>50 ðÞ ¼ 1/C0PX 4ðÞ /C20 50 ðÞ ¼ 1/C0X50
x¼0e/C06060x
x!¼1/C0:108¼:892
Next, we ﬁnd the probability that more than 10 critical patients arrive in the next 4 h. Let X1(t)
denote the number of critical (“Type 1”) patients that arrive within thours. By the previous
proposition, X1(t) is a Poisson process with rate parameter pλ¼.20(15) ¼3, so X1(4) is Poisson
with mean 3(4) ¼12. Thus,
PX 14ðÞ>10 ðÞ ¼ 1/C0PX 14ðÞ /C20 10 ðÞ ¼ 1/C0X10
x¼0e/C01212x
x!¼1/C0:347¼:653
Finally, to ﬁnd the probability that more than 10 critical patients and more than 40 noncritical
patients arrive in the next 4 h, let X2(t) denote the number of noncritical (“Type 2”) patients that arrive
within thours. Then X2(t) is also a Poisson process, but with rate parameter (1 /C0.20)(15) ¼12.
Thus X2(4) ~ Poisson(48); moreover, X2(4) is independent of X1(4). Therefore,
PX 14ðÞ>10\X24ðÞ>40 ðÞ ¼ PX 14ðÞ>10 ðÞ /C1 PX 24ðÞ>40 ðÞ ¼ :653ðÞ :862ðÞ ¼ :563
The calculation of P(X2(4)>40) is similar to those displayed above. ■
7.5.3 Alternative Definition of a Poisson Process
The deﬁnition of a Poisson process at the beginning of this section almost seems a tautology, since we
said X(t) is a Poisson process if it has a Poisson distribution. The following theorem provides an
alternative way to deﬁne a Poisson counting process.528 7 Random Processes
THEOREM
Consider the experiment of observing randomly occurring events of some type along continu-
ous time. Deﬁne X(0)¼0, and deﬁne X(t) for t>0 to be the number of events that occur in the
time interval (0, t]. Suppose X(t) has the following properties:
1.X(t) has independent and stationary increments.
2. There exists λ>0 such that in any time interval of length h, the probability that exactly one
event occurs is λh+o(h).4
3. The probability of more than one event occurring in an interval of length hiso(h).
Then X(t) is a Poisson counting process with rate parameter λ.
Proof Because of the stationarity assumption, it sufﬁces to consider a time interval beginning at
time 0. Let Pk(t) denote the probability that exactly kevents occur in the interval [0, t]. First consider
P0(t+h), the probability that no events occur during the ﬁrst t+hunits of time. In order for this to
happen, no events must occur in [0, t] and also no events must occur during the next hunits of time.
Since these two time intervals are nonoverlapping, the number of events that occur in the ﬁrst interval
is independent of the number that occur in the second interval. Thus
P0tþhðÞ ¼ P0tðÞ/C1Pðno events in an interval of length hÞ
¼P0tðÞ/C1/C2
1/C0Pðexactly one event Þ/C0Pðat least two events Þ/C3
¼P0tðÞ/C1/C2
1/C0ðλhþoðhÞÞ /C0 oðhÞ/C3
¼P0tðÞ/C1 1/C0λh/C0ohð ½Þ /C0 oðhÞ/C3
¼P0tðÞ/C1 1/C0λhþohðÞ ½/C138
Rearranging this expression gives
P0tþhðÞ /C0 P0tðÞ
h¼/C0λP0tðÞþohðÞ
h
Now taking the limit as h!0 gives the derivative of P0(t):
P0
0tðÞ¼/C0 λP0tðÞ
This differential equation has the unique solution P0(t)¼ce/C0λt, where the constant cis determined
by the initial condition P0(0)¼1. This implies that c¼1 and thus that P0(t)¼e/C0λt, which sure
enough is the probability of no events when the distribution is Poisson with parameter λt.
Now consider Pk(t) for general k. In order to have kevents occur in the interval [0, t+h], it must
be the case that either (1) kevents occur in [0, t] and none in the next htime units, or (2) k/C01 events
occur in [0, t] and one occurs in the next htime units, or (3) For l/C212,k/C0loccur in [0, t] and loccur
in the next htime units. By condition 3 in the theorem, the probability of the event in (3) is o(h).
Writing Pk(t+h) as a sum of probabilities corresponding to cases (1), (2), and (3), rearranging,
dividing by h, and taking the limit as h!0 gives the following system of differential equations:
Pk0tðÞ¼/C0 λPktðÞþ λPk/C01tðÞ k¼1, 2, 3, ...
4Readers not familiar with o(h) notation should consult Appendix B.7.5 Poisson Processes 529
Letting Qk(t)¼Pk(t)eλt, the above differential equation becomes Qk0(t)¼λQk/C01(t). This system
can be solved recursively starting with Q0(t)¼1 (because P0(t)¼e/C0λt) to give Qk(t)¼λktk/k!,
whence
PktðÞ¼λtðÞke/C0λt
k!k¼0, 1, 2, ...■
In this chapter, we are discussing temporal stochastic processes—that is, random processes that are
functions of time. For some of these processes, in particular the Poisson counting process, there exist
spatial analogues. A spatial Poisson process models the occurrence of “random” events in space,
rather than in time (e.g., the location of ﬂaws on an integrated circuit, or of trees in a forest). Analogous
to the preceding theorem, suppose these random events meet the following conditions: (1) the numbers
of events in nonoverlapping regions of space are independent; (2) the probability of exactly one event
in a region of area hisλh+o (h) for some λ>0; and (3) the probability of more than one event is a
region of area hiso(h). Then a similar proof to the one above shows that the random variable
X(R)¼number of events in region Rhas a Poisson distribution with mean λ/C1(area of R).
7.5.4 Nonhomogeneous Poisson Processes
The Poisson process considered thus far is characterized by a constant rate λat which events occur per
unit time. A generalization of this is to suppose that the probability of exactly one event occurring in
the interval ( t,t+h]i sλ(t)/C1h+o (h) for some function λ(t). That is, we replace λin condition 2 of the
preceding theorem with a nonnegative function λ(t). It can then be shown that the number of events
occurring during an interval ( t1,t2] has a Poisson distribution with mean
EX t 2ðÞ /C0 Xt1ðÞ ½/C138 ¼ðt2
t1λtðÞdt ð7:7Þ
The occurrence of events over time in this situation is called a nonhomogeneous Poisson process ,
and λ(t) is called the intensity function of the process. Notice that the special case λ(t)¼λ
(a constant) returns us to the usual, “homogeneous” case; in particular, from Eq. ( 7.7) we immediately
have μX(t)¼λt.
Example 7.29 The article “Inference Based on Retrospective Ascertainment” ( J. Amer. Statist.
Assoc ., 1989: 360-372) considers the intensity function
λtðÞ¼ eaþbt
as appropriate for events involving transmission of HIV (the AIDS virus) via blood transfusions.
Suppose that a¼2 and b¼0.6 (close to values suggested in the paper), with time in years. What is
the expected number of events in the ﬁrst 4 years? In the time interval (2, 6]? What is the probability
that at most 20 events occur in ﬁrst 18 months?
To determine the expectation in any interval, we apply Eq. ( 7.7). The expected number of events in
the interval (0, 4] is
EX 4ðÞ /C0 X0ðÞ ½/C138 ¼ð4
0e2þ0:6tdt¼123:44,
while the expected number of events in (2, 6] equalsÐ6
2e2+0.6 tdt¼409.82. Notice that the expected
numbers of events for these two time intervals are quite different, even though each interval has530 7 Random Processes
length 4 years; this illustrates that a nonhomogeneous Poisson process does nothave stationary
increments.
Finally, the number of events in the ﬁrst 18 months (1.5 years), X(1.5), has a Poisson distribution
with parameter
μ¼EX 1:5ðÞ½/C138 ¼ EX 1:5ðÞ /C0 X0ðÞ ½/C138 ¼ð1:5
0e2þ0:6tdt¼17:975
Therefore, P(X(1.5)/C2020)¼P20
x¼0e/C017.97517.975x/x!¼.733. ■
7.5.5 The Poisson Telegraphic Process
We end this section with a brief discussion of the Poisson telegraphic process (orPoisson tele-
graph ), a popular model in engineering for noise in a binary channel. Suppose we have events
occurring according to the conditions of a Poisson process. Deﬁne a new random process, N(t), as
follows: N(0)¼/C01 with probability .5 and +1 with probability .5; when a random event occurs, N(t)
switches parity (i.e., from /C01 to +1 or vice versa). A sample function appears in Fig. 7.19; the x’s
through the middle indicate the time occurrences of the random events (notice these are precisely
where the process switches parity).
The statistical properties of the Poisson telegraph are catalogued in the following proposition.
PROPOSITION
LetN(t) be a Poisson telegraphic process with rate parameter λ.
1. For all t/C210,N(t)i s+ 1o r /C01 with probability .5 each. (Thus, a Poisson telegraph has the
same distribution at all time-points.)
2.μN(t)¼0 and σN(t)¼1 for all t/C210.
3.N(t) is WSS, and RNN(τ)¼CNN(τ)¼e/C02λ|τ|.
The proofs of these statements are left as exercises (see Exercises 69–70 at the end of this section).−101
tx(t) Fig. 7.19 One sample
function of a Poisson
telegraph7.5 Poisson Processes 531
We more commonly think of the symbols 0 and 1 in binary communication, rather than /C01 and +1.
The Poisson telegraph described above can be easily modiﬁed: let N∗(t)¼.5[N(t) + 1], so that N∗(t)
takes on the values 0 and 1. We call N∗(t)aPoisson 0-1 telegraph . In Exercise 71 of this chapter, you
are asked to derive the properties of the Poisson 0-1 telegraph.
7.5.6 Exercises: Section 7.5(53–72)
53. The number of requests for assistance received by a towing service is a Poisson process with rate
λ¼4 per hour.
(a) Compute the probability that exactly ten requests are received during a particular 2-h period.
(b) If the operators of the towing service take a 30-min break for lunch, what is the probability
that they do not miss any calls for assistance?
(c) How many calls would you expect during their break?
54. During the daily lunch rush, arrivals at the drive-thru at a nearby fast food restaurant follow a
Poisson process with a rate of 0.8 customers per minute.
(a) What is the expected number of customers in 1 h, and what is the corresponding standard
deviation?
(b) The drive-thru’s workers can’t handle more than 10 customers in any 5-min span. Deter-
mine the probability that too many customers arrive for the workers to handle between
12:15 p.m. and 12:20 p.m.
(c) A customer has just arrived. What is the probability another customer will arrive within the
next 30 s?
(d) The 100th lunch customer, starting at 12:00 p.m., gets a free meal. What is the expected
arrival time of that lucky customer, and what is the standard deviation of that time?
55. Packets arrive at a certain node on the university’s intranet at 10 packets per minute, on average.
Assume packet arrivals meet the assumptions of a Poisson process.
(a) Calculate the probability that exactly 15 packets arrive in the next 2 min.
(b) Find an expression for the probability that more than 75 packets arrive in the next 5 min.
(c) Calculate the probability that the next packet will arrive in less than 15 s.
(d) What is the average time between successive packet arrivals?
(e) Calculate the probability that the ﬁfth packet arrives in less than 45 s.
56. The article “Reliability-Based Service-Life Assessment of Aging Concrete Structures” ( J. Struct.
Engrg ., 1993: 1600–1621) suggests that a Poisson process can be used to represent the occur-
rence of structural loads over time. Suppose the mean time between occurrences of loads is
.5 year.
(a) How many loads can be expected to occur during a 2-year period?
(b) What is the probability that more than ﬁve loads occur during a 2-year period?
(c) How long must a time period be so that the probability of no loads occurring during that
period is at most .1?
57. Travelers arrive at an airport shuttle station according to a Poisson process with rate λ. The shuttle
vehicle will depart only once ktravelers have arrived. Assuming that there are no travelers
waiting at time 0, what is the expected duration of time until the next shuttle vehicle departs?
58. The parking lot for a local ballpark has two entrances (east and west). In the hour before a game,
cars entering the lot from east and west form two independent Poisson processes with rates 10 per
minute and 15 per minute, respectively.
(a) What is the expected number of cars entering the parking lot in any 10-min span, and what is
the corresponding standard deviation?532 7 Random Processes
(b) In any particular minute, what is the probability exactly 12 cars enter from each side?
(c) What is the probability that exactly 24 cars enter the lot in any particular minute?
(d) Write an expression for the probability that, in any particular minute, the same number of
cars enter through the east side and the west side.
59. Orders are submitted to a certain online business according to a Poisson process with rate 3 orders
per hour.
(a) Given that 4 orders are submitted during the time interval [0, 2], what is the probability that
10 orders are submitted in the interval [0, 5]?
(b) More generally, consider two ﬁxed times s<tand two nonnegative integers m<n. Given
thatmorders are submitted by time s, what is the probability that norders are submitted by
time t?
60. Automobiles arrive at a vehicle equipment inspection station according to a Poisson process with
rateλ¼10 per hour. Suppose that with probability .5 an arriving vehicle will have no equipment
violations.
(a) What is the probability that exactly ten vehicles arrive during the hour and all ten have no
violations?
(b) For any ﬁxed y/C2110, what is the probability that exactly yvehicles arrive during the hour,
of which ten have no violations?
(c) What is the probability that ten “no-violation” cars arrive during the next hour? [ Hint: Sum
the probabilities in (b) from y¼10 to 1.]
61. A certain component is subject to electrical surges over time. Suppose that these surges occur
according to a Poisson process with rate λ. Suppose also that with probability p, any particular
surge will disable the component. What is the probability that the component survives (is not
disabled) throughout the period [0, t]? [Hint: Make appropriate independence assumptions.]
62. Suppose events occur according to a Poisson process with rate λ.
(a) Given that nevents have occurred in the interval [0, n], what is probability that xof these
events occurred in [0, 1]? [ Hint: LetX(t) be the Poisson process, and write the conditional
probability of interest in terms of X(t). Then apply the deﬁnition of conditional probability.]
(b) Given that nevents have occurred in the interval [0, n], what is the limiting conditional
distribution of the number of events in [0, 1] as n!1 ?
63. There is one hospital at the northern end of a particular county and another hospital at the
southern end of the county. Suppose that arrivals to each hospital’s emergency room occur
according to a Poisson process with the same rate λand that the two arrival processes are
independent of one another. Starting at time t¼0, let Ybe the elapsed time until at least one
arrival has occurred at each of the two emergency rooms. Determine the probability distribution
ofY.
64. Suppose that ﬂaws occur along a cable according to a Poisson process with parameter λ.A
segment of this cable of length Yis removed, where Yhas an exponential distribution with
parameter θ. Determine the distribution of the number of ﬂaws that occur in this random-length
segment. [ Hint: Let Xbe the number of ﬂaws on this segment. Condition on Y¼yto obtain
P(X¼xjY¼y). Then “uncondition” using the Law of Total Probability (multiply by the pdf of
Yand integrate). The gamma integral (3.5) will prove useful.]
65. Starting at time t¼0, certain events occur at random with inter-arrival times T1,T2, and so on as
in Fig. 7.18. Deﬁne X(t)¼the number of arrivals in (0, t]; if we assume the Tnare iid (but not
necessarily exponentially distributed), then X(t) is called a renewal process.
(a) Show that a renewal process whose inter-arrival times are iid exponential rvs is a Poisson
process. That is, show that if the Tnare iid exponential( λ) rvs then X(t) has a Poisson( λt)
distribution.7.5 Poisson Processes 533
(b) The elementary renewal theorem states that, for any renewal process,
lim
t!1EX tðÞ½/C138
t¼1
ET n½/C138
Show that this is trivially true for a Poisson process.
[Note : A stronger version of the renewal theorem actually shows X(t)/tconverges in
probability to the constant 1/ E[Tn].]
66. Let X(t) count the number of events of a certain type in the time interval (0, t], and suppose X(t)
can be modeled by a nonhomogeneous Poisson process with intensity function λ(t).
(a) Does X(t) have stationary increments? Why or why not?
(b) Find the mean and variance functions of X(t).
(c) What is the probability that no events occur in the time interval (0, t]?
67. A certain repair facility is open for 8 hours on a particular day. Customers arrive according to a
nonhomogeneous Poisson process with intensity function λ(t)¼tfor 0 /C20t<1,¼1 for 1 /C20t
<7, and ¼8/C0tfor 7/C20t/C208.
(a) What is the probability that no customers arrive in both the ﬁrst and last hours and that
4 customers arrive in the middle 6 hours?
(b) What is the probability that the same number of customers arrive in the ﬁrst hour, middle
6 hours, and last hour?
68. During the ﬁrst round of enrollment, students begin registering for classes at the top of each hour.
There’s a mad rush at the beginning of the hour, and then logins taper off. Let X(t)¼the number
of logins tminutes into the hour, and suppose X(t) can be modeled by a nonhomogeneous Poisson
process with intensity function λ(t)¼500/( t+1 )2for 0 <t<60.
(a) What is the expected number of students that will log into the registration system in the ﬁrst
5 min of the hour? In the last 5 min of the hour?
(b) What is the probability that no students log in during the last 5 min of an hour?
(c) The registration system will crash if more than 450 students log in during any 5-min period.
What is the probability that this occurs in the ﬁrst 5 min of an hour? (You will need to use
software or a Central Limit Theorem approximation to determine this probability.)
69. Consider a Poisson telegraphic process N(t) with rate parameter λ.
(a) Let p¼P(an even number of events occur in (0, t]). Explain why, for t>0,
PNtðÞ¼þ 1/C12/C12N0ðÞ ¼ þ 1/C0/C1
¼pand PNtðÞ¼þ 1/C12/C12N0ðÞ ¼ /C0 1/C0/C1
¼1/C0p:
(b) Use (a) and the Law of Total Probability to show that P(N(t)¼+1)¼.5 for all t/C210.
(Since the only other possible value of N(t)i s/C01, this establishes property 1 of the last
proposition of this section.)
(c) Establish property 2 of the Poisson telegraph, i.e., that μN(t)¼0 and σN(t)¼1 for all t/C210.
70. (a) Consider a Poisson process with parameter λ. Show that the probability that an even number
of events (0, 2, 4, ...) occurs in any interval ( t, t + τ] is equal to (1 + e/C02λτ)/2.
(b) Let N(t) be a Poisson telegraphic process with parameter λ. By considering the possible
values of the product N(t)N(t+τ), show that the autocorrelation function of N(t)i se/C02λ|τ|.
[Hint: Use (a).]
71. A Poisson 0-1 telegraph N∗(t) is constructed as follows: N∗(0) equals 0 or 1 with probability .5
each, and then N∗(t) switches parity upon the occurrence of an event in a Poisson process. Find
the pmf, mean, variance, autocovariance function, and autocorrelation function of N∗(t). [Hint:
Use the relationship N∗(t)¼.5[N(t) + 1], where N(t) is an ordinary Poisson telegraph.]534 7 Random Processes
72. This exercise outlines a proof that the time to the nth event of a Poisson process has an Erlang
distribution.
(a) Let Yndenote the time to the nth event in a Poisson process. Explain why, for any time
y>0,P(Yn>y)¼P(fewer than nevents occur in the time interval (0, y]).
(b) Suppose the Poisson process has rate parameter λ. Use (a) to write an expression for the cdf
ofYn.[Hint: the right-hand side of (a) can be written as a ﬁnite sum using the deﬁnition of a
Poisson process.]
(c) Differentiate your answer to part (b) to obtain the pdf of Yn, and verify that it is an Erlang pdf
with parameters nandλ(aka the gamma distribution with α¼nandβ¼1/λ).
7.6 Gaussian Processes
We introduced the normal or Gaussian distribution in Chap. 3and then extended it to a multivariate
distribution in Chap. 4. Here, we consider the extension of the normal distribution to random
processes. Engineers commonly use such models for noise in audio signals and the (seemingly)
random motion of small particles. We’ll explore both of these applications shortly.
DEFINITION
A random process X(t)i sa Gaussian process if for all time points, t1,...,tnthe random
variables X(t1),...,X(tn) have a multivariate normal distribution (as deﬁned in Sect. 4.7). In
particular, the distribution of X(t) at any time point tis normal.
As discussed in Sect. 4.7, we can also characterize a joint Gaussian distribution by requiring that
all linear combinations of the random variables be Gaussian. Applying that characterization here, we
have an alternate deﬁnition of a Gaussian process: X(t) is a Gaussian process iff all linear
combinations of X(t1),...,X(tn) have a normal distribution for n¼1, 2, 3, ..., and all time-points
t1,...,tn.
In Sect. 7.3, we distinguished strict-sense stationary processes from wide-sense stationary pro-
cesses. We noted that a strict-sense stationary process is automatically WSS, but not vice versa.
However, suppose that a Gaussian process X(t) is WSS: this implies the mean and covariance
structure of X(t) are time-invariant. But we know from Sect. 4.7that mean and covariance completely
characterize a joint Gaussian distribution; all other statistical properties can be derived from these
two. Thus, if a Gaussian process is WSS, allof its statistical properties must be time-invariant.
PROPOSITION
Suppose X(t) is a Gaussian process. Then X(t) is wide-sense stationary if, and only if,X(t)i s
strict-sense stationary.
Example 7.30 The noise X(t) (measured in decibels) in an audio signal is modeled as a wide-sense
stationary Gaussian process, with mean zero and autocovariance function
CXXτðÞ ¼ 0:04e/C0τjj=10
Let’s ﬁrst investigate the distributions of X(3) and X(8), the noise three and eight seconds into the
audio signal, respectively. Since X(t) is a Gaussian process, by deﬁnition X(3) is a Gaussian random7.6 Gaussian Processes 535
variable; we merely have to specify its mean and standard deviation. We are given that X(t) is a mean-
zero process, so in particular E[X(3)]¼0. We can extract the variance of X(3) from the
autocovariance using a property of WSS processes:
VarX3ðÞðÞ ¼ σ2
X¼CXX0ðÞ ¼ 0:04e/C0j0j=10¼0:04
Therefore, X(3) ~ N(0, 0.2). Moreover, since X(t) is stationary, this is also the distribution of X(8).
Next, notice that X(8)/C0X(3) is a linear combination of X(3) and X(8); therefore, since X(t)i s
a Gaussian process, the random variable X(8)/C0X(3) is also Gaussian. Its mean is simply E[X(8)/C0
X(3)]¼0/C00¼0. Computing the variance takes a bit more effort:
VarðX8ðÞ /C0 Xð3ÞÞ ¼ VarðXð8Þ Þþð /C0 1Þ2VarðXð3ÞÞ þ 2ð1Þð/C01ÞCovðXð8Þ,Xð3ÞÞ
¼VarðX8ðÞ Þ þ VarðXð3ÞÞ /C0 2CovðXð8Þ,Xð3ÞÞ
¼CXX0ðÞþCXXð0Þ/C02CXXð/C05Þ since τ¼3/C08¼/C05
¼0:04þ0:04/C02/C10:04e/C0j/C05j=10
¼0:08 1/C0e/C01=2/C0/C1
¼:0315
Therefore, X8ðÞ /C0 X3ðÞ /C24 N0;ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:0315p/C0/C1
. Finally, we can use this distribution to ﬁnd the proba-
bility the noise at t¼8 is more than 0.3 dB above the noise at t¼3:
PðX8ðÞ>0:3þXð3ÞÞ ¼ PðXð8Þ/C0Xð3Þ>0:3Þ¼1/C0PðXð8Þ/C0Xð3Þ/C200:3Þ
¼1/C0Φ0:3/C00ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
:0315p/C18/C19
¼1/C0Φ1:69ðÞ ¼ :0455■
7.6.1 Brownian Motion
In Example 7.4, we introduced the idea of Brownian motion, a model for the seemingly random
behavior of a dust particle on a liquid surface. Physicists also use the Brownian motion model to
describe a variety of physical processes, including the motion of some celestial bodies in response to
gravitational forces. A precise mathematical construction of Brownian motion is beyond the scope of
this book; however, we characterize Brownian motion in the following deﬁnition.
DEFINITION
A (one-dimensional) Brownian motion process (also called a Wiener process ) with parameter
α>0, denoted B(t), is a Gaussian process with the following properties:
1.μB(t)/C170; that is, Brownian motion is a mean-zero process.
2.σB2(t)¼αt,s oσBtðÞ¼ﬃﬃﬃﬃαtp.
3.B(t) has stationary and independent increments.
Ifα¼1,B(t) is called standard Brownian motion .5
5Albert Einstein showed in 1905 from physical considerations that the conditional pdf of B(t0+t) given B(t0)¼xmust
satisfy the partial differential equation ∂f=∂t¼1
2α/C1∂2f=∂x2, where the “diffusion constant” αinvolves a gas constant,
temperature, a coefﬁcient of friction, and Avogadro’s number. He also showed that the unique solution to this PDE is
the normal pdf.536 7 Random Processes
It follows from the stationary increments property that BtþτðÞ /C0 BtðÞ/C24 N0;ﬃﬃﬃﬃﬃατpðÞ for any τ>0.
Figure 7.20 shows several sample functions of Brownian motion. It is important to note that while
B(t) has stationary increments , it is not itself a stationary process (the variance of Brownian motion
depends on t).
Because Brownian motion is nota stationary process, we expect the autocovariance function will
depend on “absolute” time ( tands) rather than “relative” time τ. The autocovariance and autocorre-
lation functions of Brownian motion are
CBBt;sðÞ ¼ RBBt;sðÞ ¼ α/C1min t;sðÞ
The derivation is similar to that of the Poisson process autocovariance function in the previous
section.
Brownian motion actually shares several features with the Poisson process of the previous section:
both have initial value 0 (with probability 1), stationary and independent increments, and variance
proportional to time. In fact, it can be shown (see Exercise 85 at the end of this section) that any
random process having a constant initial value along with stationary and independent increments
must necessarily have a variance function that’s linear in t.
Since Brownian motion is a one-dimensional random process, clearly it can only describe particle
motion in a single direction. The random motion of particles on a surface or through space can be
described by 2- or 3-dimensional Brownian motion processes, for which it’s assumed the motion
along each dimensional axis is an independent, one-dimensional Brownian motion process.
Example 7.31 Consider the movement of a particle along a single axis, governed by Brownian
motion with parameter α¼4. Let’s begin by identifying the probability distribution of the particle’s
displacement from time t¼1 s to time t¼4 s. If we write B(t) for the process, then we wish to know
the distribution of B(4)/C0B(1). Applying the comment below the deﬁnition of Brownian motion with
τ¼4/C01¼3, we have B4ðÞ /C0 B1ðÞ /C24 N0;ﬃﬃﬃﬃﬃ
12p/C0/C1
. (An alternative derivation uses a similar
approach to Example 7.30 and the autocovariance function mentioned above.)
The particle’s displacement from time t¼2 s to time t¼5 s has this same distribution, since both
increments span a time length of τ¼3 and, by Property 3 of the deﬁnition, B(t) has stationary
increments. However, the increments B(5)/C0B(2) and B(4)/C0B(1) are notindependent: while
Property 3 states that Brownian motion has independent increments, the two time intervals (2, 5]
and (1, 4] overlap.0x(t)
tFig. 7.20 Brownian
motion7.6 Gaussian Processes 537
Finally, what is the probability that the particle is displaced more than 10 units in the time interval
(1, 4]? Since the question does not indicate whether the displacement is positive or negative (relative
to the axis), we’re really interested in determining P(jB(4)/C0B(1)j>10). Because the distribution of
B(4)/C0B(1) is symmetric about 0, we may proceed as follows:
Pð/C12/C12B4ðÞ /C0 Bð1Þ/C12/C12>10Þ¼2PðBð4Þ/C0Bð1Þ>10Þ by symmetry
¼2/C2
1/C0PðB4ðÞ /C0 Bð1Þ/C2010Þ/C3
¼21/C0Φ10/C00ﬃﬃﬃﬃﬃ
12p/C18/C19/C20/C21
¼2/C2
1/C0Φ2:89ðÞ/C3
¼:0038■
7.6.2 Brownian Motion as a Limit
The Brownian motion process described above can actually be constructed as the limit of a discrete-
time random process—speciﬁcally, the simple symmetric random walk Snof Example 7.24. We will
shrink both the time increment and the size of a jump in this random walk as follows: for some h>0
andΔx>0, suppose at times h,2h,3h, etc. the walk moves + Δxor/C0Δxwith probability .5 each.
Then, with [ ] denoting the greatest-integer function, the random process B(t) deﬁned by
BtðÞ¼ ΔxðÞ X1þ/C1/C1/C1þ ΔxðÞ Xt=h½/C138¼ΔxðÞ St=h½/C138
indicates the location of the random walk at time t. The coefﬁcient Δxchanges the motion increment
from/C61t o/C6Δx; the time index n¼[t/h] equals the number of moves (equivalently, the number of
h-second time intervals) in the interval [0, t].
From the properties of the random walk,
μBtðÞ¼ EΔxðÞ St=h½/C138/C2/C3
¼ΔxðÞ ES t=h½/C138/C2/C3
¼ΔxðÞ /C1 0¼0andσ2
BtðÞ¼ VarΔxðÞ St=h½/C138/C0/C1
¼ΔxðÞ2Var St=h½/C138/C0/C1
¼ΔxðÞ2/C1t=h½/C138
Moreover, the Central Limit Theorem tells us that, for large values of [ t/h], the distribution of B(t)
is approximately normal.
Up to now, the choices of handΔxhave been arbitrary. But suppose we choose Δx¼ﬃﬃﬃﬃﬃﬃ
αhp
for
some α>0. Then, if we shrink hto 0 (effectively moving from discrete time to continuous time), B(t)
will be normally distributed with mean 0 and variance
lim
h!0αht=h½/C138 ¼ αt
The properties of independent and stationary increments clearly follow as consequences of the iid
steps in the random walk. Thus, B(t) becomes a Brownian motion process as h!0.
7.6.3 Further Properties of Brownian Motion
Consider some ﬁxed value x0>0 and a ﬁxed time t0. The maximum value of B(t) during the time
interval 0 /C20t/C20t0is a random variable M. What is the probability that Mexceeds the threshold x0?
Figure 7.21 shows two sample paths, one for which the level x0is exceeded prior to t0and one for
which this does not occur.538 7 Random Processes
Let’s focus on a path b(t) that does reach x0during the speciﬁed time interval. Corresponding to
this path, we now create a new path by reﬂecting about the line y¼x0the part of b(t) that lies to the
right of the ﬁrsttime it reaches x0. Denote the ﬁrst time at which the original path reaches x0byT.
Then for t/C20Tthe new path—call it b*(t)—is identical to the original path. But for any time t>T,
it’s easy to show that the reﬂected path is given by b*(t)¼2x0/C0b(t). The original path and its
reﬂected path are illustrated in Fig. 7.22.
Notice that the maximum level for each of these paths exceeds x0, and exactly one of these two
paths has level exceeding x0at time t0. That is, for every sample path that exceeds level x0at time t0,
there are two sample paths whose maxima on [0, t0] exceed x0, the original path and the reﬂected path.
Put another way, for each pair of sample paths whose level exceeds x0some time before t0, one being
the reﬂection of the other about x0subsequent to its ﬁrst “hitting time,” there is exactly one sample
path satisfying B(t0)>x0.
Now given that B(T)¼x0, consider the pdf of the level B(t) at some time subsequent to T. Because
Brownian motion has independent increments, the process “begins anew” at time T, except that its
Gaussian behavior starts at x0rather than at 0. The symmetry of the normal distribution implies that
the pdf at a level above x0at the future time is the same as the pdf at a level that is below x0by the
same amount. That is, the original path and the reﬂected path are equally likely. Melding this equally
likely property with the result of the previous paragraph, establishing a one to one correspondence
between pairs of reﬂected paths crossing x0and paths whose level exceeds x0at time t0, gives the
following result.t0x0x(t)
M
M
t
Fig. 7.21 A sample path for which M¼max
0/C20t/C20t0BtðÞ>x0and a path for which M<x0
x0b*(t)
b(t)
t0 Tt
Fig. 7.22 A sample path crossing x0before time t0and its paired reﬂected path7.6 Gaussian Processes 539
PROPOSITION
LetB(t) be Brownian motion and, for t0>0, let M¼max
0/C20t/C20t0BtðÞ. Then
PM >x0 ðÞ ¼ 2PBt 0ðÞ>x0 ðÞ ¼ 21/C0Φx0ﬃﬃﬃﬃﬃﬃαt0p/C18/C19/C20/C21
ð7:8Þ
The second equality in the proposition comes from the fact that Bt0ðÞ /C24 N0;ﬃﬃﬃﬃﬃﬃαt0pðÞ . Replacing x0
with mon both sides of Eq. ( 7.8) and differentiating with respect to m, we can determine the pdf of
this rv:
fMmðÞ ¼2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2παt0p e/C0m2=2αt0ðÞm>0
The foregoing proposition also allows us to obtain the distribution of the random variable T¼the
ﬁrst time at which the process hits level x0. To see this, note that a sample path will ﬁrst hit level x0by
time t0iff the maximum level of the path during the time interval [0, t0] is at least x0. In symbols,
T/C20t0iffM/C21x0. Since the probability of the latter event is what appears in the last proposition box,
we immediately have the following result.
PROPOSITION
LetTbe the ﬁrst time that a Brownian motion process reaches level x0. Then
FTtðÞ¼ PT/C20t ðÞ ¼ 21/C0Φx0ﬃﬃﬃﬃαtp/C18/C19/C20/C21
,
from which it follows that
fTtðÞ¼x0ﬃﬃﬃﬃﬃﬃﬃﬃ
2παp t/C03=2e/C0x2
0=2αtt>0
Figure 7.23 shows the probability distribution of the “hitting time” T.Exercise 84 asks you to show
this is a valid probability distribution and to derive the pdf from the cdf.
fT(t)
tFig. 7.23 pdf of the
hitting time, T, for
Brownian motion540 7 Random Processes
7.6.4 Variations on Brownian Motion
LetB(t) denote a standard Brownian motion process (i.e., with α¼1), and let μand σ>0b e
constants. Brownian motion with drift is the process X(t)¼μt+σB(t).X(t) also has stationary an
independent increments; an increment X(t+τ)/C0X(t) is normally distributed with mean μτand
variance σ2τ. Brownian motion with drift has many interesting applications and properties. For
example, suppose the drift parameter μis negative. Then over time X(t) will tend toward ever
lower values. It can be shown that M, the maximum level attained over all time t/C210, has an
exponential distribution with parameter 2| μ|/σ2.
Standard Brownian motion and Brownian motion with drift both allow for positive and negative
values of the process. Thus they are not typically acceptable models for the behavior of the price of
some asset over time. A stochastic process Z(t) is called geometric Brownian motion with drift
parameter αifX(t)¼ln[Z(t)] is a Brownian motion process with drift having mean parameter
μ¼α/C0σ2/2 and standard deviation parameter σ. Since Z(t)¼exp(X(t)), a geometric Brownian
motion process will be nonnegative. Any particular sample path will show random ﬂuctuations about
a long-term exponential decay or growth curve.
Geometric Brownian motion is a popular model for the pricing of assets. Let X(t) be the price of an
asset at time t. The ratio X(t)/X(0) is the proportion by which the price has increased or decreased
between time 0 and time t. In the same way that we obtained Brownian motion as a limit of a simple
symmetric random walk, geometric Brownian motion can be obtained as a limit in which the price at
each time point either increases by a multiplicative factor uor goes down by another particular
multiplicative factor d. The limit is taken as the number of price changes during (0, t] gets arbitrarily
large, while the factors uanddget closer and closer to 1 and the two probabilities associated with
uanddapproach .5. Geometric Brownian motion is the basis for the famous Black-Scholes option
pricing formula that is used extensively in quantitative ﬁnance. This formula speciﬁes a fair price for
a contract allowing an investor to purchase an asset at some future time point for a particular price
(e.g., a contract permitting an investor to purchase 100 shares of Facebook stock at a price of $20 per
share 3 months from now).
7.6.5 Exercises: Section 7.6(73–85)
73. Let X(t) be a wide-sense stationary Gaussian process with mean μX¼13 and autocovariance
function CXX(τ)¼9cos( τ/5).
(a) Calculate P(X(10) <5).
(b) Calculate P(X(10) <X(8) + 2).
74. Let Y(t) be a WSS Gaussian process with mean μY¼/C05 and autocorrelation function RYY(τ)
¼(25τ2+ 34)/(1 + τ2). Determine each of the following:
(a) Var( Y(t))
(b) P(Y(2)>5)
(c) P(jY(2)j>5)
(d) P(Y(6)/C0Y(2)>5)
75. The voltage noise N(t) in a certain analog signal is modeled by a Gaussian process with mean 0 V
and autocorrelation function RNN(t, s)¼1/C0jt/C0sj/10 for jt/C0sj/C2010 (and zero otherwise).
(a) Is N(t) stationary? How can you tell?
(b) Determine P(jN(t)j>1).
(c) Determine P(jN(t+5 )/C0N(t)j>1).7.6 Gaussian Processes 541
(d) Determine P(jN(t+ 15) /C0N(t)j>1).
[Hint for (b)–(d): Does your answer depend on t?]
76. The text Gaussian Processes for Machine Learning (2nd ed., 2006) discusses applications of the
“regression-style” model Y(t)¼β0+β1t+X(t)+ε, where β0andβ1are constants, the “error
term” εis aN(0,σ) rv, and X(t) is a Gaussian random process with mean 0 and covariance
function
CXXt;sðÞ ¼ κ2e/C0λt/C0sðÞ2
for suitable choices of the parameters κ>0 and λ>0.X(t) andεare assumed to be independent.
(a) Is X(t) wide-sense stationary?
(b) Is Y(t) a Gaussian process?
(c) Determine the mean, variance, and autocovariance functions of Y(t). IsY(t) WSS?
(d) What effect does the parameter κhave on Y(t)? That is, how would the behavior of Y(t)b e
different for large κversus small κ?
(e) What effect does the parameter λhave on Y(t)?
77. Consider the following model for the temperature X(t), in/C14F, measured thours after midnight on
August 1, in Bakersﬁeld, CA:
XtðÞ¼ 80þ20cosπ
12t/C015ðÞ/C16/C17
þBtðÞ,
where B(t) is a Brownian motion process with parameter α¼.2.
(a) Determine the mean and variance functions of X(t). Interpret these functions in the context
of the example.
(b) According to this model, what is the probability that the temperature at 3 pm on August
1 will exceed 102/C14F?
(c) Repeat part (b) for 3 p.m. on August 5.
(d) What is the probability that the temperatures at 3 p.m. on August 1 and August 5 will be
within 1/C14F of each other?
78. Brownian motion is sometimes used in ﬁnance to model short-term asset price ﬂuctuation.
Suppose the price (in dollars) of a barrel of crude oil varies according to a Brownian motion
process; speciﬁcally, suppose the change in a barrel’s price tdays from now is modeled by
Brownian motion B(t) with α¼.15.
(a) Find the probability that the price of a barrel of crude oil has changed by more than $1, in
either direction, after 5 days.
(b) Repeat (a) for a time interval of 10 days.
(c) Given that the price has increased by $1 in seven days, what is the probability the price will
be another dollar higher after an additional seven days?
79. Refer to the weather model in Exercise 77. Suppose a meteorologist uses the mean function of
X(t) as her weather forecast over the next week.
(a) Over the next ﬁve days, what is the probability that the actual temperature in Bakersﬁeld
will exceed the meteorologist’s prediction by more than 5/C14F? [Hint: What part of X(t)
represents her prediction error?]
(b) What is the probability that the actual temperature will exceed her prediction by 5/C14F for the
ﬁrst time by midnight on August 3 (i.e., two days after t¼0)?
80. Refer to Exercise 78, and suppose the initial price of crude oil is $110 per barrel.
(a) Over the next 30 days, what is the probability the maximum price of a barrel of crude oil will
exceed $115?542 7 Random Processes
(b) Determine the probability that the price of crude oil will hit $120 for the ﬁrst time within the
next 60 days.
81. The motion of a particle in two dimensions (e.g., a dust particle on a liquid surface) can be
modeled by using Brownian motion in each direction, horizontal and vertical. That is, if ( X(t),
Y(t)) denotes the position of a particle at time t, starting at (0, 0), we assume X(t) and Y(t) are
independent Brownian motion processes with common parameter α. This is sometimes called
two-dimensional Brownian motion.
(a) Suppose a certain particle moves in accordance with two-dimensional Brownian motion
with parameter α¼5. Find the probability the particle is more than 3 units away from (0, 0)
in each dimension at time t¼2.
(b) For the particle in (a), ﬁnd the probability the particle is more than 3 units away from (0, 0)
radially (i.e., by Euclidean distance) at time t¼2. [Hint: It can be shown that the sum of
squares of two independent N(0,σ) rvs has an exponential distribution with parameter
λ¼1/(2σ2).]
(c) Three-dimensional Brownian motion , a model for particulate movement in space, assumes
that each location coordinate ( X(t),Y(t),Z(t)) is an independent Brownian motion process
with common parameter α. Suppose a certain particle’s motion follows three-dimensional
Brownian motion with parameter α¼0.2. Find the probability that (1) the particle is more
than 1 unit away from (0, 0, 0) in each dimension at time t¼4, and (2) the particle is more
than 1 unit away from (0, 0, 0) radially at time t¼4. [Hint: The sum of squares of three
independent N(0,σ) rvs has a gamma distribution with α¼3/2 and β¼2σ2.]
82. Some forms of thermal voltage noise can be modeled by an Ornstein-Uhlenbeck process X (t),
which is the solution to the “stochastic differential equation” X0(t)+κX(t)¼σB0(t), where B(t)i s
standard Brownian motion and κ,σ>0 are constants. With the initial condition X(0)¼0, It can
be shown that X(t) is a Gaussian process with mean 0 and autocovariance function
CXXt;sðÞ ¼σ2
2κe/C0κjs/C0tj/C0e/C0κsþtðÞhi
(a) Is the Ornstein-Uhlenbeck process wide-sense stationary? Why or why not?
(b) Find the variance of X(t). What happens to the variance of X(t)a st!1 ?
(c) Let s¼t+τ. What happens to CXX(t, t + τ)a st!1 ?
(d) For s>t, determine the conditional distribution of X(s) given X(t).
83. A Gaussian white noise process is a Gaussian process N(t) with mean μN(t)¼0 and autocorre-
lation function RNNτðÞ¼ð N0=2ÞδðτÞ, where N0>0 is a constant and δ(τ) is the Dirac delta
function (see Appendix B).
(a) Is Gaussian white noise a stationary process?
(b) Deﬁne a new random process, X(t), as the integrated version of N(t):
XtðÞ¼ðt
0NsðÞds
Find the mean and autocorrelation functions of X(t). IsX(t) stationary?
84. Consider the “hitting-time” distribution FTtðÞ¼ 21/C0Φx0=ﬃﬃﬃﬃαtp/C0/C1/C2/C3
,t>0, for Brownian
motion presented in the last proposition of this section.
(a) Show that FT(t) is a valid cdf for a nonnegative rv by proving that (1) FT(t)!0a st!0+,
(2)FT(t)!1a st!1 , and (3) FT(t) is an increasing function of t.
(b) Find the median of this hitting-time distribution.7.6 Gaussian Processes 543
(c) Derive the pdf of Tfrom the cdf.
(d) Does the mean of this hitting-time distribution exist?
85. Let X(t) be a random process with stationary and independent increments and X(0) a constant.
(a) Take the variance of both sides of the expression
XtþτðÞ /C0 X0ðÞ ¼ XtþτðÞ /C0 XtðÞ ½/C138 þ XtðÞ/C0 X0ðÞ ½/C138
and use the properties of X(t) to show that Var( X(t+τ))¼Var(X(t)) + Var( X(τ)).
(b) The only solution to the functional relation g(t+τ)¼g(t)+g(τ) is a linear function:
g(t)¼atfor some constant a. Apply this fact to part (a) to conclude that any random
process with a constant initial value and stationary and independent increments must have
linear variance. (This includes both Brownian motion as well as the Poisson counting
process of the previous section.)
7.7 Continuous-Time Markov Chains
Recall from Chap. 6that a discrete-time Markov chain is a sequence of random variables X0,X1,X2,
...satisfying the Markov property on some state space (typically a set of integers). In this section, we
consider a continuous-time random process that transitions between a discrete set of states (again,
typically a set of integers) according to rules dictated by the Markov property. To be consistent with
the notation of Chap. 6, we will denote the random process as Xt, with time as a subscript, but we
could just as well use the X(t) notation from earlier sections of this chapter. For example, Xtmight be
the number of customers in a service facility, where t= 0 is the time at which the facility opened; we
then monitor the number of customers for all t/C210 rather than just at times 0, 1, 2, and so on.
As before, the Markov property says that once we know the state of the process at some time t,
the probability distribution of future states does not depend on the state of the process at any time
prior to t.
DEFINITION
A continuous-time, discrete-space random process Xt(t/C210) is a continuous-time Markov
chain if for any time tand any h>0,
PX tþh¼jjXt¼i;Xs¼isfor 0/C20s<t ðÞ ¼ PX tþh¼jjXt¼i ðÞ
We will assume throughout this section that our Markov processes are time homogeneous ; i.e., for
any time increment h>0, the probability P(Xt+h=j|Xt=i) depends on hbut not on t, so that we may
write
PijhðÞ ¼ PX tþh¼jjXt¼i ðÞ
Thus Pij(h) is the conditional probability that the state of the process htime units into the future
will be j, given that the process is presently in state i.
Example 7.32 A physician checks in on her patients in three hospital rooms and also spends time at
the nurses’ station. Identify these “states” as 0 = nurses’ station and 1, 2, 3 = the three patient rooms.
LetXtdenote the physician’s location thours into her shift. Figure 7.24 shows an example of her544 7 Random Processes
transitions between these four states across continuous time. In this ﬁgure, she begins her shift at the
nurses’ station ( X0= 0), spends some time there, and then moves periodically from room to room.
So far, this only describes a discrete-space random process. For Xtto be accurately modeled as a
Markov process, it must be the case that the probability of her being at any particular location
hminutes from now can be completely determined by her present location. Later in this section, we’ll
see several ways to characterize a continuous-time Markov chain and how to apply those character-
izations to this example. ■
Example 7.33 Our old friend the Poisson counting process with rate parameter λis an especially
simple case of a continuous-time Markov chain. Given the complete history of the process from 0 to t,
the value of Xt+his completely determined by the count of events at time tand the number of
additional events in ( t,t+h]. Speciﬁcally, for non-negative integers iandjwith i/C20j,
PijhðÞ ¼ PX tþh¼jjXt¼i ðÞ
¼P/C0
exactly j/C0ievents in t;tþh/C3/C0/C1
¼e/C0λhλhðÞj/C0i
j/C0iðÞ !
Notice that the Poisson process is “time homogeneous”—that is, the above probability depends
only on hand not on t—because of the stationary increments property. ■
Paralleling the discrete-time case, here we also have the Chapman-Kolmogorov equations; these
describe how Pij(t+h)=P(Xt+h=j|X0=i) is obtained by conditioning on the state of the process
after ttime units have elapsed. By considering all possible states for the process at time tand applying
the law of total probability,
PijtþhðÞ ¼ PX tþh¼jjX0¼i ðÞ
¼P
kPX tþh¼j\Xt¼kjX0¼i ðÞ
¼P
kPX tþh¼jjXt¼k;X0¼i ðÞ /C1 PX t¼kjX0¼i ðÞ
¼P
kPX tþh¼jjXt¼k ðÞ /C1 PX t¼kjX0¼i ðÞ
¼P
kPiktðÞ/C1PkjhðÞ
The second-to-last equality is by virtue of the Markov property: conditional on state being in state
kat time t, the state at any previous time is irrelevant to the chance of being in state jat the future time.
The ﬁnal step relies on time-homogeneity.1
023Xt
tFig. 7.24 One realization
of the Markov process Xtin
Example 7.327.7 Continuous-Time Markov Chains 545
Based on the Markov chain discussion in Chap. 6, you might imagine that our primary aim would
be to completely specify Pij(h) and then use the Chapman-Kolmogorov equations (perhaps in matrix
form) to study the evolution of the chain. We’ll explore this approach at the very end of this section.
However, an explicit expression for the transition probabilities is not always easy to come by, and so a
more common characterization involves the transition rates between states and the times spent in each
state.
7.7.1 Infinitesimal Parameters and Instantaneous Transition Rates
Just as a discrete-time Markov chain is characterized by its one-step transition probabilities, we can
completely describe a continuous-time Markov process if we can determine the likelihood that,
residing currently in state i, the process will be in state ja short time later. Since our time scale is now
continuous, we can’t really talk about “one time step”; rather, we think in terms of very short time
segments, which leads to the calculus ideas of instantaneous rates of change and ﬁrst derivatives.
It is reasonable to assume that each transition probability Pij(h) is a continuous function of h,s o
that such probabilities change smoothly as hdoes. Since by deﬁnition Pii(0) = P(Xt=i|Xt=i)=1
andPij(0) = 0 for i6¼j, continuity implies that as happroaches 0, Pii(h) approaches 1 and Pij(h)
approaches 0 when i6¼j. Rather amazingly, it turns out that the continuity assumption and the
Markov property imply that all Pijare differentiable, and in particular are differentiable at 0. The
following notation will be used for such derivatives:
/C0qi¼P0
ii0ðÞ ¼ lim
h!0PiihðÞ /C0 Pii0ðÞ
h¼lim
h!0PiihðÞ /C0 1
h
(it is convenient to denote this derivative by /C0qibecause the numerator Pii(h)/C01 is negative and so
the limit itself will be negative; qiis then positive); and, for i6¼j,
qij¼P0
ij0ðÞ ¼ lim
h!0PijhðÞ /C0 Pij0ðÞ
h¼lim
h!0PijhðÞ
h
Recall from calculus that the ﬁrst derivative of a function is the slope of the line tangent to the
function, and that this allows us to create a “ﬁrst-order” (i.e., linear) approximation to the function if
we stay on a small scale. In our development of the Poisson process Sect. 7.5, we employed o(h)
notation to represent a quantity that for small his negligible compared to h(see also Appendix B).
Using this notation in combination with the preceding derivative expressions, we have that for all
tand for hclose to 0,
PX tþh¼ijXt¼i ðÞ ¼ PiihðÞ ¼ Pii0ðÞ þ P0
ii0ðÞhþohðÞ ¼ 1/C0qihþohðÞ
PX tþh¼jjXt¼i ðÞ ¼ PijhðÞ ¼ Pij0ðÞ þ P0
ij0ðÞhþohðÞ ¼ 0þqijhþohðÞ ¼ qijhþohðÞ i6¼j
A continuous-time Markov process is characterized by these various transition probability
derivatives at time 0, which are collectively called the inﬁnitesimal parameters of the process.
We know from calculus that a derivative also represents a rate of change. Hence, we can interpret the
qijs in this fashion: qijrepresents the rate at which a Markov process transitions from state ito state j
over some very short time interval. Hence, the qijs are called the instantaneous transition rates of
the Markov process.
The parameters q0,q1,...arenottransition rates, since they are associated with time intervals in
which the process stays in the same state. (In contrast to the case of discrete-time Markov chains from546 7 Random Processes
Chap. 6, here we do not speak of “transitions” from a state into itself.) Rather, an interpretation of the
qis will be provided by the main theorem of this section.
For any state i,the inﬁnitesimal parameters qiandqijare related to each other by the following
proposition.
PROPOSITION
With qiandqijas just deﬁned,
qi¼X
j6¼iqij ð7:9Þ
Proof Because a process that’s in state iat time tmust be somewhere at time t+h,∑jPij(h) = 1 for
anyi.Take the derivative of both sides and evaluate at 0:6
1¼X
jPijhðÞ ¼ PiihðÞ þX
j6¼iPijhðÞ )
0¼P0
iihðÞ þX
j6¼iP0
ijhðÞ )
0¼P0
ii0ðÞ þX
j6¼iP0
ij0ðÞ ¼ /C0 qiþX
j6¼iqij)
qi¼X
j6¼iqij ■
Example 7.34 Consider again a Poisson process with rate parameter λ. If the process is currently in
state i(meaning ievents have occurred so far), the only possible transition is to state i+ 1 when the
next event occurs. So,
PiihðÞ ¼ Pno events in an interval of length h ðÞ ¼e/C0λhλhðÞ0
0!¼e/C0λh,
whence P0
iihðÞ ¼ /C0 λe/C0λhandqi¼/C0P0
ii0ðÞ ¼ λ. Similarly, Pi,i+1(h)=1 /C0P(no events in inter-
val) = 1 /C0e/C0λh, which implies qi,iþ1¼P0
i,iþ10ðÞ ¼ λ, and qij= 0 for j6¼i+ 1. Notice that these values
indeed satisfy Eq. ( 7.9).
In light of the fact that λrepresents the rate at which events occur (i.e., the count increases by 1),
our interpretation of qi,i+1as an instantaneous transition rate seems quite reasonable. ■
Example 7.35 (Example 7.32 continued) Suppose the physician walks from the nurses’ station to
Room 3 an average of twice per hour; this is the rateat which she transitions from state 0 to state 3.
But the foregoing discussion also indicated that the derivative qijrepresents the rate of change from
state ito state j. Therefore, we have that q03= 2 (two such transitions per hour). A complete
description of the doctor’s movements requires specifying all of the other instantaneous transition
rates as well; let’s say those are
6When the state space is inﬁnite, it can sometimes happen that qi¼1 . This will not occur for the situations considered
in this section.7.7 Continuous-Time Markov Chains 547
The remaining four inﬁnitesimal parameters of this Markov process model can be determined
using Eq. ( 7.9). For example,
q0¼X
j6¼0q0j¼q01þq02þq03¼4þ:5þ2¼6:5
Similarly, q1=8 ,q2= 7.5, and q3=4 . ■
7.7.2 Sojourn Times and Transitions
An important feature of a continuous-time Markov chain is the collection of sojourn times —the
successive time durations spent in visits to various states. In Fig. 7.24, ﬁve sojourn times are visible:
the doctor starts out at the nurses’ station (state 0), then spends some time in patient Room 1, then
back at her station, over into Room 3, and ﬁnally into Room 2. What are the distributions of these
continuous random variables?
Think back once more to the Poisson process. From the results of Sect. 7.5, the distribution of time
that a Poisson process spends in state ibefore moving to state i+ 1—that is, the sojourn time between
the occurrence of the ith event and the ( i+ 1)st event—is exponential with parameter λ. Our next
theorem says that sojourn times for anycontinuous-time Markov process are also exponentially
distributed and speciﬁes, in terms of the inﬁnitesimal parameters, the probabilities of moving to
various other states once a transition occurs.
THEOREM
1. A sojourn time for state iof a continuous-time Markov chain has an exponential distribution,
with parameter λ¼qi.
2. Once a sojourn in state ihas ended, the process next moves to a particular state j6¼iwith
probability qij/qi.
Proof Let’s ﬁrst consider the distribution of T= sojourn time in state i, and in particular the
probability that this sojourn time is at least t+h, where his very small. In order for this event to
occur, the process must remain in state icontinuously throughout the time period of length tand then
continue in this state for an additional hunits of time. That is, with F(x) denoting the cdf of Tand
G(x)=1/C0F(x),q01¼4 q02¼.5 q03¼2
q10¼3 q12¼4 q13¼1
q20¼3 q21¼.5 q23¼4
q30¼3 q31¼0 q32¼1548 7 Random Processes
GtþhðÞ ¼ 1/C0FtþhðÞ ¼ PT/C21tþh ðÞ ¼ PX u¼ifor 0 /C20u/C20tand for t/C20u/C20tþh ðÞ
¼PX u¼ifort/C20u/C20tþhjXu¼ifor 0 /C20u/C20t ðÞ /C1 PX u¼ifor 0 /C20u/C20t ðÞ
¼PX u¼ifort/C20u/C20tþhjXt¼i ðÞ /C1 PX u¼ifor 0 /C20u/C20t ðÞ by the Markov property
¼PX u¼ifort/C20u/C20tþhjXt¼i ðÞ /C1 PT/C21t ðÞ
¼PX u¼ifort/C20u/C20tþhjXt¼i ðÞ /C1 GtðÞ
Now, the probability P(Xu=ifort/C20u/C20t+h|Xt=i) is not quite Pii(h), because the latter
includes both the chance of remaining in state ithroughout [ t, t + h ] and also of making multiple
transitions that bring the process back to state iby the end of this time interval. But because his small,
the probability of two or more transitions is negligible compared to the likelihood of either making a
single transition (to some other state j) or remaining in state i. That is, these two probabilities differ by
a term that is o(h). Therefore we have
GtþhðÞ ¼ PiihðÞ þ ohðÞ ½/C138 /C1 GtðÞ¼ 1/C0qih ½/C138 /C1 GtðÞþ ohðÞ )
GtþhðÞ /C0 GtðÞ
h¼/C0qiGtðÞþohðÞ
h
Taking the limit as h!0 results in the differential equation G0(t)= – qiG(t) whose solution is
GtðÞ¼ e/C0qit. Therefore, the cdf of TisFtðÞ¼ 1/C0e/C0qit, an exponential cdf with parameter qi.T h i s
proves the ﬁrst part of the theorem.
For the second part of the theorem, we consider the probability that the process is in state jafter a
short interval of time, given that it is in state iat the beginning of that interval and is notin state iat the
end of the interval:
PX tþh¼jjXt¼i;Xtþh6¼i ðÞ ¼PX tþh¼j;Xt¼i;Xtþh6¼i ðÞ
PX t¼i;Xtþh6¼i ðÞ
¼PX t¼i;Xtþh¼j ðÞ
PX t¼i;Xtþh6¼i ðÞbecause Xtþh¼j;Xtþh6¼i fg ¼Xtþh¼j fg
¼PX tþh¼jjXt¼i ðÞ PX t¼i ðÞ
PX tþh6¼ijXt¼i ðÞ PX t¼i ðÞ
¼PX tþh¼jjXt¼i ðÞ
PX tþh6¼ijXt¼i ðÞ¼PijhðÞ
1/C0PiihðÞ
If we now divide both numerator and denominator by hand take the limit as happroaches 0, the
result is P(next in j| currently in i)=qij/qias asserted. ■
The foregoing theorem gives us a much easier way to think about Markov processes. The process
stays in its current state for an exponentially distributed amount of time; the rate parameter, which
determines the expected time, may be different for different states (the different qis). At the end of that
time, the process transitions from state ito adifferent state according to the transition probabilities
pij¼qij
qi,i6¼j
Equation ( 7.9) guarantees that for any state i, the transition probabilities pijsum to 1. In this way,
a Markov process is completely described by specifying two sets of parameters: the sojourn time
means 1/ q0,1 /q1, etc., and the end-of-sojourn transition probabilities pij. (This set is completely
equivalent to the inﬁnitesimal parameters; in particular, we can recover the instantaneous transition
rates by qij=pij/C1qi.)7.7 Continuous-Time Markov Chains 549
Example 7.36 (Example 7.35 continued) According to the preceding theorem, the time durations
spent by the physician at the nurses’ station are exponentially distributed with parameter λ=q0= 6.5.
Hence, the average length of time she spends there is 1/ λ= 1/6.5 h /C259.23 min. Similarly, the average
sojourn time in Room 3 is 1/ q3= 1/4 h = 15 min. When the doctor leaves the nurses’ station, the
chances of next visiting Rooms 1, 2, and 3 are, respectively,
p01¼q01
q0¼4
6:5¼8
13,p02¼q02
q0¼:5
6:5¼1
13,p03¼q03
q0¼2
6:5¼4
13
Similarly, when the doctor leaves Room 1, there is a 3/8 chance she’ll return to the nurses’ station,
a 4/8 probability of moving on to Room 2, and a 1/8 chance of checking the patients in Room 3.
Notice that we could also obtain these probabilities by rescaling the appropriate row of the array in
Example 7.35 so that the entries sum to 1. In general, the transition probabilities when exiting a
sojourn spent in state iare proportional to the instantaneous transition rates out of state i. ■
Example 7.37 Consider a machine that goes back and forth between working condition (state 0) and
needing repair (state 1). Suppose that the duration of working condition time is exponential with
parameter αand the duration of repair time is exponential with parameter β. From the preceding
theorem, q0=αandq1=β. Equation ( 7.9) then implies that q01=αandq10=β, from which we infer
that
P01hðÞ ¼ αhþohðÞ and P10hðÞ ¼ βhþohðÞ
That is, for very small values of h, the chance of transitioning from working condition to needing
repair in the next htime units is roughly αh, while P(working at time t+h| needing repair at
time t)/C25βhforhsmall.
Notice also that once a sojourn in the working state ( i= 0) has ended, the process moves to the
repair state ( j= 1) with probability p01=q01/q0=α/α= 1. This makes sense, since a machine leaving
the working condition has nowhere else to go except into repair. The same is true if the roles of iandj
are reversed. ■
Example 7.38 A commercial printer has four machines of a certain type. Because there are only
three employees trained to operate this kind of machine, at most three of the four can be in operation
at any given time. Once a machine starts to operate, the time until it fails is exponentially distributed
with parameter α(so the mean time until failure is 1/ α). There are unfortunately only two employees
who can repair these machines, each of whom works on just one machine at a time. So if three
machines need repair at any given time, only two of these will be undergoing repair, and if all four
machines need repair, two will be waiting to start the repair process. Time necessary to repair a
machine is exponentially distributed with parameter β(thus mean time to repair is 1/ β).
LetXtbe the number of functional machines at time t. Possible values of Xt(i.e., the states of the
process) are 0, 1, 2, 3, and 4. If the system is currently in state 1, 2, 3, or 4, one possible state transition
results from one of the working machines suddenly breaking down. Alternatively, if the system is
currently in state 0, 1, 2, or 3, the next transition might result from one of the non-functional machines
ﬁnishing the repair process. These possible transitions are depicted in the state diagram in Fig. 7.25.
01234Fig. 7.25 State diagram
for Example 7.36550 7 Random Processes
The eight non-zero instantaneous transition probabilities must be determined. Two of them follow
the derivation from the Poisson process in Example 7.34:
q10¼instantaneous failure rate for one machine ¼α
corresponding to the one working machine going downðÞ
q34¼instantaneous repair rate for one machine ¼β
in state 3 ;only one machine is currently being repaired ðÞ
Next, consider the transition from 2 working machines to just 1. For a time interval of length h,
P21hðÞ ¼ PX tþh¼1jXt¼2 ðÞ ¼ P/C0
1stworking machine breaks down [
2ndworking machine breaks down/C1
¼P1stbreaksðÞ þ P2ndbreaks/C0/C1
/C0Pboth breakðÞ
¼1/C0e/C0αh/C0/C1
þ1/C0e/C0αh/C0/C1
/C01/C0e/C0αh/C0/C12
The term (1 – e–αh) comes from the fact that for an exponentially distributed rv T,
P(T/C20t+h|T/C21t)=1– e–αh. Differentiating and substituting h= 0 gives
q21¼P0
210ð Þ¼/C1/C1/C1¼ 2α
When exactly two machines are working, the instantaneous failure rate is twice that of a single
machine (because, in effect, twice as many things can go wrong).
By similar reasoning, q32=3α. But also q43=3α—although four machines are functional, only
three are actually in operation, so it’s impossible for all four to break.
Finally, a nearly identical calculation gives q01=2β(none of the machines are working, but only
two are undergoing repair), and likewise q12=q23=2β.
From Eq. ( 7.9), the parameters of the exponential sojourn distributions are
q0¼2βq1¼αþ2βq2¼2αþ2βq3¼3αþβq4¼3α
So, for example, the length of a time interval in which exactly three machines are operating has an
exponential distribution with λ=3α+β, and the expected duration of such an interval is1/(3 α+β).
That interval ends when either the one broken machine is repaired ( Xttransitions from 3 to 4) or one
of the three working machines breaks (and the chain goes from 3 to 2). The probabilities of those two
transitions are
p34¼q34
q3¼β
3αþβand p32¼q32
q3¼3α
3αþβ
The mean repair time 1/ βwill be small when repair times are very fast, in which case βitself is
large and p34might be the higher probability. Otherwise, p32is apt to be larger, since any of three
machines could break down in the time the one non-functional machine is being repaired. ■
A continuous-time Markov chain for which the only possible transitions from state iare either to
state i/C01 or state i+ 1 is called a birth and death process . The Poisson process is an example of a
pure birth process—no deaths are allowed. In Example 7.38, a birth occurs when a machine ﬁnishes
repair, and a death occurs when a machine breaks down. Thus, starting from state 0 only a birth is
possible, starting from state 4 only a death is possible, and either a birth or a death is possible when
starting from state 1, 2, or 3.7.7 Continuous-Time Markov Chains 551
7.7.3 Long-Run Behavior of Continuous-Time Markov Chains
Consider ﬁrst a continuous-time Markov chain for which the state space is ﬁnite and consists of the
states 0, 1, 2, 3, ...,N. Then we already know that
lim
h!0PiihðÞ /C0 1
h¼/C0qi, lim
h!0PijhðÞ
h¼qijfori6¼j
Let’s now create a matrix of these parameters—the exponential sojourn parameters and the
instantaneous transition rates—in which the diagonal elements are the /C0qis and the off-diagonal
elements are the qijs. Here is the matrix in the case N=4 :
Q¼/C0q0q01 q02 q03 q04
q10/C0q1q12 q13 q14
q20 q21/C0q2q23 q24
q30 q31 q32/C0q3q34
q40 q41 q42 q43/C0q42
666643
77775
Equation ( 7.9) implies that the sum of every row in this matrix of parameters is zero (since each qi
is the sum of the other qijs in its row). This matrix Qis sometimes called the generator matrix of the
Markov process.
Next, deﬁne a transition matrix P(t) whose ( i, j)th entry is the transition probability Pij(t)=
P(Xt=j|X0=i). Analogous to the discrete case, the Chapman-Kolmogorov equations can be
rendered in terms of the transition matrix: P(t+h)=P(t)P(h).We now consider the derivative of
the transition matrix at time t:
P0tðÞ¼ lim
h!0PtþhðÞ /C0 PtðÞ
h¼lim
h!0PtðÞPhðÞ /C0 PtðÞ
h¼PtðÞlim
h!0PhðÞ /C0 I
h/C26/C27
From the earlier derivatives, the limit of the matrix inside braces is precisely the generator matrix
Q. Thus we obtain the following system of so-called “forward” differential equations in the transition
probabilities:
P0tðÞ¼ PtðÞQ ð7:10Þ
where P0(t) is the matrix of derivatives of the transition probabilities.
As in the discrete case, if the chain is irreducible, i.e. all states communicate with one another, then
Pij(t)>0 for every pair of states and lim t!1Pij(t) exists and equals a value πjindependent of the
initial state. Thus as t!1 ,P0(t) approaches a matrix consisting entirely of 0s (because the
probabilities themselves are approaching constants independent of t) and P(t) itself approaches a
matrix each of whose rows is π=[π0,π1,...,πN]. Applying these statements to (7.10) and taking the
top row (or any row) of each side, the vector of stationary probabilities must then satisfy 0=πQ,a s
well as ∑πj= 1. Slight rearrangement of the equations gives
π0q0¼π1q10þπ2q20þ/C1/C1/C1þ πNqN0
π1q1¼π0q01þπ2q21þ/C1/C1/C1þ πNqN1
/C1/C1/C1
πNqN¼π0q0Nþπ1q1Nþ/C1/C1/C1þ πN/C01qN/C01,N
π0þπ1þπ2þ/C1/C1/C1þ πN¼1
Consider the ﬁrst of these equations. The left hand side gives the long-run rate at which the process
leaves state 0, and the right hand side is the sum of rates at which the process goes from some other552 7 Random Processes
state to state 0. So, the equation says that the long-run rate out of that state equals the long-run rate
into the state. The other equations have analogous interpretations.
Example 7.39 (Example 7.38 continued) The generator matrix for the printing machines scenario is
Q¼/C02β 2β 00 0
0/C0αþ2β ðÞ 2β 00
02 α/C02αþ2β ðÞ 2β 0
003 α/C03αþβ ðÞ β
00 0 3 α/C03α2
666643
77775
The stationary distribution of the chain satisﬁes 0=πQ. Expanding these matrices, the resulting
system of equations is
/C02βπ0þαπ1¼0, 2βπ0/C0αþ2β ðÞ π1þ2απ2¼0, 2 βπ1/C02αþ2β ðÞ π2þ3απ3¼0
2βπ2/C03αþβ ðÞ π3þ3απ4¼0,βπ3/C03απ4¼0
The ﬁrst equation immediately gives π1=( 2β/α)π0. Then substituting this expression for π1into
the second equation and doing a bit of algebra results in π2=( 2β2/α2)π0. Now substitute this
expression into the third equation, solve for π3in terms of π0, and obtain an expression for π4in
terms of π0. The stationary probabilities are then
π0,π1¼2β
απ0,π2¼2β2
α2π0,π3¼4β3
3α3π0,π4¼4β4
9α4π0
Finally, the fact that the sum of all ﬁve πs equals 1 gives an expression for π0:
π0¼1
1þ2β
αþ2β2
α2þ4β3
3α3þ4β4
9α4
Consider two different speciﬁc cases: (i) α=1 ,β= 2, (ii) α=2 ,β= 1. In the ﬁrst case, machines get
repaired more quickly than they fail, and in the second case the opposite is true. Here are the
stationary probabilities:
(i)π0= .0325, π1= .1300, π2= .2600, π3= .3466, π4= .2310
(ii)π0= .3711, π1= .3711, π2= .1856, π3= .0619, π4= .0103
In the ﬁrst case, the mean number of machines in operating condition is ∑iπi= 2.614, and in the
second case it is only .969. ■
Under quite general conditions, the forward system of differential equations ( 7.10) is valid for a
birth and death process even when the state space is inﬁnite (i.e., when there is no upper bound on the
population size). Furthermore, the stationary distribution exists and has a rather simple form. Let
θ0¼1,θ1¼q01
q10,θ2¼q01q12
q10q21,θ3¼q01q12q23
q10q21q32, ... ð7:11Þ
Then π1=θ1π0,π2=θ2π0,π3=θ3π0,..., and π0=1 /∑θiprovided that the sum in the
denominator is ﬁnite.7.7 Continuous-Time Markov Chains 553
Example 7.40 Customers arrive at a service facility according to a Poisson process with rate
parameter λ(so the times between successive arrivals are independent and exponentially distributed,
each with parameter λ). The facility has only one server, and the service time for any particular
customer is exponentially distributed with parameter μ. This is often referred to as an M/M/1 queue ,
where M stands for Markovian. Let Xtrepresent the number of customers in the system at time t.
The mean time between successive arrivals is 1/ λ, and the mean time for a service to be completed
is 1/μ. Intuitively if 1/ μ>1/λ(i.e. if μ<λ), then customers will begin to pile up in the system and
there won’t be a limiting distribution because the number of customers in the system will grow
arbitrarily large over time. We therefore restrict consideration to the case λ<μ(the case in which
λ=μis a bit tricky).
The inﬁnitesimal birth parameters are qi,i+1=λfori=0 ,1 ,2 , ..., since a birth occurs when a new
customer enters the facility, and the inﬁnitesimal death parameters are qi+1,i=μfori=1 ,2 ,3 , ...,
since a death occurs when a customer ﬁnishes service. Substituting into (7.11),
θk¼λk
μkk¼0, 1, 2, 3, ...,π0¼1
X1
k¼0λ
μ/C18/C19k¼1/C0λ
μ
πk¼π0θk¼1/C0λ
μ/C18/C19λ
μ/C18/C19k
k¼0, 1, 2, 3, ...
This is similar to a geometric distribution with p=1/C0λ/μ, except that the terms start at k= 0 rather
than k= 1. Nevertheless, we can quickly determine that the mean number of customers in the system
is∑kπk=(λ/μ)/(1/C0λ/μ)=λ/(μ/C0λ). ■
7.7.4 Explicit Form of the Transition Matrix
In Example 7.33, the deﬁnition of a Poisson process allowed us to write explicit expressions for the
transition probabilities explicitly: substituting tforhin that example, we have for all non-negative
integers iandjthat
PijtðÞ¼e/C0λtλtðÞj/C0i
j/C0iðÞ !i/C20j
0 i>j8
<
:
Exercises 90–92 provide examples where Pij(t) can be determined from a system of differential
equations based on the problem description. But since Markov processes are often speciﬁed in terms
of their inﬁnitesimal parameters, it’s desirable to have a method for determining the functions Pij(t)—
or, equivalently, the matrix P(t) of those functions—solely from the qis and qijs. We indicate such a
method below.
The forward system of differential equations was obtained by decomposing the time interval from
0t ot+hinto the interval from 0 to tand the interval from ttot+h. A“backward” system of equations
results from considering the two intervals [0, h] and ( h,t+h] and again using the Chapman-
Kolmogorov equations: P(t+h)=P(h)P(t). The derivative of the transition matrix at time tis then
lim
h!0PtþhðÞ /C0 PtðÞ
h¼lim
h!0PhðÞPtðÞ/C0 PtðÞ
h¼lim
h!0PhðÞ /C0 I
h/C26/C27
PtðÞ554 7 Random Processes
The matrix limit is again Q, giving the following system of equations:
P0tðÞ¼ QPtðÞ ð 7:12Þ
Contrast the “backward” equation ( 7.12) with the “forward” equation ( 7.10): the two matrices on
the right-hand side are simply reversed. Of course, in general matrices do not commute, so one
equation does not follow from the other; that both QP(t) and P(t)Qequal P0(t) is a consequence of the
Markov property.
Now recall from elementary differential equations that the solution to the equation f0(t)=cf(t)i s
f(t)=ect, and also that the inﬁnite series expansion for ektis 1þP1
k¼1cktk=k!. By analogy, the
solution to our system of backward equations ( 7.12)i s
PtðÞ¼ eQt¼IþX1
k¼1Qktk
k!
Example 7.41 (Example 7.37 continued) Let’s return to the scenario involving a single machine
which is either working or undergoing repair, where time until failure has an exponential distribution
with parameter αand repair time is exponentially distributed with parameter β. The matrix of
inﬁnitesimal parameters is
Q¼/C0αα
β/C0β/C20/C21
It is easily veriﬁed that Qk=[/C0(α+β)]k–1Q, from which
PtðÞ¼ I/C01
αþβX1
k¼1/C0αþβðÞ½/C138ktk
k!Q
¼I/C01
αþβe/C0αþβðÞ t/C01hi
Q
¼1/C0α
αþβ1/C0e/C0αþβðÞ t/C16/C17α
αþβ1/C0e/C0αþβðÞ t/C16/C17
β
αþβ1/C0e/C0αþβðÞ t/C16/C17
1/C0β
αþβ1/C0e/C0αþβðÞ t/C16/C172
6643
775
We now have a completely explicit formula for the transition probabilities of the Markov process
for any time duration t. Notice that the sum of each row in the transition matrix is 1, as required.
This explicit form of P(t) also allows us to investigate the chain’s long-run behavior. Speciﬁcally,
ast!1 ,
PtðÞ! Iþ1
αþβQ¼β=αþβðÞ α=αþβðÞ
β=αþβðÞ α=αþβðÞ/C20/C21
Thus the stationary distribution is given by π0=β/(α+β) and π1=α/(α+β), which could also
have been obtained by solving πQ=0 ,π0+π1=1 . ■7.7 Continuous-Time Markov Chains 555
7.7.5 Exercises: Section 7.7(86–97)
86. The authors of the article “A Multi-State Markov Model for a Short-Term Reliability Analysis of
a Power Generating Unit” ( Reliab. Eng. and Sys. Safety , 2012: 1–6) modeled the transitions of a
particular coal-ﬁred generating unit through four states, characterized by the unit’s capacity:
0¼complete failure, (0 MW of power), 1 ¼247 MW, 2 ¼482 MW, and 3 ¼575 MW (full
power). Observation of the unit over an extended period of time yielded the following instanta-
neous transition rates:
q01¼.0800 q02¼.0133 q03¼0
q10¼.0294 q12¼.3235 q13¼.0294
q20¼0 q21¼.0288 q23¼.3558
q30¼.0002 q31¼.0001 q32¼.0007
(a) Determine the complete generator matrix Qof this Markov process.
(b) Determine the stationary probabilities of this process.
(c) What is the long-run expected output of this particular unit, in megawatts?
87. Potential customers arrive at a service facility according to a Poisson process with rate λ.
However, an arrival will enter the facility only if there is no one already being served, and
otherwise will disappear (there is no waiting room!). Once a customer enters the facility, service
is carried out in two stages. The time to complete the ﬁrst stage of service is exponentially
distributed with parameter λ1. A customer completing the ﬁrst stage of service immediately
enters the second stage, where the distribution of time to complete service is exponential with
parameter λ2.
(a) Deﬁne appropriate states, and then identify the qis and qijs.
(b) Determine the stationary probabilities when λ¼1,λ1¼3,λ2¼2.
(c) Determine the stationary probabilities when λ¼1,λ1¼2,λ2¼3.
(d) Determine the stationary probabilities when λ¼4,λ1¼2,λ2¼1.
88. Return to the scenario of the previous exercise, and now suppose that the facility has a waiting
area that will accommodate one customer. A customer in the waiting area cannot begin the ﬁrst
stage of service until the previous customer has completed both stages.
(a) Deﬁne appropriate states, and then identify the qis and qijs. [Hint: The chain now has ﬁve
possible states.]
(b) Determine the stationary probabilities when λ¼1,λ1¼3,λ2¼2.
(c) Determine the stationary probabilities when λ¼1,λ1¼2,λ2¼3.
(d) Determine the stationary probabilities when λ¼4,λ1¼2,λ2¼1.
89. Reconsider the scenario of Exercise 87. Now suppose that a customer who ﬁnishes stage 2 service
leaves the facility with probability .8, but with probability .2 returns to stage 1 for rework because
of deﬁcient service and then proceeds again to stage 2.
(a) Deﬁne appropriate states, and then identify the qis and qijs.
(b) Determine the stationary probabilities when λ¼1,λ1¼3,λ2¼2.
(c) Determine the stationary probabilities when λ¼1,λ1¼2,λ2¼3.
(d) Determine the stationary probabilities when λ¼4,λ1¼2,λ2¼1.
(e) What is the expected total time that a customer remains in the facility once he/she has
entered?556 7 Random Processes
90. The Yule Process is a special case of a birth and death process in which only births occur; each
member of the population at time thas probability βh+o(h) of giving birth to an additional
member during a short time interval of length hindependently of what happens to any other
member of the population at that time (so there is no interaction among population members). Let
Xtdenote the population size at time t.
(a) Show that if the population size is currently n, then the probability of a birth in the next
interval of length hisnβh+o(h), and that the probability of no births in the next interval of
length his 1/C0nβh+o(h).
(b) Relate Pij(t+h) to the transition probabilities at time t, and take an appropriate limit to
establish a differential equation for Pij(t). [Hint: IfXt+h¼jandhis small, there are only two
possible values for Xt. Your answer should relate Pij0(t),Pij(t), and Pi,j/C01(t).]
(c) Assuming that there is one individual alive at time 0, show that a solution to the differential
equation in (b) is P1n(t)¼e/C0βt(1/C0e/C0βt)n/C01. (In fact, this is the only solution satisfying the
initial condition.)
(d) Determine the expected population size at the t, assuming X0¼1. [Hint: What type of
probability distribution is P1n(t)?]
91. Another special case of a birth and death process involves a population consisting of
Nindividuals. At time t¼0 exactly one of these individuals is infected with a particular disease,
and the other N/C01 are candidates for acquiring the disease (susceptibles). Once infected,
an individual remains so forever. In any short interval of time h, the probability that any
particular infected individual will transmit the disease to any particular non-diseased individual
isβh+o(h). Let Xtrepresent the number of infected individuals at time t.Specify the birth
parameters for this process. [ Hint: Use the differential equation from the last exercise.]
92. At time t¼0 there are Nindividuals in a population. Let Xtrepresent the number of individuals
alive at time t.Alinear pure death process is one in which the probability that any particular
individual alive at time tdies in a short interval of length hisβh+o(h); no births can occur,
deaths occur independently, and there is no immigration into the population.
(a) Obtain a differential equation for the transition probabilities of this process, and then show
that the solution is PNntðÞ¼N
n/C18/C19
e/C0nβt1/C0e/C0βt/C0/C1N/C0n.
(b) What is the expected population size at time t?[Hint: According to (a), what type of
probability distribution is PNn(t)?]
93. A radioactive substance emits particles over time according to a Poisson process with parameter λ.
Each emitted particle has an exponentially distributed lifetime with parameter β, and the lifetime
of any particular particle is independent of that of any other particle. Let Xtbe the number of
particles that exist at time t. Assuming that X0¼0, specify the parameters of this birth and death
process.
94. Consider a machine shop that has three machines of a particular type. The time until any one of
these machines fails is exponentially distributed with mean lifetime 10 h, and machines fail
independently of one another. The shop has a single individual capable of repairing these
machines. Once a machine fails, it will immediately begin service provided that the other two
machines are still working; otherwise it will wait in a repair queue until the repair person has
ﬁnished work on any other machines that need service. Time to repair is exponentially distributed
with expected repair time 2 h. Obtain the stationary probabilities and determine the expected
number of machines operating under stationary conditions.
95. A system consists of two independent components connected in parallel, so the system will
function as long as at least one of the components functions. Component A has an exponentially7.7 Continuous-Time Markov Chains 557
distributed lifetime with parameter α0. Once it fails, it immediately goes into repair, and its repair
time is exponentially distributed with parameter α1. Similarly, component B has an exponentially
distributed lifetime with parameter β0and an exponentially distributed repair time with parameter
β1. Determine the stationary probabilities for the corresponding continuous time Markov chain,
and then the probability that the system is operating.
96. The article “Optimal Preventive Maintenance Rate for Best Availability with Hypo-Exponential
Failure Distribution” ( IEEE Trans. on Reliability , 2013: 351-361) describes the following model for
maintenance of a particular machine. The machine naturally has three states: 0 ¼“up” (i.e., fully
operational), 1 ¼ﬁrst stage degraded, and 2 ¼second stage degraded. A machine in state 2 requires
corrective maintenance, which restores the machine to the “up” state. But the machine’s operators
can voluntarily put a machine currently in states 0 or 1 into one other state, 3 ¼preventive
maintenance. The cited article gives the following instantaneous transition rates:
q01¼λ1 q02¼0 q03¼δ
q10¼0 q12¼λ2 q13¼δ
q20¼μ q21¼0 q23¼0
q30¼mq 31¼0 q32¼0
The parameter δ/C210 is called the trigger rate for preventive maintenance and is controlled by
the machine’s operator.
(a) Draw a state diagram for this Markov process.
(b) Interpret the parameters λ1,λ2,μ, and m.
(c) Determine the stationary distribution of this chain.
The machine can be operated in both states 0 and 1, and so the availability of the
machine, A(δ), is deﬁned to be the sum of the stationary probabilities for those two states.
(d) Show that
AδðÞ ¼ 1þδ
mþλ1λ2
μλ 1þλ2þδ ðÞ/C20/C21/C01
(e) Determine the value of δthat maximizes the long-run proportion of time the machine is
available for use. [ Hint: You’ll have to consider two separate cases, depending on whether a
certain quadratic equation has any positive solutions.]
97. A discrete-time Markov chain, i.e., the type investigated in Chap. 6, can be obtained from a
continuous-time chain by sampling it every htime units. That is, for n¼0, 1, 2, ...we deﬁne
Yn¼Xnh,
where Xtis a Markov process. For example, the doctor’s movements in Example 7.33 could be
observed every 6 min ( h¼1/10 h), and a discrete-time Markov chain could be deﬁned by
Yn¼Xn/10¼the nurses’ location at the nth observed time.
(a) Let Pbe the one-step transition matrix for Yn,s ot h e( i, j)th entry of Pispij¼
P(Yn+1¼jjYn¼i). Show that pij/C25qijhfori6¼jandpii/C25(1/C0qi)h, where the qijs and
qis are the inﬁnitesimal parameters of Xtand the approximations are on the order o(h).
(b) Suppose Ynis a regular chain, and that the one-step transition probabilities in part (a) are
exact (rather than just o(h)-approximate). Show that the stationary distribution of Ynis
identical to that of its continuous-time version Xt.[Hint: Use part (a) to show that the
equations πP¼πfrom Chap. 6andπQ¼0from this section are one and the same.]558 7 Random Processes
7.8 Supplementary Exercises (98–114)
98. Let X(t) be a WSS random process.
(a) Show that
VarXtþτðÞ /C0 XtðÞ ½/C138 ¼ EX t þτðÞ /C0 XtðÞ ðÞ2hi
¼2RXX0ðÞ /C0 RXXτðÞ ½/C138
¼2CXX0ðÞ /C0 CXXτðÞ ½/C138 :
(b) Show that if CXX(d)¼CXX(0) for any d6¼0, then X(t)i smean square periodic , that is,
E[(X(t+d )/C0X(t))2]¼0.
(c) Show that if CXX(d)¼CXX(0) for any d6¼0, then CXX(τ) is periodic. (A similar property
holds for RXX.) [Hint: Consider the covariance of X(0) and X(τ+d)/C0X(τ), and use the
fact that jCov( U, V)j/C20SD(U)/C1SD(V) for any two rvs UandV.]
99. Consider the following model for binary voltage noise: let V1,V2,...be independent rvs with
P(Vn¼+1)¼P(Vn¼/C01)¼.5. Then deﬁne X(t)¼Vnforn/C01/C20t<n, i.e., V1is trans-
mitted for 0 /C20t<1,V2is transmitted for 1 /C20t<2, and so on.
(a) Find the mean function of X(t).
(b) Find the autocovariance function of X(t). [Hint: Consider separate cases depending on
whether or not tandslie in the same unit interval, e.g., [1, 2).]
100. Modify the previous exercise as follows: let T0~ Unif[0, 1] be independent of the Vns. Then the
random process X(t) equals V1forT0/C20t<T0+1 ,V2is transmitted for T0+1/C20t<T0+2 ,
and so on.
(a) Find the mean function of X(t).
(b) Find the autocorrelation function of X(t). [Hint: First ﬁnd the conditional distribution of
X(t) and X(s) given T0¼t0.]
101. Deﬁne a collection of random processes by Xk(t)¼Akcos(ωkt)+Bksin(ωkt) for k¼1, 2, ...,n,
where the coefﬁcients A1,...,An,B1,...,Bnare iid Unif[ /C01, 1] rvs, and the frequencies ω1,...,
ωnare constants. Let Y(t)¼X1(t)+...+Xn(t).
(a) Find the mean and autocovariance functions of Xk(t) for k¼1, 2, ...,n.
(b) Find the mean and autocovariance functions of Y(t). IsY(t)W S S ?
102. Let Θ1,...,Θnbe iid Unif( /C0π,π] rvs and deﬁne a random process X(t)b y
XtðÞ¼Xn
k¼1aksinωktþΘk ðÞ
IsX(t) wide-sense stationary?
103. Let X(t)¼cos(Ωt+Θ), where ΩandΘare independent rvs, Θ~ Unif( /C0π,π], and Ωequals ωk
with probability pkfork¼1, 2, ...,n(i.e.,Ωis a discrete rv).
(a) Find the mean function of X(t).
(b) Find the autocovariance function of X(t).
(c) Is X(t) WSS?
104. Let X(t) be a WSS random process, and let Y(t)¼X(t/C0d), ad-second delayed version of X(t).
(a) Find the mean and autocorrelation functions of Y(t) in terms of those of X(t).
(b) Is Y(t) WSS?
(c) Find the cross-correlation RXY(t, t + τ). Are X(t) and Y(t) jointly WSS?
105. A rotor within a certain manufacturing machine must be replaced every 125 h of use, on
average. Let Xndenote the lifetime of the nth rotor ( n¼1, 2, 3, ...), and suppose the Xns are7.8 Supplementary Exercises (98–114) 559
iid exponential rvs with mean 125 h. (In this context, the “time” index nactually counts rotors,
not hours or some other time unit.)
(a) Deﬁne Sn¼X1+/C1/C1/C1+Xn. Interpret Snin this context.
(b) Find the mean, variance, autocorrelation, and autocovariance functions of Sn.
(c) Use the Central Limit Theorem to determine the approximate distribution of S50and to
approximate P(S50/C216240), the chance that 50 rotors will be sufﬁcient to operate the
machine for 3 years (40 h per week, 52 weeks a year).
106. Let X(t) be a WSS random process with mean μXand autocovariance function CXX(τ).
(a) Show that E[hX(t)iT]¼μXfor all T.[Note: Since this is the ensemble average of X(t), it
follows that X(t) is mean ergodic iff Var( hX(t)iT)!0a sT!1 .]
(b) It is straightforward to show that
Var XtðÞT/C10/C11/C0/C1
¼1
4T2ðT
/C0TðT
/C0TCXXs/C0tðÞ dtds
Make the substitution τ¼s/C0tto prove
Var XtðÞhiT/C0/C1
¼1
2Tð2T
/C02TCXXτðÞ 1/C0τjj
2T/C18/C19
dτ,
so that X(t) is mean ergodic iff this integral converges to 0 as T!1 . [This can be a useful
test for ergodicity when a model is speciﬁed in terms of its covariance function and no
explicit form of X(t) is available.]
(c) Show that X(t) is mean ergodic if1
2Tð2T
/C02TCXXτðÞdτ!0a sT!1 .
107. Let Xnbe a WSS random sequence, and deﬁne Yn¼Xn/C0Xn/C01.I sYnalso WSS?
108. Let Xnbe iid, with mean 0 and variance σ2. Deﬁne a kth-order moving average sequence Ynby
Yn¼α1Xnþ/C1/C1/C1þ αkXn/C0kþ1
where the nonnegative constants αiare such that α1+/C1/C1/C1+αk¼1.
(a) Find the mean function of Yn.
(b) Find the variance function of Yn.
(c) Find the autocovariance function of Yn.
(d) Is Ynwide-sense stationary?
(e) Find the correlation coefﬁcient ρ(Yn,Yn+k).
109. Suppose that noise impulses occur on a telephone line at random, with a mean rate λper second.
Assume the occurrence of noise impulses meet the conditions of a Poisson process.
(a) Find the probability that no noise impulses occur during the transmission of a t-second
message.
(b) Suppose that the message is encoded so that errors caused by a single noise impulse can be
corrected. What is the probability that a t-second message is either error-free or
correctable?
(c) Suppose the error correction protocols can reset themselves so long as successive noise
impulses are more than εseconds apart. What is the probability the next noise impulse will
be corrected?
110. A bus has just departed from a certain New York City bus stop. Passengers for the next bus
arrive according to a Poisson process with rate 3 per minute. Suppose the arrival time Yof the
next bus has a uniform distribution on the interval [0, 5].560 7 Random Processes
(a) Given that Y¼y, what is the expected number of passengers at the stop for this next bus?
(b) Use the result of (a) along with the Law of Total Expectation to determine the expected
number of passengers at this stop when the next bus arrives.
(c) Given that Y¼y, determine the (conditional) variance of the number of passengers at the
stop for this next bus. Then use the Law of Total Variance to determine the standard
deviation of the number of passengers at this stop for the next bus.
111. Starting at time t¼0, commuters arrive at a subway station according to a Poisson process with
rateλper minute .The subway fare is $2. Suppose this fare is “exponentially discounted” back to
time 0; that is, if a commuter arrives at time t, the resulting discounted fare is 2 e/C0αt, where αis
the “discount rate.”
(a) If ﬁve commuters arrive in the ﬁrst t0minutes, what is the expected value of the total
discounted fare collected from these ﬁve individuals? [ Hint: Recall that for a Poisson
process, conditional on any particular number of events occurring in some time interval,
each event occurrence time is uniformly distributed over that interval.]
(b) What is the expected value of the total discounted fare collected from customers who arrive
in the ﬁrst t0minutes? [ Hint: Conditioning on the number of commuters who arrive, use an
expected value argument like that employed in (a), and then apply the Law of Total
Probability.]
112. Individuals enter a museum exhibit according to a Poisson process with rate λ. The amount of
time any particular individual spends in this exhibit is a random variable having an exponential
distribution with parameter θ, and these exhibit-viewing times are independent of one another.
LetY(t) denote the number of individuals who have entered the exhibit prior to time tand are
still viewing the exhibit, and let Z(t) denote the number of individuals who have entered the
exhibit and departed by time t.
(a) Obtain an expression for P(Y(t)¼6 and Z(t)¼4).
(b) Generalize the argument leading to the expression of (a) to obtain the joint pmf of the two
random variables Y(t) and Z(t).
113. According to the article “Reliability Evaluation of Hard Disk Drive Failures Based on Counting
Processes” ( Reliability Engr. and System Safety , 2013: 110–118), particles accumulating on a
disk drive come from two sources, one external and the other internal. The article proposed a
model in which the internal source contains a number of loose particles Mhaving a Poisson
distribution with mean value μ; when a loose particle releases, it immediately enters the drive,
and the release times are iid with cumulative distribution function G(t). Let X(t) denote the
number of loose particles not yet released at a particular time t. Show that X(t) has a Poisson
distribution with parameter μ[1/C0G(t)]. [Hint:L e t Y(t) denote the number of particles
accumulated on the drive from the internal source by time t, so that X(t)+Y(t)¼M. Obtain
an expression for P(X(t)¼x,Y(t)¼y), and then sum over y.]
114. Suppose the strength of a system is a nonnegative rv Ywith pdf g(y).The system experiences
shocks over time according to a Poisson process with rate λ. Let Xidenote the magnitude of the
ith shock, and suppose the Xis are iid with cdf F(x). If when the ith shock occurs, Xi>Y, then
the system immediately fails; otherwise it continues to operate as though nothing happened. Let
S(t) denote the number of shocks in [0, t], and let Tdenote the system lifetime.
(a) Determine the probability P(T>tjY¼yand S(t)¼n). [Hint: Your answer should
involve y, n, and the cdf F.]
(b) Apply the Law of Total Probability along with (a), to determine P(T>tjY¼y).7.8 Supplementary Exercises (98–114) 561
(c) Obtain an integral expression for the probability that the system lifetime exceeds t.[Hint:
Write P(T>t) as a double integral involving the joint pdf of TandY. Then simplify to a
single integral using (b).]
(Based on the article “On Some Comparisons of Lifetimes for Reliability Analysis,” Reliability
Engr. and Safety Analysis , 2013: 300–304.)562 7 Random Processes
Introduction to Signal Processing8
The previous chapter introduced the concept of a random process and explored in depth the temporal
(i.e., time-related) properties of such processes. Many of the speciﬁc random processes introduced in
Chap. 7are used in modern engineering to model noise or other unpredictable phenomena in signal
communications. In this chapter, we investigate the frequency -related properties of random pro-
cesses, with a particular emphasis on power and ﬁltering.
Section 8.1introduces the power spectral density , which describes how the power in a random
signal is distributed across all possible frequencies. This ﬁrst section also discusses so-called white
noise processes, which are best described in terms of a frequency distribution. In Sect. 8.2, we look at
ﬁlters; or, more precisely, linear, time-invariant (LTI) systems. We explore some techniques for
ﬁltering random signals, including the use of so-called “ideal” ﬁlters. Finally, Sect. 8.3reexamines
these topics in the context of discrete-time signals.
We assume throughout this chapter that readers have some familiarity with (nonrandom) signals
and frequency representations. In particular, knowledge of Fourier transforms and LTI systems will
be critical to understanding our exposition. Appendix B includes a brief summary of the properties of
Fourier transforms; Sect. 8.2includes a short discussion of LTI systems.
8.1 Power Spectral Density
In Chap. 7, we considered numerous models for random processes X(t) as well as several ways to
quantify the statistical properties of such processes (the mean, variance, autocovariance, and auto-
correlation functions). All of these statistical functions describe the behavior of X(t) in the time
domain. Now we turn our attention to properties of a random process that can be described in the
frequency domain .
At the outset, some basic notation and conventions must be established. First, the letter jwill
denoteﬃﬃﬃﬃﬃﬃ ﬃ
/C01p
in order to be consistent with engineering practice (some readers may be more familiar
with the symbol i). Second, we will denote frequency by f, whose units are Hertz (1/s). For those more
familiar with radian frequency ω, the two are of course related by ω¼2πf. Third, throughout this
chapter X(t) will represent a random current waveform through a 1- Ωimpedance. This is a standard
convention in signal processing; it has the advantage that we can talk about current and voltage
interchangeably (since V¼IR). Finally, we will assume that all random processes are wide-sense
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6_8563
stationary (WSS) unless otherwise noted, because this is a key assumption for the main theorem of
this section.
Our ultimate goal is to describe how the power in a random process is distributed across the
frequency spectrum. From basic electrical engineering, we know P¼I2R, where P¼power, I¼
current ¼X(t), and R¼resistance ¼1Ω. Hence, we may think of I2R¼X2(t) as the “instantaneous
power” in the random process at time t.
DEFINITION
LetX(t) be a WSS random process. The (ensemble) average power (also called the expected
power )o fX(t), denoted by PX,i s
PX¼EX2tðÞ/C2/C3
The average power of X(t) is related to its autocorrelation function by
PX¼RXX0ðÞ
Notice we may write PXrather than PX(t), i.e., the ensemble average power of X(t) does not vary
with time. This is due to the assumption of wide-sense stationarity. To see why PXequals RXX(0),
recall that for WSS processes we have RXX(τ)¼E[X(t)X(t+τ)], which does not depend on t. Setting
τ¼0 immediately gives RXX(0)¼E[X2(t)]¼PX.
Example 8.1 In Chap. 7, we introduced the random process X(t)¼A0cos(ω0t+Θ), where the phase
shiftΘis uniformly distributed on the interval ( /C0π,π]. We showed that X(t) is WSS, with mean
μX¼0 and autocovariance function CXX(τ)¼(A02/2)cos( ω0τ), from which RXX(τ)¼(A02/2)cos( ω0τ)
as well. Thus, the ensemble average power of the phase variation process is
PX¼RXX0ðÞ ¼A2
0
2cosω0/C10ðÞ ¼A2
0
2
This formula for the average power of a sinusoid is well known to electrical engineers. ■
Now we turn to describing how the expected power PXin a random process is distributed across
the frequency domain. For example, is this power concentrated at just a few frequencies, or across a
very large frequency band? Typically in engineering, we move from the time domain tto the
frequency domain fby taking the Fourier transform of our time-dependent function. Because of
some technical issues related to the existence of certain integrals that arise in connection with random
processes, we must proceed carefully here. To begin, deﬁne a truncated version of a random process
X(t)b y
XTtðÞ¼XtðÞ/C12/C12t/C12/C12/C20T
0 otherwise/C26
This function is square-integrable with respect to t, and so its Fourier transform exists. Deﬁne
FTfðÞ¼ FXTtðÞfg ¼ð1
/C01XTtðÞe/C0j2πftdt¼ðT
/C0TXtðÞe/C0j2πftdt564 8 Introduction to Signal Processing
Parseval’s Theorem then connects the integrals of XT(t) and FT(f):Ð1
/C01jFT(f)j2df¼Ð1
/C01jXT(t)j2dt
¼ÐT
/C0TX2(t)dt, where the absolute value bars denote the magnitude of a possibly complex number.
(Since X2(t) is real-valued and nonnegative, those bars may be dropped.) Divide both sides by 2 T:
ð1
/C01/C12/C12FTfðÞ/C12/C122
2Tdf¼ðT
/C0TX2tðÞ
2Tdt¼1
2TðT
/C0TX2tðÞdt ð8:1Þ
Since the right-most expression in Eq. ( 8.1) gives the average power in X(t) across the interval
[/C0T, T], so does the far left term, and it follows that the integrand jFT(f)j2/2Tdescribes how that
average power is distributed in the frequency domain. In fact, jFT(f)j2has units of energy, and so the
units on jFT(f)j2/2Tare energy/time ¼power. We still need to remove the truncation of the original
X(t), and it is desirable to take the ensemble average of this power representation.
DEFINITION
Thepower spectral density (psd) ,o rpower spectrum , of a random process X(t) is deﬁned by
SXXfðÞ¼ lim
T!1E/C12/C12FTfðÞ/C12/C122
2T/C20/C21
As may be evident by the preceding development, applying this deﬁnition is typically extremely
difﬁcult in practice. Thankfully, for wide-sense stationary processes, there is a simpler method for
calculating the power spectral density SXX(f). The formula, presented in the following theorem, is
hinted at by the fact that the average power itself can be found through the autocorrelation function,
PX¼RXX(0), as noted before. It was ﬁrst discovered by Albert Einstein but is more commonly
attributed to Norbert Wiener and Aleksandr Khinchin.
WIENER–KHINCHIN THEOREM
IfX(t) is a wide-sense stationary random process, then
SXXfðÞ¼ FRXXτðÞfg
A proof of this theorem appears at the end of this section.
Example 8.2 LetX(t)¼220cos(2000 πt+Θ), an example of the phase variation process from Exam-
ple 8.1 (with A0¼220 and ω0¼2000π). The expected power of this signal is A02/2¼24,200 ¼
24.2 kW. It’s clear from the formula for X(t) that it broadcasts this 24.2 kW signal at a single frequency
of 2000 πradians, or 1 kHz. Thus, we anticipate that all the power in X(t) is concentrated at 1 kHz.
To verify this, use the autocorrelation function from Example 8.1 and apply the Wiener–Khinchin
Theorem:
SXXfðÞ ¼ FRXXτðÞfg ¼FA2
0
2cosω0τðÞ/C26/C27
¼F24,200 cos 2000 πτ ðÞ fg
Use the linear property of Fourier transforms, and then apply the known transform of the cosine
function:8.1 Power Spectral Density 565
SXXfðÞ ¼ 24,200 Fcos 2000 πτ ðÞfg
¼24,200 /C11
2/C2
δf/C01000ðÞ þ δ/C0
fþ1000/C1/C3
¼12,100/C2
δf/C01000ðÞ þ δ/C0
fþ1000/C1/C3
where δ() denotes an impulse function (see Appendix B for more information). Figure 8.1shows this
power spectral density, which consists of two impulses located at /C61000 Hz, each with intensity
12,100. Of course, in practice, the frequency /C01000 Hz is really the same as +1000 Hz, and so the
power spectrum of this random process in the positive frequency domain is located solely at 1000 Hz;
the impulse at this lone frequency carries intensity 12,100 + 12,100 ¼24,200, the ensemble average
power of the signal.
Because the Fourier transform results in part of the power spectral density being represented at
negative frequencies, the psd is sometimes called a two-sided power spectrum . Next we will explore
this property and others more thoroughly.
8.1.1 Properties of the Power Spectral Density
The following proposition describes several basic properties of SXX(f) and indicates how the psd is
related to average power.
PROPOSITION
LetSXX(f) be the power spectral density of a WSS random process X(t).
1.SXX(f) is real-valued and nonnegative.
2.SXX(f) is an even function, i.e., SXX(/C0f)¼SXX(f). (This is the “two-sided” nature of the
psd.)
3.ð1
/C01SXXfðÞdf¼PX, the ensemble average power in X(t).
Proof Property 1 follows from the deﬁnition of the psd: even though the Fourier transform FT(f)
may be complex-valued, jFT(f)j2/2Tmust be real and nonnegative. Since SXX(f) is the limit of the
expected value of jFT(f)j2/2T, it must also be real and nonnegative.
To prove property 2, we invoke the Wiener–Khinchin Theorem. Since RXX(τ) is even, we can
simplify its Fourier transform:fSXX(f)
−1000 1000(12,100) (12,100)
Fig. 8.1 The power spectral density of Example 8.2 ■566 8 Introduction to Signal Processing
SXXfðÞ ¼ FRXXτðÞfg ¼ð1
/C01RXXτðÞe/C0j2πfτdτ¼ð1
/C01RXXτðÞcos 2πfτðÞ dτ ð8:2Þ
The sine component of the complex exponential drops out because RXX(τ) is even. From Eq. ( 8.2)
it is clear that SXX(f) is both real-valued and an even function of f, since cosine is even.
Property 3 also follows from the Wiener–Khinchin Theorem: writing the autocorrelation function
as the inverse Fourier transform of the pdf, we have
RXXτðÞ¼ F/C01/C8
SXX/C0
f/C1/C9
¼ð1
/C01SXXfðÞeþj2πfτdf)
PX¼RXX0ðÞ ¼ð1
/C01SXXfðÞeþj2πf0ðÞdf¼ð1
/C01SXXfðÞdf ■
The foregoing proposition gives some insight into the interpretation of the power spectral density.
As stated previously, SXX(f) describes how the ensemble average power in X(t) is distributed across
the frequency domain. Since power must be real and nonnegative, so must the power spectrum.
Property 3 shows why SXX(f) is called a “density”: if we integrate this function across its entire
domain, we recover the total (expected) power in the signal, PX, much in the same way that
integrating a pdf from /C01 to1returns the total probability of 1. Property 3 also indicates the
appropriate units for the psd: since the integral is performed with respect to f(Hertz) and the result is
power (watts), the correct units for the power spectral density are watts per Hertz (W/Hz) .
Property 2 makes precise the two-sided nature of a psd. The power spectrum SXX(f) will always be
symmetric about f¼0; this is a by-product of how Fourier transforms are computed and the fact that
autocorrelation functions are always symmetric in τ. But we must make sense of it in terms of “true”
(i.e., nonnegative) frequencies. Look back at Example 8.2: that particular phase variation random
process had a power spectral density consisting of two impulses, each of intensity 12,100 W/Hz, at
/C61 kHz. We can interpret the impulse at /C01000 by mentally “folding” the power spectrum along
the vertical axis, left to right, so that the two impulses line up at +1 kHz with a total intensity of
24,200 W/Hz. Integrating that impulse dfrecovers the ensemble average power of 24.2 kW.
Our next example illustrates a more general psd, including components other than impulses.
Example 8.3 Partitioning a power spectrum Suppose X1(t) and X2(t) are independent, zero-mean,
WSS random processes with autocorrelation functions
R11τðÞ¼ 2000tri 10, 000 τ ðÞ ;R22τðÞ¼ 650cos 40, 000 πτ ðÞ
Deﬁne a new random process by X(t)¼X1(t)+X2(t) + 40. We encountered this random process
in Example 7.18, from which we know that X(t) is WSS with a mean of μX¼40 and an autocorrela-
tion function of
RXXτðÞ¼ R11τðÞ þ R22τðÞ þ 402
¼2000tri 10, 000 τ ðÞ þ 650cos 40, 000 πτ ðÞ þ 1600
First, let’s ﬁnd the ensemble average power in X(t):
PX¼RXX0ðÞ ¼ 2000þ650þ1600¼4250W
Recall from Example 7.18 that X(t) consists of three pieces: the aperiodic component X1(t), the
periodic component X2(t), and the dc offset of 40. These deliver a total of 4.25 kW of power: 2000 W
from X1(t), 650 W from X2(t), and 1600 W from the dc power offset (recall that we always assume
R¼1Ω,s oP¼I2R¼402(1)¼1600 for that term).8.1 Power Spectral Density 567
Next, let’s see how this 4250 W of power is distributed in the frequency domain by determining the
power spectral density of X(t). Apply the Wiener–Khinchin Theorem:
SXXfðÞ ¼ F/C8
RXX/C0
τ/C1/C9
¼F/C8
2000tri/C0
10,000 τ/C1
þ650 cos/C0
40,000 πτ/C1
þ1600/C9
¼2000F/C8
tri 10,000 τ ðÞ/C9
þ650F/C8
cos/C0
40,000 πτ/C1/C9
þF/C8
1600/C9
To evaluate each of these three Fourier transforms, we use the table of Fourier pairs in Appendix
B. The last two are straightforward, while the transform of tri(10,000 τ) requires the rescaling property
with a¼10,000. Since the Fourier transform pair of tri( t) is sinc2(f), the ultimate result is
SXXfðÞ ¼ 2000/C11
10;000jjsinc2 f
10,000/C18/C19
þ650/C11
2/C2
δ/C0
f/C020,000/C1
þδ/C0
fþ20,000/C1/C3
þ1600δ/C0
f/C1
¼0:2sinc2 f
10,000/C18/C19
þ325/C2
δf/C020,000ðÞ þ δ/C0
fþ20,000/C1/C3
þ1600δ/C0
f/C1
A graph of this power spectrum appears in Fig. 8.2. Notice the graph is symmetric about the
vertical axis f¼0, as guaranteed by property 2 of the previous proposition. The psd consists of three
elements, corresponding to the three components of the original signal. The power spectrum of the
aperiodic component appears as a continuous function (a true “density”) that vanishes as | f|!1 .
This is sometimes referred to as the dissipative component of the psd. The periodic component of the
signal has psd equal to a pair of impulses (sometimes called split impulses) at its fundamental
frequency—here, 40,000 πradians, or 20 kHz. Finally, the direct current corresponds to a “frequency”
off¼0; thus, the dc power offset of 1600 W is represented by 1600 δ(f), an impulse at f¼0.
(1600) (325) (325)
0.05
−40000 −30000 −20000 20000 30000 40000 −10000 10000 0fSXX(f)
0.100.150.20
Fig. 8.2 Power spectral density of Example 8.3 ■568 8 Introduction to Signal Processing
As illustrated in the foregoing example, a power spectral density generally consists of at most three
pieces, {dissipative components} + {periodic components} + {dc power offset}, and the last two will
be comprised of impulses.
8.1.2 Power in a Frequency Band
Suppose we wish to determine how much of the power in a random signal lies within a particular
frequency band; this is, as it turns out, a primary purpose of the psd. For frequencies f1andf2with
0<f1<f2, letPX[f1,f2] denote the expected power in X(t) in the band [ f1,f2]. Then, to account for
the two sides of the power spectrum, we calculate as follows:
PXf1;f2½/C138 ¼ðf2
f1SXXfðÞdfþð/C0f1
/C0f2SXXfðÞdf¼2ðf2
f1SXXfðÞdf ð8:3Þ
The last two expressions in Eq. ( 8.3) are equal because SXX(f) is an even function. Figure 8.3a
shows a generic power spectrum. Figure 8.3b shows the calculation of power in a band, accounting
for the two sides of the psd; it’s clear that we could simply double the right-hand area and get the same
result.
Extra care must be taken to ﬁnd the power in X(t) below some frequency f2, i.e., between 0 and f2
including the possible dc power offset at f¼0. When we “fold” the negative frequencies over to the
positive side, any power represented by an impulse at f¼0i snotduplicated. Therefore, we cannot
simply double the entire integral of SXX(f) from 0 to f2; we must count the dc power offset a single
time, and then integrate the rest of the psd. Written mathematically,
PX0;f2½/C138 ¼ð/C0f2
f2SXXfðÞdf¼dc power offsetðÞ þ 2ðf2
0þSXXfðÞdf ð8:4Þ
The lower limit 0+in Eq. ( 8.4) indicates that the integral term does not include an impulse at zero,
should one exist.
Example 8.4 For the random process X(t) in Example 8.3, let’s ﬁrst ﬁnd the ensemble average power
in the band from 10 to 30 kHz. With f1¼10,000 and f2¼30,000, we proceed as follows:SXX(f)
f−f2 f2 −f1ff1SXX(f)
PX[f1, f2]ab
Fig. 8.3 (a) A generic power spectral density; ( b) the ensemble average power in a speciﬁed frequency band8.1 Power Spectral Density 569
PX10;000;30;000 ½/C138 ¼ 2ð30;000
10;000SXXfðÞdf
¼2ð30;000
10;0000:2sinc2 f
10, 000/C18/C19
þ325δf/C020, 000ðÞ þ δfþ20, 000ðÞ ½/C138 þ 1600δfðÞ/C20/C21
df
¼2ð30;000
10;0000:2sinc2 f
10, 000/C18/C19
dfþ2ð30;000
10;000325δf/C020, 000ðÞ df
þ2ð30;000
10;000325δfþ20, 000ðÞ dfþ2ð30;000
10;0001600δfðÞdf
To evaluate the integrals of the three impulses, we use the sifting property (see Appendix B); since
the speciﬁed frequencies of the last two impulses lie outside the band [10,000, 30,000], those two
integrals are zero. The calculation continues
PX10;000;30;000 ½/C138 ¼ 2ð30;000
10;0000:2sinc2 f
10, 000/C18/C19
dfþ2/C0
325/C1
þ2/C0
0/C1
þ2/C0
0/C1
¼0:4ð30;000
10;000sin2πf=10;000 ðÞ
πf=10;000 ðÞ2dfþ650¼127:17þ650¼777:17W
This last integration of the sinc2function requires software (or an advanced calculator). Next, let’s
ﬁnd the average power in X(t) concentrated below 10 kHz. We must remember to include the impulse
representing the dc power offset at f¼0, but only once. Also, we can ignore the impulses at
/C620 kHz, since they lie outside our desired range. Applying Eq. ( 8.4),
PX0;10;000 ½/C138 ¼ð10;000
/C010;000SXXfðÞdf¼1600þ2ð10;000
0þSXXfðÞdf
¼1600þ2ð10;000
00:2 sin c2 f
10;000/C18/C19
df
¼1600þ1805 :65¼3405 :65W
Again, a numerical integration tool is required. ■
8.1.3 White Noise Processes
As mentioned previously, engineers frequently use random process models in an attempt to describe
the noise acquired by an intended signal during transmission. One of the simplest models, called
white noise , can most easily be described by its frequency representation (as opposed to the time-
domain models of Chap. 7).
DEFINITION
A random process N(t)i s(pure) white noise if there exists a constant N0>0, called the
intensity parameter , such that the psd of N(t)i s
SNNfðÞ ¼N0
2/C01 <f<1
As a special case, N(t) is called Gaussian white noise ifN(t) is a Gaussian process as deﬁned
in Sect. 7.6and its psd is as above.570 8 Introduction to Signal Processing
The power spectral density of pure white noise appears in Fig. 8.4a. A white noise model assumes
that all frequencies appear at equal power intensity throughout the entire spectrum. In that sense, it is
analogous to white light (all frequencies at equal intensity), which gives white noise its name.
White noise processes can also be partially described in the time domain through the autocorrela-
tion function:
RNNτðÞ¼ F/C01SXXfðÞfg ¼F/C01N0
2/C26/C27
¼N0
2δτðÞ
Figure 8.4b shows this autocorrelation function. From property 5 of the main proposition in Sect.
7.3, it follows that the mean of a white noise process is μN¼0. (That’s also evident from the psd
itself, since it lacks an impulse at f¼0 that would correspond to a dc power offset.) Thus, the
autocovariance function of pure white noise is also CNN(τ)¼RNN(τ)¼(N0/2)δ(τ).
This has a rather curious consequence: since δ(τ)¼0f o r τ6¼0,CNN(τ)¼(N0/2)δ(τ) implies that the
random variables N(t)a n d N(t+τ) are uncorrelated except when τ¼0. IfN(t) is Gaussian white noise,
then N(t)a n d N(t+τ) are independent for τ6¼0 (since uncorrelated implies independent for normal
rvs), even if the two times are very close together. That is, a pure white noise process has the property that
its location at any given time is completely uncorrelated with, say, its location the nanosecond before!
Although the pure white noise model is commonly used in engineering practice, no such process
can exist in physical reality. In order for the description in the preceding paragraph to be true, the
process would have to “move” inﬁnitely quickly, thus requiring inﬁnite power. This can be seen
directly from the deﬁnition:
PX¼ð1
/C01SXXfðÞdf¼ð1
/C01N0
2df¼1
That is, the area under the curve in Fig. 8.4a is inﬁnite. So, why use a model for a process that
cannot exist? As we’ll see in Sect. 8.2, when a white noise process is passed through certain ﬁlters, the
resulting output will have ﬁnite power. Thus, if we are interested in analyzing the ﬁltered version of
our communication noise, using the very simple model of pure white noise for the input is not
unreasonable.
Though pure white noise cannot exist, various types of band-limited white noise are physically
realizable. Two types of band-limited white noise, called lowpass white noise andbandpass white
noise , are depicted in Fig. 8.5.1Notice that the area under both of these power spectral densities is
ﬁnite, and thus the corresponding random processes both have ﬁnite power.SNN(f) RNN(t)
(N0/2) N0/2
f0tab
Fig. 8.4 Pure white noise: ( a) power spectral density; ( b) autocorrelation function
1Readers already familiar with ﬁlters will recognize the terms “lowpass” and “bandpass.” We will see these terms again
in the next section.8.1 Power Spectral Density 571
8.1.4 Power Spectral Density for Two Processes
For two jointly WSS random processes X(t) and Y(t), the cross-power spectral density of X(t) with Y
(t)is deﬁned by
SXYfðÞ ¼ FRXYτðÞfg ,
where RXY(τ) is the cross-correlation function deﬁned in Sect. 7.2. A similar deﬁnition can be made
forSYX(f). Since RXY(τ) is generally not an even function of τ, the cross-power spectral density need
not be real-valued. When X(t) and Y(t) are orthogonal random processes, RXY(τ)¼0 by deﬁnition and
soSXY(f)¼0. The cross-power spectral density gives information about the distribution of the
power generated by combining X(t) and Y(t), above and beyond their individual power spectra, when
X(t) and Y(t) are not orthogonal. See Exercise 16.
Proof of the Wiener–Khinchin Theorem The deﬁnition of SXX(f) involves the squared magnitude
of a complex function; from the theory of complex numbers, we know that jzj2¼z/C1z*, where *
denotes the complex conjugate. The proof then proceeds as follows:
SXXfðÞ ¼ lim
T!1E/C12/C12FTfðÞ/C12/C122
2T/C20/C21
¼lim
T!11
2TE/C2
FT/C0
f/C1
F*
T/C0
f/C1/C3
¼lim
T!11
2TEðT
/C0TXsðÞe/C0j2πfsdsðT
/C0TXtðÞe/C0j2πftdt/C18/C19 *"#
¼lim
T!11
2TEðT
/C0TXtðÞe/C0j2πfsdsðT
/C0TXtðÞej2πftdt/C20/C21
¼lim
T!11
2TEðT
/C0TðT
/C0TXsðÞXtðÞe/C0j2πfs/C0tðÞdtds/C20/C21
Next, pass the expected value into the integrand (which is permissible because the integral
converges), and use the fact that wide-sense stationarity implies E[X(s)X(t)]¼RXX(s/C0t):
ðT
/C0TðT
/C0TEXsðÞXtðÞe/C0j2πfs/C0tðÞhi
dtds¼ðT
/C0TðT
/C0TEX sðÞXtðÞ ½/C138 e/C0j2πfs/C0tðÞdtds
¼ðT
/C0TðT
/C0TRXXs/C0tðÞ e/C0j2πfs/C0tðÞdtds
Now make the change of variables τ¼s/C0t(i.e., s¼t+τ), under which the region of integra-
tion becomes the parallelogram pictured in Fig. 8.6.SNN (f) SNN (f)
B B
B −Bab
f f
−f0 f0N0/2 N0/2
Fig. 8.5 Examples of band-limited white noise: ( a) lowpass; ( b) bandpass572 8 Introduction to Signal Processing
Integrating in the order dt dτyields the sum of two integrals:
SXXfðÞ ¼ lim
T!11
2Tð0
/C02TðτþT
/C0TRXXτðÞe/C0j2πfτdtdτþð2T
0ðT
τ/C0TRXXτðÞe/C0j2πfτdtdτ/C20/C21
¼lim
T!11
2Tð0
/C02TRXXτðÞe/C0j2πfτ2Tþτ ðÞ dτþð2T
0RXXτðÞe/C0j2πfτ2T/C0τ ðÞ dτ/C20/C21
¼lim
T!11
2Tð2T
/C02TRXXτðÞe/C0j2πfτ2T/C0τjj ðÞ dτ
¼lim
T!1ð2T
/C02TRXXτðÞe/C0j2πfτ1/C0τjj
2T/C18/C19
dτ
¼lim
T!1ð1
/C01RXXτðÞe/C0j2πfτqTτðÞdτ
where qT(τ)¼1/C0|τ|/2Tfor |τ|/C202Tand 0 otherwise. Since qT(τ)!1a s T!1 for all τ,w e
conclude that
SXXfðÞ¼ð1
/C01RXXτðÞe/C0j2πfτdτ¼FRXXτðÞfg ,
as claimed. ■
8.1.5 Exercises: Section 8.1(1–21)
1. The function rect( τ) satisﬁes all the properties of an autocorrelation function for a WSS process
that were speciﬁed in the main proposition of Sect. 7.3: rect( τ) is even, has its maximum value at
0, vanishes as τ!1 . However, rect( τ) cannot be the autocorrelation of a WSS random process.
Why not? [ Hint: Consider the resulting psd.] This demonstrates that the properties listed in that
proposition do notfully characterize the types of functions that can be autocorrelations.
2. Let A(t) be a WSS random process with autocorrelation function RAA(τ) and power spectral
density SAA(f). Deﬁne an “amplitude modulated” version of A(t)b y
XtðÞ¼ AtðÞcos 2πf0tþΘ ðÞ ,
where Θ~ Unif( /C0π,π] and is independent of A(t).t
(0, T)
(0, −T)(2T, T)
(−2T, T)tFig. 8.6 Region of
integration for the proof of
the Wiener–Khinchin
Theorem8.1 Power Spectral Density 573
(a) Find the mean and autocorrelation functions of X(t).
(b) Find the power spectral density of X(t).
(c) Find an expression for the expected power in X(t).
3. Suppose X(t) is a wide-sense stationary process with the following autocorrelation function:
RXXτðÞ¼ 250þ1000exp /C04/C2106τ2/C0/C1
(a) Find and graph the power spectral density of X(t).
(b) Find the ensemble average power in X(t) between 500 Hz and 1 kHz.
(c) Find the ensemble average power in X(t) below 200 Hz.
4. Let A(t) be a WSS waveform with autocorrelation function RAA(τ)¼2400sinc(2000 τ). Deﬁne a
new random process X(t)b y
XtðÞ¼ 20þAtðÞcos 5000 πtþΘ ðÞ
where Θis uniform on ( /C0π,π] and independent of A(t).
(a) Find the mean function of X(t).
(b) Find the autocorrelation function of X(t). IsX(t) WSS?
(c) Find the expected power in X(t).
(d) Find and sketch the power spectral density of X(t).
(e) Find the expected power in X(t) in the frequency band from 2 to 3 kHz.
5. Suppose X(t) is a wide-sense stationary random process with power spectral density SXX(f)¼
0.2 exp( /C0π2f2/1012).
(a) Sketch the psd, and ﬁnd the expected power in X(t).
(b) Find the expected power in X(t) above 10 kHz.
(c) Find the autocorrelation function of X(t) and verify your answer to (a).
6. Let X(t) be a WSS random process with mean μX¼32.6 and autocovariance function
CXX(τ)¼12,160sinc2(40,000 τ).
(a) Find and sketch the power spectral density of X(t).
(b) Find the expected power in X(t) below 20 kHz.
(c) Find the expected power in X(t) between 10 and 30 kHz.
(d) Find the total expected power in X(t).
7. Let N(t)b e lowpass white noise , i.e., N(t) is WSS with power spectral density given by
SNN(f)¼N0/2 for jfj/C20Band 0 otherwise (see Fig. 8.5a).
(a) Find the expected power in N(t).
(b) Find the autocorrelation function of N(t).
8. Let N(t)b e bandpass white noise , i.e., N(t) is WSS with power spectral density given by
SNN(f)¼N0/2 for f0/C0B/2/C20jfj/C20f0+B/2 and 0 otherwise (see Fig. 8.5b).
(a) Find the expected power in N(t).
(b) Find the autocorrelation function of N(t).
9. Let N(t) be a Poisson telegraphic process with parameter λas deﬁned in Sect. 7.5, and consider
Y(t)¼A0N(t) for some constant A0>0.
(a) Find the autocorrelation function of Y(t).
(b) Find and sketch the power spectral density of Y(t).
(c) Find the expected power in Y(t).
(d) What proportion of the expected power in Y(t) lies below the frequency λHz?
10. Let X(t) have power spectral density SXX(f)¼N0/C0jfj/Aforjfj/C20B(and zero otherwise),
where B<N0A.
(a) Find the expected power in X(t).574 8 Introduction to Signal Processing
(b) Find the autocorrelation function of X(t).
[Hint: It may be helpful to sketch SXX(f) ﬁrst.]
11. Suppose a random process X(t) has autocorrelation function RXX(τ)¼100e/C0jτj+5 0 e/C0jτ/C01j+
50e/C0jτ+1j.
(a) Find the expected power in X(t).
(b) Find and sketch the power spectral density of X(t).
(c) Find the expected power in X(t) below 1 Hz.
12. Let X(t) be a WSS random process, and deﬁne a d-second delay of X(t)b yY(t)¼X(t/C0d). Find
the mean, autocorrelation, and power spectrum of Y(t) in terms of those of X(t).
13. Let X(t) be a WSS random process, and deﬁne a d-second “moving window” process by
W(t)¼X(t)–X(t/C0d). Find the mean, autocorrelation, and power spectrum of W(t) in terms
of those of X(t).
14. Let X(t) and Y(t) be jointly WSS random processes. Show that SXY(f)¼SY*
X(f).
15. Let X(t) and Y(t) be orthogonal and WSS random processes, and deﬁne Z(t)¼X(t)+Y(t).
(a) Are X(t) and Y(t) jointly WSS? Why or why not?
(b) Is Z(t) WSS?
(c) Find the psd of Z(t).
16. Let X(t)a n d Y(t)b enon-orthogonal, jointly WSS random processes, and deﬁne Z(t)¼X(t)+Y(t).
(a) Find the autocorrelation function of Z(t). IsZ(t) WSS?
(b) Find the power spectral density of Z(t), and explain why this expression is real-valued.
17. Let X(t) and Y(t) be independent WSS random processes, and deﬁne Z(t)¼X(t)Y(t).
(a) Show that Z(t) is also WSS.
(b) Find the psd of Z(t).
18.Pink noise , also called 1/ fnoise, is characterized by the power spectrum SNN(f)¼1/jfjforf6¼0.
(a) Explain why such a process is not physically realizable.
(b) Consider a band-limited pink noise process with psd SNN(f)¼1/jfjforf0/C20jfj/C20f1. Find
the expected power of such a random process.
(c) A “generalized pink noise” process has the psd SNN(f)¼N0/(2jfj1+β)f o rjfj>f0and 0
otherwise, where 0 <β<1. Find the expected power of such a random process.
19.Highpass white noise is characterized by the power spectrum SNN(f)¼N0/2 for jfj>Band
0 otherwise. Is highpass white noise a physically realizable process? Why or why not?
20. The ac power spectral density (ac-psd) of a WSS random process is deﬁned as the Fourier
transform of its auto covariance function:
Sac
XXfðÞ¼ FCXXτðÞfg
(a) By using the relationship between CXX(τ) and RXX(τ), develop an equation relating the psd of
a random process to its ac-psd.
(b) Find the ac-psd for the random process of Example 8.3.
(c) Explain why the term “ac power spectral density” is appropriate.
21. Exercise 36 of Chap. 7presented a random process of the form X(t)¼A/C1Y(t), where Ais a
random variable and Y(t) is an ergodic, WSS random process independent of A. It was shown that
X(t) is WSS but notergodic.
(a) Find the psd of X(t).
(b) Find the ac-psd of X(t). (See the previous exercise.)
(c) Does the ac-psd of X(t) include an impulse at zero? What does this say about our interpreta-
tion of “dc power offset” for non-ergodic processes?8.1 Power Spectral Density 575
8.2 Random Processes and LTI Systems
For any communication system to be effective, one must be able to successfully distinguish the
intended signal from the noise it encounters during transmission. If we understand enough about the
statistical properties of that noise, then in theory a ﬁlter can be constructed to minimize noise effects,
thereby making the signal easier to “hear.” This section gives a very brief overview of ﬁlters2and then
investigates aspects of applying a ﬁlter to a random, continuous-time signal.
In communication theory, a system refers to anything that operates on a signal. We will denote a
generic system by the letter L. If we let x(t) and y(t) denote the input and output of this system,
respectively, then we may write
ytðÞ¼ LxtðÞ½/C138
where L[] denotes the application of the system to a signal. One particular class of systems is of the
greatest interest, since they form the backbone of ﬁltering.
DEFINITION
Alinear, time-invariant (LTI) system Lsatisﬁes the following two properties:
1. (Linearity) For all functions x1(t) and x2(t) and all constants a1anda2,
La 1x1tðÞþ a2x2tðÞ ½/C138 ¼ a1Lx1tðÞ½/C138 þ a2Lx2tðÞ½/C138
2. (Time invariance) For all d>0, if y(t)¼L[x(t)], then y(t/C0d)¼L[x(t/C0d)].
Part 2 of this deﬁnition says that it does not matter on an absolute time scale when we apply the
LTI system to x(t); the response will be the same, other than the time delay. As it turns out, an LTI
system can be completely characterized by its effect on an impulse, essentially because a signal can
generally be decomposed into a weighted sum of impulses, and then we may apply linearity. With
this in mind, an LTI system is described in the time domain by its impulse response (function) ,
denoted h(t):
htðÞ¼ LδtðÞ½/C138
It can be shown (see Chap. 6of the reference by Ambardar) that if Lis an LTI system with impulse
response h(t), then the input and output signals of Lare related by a convolution operation:
ytðÞ¼ xtðÞHhtðÞ¼ð1
/C01xsðÞht/C0sðÞ ds¼ð1
/C01xt/C0sðÞ hsðÞds ð8:5Þ
The same relationship holds for random signals, i.e., if X(t) is the random input to an LTI system
andY(t) the output, then Y(t)¼X(t)Hh(t).
The appearance of a convolution operator suggests it would be desirable to apply a transform to
Eq. ( 8.5). The Fourier transform of the impulse response, denoted H(f), is called the transfer
function of the LTI system:
2Readers interested in a thorough treatment of ﬁlters and other systems should consult the reference by Ambardar.576 8 Introduction to Signal Processing
HfðÞ ¼ FhtðÞfg
Fordeterministic signals, we may then write Y(f)¼X(f)H(f), where X(f) and Y(f) denote the
Fourier transforms of x(t) and y(t), respectively. However, Fourier transforms of random signals do
not exist (due to convergence issues), so the transfer function H(f) cannot be deﬁned as the ratio of
the output and input in the frequency domain as one commonly does in other engineering situations.
Still, the transfer function will prove critical in determining how the power in a random signal X(t)i s
“transferred” by an LTI system, as we will see shortly.
8.2.1 Statistical Properties of the LTI System Output
The following proposition summarizes the relationships between the statistical properties of the
random input signal X(t) of an LTI system and the corresponding output signal Y(t). Here X(t) is again
assumed to be wide-sense stationary.
PROPOSITION
LetLbe an LTI system with impulse response h(t) and transfer function H(f). Suppose X(t)i sa
wide-sense stationary process and let Y(t)¼L[X(t)], the output of the LTI system applied to
X(t). Then X(t) and Y(t) are jointly WSS, with the following properties.
Time domain Frequency domain
1.μY¼μX/C1ð1
/C01hsðÞds 1.μY¼μX/C1H(0)
2.RYY(τ)¼RXX(τ)Hh(τ)Hh(/C0τ)2 . SYY(f)¼SXX(f)/C1jH(f)j2
3.PY¼RYY(0) 3.PY¼ð1
/C01SYYfðÞdf
4.RXY(τ)¼RXX(τ)Hh(τ)4 . SXY(f)¼SXX(f)/C1H(f)
The quantity jH(f)j2in property 2 is called the power transfer function of the LTI system.
Proof Using the convolution relationship between X(t) and Y(t),
YtðÞ¼ XtðÞHhtðÞ¼ð1
/C01Xt/C0sðÞ hsðÞds)EY tðÞ½/C138 ¼ Eð1
/C01Xt/C0sðÞ hsðÞds/C20/C21
¼ð1
/C01EX t /C0sðÞ½/C138 hsðÞds
Since X(t) is WSS, the expression E[X(t/C0s)] is just a constant, μX, from which E[Y(t)]¼
μXÐ1
/C01h(s)ds, as desired. Since this expression does not depend on t, we deduce that the mean of
Y(t) is constant (and we may denote it μY). This establishes property 1 in the time domain. For the
parallel result in the frequency domain, simply note that since HfðÞ¼ FhtðÞfg , it follows from
the deﬁnition of the Fourier transform thatÐ1
/C01h(s)ds¼H(0).
A similar (but vastly more tedious) derivation yields property 2 in the time domain (see Exercise
31). The right-hand side establishes that the autocorrelation of Y(t) depends only on τand not t, and
therefore Y(t) is indeed WSS. Hence, the Wiener–Khinchin Theorem applies to Y(t), and taking the
Fourier transform of both sides gives8.2 Random Processes and LTI Systems 577
F/C8
RYYτðÞ/C9
¼F/C8
RXX/C0
τ/C1
Hh/C0
τ/C1
Hh/C0
/C0τ/C1/C9
)
SYYfðÞ ¼ F/C8
RXX/C0
τ/C1/C9
F/C8
h/C0
τ/C1/C9
F/C8
h/C0
/C0τ/C1/C9
¼SXXfðÞH/C0
f/C1
H∗/C0
f/C1
,
where H*(f) denotes the complex conjugate of H(f). Now, recall that for any complex number z,
z/C1z*¼jzj2. We immediately have H(f)H*(f)¼jH(f)j2, completing property 2 in the frequency
domain.
Both the time and frequency versions of property 3 follow immediately from Sect. 8.1and the fact
thatY(t) is WSS. The proofs of property 4 in the time and frequency domain are parallel to those of
property 2. ■
The frequency domain properties of the previous theorem are the most illuminating. Property
1 says the dc offset of X(t),μX, is “transferred” to the dc offset of Y(t) by evaluating the transfer
function H(f) at 0. This makes sense, since the dc offset corresponds to the frequency f¼0. Notice
in particular that if μX¼0, necessarily μY¼0; an LTI system cannot introduce a dc offset if none
exists in the input signal.
Property 2 states that the power spectrum of the output of an LTI system is obtained from the input
psd through multiplication by the quantity jH(f)j2, hence the name “power transfer function.” Similar
to the preceding discussion about dc offset, observe that if X(t) carries no power at some particular
frequency f(so that SXX(f)¼0), then SYY(f) will be zero there as well. An LTI system cannot
introduce power to any frequency that did not appear in the input signal.
Example 8.5 One of the simplest ﬁlters is an RC circuit , an LTI system whose impulse response is
given by
htðÞ¼1
RCe/C0t=RCutðÞ
where u(t) is the unit step function, equal to 1 for t/C210 and zero otherwise. (The product RCof the
resistance and the capacitance is called the time constant of the circuit, since its units are seconds. The
unit step function makes h(t) equal 0 for t<0; engineers call this a causal ﬁlter .) Suppose we have
such a circuit with time constant RCand that we model the input to our system as a pure white noise
process with power spectral density SXX(f)¼N0/2 W/Hz. Let’s investigate the properties of the
output, Y(t).
First, since white noise has mean zero, it follows that μY¼0 as well (property 1). Now we need the
transfer function of the system:
HfðÞ¼ FhtðÞfg ¼1
RCF e/C0t=RCutðÞno
¼1
RC0!
1=RCþj2πf ðÞ0þ1¼1
1þj2πfRC
Next, we ﬁnd the psd of Y(t) using property 2:
SYYfðÞ¼ SXXfðÞ /C1j HfðÞj2¼N0
21
1þj2πfRC/C12/C12/C12/C12/C12/C12/C12/C122
¼N0
2/C112
12þ2πfRCðÞ2¼N0=2
1þ2πfRCðÞ2
Figure 8.7displays this power spectral density. Finally, the ensemble average power of Y(t)i s
given by578 8 Introduction to Signal Processing
PY¼ð1
/C01SYYfðÞdf¼ð1
/C01N0=2
1þ2πfRCðÞ2df¼N0
2ð1
/C01df
1þ2πfRCðÞ2¼N0
4RC
where the integral is evaluated by the substitution x¼2πfRCand the fact that the antiderivative of
1/(1 + x2) is arctan( x).
We ﬁnd that, even though the input signal had (theoretically) inﬁnite power, the output Y(t) has
ﬁnite power, directly proportional to the intensity of the input and inversely proportional to the time
constant of the circuit. (As an exercise, see if you can verify that the units on the ﬁnal expression for
power are indeed watts.) ■
Example 8.6 An LTI system has an impulse response of h(t)¼t2e/C0tu(t). The input to this system is
the random process
XtðÞ¼ Vþ500 cos 2 /C2106πtþΘ/C0/C1
,
where VandΘare independent random variables, Θis uniformly distributed on ( /C0π,π], and Vhas
mean 60 and variance 12. It was shown in Exercise 25 of Chap. 7that X(t) is WSS, with mean
μX¼μV¼60 and autocorrelation function RXX(τ)¼3612 + 125,000cos(2 /C2106πτ). (Depending
on whether we choose to interpret X(t) as a voltage or current waveform, the units on the mean are
either volts or amperes.) Applying the Wiener–Khinchin Theorem, the psd of X(t)i s
SXXfðÞ¼ F/C8
RXX/C0
τ/C1/C9
¼F/C8
3612þ125,000 cos 2 /C2106πτ/C0/C1 /C9
¼3612δ/C0
f/C1
þ62,500 δ/C0
f/C0106/C1
þ62,500 δ/C0
fþ106/C1
Since X(t) consists of a (random) dc offset and a periodic component, the power spectrum of X(t)i s
comprised entirely of impulses. Now let Y(t) denote the output of the LTI system. To deduce the
properties of Y(t) requires the transfer function, H(f), of the LTI system. Using the table of Fourier
transforms in Appendix B,
HfðÞ¼ FhtðÞfg ¼Ft2e/C0tutðÞ/C8/C9
¼2!
1þj2πf ðÞ2þ1¼2
1þj2πf ðÞ3SYY(f)
fN0/2Fig. 8.7 Power spectral
density of Y(t)
in Example 8.58.2 Random Processes and LTI Systems 579
According to property 1 of the earlier proposition, the mean of the output signal Y(t) is given by
μY¼μX/C1H0ðÞ ¼ 60/C12
1þj2π/C10 ðÞ3¼120
To ﬁnd the psd of Y(t), we must ﬁrst calculate the power transfer function of the LTI system:
/C12/C12HfðÞ/C12/C122¼/C12/C12/C12/C12/C122
1þj2πf ðÞ3/C12/C12/C12/C12/C122
¼/C12/C122/C12/C122
/C12/C121þj2πf/C12/C122/C0/C13¼4
1þ2πfðÞ2/C0/C13
Since the input power spectrum consists of impulses, so does the output power spectrum; the
coefﬁcients on the impulses are found by evaluating the power transfer function at the appropriate
frequencies:
SYYfðÞ ¼ SXX/C0
f/C1/C12/C12H/C0
f/C1/C12/C122
¼3612δfðÞ/C12/C12H/C0
f/C1/C12/C122þ62, 500 δ/C0
f/C0106/C1/C12/C12H/C0
f/C1/C12/C122þ62, 500 δ/C0
fþ106/C1/C12/C12H/C0
f/C1/C12/C122
¼3612δfðÞ/C12/C12H/C0
0/C1/C12/C122þ62, 500 δ/C0
f/C0106/C1/C12/C12H/C0
106/C1/C12/C122þ62, 500 δ/C0
fþ106/C1/C12/C12H/C0
/C0106/C1/C12/C122
¼3612δfðÞ /C14
1þ02/C0/C13þ62, 500 δ/C0
f/C0106/C1
/C14
/C0
1þ2/C2106π/C0/C12/C13
þ62, 500 δfþ106/C0/C1
/C14
/C0
1þ/C0 2/C2106π/C0/C12/C13
¼14, 448 δfðÞ þ 4/C210/C036δ/C0
f/C0106/C1
þ4/C210/C036δ/C0
fþ106/C1
The effect of the LTI system is to “ramp up” the dc power and to effectively eliminate the power at
1 MHz. In particular, the expected power in the output signal Y(t)i s
PY¼ð1
/C01SYYfðÞdf¼14, 448 þ24/C210/C036/C0/C1
/C2514:448kW,
with essentially all of the power coming from the dc component. ■
8.2.2 Ideal Filters
The goal of a ﬁlter is, of course, to eliminate (“ﬁlter out”) whatever noise has accumulated during the
transmission of a signal. At the same time, we do not want our ﬁlter to affect the intended signal, lest
information be lost. Ideally, we would know at what frequencies the noise in our transmission exists,
and then a ﬁlter would be designed that completely eliminates those frequencies while preserving all
others. (If the frequency band of the noise overlaps that of the signal, one can modulate the signal so
that the two frequency bands are disjoint.)580 8 Introduction to Signal Processing
DEFINITION
An LTI system is an ideal ﬁlter if there exists some set of frequencies, Fpass, such that the
system’s power transfer function is given by
/C12/C12HfðÞ/C12/C122¼1 for f2Fpass
0 otherwise/C26
If we let X(t) denote the input to the system (which may consist of both signal and noise) and Y(t)
the output, then for an ideal ﬁlter we have
SYYfðÞ ¼ SXXfðÞ/C12/C12HfðÞ/C12/C122¼SXXfðÞ forf2Fpass
0 otherwise/C26
In other words, the power spectrum of X(t) within the band Fpassis unchanged by the ﬁlter, while
everything in X(t) lying outside that band is completely eliminated. Thus, the obvious goal is to select
Fpassto include all frequencies in the signal and exclude all frequencies in the accumulated noise.
Figure 8.8displays | H(f)| for four different types of ideal ﬁlters. To be consistent with the
two-sided nature of power spectral densities, we present the graphs for /C01 <f<1, even though
plots starting at f¼0 are more common in engineering practice. Figure 8.8a shows a lowpass ﬁlter ,
which preserves the signal up to some threshold B. Under our notation, Fpass¼[0,B] for an ideal
lowpass ﬁlter. The ideal highpass ﬁlter of Fig. 8.8b does essentially the opposite, preserving
frequencies above B . Figure 8.8c, d illustrate a bandpass ﬁlter and a bandstop ﬁlter (also called a
notch ﬁlter ), respectively.
The previous section brieﬂy mentioned band-limited white noise processes, wherein we also used
the terms “lowpass” and “bandpass.” These models inherit their names from the aforementioned
ﬁlters, e.g., if pure white noise passes through an ideal bandpass ﬁlter, the result is called bandpass
white noise.|H(f)|
B -Ba
f1|H(f)| b
f
-B B1
|H(f)| d
f
-f0 f01|H(f)| c
f
−f0 f01
Fig. 8.8 Ideal ﬁlters: ( a) lowpass; ( b) highpass; ( c) bandpass; ( d) bandstop8.2 Random Processes and LTI Systems 581
Example 8.7 A WSS random signal X(t) with autocorrelation function given by RXX(τ)¼250 +
1500exp( /C01.6/C2109τ2) is passed through an ideal lowpass ﬁlter with B¼10 kHz (i.e., 104Hz).
Before considering the effect of the ﬁlter, let’s investigate the properties of the input signal X(t). The
ensemble average power of the input is PX¼RXX(0)¼250 + 1500 ¼1750 W; moreover, we
recognize that 250 W represents the dc power offset while the other 1500 W comes from an aperiodic
component. Applying the Wiener–Khinchin Theorem, the input power spectral density is given by
SXXfðÞ¼ FRXXτðÞfg ¼F250þ1500exp 1 :6/C2109τ2/C0/C1 /C8/C9
¼250δfðÞ þ 1500Fexp/C01:6/C2109τ2/C0/C1/C8/C9
The second Fourier transform requires the rescaling property; however, we must be careful in
identifying the rescaling constant. If we rewrite 1.6 /C2109τ2as (4/C2104τ)2, we see that the appro-
priate rescaling constant is actually a¼4/C2104. Continuing,
SXXfðÞ ¼ 250δ/C0
f/C1
þ1500F/C8
exp/C0
/C0/C0
4/C2104τ/C12/C1/C9
¼250δfðÞ þ 1500/C11
4/C2104ﬃﬃﬃπpexp/C0
/C0π2/C0
f=4/C2104/C12/C1
¼250δfðÞ þ3ﬃﬃﬃπp
80exp/C0
/C0π2f2=1:6/C2109/C1
This psd appears in Fig. 8.9a. Now let’s apply the ﬁlter, and as usual let Y(t) denote the output.
Then, based on the preceding discussion, the psd of Y(t) is given by
SYYfðÞ ¼SXXfðÞ f2Fpass
0 otherwise/C26
¼250δfðÞ þ3ﬃﬃﬃπp
80e/C0π2f2=1:6/C2109/C12/C12f/C12/C12/C20104Hz
0 otherwise(
Figure 8.9b shows the output power spectrum, which is identical to SXX(f) in the preserved band
[0, 104] and zero everywhere else.
The ensemble average power of the output signal Y(t) is calculated by taking the integral of SYY(f),
which in this case requires numerical integration by a calculator or computer:ffSXX (f) SYY (f)
(250)
-30000 -20000 -10000 -30000 -20000 -10000 10000 20000 30000 10000 20000 30000 00(250)ab
Fig. 8.9 Power spectral densities for Example 8.7: ( a) input signal; ( b) output signal582 8 Introduction to Signal Processing
PY¼ð1
/C01SYYfðÞdf¼ð104
/C0104250δfðÞ þ3ﬃﬃﬃπp
80e/C0π2f2=1:6/C2109/C20/C21
df
¼250þ2ð104
03ﬃﬃﬃπp
80e/C0π2f2=1:6/C2109df/C25250þ1100¼1350W ■
In the preceding example, the output power from the ideal ﬁlter was less than the input power
(1350 W <1750 W). It should be clear that this will always be the case: it is impossible to achieve a
power gain with an ideal ﬁlter of any type. At best, if the entire input lies within the preserved band
Fpass, then the input and output power will be equal.
Of course, in practice one cannot actually construct an “ideal” ﬁlter—there is no engineering
system that will perfectly cut off a signal at a prescribed frequency. But many simple systems can
approximate our ideal. For instance, consider Example 8.5: the power transfer function of that RC
ﬁlter is identical to Fig. 8.7(except that the height at f¼0 is 1 rather than N0/2). This bears some
weak resemblance to the picture for an ideal lowpass ﬁlter in Fig. 8.8a. In fact, a more general class of
LTI systems called Butterworth ﬁlters can achieve an even more “squared off” appearance; the nth-
order Butterworth ﬁlter has a power transfer function of the form
/C12/C12HfðÞ/C12/C122¼α
1þβ2πfðÞ2n,
where the constants αandβcan be derived from the underlying circuit. The RC ﬁlter of Example 8.5 is a
“ﬁrst-order” (i.e., n¼1) Butterworth ﬁlter. The books by Peebles and Ambardar listed in the references
provide more information. Examples of these power transfer functions are displayed in Fig. 8.10.
8.2.3 Signal Plus Noise
For a variety of physical reasons, it is common in engineering practice to assume that communication
noise is additive , i.e., if our intended signal X(t) experiences noise N(t) during transmission, then the
received transmission (prior to any ﬁltering) has the form X(t)+N(t). We assume throughout this
subsection that X(t) and N(t) are independent, WSS random processes and that E[N(t)]¼0 (i.e., the
noise component does not contain a dc offset, a standard engineering assumption).3|H(f)|2|H(f)|2|H(f)|2
f f f
n = 1 n = 2 n = 4
Fig. 8.10 Power transfer functions for Butterworth ﬁlters (approximations to ideal ﬁlters)
3Please note: The case of a deterministic signal x(t) must be handled somewhat differently. Consult the reference by
Ambardar for details.8.2 Random Processes and LTI Systems 583
The mean of the input process is given by E[X(t)+N(t)]¼E[X(t)] + E[N(t)]¼μX+0¼μX, the
dc offset of the input signal. Computing the autocorrelation of the input process relies on the assumed
independence:
RinτðÞ ¼ E/C2/C0
X/C0
t/C1
þN/C0
t/C1/C1
/C1/C0
X/C0
tþτ/C1
þN/C0
tþτ/C1/C1/C3
¼E/C2
XtðÞX/C0
tþτ/C1/C3
þE/C2
X/C0
t/C1
N/C0
tþτ/C1/C3
þE/C2
N/C0
t/C1
X/C0
tþτ/C1/C3
þE/C2
N/C0
t/C1
N/C0
tþτ/C1/C3
¼RXXτðÞ þ E/C2
X/C0
t/C1/C3
E/C2
N/C0
tþτ/C1/C3
þE/C2
N/C0
t/C1/C3
E/C2
X/C0
tþτ/C1/C3
þRNN/C0
τ/C1
¼RXXτðÞ þ E/C2
X/C0
t/C1/C3
0þ0E/C2
X/C0
tþτ/C1/C3
þRNN/C0
τ/C1
since μN¼0
¼RXXτðÞ þ RNNτðÞ
Then, by the Wiener–Khinchin Theorem, the input power spectrum is
SinfðÞ ¼ FRXXτðÞ þ RNNτðÞ fg ¼SXXfðÞ þ SNNfðÞ
Now we imagine passing the random process X(t)+N(t) through some LTI system L(presumably
a ﬁlter intended to reduce the noise). The foregoing assumptions make the analysis of the output
process quite straightforward. To start, the linearity property allows us to regard the system output as
the sum of two parts:
LX tðÞþNtðÞ ½/C138 ¼LX tðÞ½/C138 þLN tðÞ½/C138
That is, we may identify L[X(t)] and L[N(t)] as the output signal and output noise, respectively.
These two output processes are also independent and WSS. Letting H(f) denote the transfer function
of the LTI system, the mean of the output signal and output noise are, respectively,
μLX½/C138¼ELXt ðÞ½/C138ðÞ ¼ μXH0ðÞ,μLN½/C138¼ELNt ðÞ½/C138ðÞ ¼ μNH0ðÞ ¼ 0
The mean of the overall output process is, by linearity, μXH(0) + 0 ¼μXH(0). Similarly, the
power spectral density of the output process is
SoutfðÞ ¼ SinfðÞ/C12/C12HfðÞ/C12/C122¼SXXfðÞ/C12/C12HfðÞ/C12/C122þSNNfðÞ/C12/C12HfðÞ/C12/C122;
the two halves of this expression are the psds of the output signal and output noise.
One measure of the quality of the ﬁlter (the LTI system) involves comparing the power signal-to-
noise ratio of the input and output:
SNR in¼PX
PNversus SNR out¼PLX½/C138
PLN½/C138
A good ﬁlter should achieve a higher SNR outthan SNR inby reducing the amount of noise without
losing any of the intended signal.
Example 8.8 Suppose a random signal X(t) incurs additive noise N(t) in transmission. Assume the
signal and noise components are independent and wide-sense stationary, X(t) has autocorrelation
function RXX(τ)¼2400 + 45,000sinc2(1800 τ), and N(t) has autocorrelation function given by
RNN(τ)¼1500 e/C010,000| τ|. To ﬁlter out the noise, we pass the input X(t)+N(t) through an ideal
lowpass ﬁlter with band limit 1800 Hz.
Our input power signal-to-noise ratio is
SNR in¼PX
PN¼RXX0ðÞ
RNN0ðÞ¼2400þ45, 000
1500¼31:6584 8 Introduction to Signal Processing
The power spectral density of X(t)i s
SXXfðÞ ¼ F/C8
RXX/C0
τ/C1/C9
¼F/C8
2400þ45, 000sinc2/C0
1800 τ/C1/C9
¼2400δfðÞ þ/C0
45, 000/C11
1800trif
1800/C18/C19
¼2400δ/C0
f/C1
þ25trif
1800/C18/C19
This psd is displayed in Fig. 8.11a . Notice that the entire power spectrum of the input signal lies
within the band [0 Hz, 1800 Hz], which is precisely the preserved band of the ﬁlter. Therefore, the
ﬁlter will have no effect on the input signal; in particular, the input and output signal components will
have the same power spectral density and the same ensemble average power (47.4 kW).
On the other hand, part of the input noise will be removed by the ﬁlter. Begin by ﬁnding the psd of
the input noise:
SNNfðÞ¼ FRNNτðÞfg ¼F 1500 e/C010,000jτjno
¼1500/C121 0;000ðÞ
10;000ðÞ2þ2πfðÞ2¼3/C2107
108þ2πfðÞ2
Figure 8.11b shows the psd of the input noise, while in Fig. 8.11c we see the psd of the output noise
L[N(t)] resulting from passing N(t) through the ideal ﬁlter.
The average power in the output noise is
PLN½/C138¼2ð1800
03/C2107
108þ2πfðÞ2df¼/C1/C1/C1¼ 808:6W,
slightly more than half the original (i.e., input) noise power. As a result, the output power signal-to-
noise ratio equals
SNR out¼PLX½/C138
PLN½/C138¼47;400
808:6¼58:6−1800
−18001800(2400)
1800f
f fba
cSXX(f) Fig. 8.11 Power spectra
for Example 8.8: ( a) input
signal; ( b) input noise; ( c)
output noise8.2 Random Processes and LTI Systems 585
Because the signal and noise power spectra were so similar, it was not possible to ﬁlter out very
much noise. Assuming our model for the input noise is correct, one solution would be to modulate the
signal before transmission to a center frequency in the “tail” of the SNN(f) distribution and then
employ a bandpass ﬁlter around that center frequency (see Exercise 30). ■
8.2.4 Exercises: Section 8.2(22–38)
22. Let Y(t) be the output process from Example 8.5. Find the autocorrelation function of Y(t).
23. A WSS current waveform X(t) with power spectral density SXX(f)¼0.02 W/Hz for jfj/C20
60 kHz is the input to a ﬁlter with impulse response h(t)¼40e/C040tu(t). Let Y(t) denote the
output current waveform.
(a) Find the autocorrelation function of the input process X(t). [Hint: Draw SXX(f) ﬁrst.]
(b) Calculate the ensemble average power in the input process X(t).
(c) Find the transfer function of this ﬁlter.
(d) Find and graph the power spectral density of the output process Y(t).
(e) Determine the ensemble average power in the output process Y(t).
24. A Poisson telegraphic process N(t) with parameter λ¼2 (see Sect. 7.5) is the input to an LTI
system with impulse response h(t)¼2e/C0tu(t).
(a) Find the power spectral density of N(t).
(b) Find the transfer function of the LTI system.
(c) Find the power spectral density of the output process Y(t)¼L[N(t)].
25. A white noise process X(t) with power spectral density SXX(f)¼N0/2 is the input to an LTI
system with impulse response h(t)¼1 for 0 /C20t<1 (and 0 otherwise). Let Y(t) denote the
output.
(a) Find the mean of Y(t).
(b) Find the transfer function of the LTI system.
(c) Find the power spectral density of Y(t).
(d) Find the expected power of Y(t).
26. The random process X(t)¼A0cos(ω0t+Θ), where Θ~ Unif( /C0π,π], is the input to an LTI
system with impulse response h(t)¼Be/C0Btu(t). Let Y(t) denote the output.
(a) Determine the transfer function and power transfer function of this system.
(b) Find the power spectral density of Y(t).
(c) Determine the expected power in Y(t). How does that compare to X(t)?
27. A WSS random process X(t) with autocorrelation function RXX(τ)¼100 + 25 e/C0|τ|is passed
through an LTI system having impulse response h(t)¼te/C04tu(t). Let Y(t) denote the output.
(a) Find the power spectral density of X(t).
(b) What is the expected power of X(t)?
(c) Determine the transfer function and power transfer function of this system.
(d) Find and sketch the power spectral density of Y(t).
(e) What is the expected power of Y(t)?
28. A white noise process X(t) with power spectral density SXX(f)¼N0/2 is the input to an LTI
system with impulse response h(t)¼e/C0Btsin(ω0t)u(t). Let Y(t) denote the output.
(a) Determine the transfer function of the LTI system.
(b) Find and sketch the power spectral density of Y(t).
29. Suppose X(t) is a white noise process with power spectral density SXX(f)¼N0/2. A ﬁlter with
transfer function H(f)¼e/C0α|f|is applied to this process; let Y(t) denote the output.586 8 Introduction to Signal Processing
(a) Find the power spectral density of Y(t).
(b) Find the autocorrelation function of Y(t).
(c) Find the expected power of Y(t).
30. Let X(t) be a WSS random process with autocorrelation function RXX(τ)¼45,000sinc2(1800 τ);
this is the signal from Example 8.8 without the dc offset. Suppose X(t) encounters the noise N(t)
described in Example 8.8. Since both X(t) and N(t) are concentrated at low frequencies, it is
desirable to modulate X (t) and then use an appropriate ﬁlter. Consider the following modulation,
performed prior to transmission: Xmod(t)¼X(t)cos(4000 πt+Θ), where Θ~ Unif( /C0π,π]. The
received signal will be Xmod(t)+N(t), to which an ideal bandpass ﬁlter on the spectrum of
Xmod(t) will be applied.
(a) Find the autocorrelation function of Xmod(t).
(b) Find the power spectral density of Xmod(t).
(c) Based on (b), what would be the optimal frequency band to “pass” through a ﬁlter?
(d) Use the results of Example 8.8 to determine the expected power in L[N(t)], the ﬁltered
noise process.
(e) Compare the input and output power signal-to-noise ratios. How do these compare to the
SNRs in Example 8.8?
31. Let X(t) be the WSS input to an LTI system with impulse response h(t), and let Y(t) denote the
output.
(a) Show that the cross-correlation function RXY(τ) equals RXX(τ)Hh(τ) as stated in the main
proposition of this section. [ Hint: In the deﬁnition of RXY(τ), write Y(t+τ) as a convolution
integral. Rearrange, and then make an appropriate substitution to show that the integrand is
equal to RXX(τ/C0s)/C1h(s).]
(b) Show that the autocorrelation function of Y(t) is given by
RYYτðÞ¼ RXYτðÞHh/C0τðÞ ¼ RXXτðÞHhτðÞHh/C0τðÞ
[Hint: Write Y(t)¼X(t)Hh(t) in the deﬁnition of RYY(τ). Rearrange, and then make an
appropriate substitution to show that the integrand is equal to RXY(τ/C0s)/C1h(/C0s). Then
invoke (a).]
32. A T-second moving-average ﬁlter has impulse response h(t)¼1/Tfor 0 /C20t/C20T(and zero
otherwise).
(a) Find the transfer function of this ﬁlter.
(b) Find the power transfer function of this ﬁlter.
(c) Suppose X(t) is a white noise process with power spectral density SXX(f)¼N0/2. If X(t)i s
passed through this moving-average ﬁlter and Y(t) is the resulting output, ﬁnd the power
spectral density, expected power, and autocorrelation function of Y(t).
33. Suppose we pass band-limited white noise X(t) with arbitrary parameters N0andBthrough a
differentiator :
YtðÞ¼ LX tðÞ½/C138 ¼d
dtXtðÞ
The transfer function of the differentiator is known to be H(f)¼j2πf.
(a) Find the power spectral density of Y(t).
(b) Find the autocorrelation function of Y(t).
(c) What is the ensemble average power of the output?8.2 Random Processes and LTI Systems 587
34. A short-term integrator is deﬁned by the input-output relationship
YtðÞ¼ LX tðÞ½/C138 ¼1
Tðt
t/C0TXsðÞds
(a) Find the impulse response of this system.
(b) Find the power spectrum of Y(t) in terms of the power spectrum of X(t). [Hint: Write the
answer to (a) in terms of the rectangular function ﬁrst.]
35. Let X(t) be WSS, and let Y(t) be the output resulting from the application to X(t) of an LTI system
with impulse response h(t) and transfer function H(f). Deﬁne a new random process as the
difference between input and output: D(t)¼X(t)/C0Y(t).
(a) Find an expression for the autocorrelation function of D(t) in terms of RXXandh.
(b) Determine the power spectral density of D(t), and verify that your answer is real, symmetric,
and nonnegative.
36. An amplitude-modulated waveform can be modeled by A(t)cos(100 πt+Θ)+N(t), where A(t)i s
WSS and has autocorrelation function RAA(τ)¼80sinc2(10τ);Θ~ Unif( /C0π,π] and is indepen-
dent of A(t); and N(t) is band-limited white noise, independent of A(t) andΘ, with SNN(f)¼0.05
W/Hz for jfj<100 Hz. To ﬁlter out the noise, we pass the waveform through an ideal bandpass
ﬁlter with transfer function H(f)¼1 for 40 <jfj<60.
LetX(t)¼A(t)cos(100 πt+Θ), the signal part of the input.
(a) Find the autocorrelation of X(t).
(b) Find the ensemble average power in X(t).
(c) Find and graph the power spectral density of X(t).
(d) Find the ensemble average power in the signal part of the output.
(e) Find the ensemble average power in N(t).
(f) Find the ensemble average power in the noise part of the output.
(g) Find the power signal-to-noise ratio of the input and the power signal-to-noise ratio of the
output. Discuss what you ﬁnd.
37. A random signal X(t) incurs additive noise N(t) in transmission. The signal and noise components
are independent and wide-sense stationary, X(t) has autocorrelation function RXX(τ)¼250,000 +
120,000cos(70,000 πτ) + 800,000sinc(100,000 τ), and N(t) has power spectral density SNN(f)¼
2.5/C210/C02W/Hz for | f|/C20100 kHz. To ﬁlter out the noise, we pass the input X(t)+N(t) through
an ideal lowpass ﬁlter with transfer function H(f)¼1 for | f|/C2060 kHz.
(a) Find the ensemble average power in X(t).
(b) Find and sketch the power spectral density of X(t).
(c) Find the power spectral density of L[X(t)].
(d) Find the ensemble average power in L[X(t)].
(e) Find the ensemble average power in N(t).
(f) Find the ensemble average power in L[N(t)]. (Think about what the power spectral density
ofL[N(t)] will look like.)
(g) Find the power signal-to-noise ratio of the input and the power signal-to-noise ratio of the
output. Discuss what you ﬁnd.
38. Let X(t) be a pure white noise process with psd N0/2. Consider an LTI system with impulse
response h(t), and let Y(t) denote the output resulting from passing X(t) through this LTI system.
(a) Show that RXYτðÞ¼N0
2hτðÞ.
(b) Show that PY¼N0
2Eh, where Ehis the energy in the impulse response function, deﬁned by
Eh¼Ð1
/C01h2tðÞdt.588 8 Introduction to Signal Processing
8.3 Discrete-Time Signal Processing
Recall from Sect. 7.4that a random sequence (i.e., a discrete-time random process) Xnis said to be
wide-sense stationary if (1) its mean, μX[n], is a constant μXand (2) its autocorrelation function,
RXX[n, n + k ], depends only on the integer-valued time difference k(in which case we may denote the
autocorrelation RXX[k]). Analogous to the Wiener–Khinchin Theorem, the power spectral density of a
WSS random sequence is given by the discrete-time Fourier transform of its autocorrelation
function:
SXXFðÞ ¼X1
k¼/C01RXXk½/C138e/C0j2πFkð8:6Þ
We use parentheses around the argument Fin Eq. ( 8.6) because SXX(F) is a function on a
continuum, even though the random sequence is on a discrete index set (the integers). Similar to
the continuous case, it can be shown that SXX(F) is a real-valued, nonnegative, symmetric function of
F.(The choice of capital Fwill be explained toward the end of this section.)
Power spectral densities for random sequences differ from their continuous-time counterparts in
one key respect: the psd of a WSS random sequence is always a periodic function, with period 1. To
see this, recall that ej2πk¼1 for any integer k, and write
SXXFþ1ðÞ ¼Xþ1
k¼/C01RXXk½/C138e/C0j2πFþ1ðÞ k¼Xþ1
k¼/C01RXXk½/C138e/C0j2πFke/C0j2πk¼Xþ1
k¼/C01RXXk½/C138e/C0j2πFk¼SXXFðÞ
As a consequence, we may recover the autocorrelation function of a WSS random sequence from
its power spectrum by taking the inverse Fourier transform of SXX(F) over an interval of length 1:
RXXk½/C138 ¼ð1=2
/C01=2SXXFðÞej2πFkdF ð8:7Þ
This affects how we calculate the power in a random sequence from its power spectral density.
Analogous to the continuous-time case, we deﬁne the (ensemble) average power of a WSS random
sequence Xnby
PX¼EX2
n/C0/C1
¼RXX0½/C138 ¼ð1=2
/C01=2SXXFðÞej2πF0ðÞdF¼ð1=2
/C01=2SXXFðÞdF
That is, the expected power in a random sequence is determined by integrating its psd over one
period , not the entire frequency spectrum.
Example 8.9 Consider the Bernoulli sequence of Sect. 7.4: the Xnare iid Bernoulli rvs, a stationary
sequence with μX¼p,CXX[0]¼Var(Xn)¼p(1/C0p), and CXX[k]¼0 for k6¼0. From these, the
autocorrelation function is
RXXk½/C138 ¼ CXXk½/C138 þ μ2
X¼pk ¼0
p2k6¼0/C26
In particular, PX¼RXX[0]¼p.To determine the power spectral density, apply Eq. ( 8.6):8.3 Discrete-Time Signal Processing 589
SXXFðÞ ¼Xþ1
k¼/C01RXXk½/C138e/C0j2πFk¼RXX/C2
0/C3
e/C0j2πF0ðÞþX
k6¼0RXXk½/C138e/C0j2πFk
¼pþp2X
k6¼0e/C0j2πFk¼pþp2Xþ1
k¼/C01e/C0j2πFk/C0p2e/C0j2πF0ðÞ
¼p1/C0pðÞ þ p2Xþ1
k¼/C01e/C0j2πFk
Engineers will recognize this last summation as an impulse train (sometimes called a sampling
function orDirac comb ), from which we have
SXXFðÞ ¼ p1/C0pðÞ þ p2Xþ1
n¼/C01δF/C0nðÞ
A graph of this periodic function appears in Fig. 8.12; notice it is indeed a nonnegative, symmetric,
periodic function with period 1. Since it’s sufﬁcient to deﬁne the psd of a WSS random sequence on
the interval ( /C01/2, 1/2), we could drop all but one of the impulses and write SXX(F)¼p(1/C0p)+
p2δ(F) for/C01/2<F<1/2.
For a more general iid sequence with E[Xn]¼μXand Var( Xn)¼σX2, a similar derivation shows
thatSXX(F)¼σX2+μX2∑n¼/C01+1δ(F/C0n), orσX2+μX2δ(F) for/C01/2<F<1/2. In particular, if Xnis a
mean-zero iid sequence, the psd of Xnis just the constant σX2.
Example 8.10 Suppose Xnis a WSS random sequence with power spectral density SXX(F)¼tri(2F)
for/C01/2<F<1/2. Let’s determine the autocorrelation function of Xn.
The psd may be rewritten as SXX(F)¼1/C02|F| for /C01/2<F<1/2, which is shown in
Fig.8.13a . Apply Eq. ( 8.7):
RXXk½/C138 ¼ð1=2
/C01=2SXXFðÞej2πFkdF¼ð1=2
/C01=21/C02Fjj ðÞ ej2πFkdF
¼ð1=2
/C01=21/C02Fjj ðÞ cos 2πFkðÞ dF since 1 /C02Fjjis even ðÞ
¼2ð1=2
01/C02F ðÞ cos 2πFkðÞ dF since the intergrand is evenðÞ
Fork¼0, this is a simple polynomial integral resulting in RXX[0]¼1/2, which equals the area
under SXX(F), as required. For k6¼0, integration by parts yields−3 −2 −1 0 123SXX(F)
F
Fig. 8.12 Power spectral density of a Bernoulli sequence (Example 8.9) ■590 8 Introduction to Signal Processing
RXXk½/C138 ¼1/C0cosπkðÞ
π2k2¼2=π2k2/C0/C1
kodd
0 keven/C26
The graph of this autocorrelation function appears in Fig. 8.13b .
8.3.1 Random Sequences and LTI Systems
A discrete-time LTI system Lhas similar properties to those described in the previous section for
continuous time. If we let δ[n] denote the Kronecker delta function —i.e., δ[0]¼1 and δ[n]¼0 for
n6¼0—then a discrete-time LTI system is characterized by an impulse response4function h[n]
deﬁned by h[n]¼L[δ[n]]. If we let Xndenote the input to the LTI system and Ynthe output, so that
Yn¼L[Xn], then Ynmay be computed through discrete-time convolution:
Yn¼XnHhn½/C138 ¼X1
k¼/C01Xkhn/C0k½/C138 ¼X1
k¼/C01Xn/C0khk½/C138
Discrete-time LTI systems can be characterized in the frequency domain by a transfer function
H(F), deﬁned as the discrete-time Fourier transform of the impulse response:
HFðÞ ¼X1
n¼/C01hn½/C138e/C0j2πFn
This transfer function, like the power spectral density, is periodic in Fwith period 1. The
properties of the output sequence Ynare similar to those for Y(t) in the continuous-time case.−6 −4 −2 0 2 460.251 0.5b a
F kSXX(F) RXX[k]
− 5.0 5.0
Fig. 8.13 Graphs for Example 8.10: ( a) Power spectral density; ( b) autocorrelation function ■
4In this context, the Kronecker delta function is also commonly called the unit sample response , since it is strictly
speaking not an impulse (its value is well deﬁned at zero). It does, however, share the two key properties of a traditional
Dirac delta function (i.e., an impulse): it equals zero for all non-zero inputs, and the sum across its entire domain equals
1.8.3 Discrete-Time Signal Processing 591
PROPOSITION
LetLbe an LTI system with impulse response h[n] and transfer function H(F). Suppose Xnis a
wide-sense stationary sequence and let Yn¼L[Xn], the output of the LTI system applied to Xn.
Then Ynis also WSS, with the following properties.
Time domain Frequency domain
1.μY¼μXX1
n¼/C01hn½/C138 1.μY¼μX/C1H(0)
2.RYY[k]¼RXX[k]Hh[k]Hh[/C0k]2 . SYY(F)¼SXX(F)/C1|H(F)|2
3.PY¼RYY[0] 3.PY¼ð1=2
/C01=2SYYFðÞdF
Example 8.11 Amoving average operator can be used to “smooth out” a noisy sequence. The
simplest moving average takes the mean of two successive terms: Yn¼(Xn/C01+Xn)/2. This formula
is equivalent to passing the sequence Xnthrough an LTI system with an impulse response given by
h[0]¼h[1]¼1/2 and h[n]¼0 otherwise. The transfer function of this LTI system is
HFðÞ ¼X1
n¼/C01hn½/C138e/C0j2πFn¼1
2e/C0j2πF0ðÞþ1
2e/C0j2πF1ðÞ¼1þe/C0j2πF
2,
from which the power transfer function is
HFðÞjj2¼1þe/C0j2πF
2/C12/C12/C12/C12/C12/C12/C12/C122
¼1þcos 2 πFðÞ /C0 jsin 2 πFðÞ
2/C12/C12/C12/C12/C12/C12/C12/C122
¼1þcos 2 πFðÞ ðÞ2þsin22πFðÞ
22¼1þcos 2 πFðÞ
2
Notice that the function (1 + cos(2 πF))/2 is periodic with period 1, as required.
Suppose Xnis a WSS random sequence with power spectral density SXX(F)¼N0for |F|<1/2, as
depicted in Fig. 8.14a . Then the moving average Ynhas psd equal to
−0.5 0.5 0 −0.5 0.5 0SXX (F) SYY (F)
N0 N0
F Fab
Fig. 8.14 Power spectral density of the moving average in Example 8.11592 8 Introduction to Signal Processing
SYYFðÞ ¼ SXXFðÞ /C1/C12/C12HFðÞ/C12/C122¼N0/C11þcos 2 πFðÞ
2
The graph of this power spectral density appears in Fig. 8.14b . The ensemble average power in Yn
can be determined by integrating this function from /C01/2 to 1/2:
PY¼ð1=2
/C01=2SYYFðÞdF¼N0
2ð1=2
/C01=21þcos 2 πFðÞ ½/C138 dF¼N0
21ðÞ ¼N0
2■
8.3.2 Random Sequences and Sampling
Modern electronic systems often work with digitized signals: analog signals that have been “sampled”
at regular intervals to create a digital (i.e., discrete-time) signal. Suppose we have a continuous-time
(analog) signal X(t), which we sample every Tsseconds; Tsis called the sampling interval . That is,
we only observe X(t) at times 0, /C6Ts,/C62Ts, and so on. Then we can regard our observed (digital)
signal as a random sequence X[n] deﬁned by
Xn½/C138 ¼ Xn T sðÞ forn¼ ...,/C02,/C01, 0, 1, 2, ...
This is illustrated for a sample function in Fig. 8.15.
The following proposition ensures that the sampled version of a WSS random process is also
WSS—and, hence, that the spectral density theory presented in this chapter applies.
PROPOSITION
LetX(t) be a WSS random process, and for some ﬁxed Ts>0 deﬁne X[n]¼X(nTs). Then the
random sequence X[n] is a WSS random sequence.
The proof was requested in Exercise 45 of Chap. 7.x(t)
t 0Fig. 8.15 An analog
signal x(t) and its sampled
version x[n] (indicated by
asterisks)8.3 Discrete-Time Signal Processing 593
If the sampling interval is selected judiciously, then we may (in some sense) recover the original
signal from the digitized version. This relies on a key result from communication theory called the
Nyquist sampling theorem for deterministic signals: If a signal x(t) has no frequencies above BHz,
then x(t) is completely determined by its sample values x[n]¼x(nTs) so long as
fs¼1
Ts/C212B
The quantity fsis called the sampling rate . The Nyquist sampling theorem says that a band-limited
signal (with band limit B) can be completely recovered from its digital version, provided the sampling
rate is at least 2 B. For example, a signal with band limit B¼1 kHz ¼1000 Hz must be sampled at
least 2,000 times per second; equivalently, the sampling interval Tscan be at most 1/(2B)¼.0005 s.
The minimum sampling rate, 2 B, is sometimes called the Nyquist rate of that signal.
When Ts/C201/(2B), as required by the Nyquist sampling theorem, the original deterministic signal
x(t) may be reconstructed by the interpolation formula
xtðÞ¼X1
n¼/C01xn½/C138sinct/C0nTs
Ts/C18/C19
ð8:8Þ
The heart of the Nyquist sampling theorem is the statement that the two sides of Eq. ( 8.8) are equal.
For a band-limited random process X(t) with corresponding digital sequence X[n]¼X(nTs), we
may deﬁne a Nyquist interpolation of X(t)b y
XNyqtðÞ¼X1
n¼/C01Xn½/C138sinct/C0nTs
Ts/C18/C19
It can be shown that XNyq(t) equals the original X(t) in the “mean square sense,” i.e., that
EX NyqtðÞ/C0 XtðÞ/C0/C12hi
¼0
(This is slightly weaker than saying XNyq(t)¼X(t); in particular, there may exist a negligible set of
sample functions for which the two differ.)
There is a direct connection between the Nyquist sampling rate and the argument Fof the discrete-
time Fourier transform. Suppose a random process X(t) has band limit B, i.e., the set of frequencies
frepresented in the spectrum of X(t) satisﬁes /C0B/C20f/C20B. Provided we use a sampling rate, fs,a t
least as great as the Nyquist rate 2 B, we have:
/C0B/C20f/C20B,fs/C212B)/C01
2/C20f
fs/C201
2
If we deﬁne F¼f/fs, we have a unitless variable whose set of possible values exactly corresponds
to that of Fin the discrete-time Fourier transform. Said differently, Fin the discrete-time Fourier
transform represents a normalized frequency; we can recover the spectrum of X(t) across its original
frequency band by writing f¼F/C1fs. (In some textbooks, you will see the argument of the discrete-
time Fourier transform denoted Ω, to indicate radian measure. The variables FandΩare, of course,
related by Ω¼2πF.)594 8 Introduction to Signal Processing
8.3.3 Exercises: Section 8.3(39–50)
39. Let X[n] be a WSS random sequence. Show that the power spectral density of X[n] may be
rewritten as
SXXFðÞ ¼ RXX0½/C138 þ 2X1
k¼1RXXk½/C138cos 2πkFðÞ
40. Let X(t) be a WSS random process, and let X[n]¼X(nTs), the sampled version of X(t). Find the
power spectral density of X[n] in terms of the psd of X(t).
41. Suppose X[n] is a WSS random sequence with autocorrelation function RXX[k]¼α|k|for some
constant | α|<1. Find the power spectral density of X[n]. Sketch this psd for α¼/C0.5, 0, and .5.
42. Consider the correlated bit noise sequence described in Exercise 50 of Chap. 7:X0is 0 or 1 with
probability .5 each and, for n/C211,Xn¼Xn/C01with probability .9 and 1 /C0Xn/C01with probability
.1. It was shown in that exercise that Xnis a WSS random sequence with mean μX¼.5 and
autocorrelation function
RXXk½/C138 ¼1þ:8kjj
4
(This particular random sequence can be “time reversed” so that Xnis deﬁned for negative
indices as well.) Find the power spectral density of this correlated bit noise sequence.
43. A Poisson telegraphic process N(t) with parameter λ¼1 (see Sect. 7.5) is sampled every 5 s,
resulting in the random sequence X[n]¼N(5n). Find the power spectral density of X[n].
44.Discrete-time white noise is a WSS, mean-zero process such that XnandXmare uncorrelated for
alln6¼m.
(a) Show that the autocorrelation function of discrete-time white noise is RXX[k]¼σ2δ[k] for
some constant σ>0, where δ[k] is the Kronecker delta function.
(b) Find the power spectral density of discrete-time white noise. Is it what you’d expect?
45. Suppose Xnis a WSS random sequence with the following autocorrelation function:
RXXk½/C138 ¼1 k¼0
1
2k2kodd
0 otherwise8
>><
>>:
Determine the power spectral density of Xn.[Hint: Use Example 8.10.]
46. Let Xnbe the WSS input to a discrete-time LTI system with impulse response h[n], and let Ynbe
the output. Deﬁne the cross-correlation ofXnandYnbyRXY[n, n + k ]¼E[XnYn+k].
(a) Show that RXYdoes not depend on n, and that RXY¼RXXHh, where Hdenotes discrete-
time convolution. (This is the discrete-time version of a result from the previous section.)
(b) The cross-power spectral density SXY(F) of two jointly WSS random sequences XnandYnis
deﬁned as the discrete-time Fourier transform of RXY[k]. In the present context, show that
SXY(F)¼SXX(F)H(F), where Hdenotes the transfer function of the LTI system.
47. The WSS random sequence Xnhas power spectral density SXX(F)¼2Pfor |F|/C201/4 and 0 for
1/4<|F|<1/2.
(a) Verify that the ensemble average power in XnisP.
(b) Find the autocorrelation function of Xn.8.3 Discrete-Time Signal Processing 595
48. Let Xnhave power spectral density SXX(F), and suppose Xnis passed through a discrete-time LTI
system with impulse response h[n]¼αnforn¼0, 1, 2, ...for some constant | α|<1 (and
h[n]¼0 otherwise). Let Yndenote the output sequence.
(a) Find the mean of Ynin terms of the mean of Xn.
(b) Find the power spectral density of Ynin terms of the psd of Xn.
49. The system in Example 8.11 can be extended to an M-term simple moving average ﬁlter, with
impulse response
hn½/C138 ¼1=Mn ¼0, 1, ...,M/C01
0 otherwise/C26
LetXnbe the WSS input to such a ﬁlter, and let Ynbe the output.
(a) Write an expression for Ynin terms of the Xn.
(b) Determine the transfer function of this ﬁlter.
(c) Assuming Xnis a discrete-time white noise process (see Exercise 44), determine the
autocorrelation function of Yn.
50. A more general moving average process has the form
Yn½/C138 ¼ θ0Xn½/C138 þ θ1Xn/C01½ /C138þ/C1/C1/C1þ θMXn/C0M½/C138
for some integer Mand constants θ0,...,θM. Let the input sequence X[n] be iid, with mean 0 and
variance σ2.
(a) Find the impulse response h[n] of the LTI system that produces Y[n] from X[n].
(b) Find the transfer function of this system.
(c) Find the mean of Y[n].
(d) Find the variance of Y[n].
(e) Find the autocovariance function of Y[n].596 8 Introduction to Signal Processing
Appendix A: Statistical Tables
A.1 Binomial cdf
Table A.1 Cumulative binomial probabilities Bx ;n;pðÞ ¼Px
y¼0by;n;pðÞ
(a)n=5
p
0.05 0.10 0.20 0.25 0.30 0.40 0.50 0.60 0.70 0.75 0.80 0.90 0.95
0 .774 .590 .328 .237 .168 .078 .031 .010 .002 .001 .000 .000 .000
1 .977 .919 .737 .633 .528 .337 .188 .087 .031 .016 .007 .000 .000
x 2 .999 .991 .942 .896 .837 .683 .500 .317 .163 .104 .058 .009 .001
3 1.000 1.000 .993 .984 .969 .913 .812 .663 .472 .367 .263 .081 .023
4 1.000 1.000 1.000 .999 .998 .990 .969 .922 .832 .763 .672 .410 .226
(b)n=1 0
p
0.05 0.10 0.20 0.25 0.30 0.40 0.50 0.60 0.70 0.75 0.80 0.90 0.95
0 .599 .349 .107 .056 .028 .006 .001 .000 .000 .000 .000 .000 .000
1 .914 .736 .376 .244 .149 .046 .011 .002 .000 .000 .000 .000 .000
2 .988 .930 .678 .526 .383 .167 .055 .012 .002 .000 .000 .000 .000
3 .999 .987 .879 .776 .650 .382 .172 .055 .011 .004 .001 .000 .000
4 1.000 .998 .967 .922 .850 .633 .377 .166 .047 .020 .006 .000 .000
x 5 1.000 1.000 .994 .980 .953 .834 .623 .367 .150 .078 .033 .002 .000
6 1.000 1.000 .999 .996 .989 .945 .828 .618 .350 .224 .121 .013 .001
7 1.000 1.000 1.000 1.000 .998 .988 .945 .833 .617 .474 .322 .070 .012
8 1.000 1.000 1.000 1.000 1.000 .998 .989 .954 .851 .756 .624 .264 .086
9 1.000 1.000 1.000 1.000 1.000 1.000 .999 .994 .972 .944 .893 .651 .401
(continued)
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6597
Table A.1 (continued)
(c)n=1 5
p
0.05 0.10 0.20 0.25 0.30 0.40 0.50 0.60 0.70 0.75 0.80 0.90 0.95
0 .463 .206 .035 .013 .005 .000 .000 .000 .000 .000 .000 .000 .000
1 .829 .549 .167 .080 .035 .005 .000 .000 .000 .000 .000 .000 .000
2 .964 .816 .398 .236 .127 .027 .004 .000 .000 .000 .000 .000 .000
3 .995 .944 .648 .461 .297 .091 .018 .002 .000 .000 .000 .000 .000
4 .999 .987 .836 .686 .515 .217 .059 .009 .001 .000 .000 .000 .000
5 1.000 .998 .939 .852 .722 .402 .151 .034 .004 .001 .000 .000 .000
6 1.000 1.000 .982 .943 .869 .610 .304 .095 .015 .004 .001 .000 .000
x 7 1.000 1.000 .996 .983 .950 .787 .500 .213 .050 .017 .004 .000 .000
8 1.000 1.000 .999 .996 .985 .905 .696 .390 .131 .057 .018 .000 .000
9 1.000 1.000 1.000 .999 .996 .966 .849 .597 .278 .148 .061 .002 .000
10 1.000 1.000 1.000 1.000 .999 .991 .941 .783 .485 .314 .164 .013 .001
11 1.000 1.000 1.000 1.000 1.000 .998 .982 .909 .703 .539 .352 .056 .005
12 1.000 1.000 1.000 1.000 1.000 1.000 .996 .973 .873 .764 .602 .184 .036
13 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .995 .965 .920 .833 .451 .171
14 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .995 .987 .965 .794 .537
(d)n=2 0
p
0.05 0.10 0.20 0.25 0.30 0.40 0.50 0.60 0.70 0.75 0.80 0.90 0.95
0 .358 .122 .012 .003 .001 .000 .000 .000 .000 .000 .000 .000 .000
1 .736 .392 .069 .024 .008 .001 .000 .000 .000 .000 .000 .000 .000
2 .925 .677 .206 .091 .035 .004 .000 .000 .000 .000 .000 .000 .000
3 .984 .867 .411 .225 .107 .016 .001 .000 .000 .000 .000 .000 .000
4 .997 .957 .630 .415 .238 .051 .006 .000 .000 .000 .000 .000 .000
5 1.000 .989 .804 .617 .416 .126 .021 .002 .000 .000 .000 .000 .000
6 1.000 .998 .913 .786 .608 .250 .058 .006 .000 .000 .000 .000 .000
7 1.000 1.000 .968 .898 .772 .416 .132 .021 .001 .000 .000 .000 .000
8 1.000 1.000 .990 .959 .887 .596 .252 .057 .005 .001 .000 .000 .000
x 9 1.000 1.000 .997 .986 .952 .755 .412 .128 .017 .004 .001 .000 .000
10 1.000 1.000 .999 .996 .983 .872 .588 .245 .048 .014 .003 .000 .000
11 1.000 1.000 1.000 .999 .995 .943 .748 .404 .113 .041 .010 .000 .000
12 1.000 1.000 1.000 1.000 .999 .979 .868 .584 .228 .102 .032 .000 .000
13 1.000 1.000 1.000 1.000 1.000 .994 .942 .750 .392 .214 .087 .002 .000
14 1.000 1.000 1.000 1.000 1.000 .998 .979 .874 .584 .383 .196 .011 .000
15 1.000 1.000 1.000 1.000 1.000 1.000 .994 .949 .762 .585 .370 .043 .003
16 1.000 1.000 1.000 1.000 1.000 1.000 .999 .984 .893 .775 .589 .133 .016
17 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .996 .965 .909 .794 .323 .075
18 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .999 .992 .976 .931 .608 .264
19 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .999 .997 .988 .878 .642
(continued)598 Appendix A: Statistical Tables
Table A.1 (continued)
(e)n=2 5
p
0.05 0.10 0.20 0.25 0.30 0.40 0.50 0.60 0.70 0.75 0.80 0.90 0.95
0 .277 .072 .004 .001 .000 .000 .000 .000 .000 .000 .000 .000 .000
1 .642 .271 .027 .007 .002 .000 .000 .000 .000 .000 .000 .000 .000
2 .873 .537 .098 .032 .009 .000 .000 .000 .000 .000 .000 .000 .000
3 .966 .764 .234 .096 .033 .002 .000 .000 .000 .000 .000 .000 .000
4 .993 .902 .421 .214 .090 .009 .000 .000 .000 .000 .000 .000 .000
5 .999 .967 .617 .378 .193 .029 .002 .000 .000 .000 .000 .000 .000
6 1.000 .991 .780 .561 .341 .074 .007 .000 .000 .000 .000 .000 .000
7 1.000 .998 .891 .727 .512 .154 .022 .001 .000 .000 .000 .000 .000
8 1.000 1.000 .953 .851 .677 .274 .054 .004 .000 .000 .000 .000 .000
9 1.000 1.000 .983 .929 .811 .425 .115 .013 .000 .000 .000 .000 .000
10 1.000 1.000 .994 .970 .902 .586 .212 .034 .002 .000 .000 .000 .000
11 1.000 1.000 .998 .980 .956 .732 .345 .078 .006 .001 .000 .000 .000
x 12 1.000 1.000 1.000 .997 .983 .846 .500 .154 .017 .003 .000 .000 .000
13 1.000 1.000 1.000 .999 .994 .922 .655 .268 .044 .020 .002 .000 .000
14 1.000 1.000 1.000 1.000 .998 .966 .788 .414 .098 .030 .006 .000 .000
15 1.000 1.000 1.000 1.000 1.000 .987 .885 .575 .189 .071 .017 .000 .000
16 1.000 1.000 1.000 1.000 1.000 .996 .946 .726 .323 .149 .047 .000 .000
17 1.000 1.000 1.000 1.000 1.000 .999 .978 .846 .488 .273 .109 .002 .000
18 1.000 1.000 1.000 1.000 1.000 1.000 .993 .926 .659 .439 .220 .009 .000
19 1.000 1.000 1.000 1.000 1.000 1.000 .998 .971 .807 .622 .383 .033 .001
20 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .991 .910 .786 .579 .098 .007
21 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .998 .967 .904 .766 .236 .034
22 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .991 .968 .902 .463 .127
23 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .998 .993 .973 .729 .358
24 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 .999 .996 .928 .723Appendix A: Statistical Tables 599
A.2 Poisson cdf
Table A.2 Cumulative Poisson probabilities Px ;mðÞ ¼Px
y¼0e/C0mmy
y!
μ
.1 .2 .3 .4 .5 .6 .7 .8 .9 1.0
0 .905 .819 .741 .670 .607 .549 .497 .449 .407 .368
l .995 .982 .963 .938 .910 .878 .844 .809 .772 .736
2 1.000 .999 .996 .992 .986 .977 .966 .953 .937 .920
x 3 1.000 1.000 .999 .998 .997 .994 .991 .987 .981
4 1.000 1.000 1.000 .999 .999 .998 .996
5 1.000 1.000 1.000 .999
6 1.000
μ
2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 15.0 20.0
0 .135 .050 .018 .007 .002 .001 .000 .000 .000 .000 .000
1 .406 .199 .092 .040 .017 .007 .003 .001 .000 .000 .000
2 .677 .423 .238 .125 .062 .030 .014 .006 .003 .000 .000
3 .857 .647 .433 .265 .151 .082 .042 .021 .010 .000 .000
4 .947 .815 .629 .440 .285 .173 .100 .055 .029 .001 .000
5 .983 .916 .785 .616 .446 .301 .191 .116 .067 .003 .000
6 .995 .966 .889 .762 .606 .450 .313 .207 .130 .008 .000
7 .999 .988 .949 .867 .744 .599 .453 .324 .220 .018 .001
8 1.000 .996 .979 .932 .847 .729 .593 .456 .333 .037 .002
9 .999 .992 .968 .916 .830 .717 .587 .458 .070 .005
10 1.000 .997 .986 .957 .901 .816 .706 .583 .118 .011
11 .999 .995 .980 .947 .888 .803 .697 .185 .021
12 1.000 .998 .991 .973 .936 .876 .792 .268 .039
13 .999 .996 .987 .966 .926 .864 .363 .066
14 1.000 .999 .994 .983 .959 .917 .466 .105
15 .999 .998 .992 .978 .951 .568 .157
x 16 1.000 .999 .996 .989 .973 .664 .221
17 1.000 .998 .995 .986 .749 .297
18 .999 .998 .993 .819 .381
19 1.000 .999 .997 .875 .470
20 1.000 .998 .917 .559
21 .999 .947 .644
22 1.000 .967 .721
23 .981 .787
24 .989 .843
25 .994 .888
26 .997 .922
27 .998 .948
28 .999 .966
29 1.000 .978
30 .987600 Appendix A: Statistical Tables
A.3 Standard Normal cdf
Table A.3 Standard normal curve areas
z .00 .01 .02 .03 .04 .05 .06 .07 .08 .09
–3.4 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0002
–3.3 .0005 .0005 .0005 .0004 .0004 .0004 .0004 .0004 .0004 .0003
–3.2 .0007 .0007 .0006 .0006 .0006 .0006 .0006 .0005 .0005 .0005
–3.1 .0010 .0009 .0009 .0009 .0008 .0008 .0008 .0008 .0007 .0007
–3.0 .0013 .0013 .0013 .0012 .0012 .0011 .0011 .0011 .0010 .0010
–2.9 .0019 .0018 .0017 .0017 .0016 .0016 .0015 .0015 .0014 .0014
–2.8 .0026 .0025 .0024 .0023 .0023 .0022 .0021 .0021 .0020 .0019
–2.7 .0035 .0034 .0033 .0032 .0031 .0030 .0029 .0028 .0027 .0026
–2.6 .0047 .0045 .0044 .0043 .0041 .0040 .0039 .0038 .0037 .0036
–2.5 .0062 .0060 .0059 .0057 .0055 .0054 .0052 .0051 .0049 .0048
–2.4 .0082 .0080 .0078 .0075 .0073 .0071 .0069 .0068 .0066 .0064
–2.3 .0107 .0104 .0102 .0099 .0096 .0094 .0091 .0089 .0087 .0084
–2.2 .0139 .0136 .0132 .0129 .0125 .0122 .0119 .0116 .0113 .0110
–2.1 .0179 .0174 .0170 .0166 .0162 .0158 .0154 .0150 .0146 .0143
–2.0 .0228 .0222 .0217 .0212 .0207 .0202 .0197 .0192 .0188 .0183
–1.9 .0287 .0281 .0274 .0268 .0262 .0256 .0250 .0244 .0239 .0233
–1.8 .0359 .0352 .0344 .0336 .0329 .0322 .0314 .0307 .0301 .0294
–1.7 .0446 .0436 .0427 .0418 .0409 .0401 .0392 .0384 .0375 .0367
–1.6 .0548 .0537 .0526 .0516 .0505 .0495 .0485 .0475 .0465 .0455
–1.5 .0668 .0655 .0643 .0630 .0618 .0606 .0594 .0582 .0571 .0559
–1.4 .0808 .0793 .0778 .0764 .0749 .0735 .0722 .0708 .0694 .0681
–1.3 .0968 .0951 .0934 .0918 .0901 .0885 .0869 .0853 .0838 .0823
–1.2 .1151 .1131 .1112 .1093 .1075 .1056 .1038 .1020 .1003 .0985
–1.1 .1357 .1335 .1314 .1292 .1271 .1251 .1230 .1210 .1190 .1170
–1.0 .1587 .1562 .1539 .1515 .1492 .1469 .1446 .1423 .1401 .1379
–0.9 .1841 .1814 .1788 .1762 .1736 .1711 .1685 .1660 .1635 .1611
–0.8 .2119 .2090 .2061 .2033 .2005 .1977 .1949 .1922 .1894 .1867
–0.7 .2420 .2389 .2358 .2327 .2296 .2266 .2236 .2206 .2177 .2148
–0.6 .2743 .2709 .2676 .2643 .2611 .2578 .2546 .2514 .2483 .2451
–0.5 .3085 .3050 .3015 .2981 .2946 .2912 .2877 .2843 .2810 .2776
–0.4 .3446 .3409 .3372 .3336 .3300 .3264 .3228 .3192 .3156 .3121
–0.3 .3821 .3783 .3745 .3707 .3669 .3632 .3594 .3557 .3520 .3482
–0.2 .4207 .4168 .4129 .4090 .4052 .4013 .3974 .3936 .3897 .3859
–0.1 .4602 .4562 .4522 .4483 .4443 .4404 .4364 .4325 .4286 .4247
–0.0 .5000 .4960 .4920 .4880 .4840 .4801 .4761 .4721 .4681 .4641
(continued)Appendix A: Statistical Tables 601
Table A.3 (continued)
z .00 .01 .02 .03 .04 .05 .06 .07 .08 .09
0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359
0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753
0.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141
0.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517
0.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879
0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224
0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .7549
0.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852
0.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133
0.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389
1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621
1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830
1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015
1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177
1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9278 .9292 .9306 .9319
1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441
1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545
1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633
1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .9706
1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767
2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817
2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857
2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890
2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916
2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936
2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952
2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964
2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974
2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981
2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986
3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990
3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993
3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995
3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997
3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998602 Appendix A: Statistical Tables
A.4 Incomplete Gamma Function
Table A.4 The incomplete gamma function Gx ;aðÞ ¼ðx
01
GaðÞya/C01e/C0ydy
α
12 3 4 56789 1 0
1 .632 .264 .080 .019 .004 .001 .000 .000 .000 .000
2 .865 .594 .323 .143 .053 .017 .005 .001 .000 .000
3 .950 .801 .577 .353 .185 .084 .034 .012 .004 .001
4 .982 .908 .762 .567 .371 .215 .111 .051 .021 .008
5 .993 .960 .875 .735 .560 .384 .238 .133 .068 .032
6 .998 .983 .938 .849 .715 .554 .394 .256 .153 .084
7 .999 .993 .970 .918 .827 .699 .550 .401 .271 .170
8 1.000 .997 .986 .958 .900 .809 .687 .547 .407 .283
x 9 .999 .994 .979 .945 .884 .793 .676 .544 .413
10 1.000 .997 .990 .971 .933 .870 .780 .667 .542
11 .999 .995 .985 .962 .921 .857 .768 .659
12 1.000 .998 .992 .980 .954 .911 .845 .758
13 .999 .996 .989 .974 .946 .900 .834
14 1.000 .998 .994 .986 .968 .938 .891
15 .999 .997 .992 .982 .963 .930Appendix A: Statistical Tables 603
A.5 Critical Values for tDistributions
Table A.5 Critical values for tdistributionsCentral area
0t density curve
−t critical value t critical value
Central area
ν 80% 90% 95% 98% 99% 99.8% 99.9%
1 3.078 6.314 12.706 31.821 63.657 318.31 636.62
2 1.886 2.920 4.303 6.965 9.925 22.326 31.598
3 1.638 2.353 3.182 4.541 5.841 10.213 12.924
4 1.533 2.132 2.776 3.747 4.604 7.173 8.610
5 1.476 2.015 2.571 3.365 4.032 5.893 6.869
6 1.440 1.943 2.447 3.143 3.707 5.208 5.959
7 1.415 1.895 2.365 2.998 3.499 4.785 5.408
8 1.397 1.860 2.306 2.896 3.355 4.501 5.041
9 1.383 1.833 2.262 2.821 3.250 4.297 4.781
10 1.372 1.812 2.228 2.764 3.169 4.144 4.587
11 1.363 1.796 2.201 2.718 3.106 4.025 4.437
12 1.356 1.782 2.179 2.681 3.055 3.930 4.318
13 1.350 1.771 2.160 2.650 3.012 3.852 4.221
14 1.345 1.761 2.145 2.624 2.977 3.787 4.140
15 1.341 1.753 2.131 2.602 2.947 3.733 4.073
16 1.337 1.746 2.120 2.583 2.921 3.686 4.015
17 1.333 1.740 2.110 2.567 2.898 3.646 3.965
18 1.330 1.734 2.101 2.552 2.878 3.610 3.922
19 1.328 1.729 2.093 2.539 2.861 3.579 3.883
20 1.325 1.725 2.086 2.528 2.845 3.552 3.850
21 1.323 1.721 2.080 2.518 2.831 3.527 3.819
22 1.321 1.717 2.074 2.508 2.819 3.505 3.792
23 1.319 1.714 2.069 2.500 2.807 3.485 3.767
24 1.318 1.711 2.064 2.492 2.797 3.467 3.745
25 1.316 1.708 2.060 2.485 2.787 3.450 3.725
26 1.315 1.706 2.056 2.479 2.779 3.435 3.707
27 1.314 1.703 2.052 2.473 2.771 3.421 3.690
28 1.313 1.701 2.048 2.467 2.763 3.408 3.674
29 1.311 1.699 2.045 2.462 2.756 3.396 3.659
30 1.310 1.697 2.042 2.457 2.750 3.385 3.646
32 1.309 1.694 2.037 2.449 2.738 3.365 3.622
34 1.307 1.691 2.032 2.441 2.728 3.348 3.601
36 1.306 1.688 2.028 2.434 2.719 3.333 3.582
38 1.304 1.686 2.024 2.429 2.712 3.319 3.566
40 1.303 1.684 2.021 2.423 2.704 3.307 3.551
50 1.299 1.676 2.009 2.403 2.678 3.262 3.496
60 1.296 1.671 2.000 2.390 2.660 3.232 3.460
120 1.289 1.658 1.980 2.358 2.617 3.160 3.373
1 1.282 1.645 1.960 2.326 2.576 3.090 3.291604 Appendix A: Statistical Tables
A.6 Tail Areas of tDistributions
Table A.6 tcurve tail areasArea to the
right of tt curve
0
t
Degrees of Freedom ( ν)
t 123456789 1 0 1 1 1 2
0.0 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500
0.1 .468 .465 .463 .463 .462. .462 .462 .461 .461 .461 .461 .461
0.2 .437 .430 .427 .426 .425 .424 .424 .423 .423 .423 .423 .422
0.3 .407 .396 .392 .390 .388 .387 .386 .386 .386 .385 .385 .385
0.4 .379 .364 .358 .355 .353 .352 .351 .350 .349 .349 .348 .348
0.5 .352 .333 .326 .322 .319 .317 .316 .315 .315 .314 .313 .313
0.6 .328 .305 .295 .290 .287 .285 .284 .283 .282 .281 .280 .280
0.7 .306 .278 .267 .261 .258 .255 .253 .252 .251 .250 .249 .249
0.8 .285 .254 .241 .234 .230 .227 .225 .223 .222 .221 .220 .220
0.9 .267 .232 .217 .210 .205 .201 .199 .197 .196 .195 .194 .193
1.0 .250 .211 .196 .187 .182 .178 .175 .173 .172 .170 .169 .169
1.1 .235 .193 .176 .167 .162 .157 .154 .152 .150 .149 .147 .146
1.2 .221 .177 .158 .148 .142 .138 .135 .132 .130 .129 .128 .127
1.3 .209 .162 .142 .132 .125 .121 .117 .115 .113 .111 .110 .109
1.4 .197 .148 .128 .117 .110 .106 .102 .100 .098 .096 .095 .093
1.5 .187 .136 .115 .104 .097 .092 .089 .086 .084 .082 .081 .080
1.6 .178 .125 .104 .092 .085 .080 .077 .074 .072 .070 .069 .068
1.7 .169 .116 .094 .082 .075 .070 .065 .064 .062 .060 .059 .057
1.8 .161 .107 .085 .073 .066 .061 .057 .055 .053 .051 .050 .049
1.9 .154 .099 .077 .065 .058 .053 .050 .047 .045 .043 .042 .041
2.0 .148 .092 .070 .058 .051 .046 .043 .040 .038 .037 .035 .034
2.1 .141 .085 .063 .052 .045 .040 .037 .034 .033 .031 .030 .029
2.2 .136 .079 .058 .046 .040 .035 .032 .029 .028 .026 .025 .024
2.3 .131 .074 .052 .041 .035 .031 .027 .025 .023 .022 .021 .020
2.4 .126 .069 .048 .037 .031 .027 .024 .022 .020 .019 .018 .017
2.5 .121 .065 .044 .033 .027 .023 .020 .018 .017 .016 .015 .014
2.6 .117 .061 .040 .030 .024 .020 .018 .016 .014 .013 .012 .012
2.7 .113 .057 .037 .027 .021 .018 .015 .014 .012 .011 .010 .010
2.8 .109 .054 .034 .024 .019 .016 .013 .012 .010 .009 .009 .008
2.9 .106 .051 .031 .022 .017 .014 .011 .010 .009 .008 .007 .007
3.0 .102 .048 .029 .020 .015 .012 .010 .009 .007 .007 .006 .006
3.1 .099 .045 .027 .018 .013 .011 .009 .007 .006 .006 .005 .005
3.2 .096 .043 .025 .016 .012 .009 .008 .006 .005 .005 .004 .004
3.3 .094 .040 .023 .015 .011 .008 .007 .005 .005 .004 .004 .003
3.4 .091 .038 .021 .014 .010 .007 .006 .005 .004 .003 .003 .003
3.5 .089 .036 .020 .012 .009 .006 .005 .004 .003 .003 .002 .002
3.6 .086 .035 .018 .011 .008 .006 .004 .004 .003 .002 .002 .002
3.7 .084 .033 .017 .010 .007 .005 .004 .003 .002 .002 .002 .002
3.8 .082 .031 .016 .010 .006 .004 .003 .003 .002 .002 .001 .001
3.9 .080 .030 .015 .009 .006 .004 .003 .002 .002 .001 .001 .001
4.0 .078 .029 .014 .008 .005 .004 .003 .002 .002 .001 .001 .001Appendix A: Statistical Tables 605
Degrees of Freedom ( ν)
t 13 14 15 16 17 18 19 20 21 22 23 24
0.0 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500
0.1 .461 .461 .461 .461 .461 .461 .461 .461 .461 .461 .461 .461
0.2 .422 .422 .422 .422 .422 .422 .422 .422 .422 .422 .422 .422
0.3 .384 .384 .384 .384 .384 .384 .384 .384 .384 .383 .383 .383
0.4 .348 .347 .347 .347 .347 .347 .347 .347 .347 .347 .346 .346
0.5 .313 .312 .312 .312 .312 .312 .311 .311 .311 .311 .311 .311
0.6 .279 .279 .279 .278 .278 .278 .278 .278 .278 .277 .277 .277
0.7 .248 .247 .247 .247 .247 .246 .246 .246 .246 .246 .245 .245
0.8 .219 .218 .218 .218 .217 .217 .217 .217 .216 .216 .216 .216
0.9 .192 .191 .191 .191 .190 .190 .190 .189 .189 .189 .189 .189
1.0 .168 .167 .167 .166 .166 .165 .165 .165 .164 .164 .164 .164
1.1 .146 .144 .144 .144 .143 .143 .143 .142 .142 .142 .141 .141
1.2 .126 .124 .124 .124 .123 .123 .122 .122 .122 .121 .121 .121
1.3 .108 .107 .107 .106 .105 .105 .105 .104 .104 .104 .103 .103
1.4 .092 .091 .091 .090 .090 .089 .089 .089 .088 .088 .087 .087
1.5 .079 .077 .077 .077 .076 .075 .075 .075 .074 .074 .074 .073
1.6 .067 .065 .065 .065 .064 .064 .063 .063 .062 .062 .062 .061
1.7 .056 .055 .055 .054 .054 .053 .053 .052 .052 .052 .051 .051
1.8 .048 .046 .046 .045 .045 .044 .044 .043 .043 .043 .042 .042
1.9 .040 .038 .038 .038 .037 .037 .036 .036 .036 .035 .035 .035
2.0 .033 .032 .032 .031 .031 .030 .030 .030 .029 .029 .029 .028
2.1 .028 .027 .027 .026 .025 .025 .025 .024 .024 .024 .023 .023
2.2 .023 .022 .022 .021 .021 .021 .020 .020 .020 .019 .019 .019
2.3 .019 .018 .018 .018 .017 .017 .016 .016 .016 .016 .015 .015
2.4 .016 .015 .015 .014 .014 .014 .013 .013 .013 .013 .012 .012
2.5 .013 .012 .012 .012 .011 .011 .011 .011 .010 .010 .010 .010
2.6 .011 .010 .010 .010 .009 .009 .009 .009 .008 .008 .008 .008
2.7 .009 .008 .008 .008 .008 .007 .007 .007 .007 .007 .006 .006
2.8 .008 .007 .007 .006 .006 .006 .006 .006 .005 .005 .005 .005
2.9 .006 .005 .005 .005 .005 .005 .005 .004 .004 .004 .004 .004
3.0 .005 .004 .004 .004 .004 .004 .004 .004 .003 .003 .003 .003
3.1 .004 .004 .004 .003 .003 .003 .003 .003 .003 .003 .003 .002
3.2 .003 .003 .003 .003 .003 .002 .002 .002 .002 .002 .002 .002
3.3 .003 .002 .002 .002 .002 .002 .002 .002 .002 .002 .002 .001
3.4 .002 .002 .002 .002 .002 .002 .002 .001 .001 .001 .001 .001
3.5 .002 .002 .002 .001 .001 .001 .001 .001 .001 .001 .001 .001
3.6 .002 .001 .001 .001 .001 .001 .001 .001 .001 .001 .001 .001
3.7 .001 .001 .001 .001 .001 .001 .001 .001 .001 .001 .001 .001
3.8 .001 .001 .001 .001 .001 .001 .001 .001 .001 .000 .000 .000
3.9 .001 .001 .001 .001 .001 .001 .000 .000 .000 .000 .000 .000
4.0 .001 .001 .001 .001 .000 .000 .000 .000 .000 .000 .000 .000606 Appendix A: Statistical Tables
t 25 26 27 28 29 30 35 40 60 120 1(¼z)
0.0 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500 .500
0.1 .461 .461 .461 .461 .461 .461 .460 .460 .460 .460 .460
0.2 .422 .422 .421 .421 .421 .421 .421 .421 .421 .421 .421
0.3 .383 .383 .383 .383 .383 .383 .383 .383 .383 .382 .382
0.4 .346 .346 .346 .346 .346 .346 .346 .346 .345 .345 .345
0.5 .311 .311 .311 .310 .310 .310 .310 .310 .309 .309 .309
0.6 .277 .277 .277 .277 .277 .277 .276 .276 .275 .275 .274
0.7 .245 .245 .245 .245 .245 .245 .244 .244 .243 .243 .242
0.8 .216 .215 .215 .215 .215 .215 .215 .214 .213 .213 .212
0.9 .188 .188 .188 .188 .188 .188 .187 .187 .186 .185 .184
1.0 .163 .163 .163 .163 .163 .163 .162 .162 .161 .160 .159
1.1 .141 .141 .141 .140 .140 .140 .139 .139 .138 .137 .136
1.2 .121 .120 .120 .120 .120 .120 .119 .119 .117 .116 .115
1.3 .103 .103 .102 .102 .102 .102 .101 .101 .099 .098 .097
1.4 .087 .087 .086 .086 .086 .086 .085 .085 .083 .082 .081
1.5 .073 .073 .073 .072 .072 .072 .071 .071 .069 .068 .067
1.6 .061 .061 .061 .060 .060 .060 .059 .059 .057 .056 .055
1.7 .051 .051 .050 .050 .050 .050 .049 .048 .047 .046 .045
1.8 .042 .042 .042 .041 .041 .041 .040 .040 .038 .037 .036
1.9 .035 .034 .034 .034 .034 .034 .033 .032 .031 .030 .029
2.0 .028 .028 .028 .028 .027 .027 .027 .026 .025 .024 .023
2.1 .023 .023 .023 .022 .022 .022 .022 .021 .020 .019 .018
2.2 .019 .018 .018 .018 .018 .018 .017 .017 .016 .015 .014
2.3 .015 .015 .015 .015 .014 .014 .014 .013 .012 .012 .011
2.4 .012 .012 .012 .012 .012 .011 .011 .011 .010 .009 .008
2.5 .010 .010 .009 .009 .009 .009 .009 .008 .008 .007 .006
2.6 .008 .008 .007 .007 .007 .007 .007 .007 .006 .005 .005
2.7 .006 .006 .006 .006 .006 .006 .005 .005 .004 .004 .003
2.8 .005 .005 .005 .005 .005 .004 .004 .004 .003 .003 .003
2.9 .004 .004 .004 .004 .004 .003 .003 .003 .003 .002 .002
3.0 .003 .003 .003 .003 .003 .003 .002 .002 .002 .002 .001
3.1 .002 .002 .002 .002 .002 .002 .002 .002 .001 .001 .001
3.2 .002 .002 .002 .002 .002 .002 .001 .001 .001 .001 .001
3.3 .001 .001 .001 .001 .001 .001 .001 .001 .001 .001 .000
3.4 .001 .001 .001 .001 .001 .001 .001 .001 .001 .000 .000
3.5 .001 .001 .001 .001 .001 .001 .001 .001 .000 .000 .000
3.6 .001 .001 .001 .001 .001 .001 .000 .000 .000 .000 .000
3.7 .001 .001 .000 .000 .000 .000 .000 .000 .000 .000 .000
3.8 .000 .000 .000 .000 .000 .000 .000 .000 .000 .000 .000
3.9 .000 .000 .000 .000 .000 .000 .000 .000 .000 .000 .000
4.0 .000 .000 .000 .000 .000 .000 .000 .000 .000 .000 .000Appendix A: Statistical Tables 607
Appendix B: Background Mathematics
B.1 Trigonometric Identities
cosaþbðÞ ¼ cosaðÞcosbðÞ /C0 sinaðÞsinbðÞ
cosa/C0bðÞ ¼ cosaðÞcosbðÞ þ sinaðÞsinbðÞ
sinaþbðÞ ¼ sinaðÞcosbðÞ þ cosaðÞsinbðÞ
sina/C0bðÞ ¼ sinaðÞcosbðÞ /C0 cosaðÞsinbðÞ
cosaðÞcosbðÞ ¼½cosaþbðÞ þ cosa/C0bðÞ ½/C138
sinaðÞsinbðÞ ¼½cosa/C0bðÞ /C0 cosaþbðÞ ½/C138
B.2 Special Engineering Functions
uxðÞ ¼1x/C210
0x<0/C26
0x1
rectxðÞ ¼1/C12/C12x/C12/C12/C200:5
0/C12/C12x/C12/C12>0:5/C26
−1 −0.5 0 0.5 1x1
(continued)
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6609
trixðÞ ¼1/C0/C12/C12x/C12/C12/C12/C12x/C12/C12/C201
0/C12/C12x/C12/C12>1/C26
1
−10 1x
sinc xðÞ¼sinπxðÞ
πxx6¼0
1 x¼08
<
:
−3 011
23x
−1 −2
B.3o(h) Notation
The symbol o(h) denotes any function of hwhich has the property that
lim
h!0ohðÞ
h¼0
Informally, this property says that the value of the function approaches 0 even faster than
happroaches 0.
For example, consider the function f(h)¼h3. Then f(h)/h¼h2, which does indeed approach 0 as
h!0. On the other hand, fhðÞ ¼ﬃﬃﬃ
hp
does not have the o(h) property, since fhðÞ=h¼1=ﬃﬃﬃ
hp
, which
approaches 1ash!0+. Likewise, sin( h) does not have the o(h) property: from calculus, sin( h)/
h!1a sh!0.
Note that the sum or difference of two functions that have this property also has this property:
o(h)/C6o(h)¼o(h). The two o(h) functions need not be the same as long as they both have the
property. Similarly, the product of two such functions also has this property: o(h)/C1o(h)¼o(h).
B.4 The Delta Function
TheDirac delta function ,δ(x), also called an impulse orimpulse function , is such that δ(x)¼0 for
x6¼0 and
ð1
/C01δxðÞdx¼1
More generally, an impulse at location x0with intensity aisa/C1δ(x/C0x0). An impulse is often
graphed as an arrow, with the intensity listed in parentheses, as in the accompanying ﬁgure. The
height of the arrow is meaningless; in fact, the “height” of an impulse is + 1.610 Appendix B: Background Mathematics
x0x(a)
Properties of the delta function:
Basic integral:ð1
/C01δxðÞdx¼1, soð1
/C01aδx/C0x0 ðÞ dx¼a
Antiderivative:ðx
/C01δtðÞdt¼uxðÞ
Rescaling: δcxðÞ ¼δxðÞ
cjjforc6¼0
Sifting:ð1
/C01gxðÞδx/C0x0 ðÞ dx¼gx0ðÞ
Convolution: g(x)★δ(x/C0x0)¼g(x/C0x0)
B.5 Fourier Transforms
TheFourier transform of a function g(t), denoted FgtðÞfg orG(f), is deﬁned by
GfðÞ¼ FgtðÞfg ¼ð1
/C01gtðÞe/C0j2πftdt
where j¼ﬃﬃﬃﬃﬃﬃ ﬃ
/C01p
. The Fourier transform of g(t) exists provided that the integral of g(t) is absolutely
convergent; i.e.,ð1
/C01/C12/C12gtðÞ/C12/C12dt<1.
Theinverse Fourier transform of a function G(f), denoted F/C01GfðÞfg org(t), is deﬁned by
gtðÞ¼F/C01GfðÞfg ¼ð1
/C01GfðÞeþj2πftdf
Properties of Fourier transforms:
Linearity: Fa1g1tðÞþ a2g2tðÞ fg ¼a1G1fðÞ þ a2G2fðÞ
Rescaling:Fga tðÞfg ¼1
ajjGf
a/C18/C19
Duality: FgtðÞfg ¼GfðÞ) FGtðÞfg ¼g/C0fðÞ
Time shift: Fgt/C0t0ðÞfg ¼ GfðÞe/C0j2πft0
Frequency shift: FgtðÞej2πf0t/C8/C9
¼Gf/C0f0 ðÞ
Time convolution: Fg1tðÞ★g2tðÞ fg ¼G1fðÞG2fðÞ
Frequency convolution: Fg1tðÞg2tðÞ fg ¼ G1fðÞ★G2fðÞAppendix B: Background Mathematics 611
Fourier transform pairs:
g(t) G(f)
1 δ(f)
u(t) 1
2δfðÞ þ1
j2πf
cos(2 πf0t) 1
2δf/C0f0 ðÞ þ δfþf0 ðÞ ½/C138
sin(2πf0t) 1
2jδf/C0f0 ðÞ /C0 δfþf0 ðÞ ½/C138
tke/C0atu(t),a>0,k¼0, 1, 2, ... k!
aþj2πf ðÞkþ1
e/C0a|t|,a>0 2a
a2þ2πfðÞ2
e/C0t2 ﬃﬃﬃπpe/C0π2f2
rect(t) sinc( f)
tri(t) sinc2(f)
B.6 Discrete-Time Fourier Transforms
Thediscrete-time Fourier transform (DTFT) of a function g[n] is deﬁned by
GFðÞ ¼X1
n¼/C01gn½/C138e/C0j2πFn
The DTFT of g[n] exists provided that g[n] is absolutely summable; i.e.,X1
n¼/C01gn½/C138jj <1.
Theinverse DTFT of a function G(F) is deﬁned by
gn½/C138 ¼ð1=2
/C01=2GFðÞeþj2πFndF
Properties of DTFTs: (an arrow indicates application of the DTFT)
Periodicity: G(F+m)¼G(F) for all integers m; i.e., G(F) has period 1
Linearity: a1g1[n]+a2g2[n]!a1G1(F)+a2G2(F)
Time shift: gn/C0n0 ½/C138 ! GFðÞe/C0j2πFn0
Frequency shift: gn½/C138ej2πF0n!GF/C0F0 ðÞ
Time convolution: g1[n]★g2[n]!G1(F)G2(F)
Frequency convolution:
g1n½/C138g2n½/C138 !ð1=2
/C01=2G1ϕðÞG2F/C0ϕðÞ dϕ(periodic convolution of G1andG2)612 Appendix B: Background Mathematics
DTFT pairs:
g[n] G(F)
1 δ(F)
δ[n]1
u[n] 1
2δFðÞ þ1
1/C0e/C0j2πF
cos(2 πF0n) 1
2δF/C0F0 ðÞ þ δFþF0 ðÞ ½/C138
sin(2πF0n) 1
2jδF/C0F0 ðÞ /C0 δFþF0 ðÞ ½/C138
α|n|,|α|<1 1/C0α2
1þα2/C02αcos 2 πFðÞ
αnu[n], |α|<1 1
1/C0αe/C0j2πFAppendix B: Background Mathematics 613
Appendix C: Important Probability
Distributions
C.1 Discrete Distributions
For discrete distributions, the speciﬁed pmf and cdf are valid on the range of the random variable. The
cdf and mgf are only provided when simple expressions exist for those functions.
Binomial (n,p) X~Bin(n,p)
range: {0, 1, ...,n}
parameters: n,n¼0, 1, 2, ...(number of trials)
p,0<p<1 (success probability)
pmf:bx;n;pðÞ ¼n
x/C18/C19
px1/C0pðÞn/C0x
cdf: B(x;n,p) (see Table A.1)
mean: np
variance: np(1/C0p)
mgf: (1 /C0p+p et)n
Note:T h e n¼1 case is called a Bernoulli distribution.
Geometric ( p)
range: {1, 2, 3, ...}
parameter: p,0<p<1 (success probability)
pmf: p(1/C0p)x/C01
cdf: 1 /C0(1/C0p)x
mean: 1
p
variance: 1/C0p
p2
mgf: pet
1/C01/C0pðÞ et
Note: Other sources deﬁned a geometric rv to be the number of failures preceding the ﬁrst success
in independent and identical trials. See Sect. 2.6for details.
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6615
Hypergeometric
(n, M, N )X~Hyp(n,M,N)
range: {max(0, n/C0N+M ),..., min( n,M)}
parameters: n,n¼0, 1, ...,N(number of trials)
M,M¼0, 1, ...,N(population number of
successes)
N,N¼1, 2, 3, ...(population size)
pmf:hx;n;M;N ðÞ ¼M
x/C18/C19
N/C0M
n/C0x/C18/C19
N
n/C18/C19
cdf: H(x;n, M, N )
mean:n/C1M
N
variance:n/C1M
N/C11/C0M
N/C18/C19
/C1N/C0n
N/C01
Note: With the understanding that/C16a
b/C17
¼0 for a<b, the range of the hypergeometric distribu-
tion can be simpliﬁed to {0, ...,n}.
Negative Binomial (r,p) X~NB(r,p)
range: { r,r+1 ,r+2 , ...}
parameters: r,r¼1, 2, ...(desired number of successes)
p,0<p<1 (success probability)
pmf:nb x ;n;pðÞ ¼x/C01
r/C01/C18/C19
pr1/C0pðÞx/C0r
mean:r
p
variance: r1/C0pðÞ
p2
mgf: pet
1/C01/C0pðÞ et/C20/C21r
Notes :T h e r¼1 case corresponds to the geometric distribution.
Other sources deﬁned a negative binomial rv to be the number of failures preceding the rth success
in independent and identical trials. See Sect. 2.6for details.
Poisson ( μ)
range: {0, 1, 2, ...}
parameter: μ,μ>0 (expected number of events)
pmf:px;μðÞ ¼e/C0μμx
x!
cdf: P(x;μ) (see Table A.2)
mean: μ
variance: μ
mgf: eμet/C01ðÞ616 Appendix C: Important Probability Distributions
C.2 Continuous Distributions
For continuous distributions, the speciﬁed pdf and cdf are valid on the range of the random variable.
The cdf and mgf are only provided when simple expressions exist for those functions.
Beta ( α,β,A ,B )
range: [ A,B]
parameters: α,α>0 (ﬁrst shape parameter)
β,β>0 (second shape parameter)
A,/C01 <A<B(lower bound)
B,A<B<1(upper bound)
pdf: 1
B/C0A/C1ΓαþβðÞ
ΓαðÞ /C1ΓβðÞx/C0A
B/C0A/C18/C19α/C01B/C0x
B/C0A/C18/C19β/C01
mean:AþB/C0AðÞ /C1α
αþβ
variance: B/C0AðÞ2αβ
αþβðÞ2αþβþ1 ðÞ
Notes :T h e A¼0,B¼1 case is called the standard beta distribution.
Theα¼1,β¼1 case in the uniform distribution.
Exponential ( λ)
range: (0, 1)
parameter: λ,λ>0 (rate parameter)
pdf: λe/C0λx
cdf: 1 /C0e/C0λx
mean: 1
λ
variance: 1
λ2
mgf: λ
λ/C0tt<λ
Note: A second parameter γ, called a threshold parameter , can be introduced to shift the density
curve away from x¼0. In that case, X/C0γhas an exponential distribution.
Gamma ( α,β)
range: (0, 1)
parameters: α,α>0 (shape parameter)
β,β>0 (scale parameter)
pdf: 1
ΓαðÞβαxα/C01e/C0x=β
cdf:Gx
β;α/C18/C19
(see Table A.4)
mean: αβ
variance: αβ2
mgf: 1
1/C0βt/C18/C19α
t<1/β
Notes :T h e α¼1,β¼1/λcase corresponds to the exponential distribution.
Theβ¼1 case is called the standard gamma distribution.
Theα¼n(an integer), β¼1/λcase is called the Erlang distribution.
A third parameter γ, called a threshold parameter , can be introduced to shift the density curve
away from x¼0. In that case, X/C0γhas the two-parameter gamma distribution described above.Appendix C: Important Probability Distributions 617
Lognormal ( μ,σ)
range: (0, 1)
parameters: μ,/C01 <μ<1(ﬁrst shape parameter)
σ,σ>0 (second shape parameter)
pdf: 1ﬃﬃﬃﬃﬃ
2πp
σxe/C0lnxðÞ /C0 μ ½/C1382=2σ2ðÞ
cdf:ΦlnxðÞ /C0 μ
σ/C20/C21
mean: eμþσ2=2
variance:e2μþσ2/C1eσ2/C01/C16/C17
Note: A third parameter γ, called a threshold parameter , can be introduced to shift the density
curve away from x¼0. In that case, X/C0γhas the two-parameter lognormal distribution described
above.
Normal (μ,σ)[or Gaussian (μ,σ)] X~N(μ,σ)
range: ( /C01,1)
parameters: μ,/C01 <μ<1(mean)
σ,σ>0 (standard deviation)
pdf:1
σﬃﬃﬃﬃﬃ
2πp e/C0x/C0μðÞ2=2σ2ðÞ
cdf: Φx/C0μ
σ/C16/C17
(see Table A.3)
mean: μ
variance: σ2
mgf: eμtþσ2t2=2
Note:T h e μ¼0,σ¼1 case is called the standard normal or zdistribution.
Uniform (A,B) X~Unif[A,B]
range: [ A, B]
parameters: A,/C01 <A<B(lower bound)
B,A<B<1(upper bound)
pdf: 1
B/C0A
cdf: x/C0A
B/C0A
mean: AþB
2
variance: B/C0AðÞ2
12
mgf: eBt/C0eAt
B/C0AðÞ tt6¼0
Note:T h e A¼0,B¼1 case is called the standard uniform distribution.
Weibull ( α,β)
range: (0, 1)
parameters: α,α>0 (shape parameter)
β,β>0 (scale parameter)
pdf:α
βαxα/C01e/C0x=βðÞα
cdf: 1/C0e/C0x=βðÞα
(continued)618 Appendix C: Important Probability Distributions
mean:β/C1Γ/C18
1þ1
α/C19
variance:
β2Γ1þ2
α/C18/C19
/C0Γ1þ1
α/C18/C19/C20/C212()
Note: A third parameter γ, called a threshold parameter , can be introduced to shift the density
curve away from x¼0. In that case, X/C0γhas the two-parameter Weibull distribution described
above.
C.3 Matlab and R Commands
Table C.1indicates the template for Matlab and R commands related to the “named” probability
distributions. In Table C.1,
x¼input to the pmf, pdf, or cdf
p¼left-tail probability (e.g., p¼.5 for the median, or .9 for the 90th percentile)
N¼simulation size; i.e., the length of the vector of random numbers
pars ¼the set of parameters, in the order prescribed
name ¼a text string specifying the particular distribution
Table C.2catalogs the names and parameters for a variety of distributions.
Table C.1 Matlab and R syntax for probability distribution commands
Matlab R
pmf/pdf namepdf(x,pars) dname(x,pars)
cdf namecdf(x,pars) pname(x,pars)
Quantile nameinv(p,pars) qname(p,pars)
Random #s namernd(pars,N,1) rname(N,pars)
Table C.2 Names and parameter sets for major distributions in Matlab and R
Matlab R
Distribution name pars name pars
Binomial bino n, p binom n, p
Geometricageo p geom p
Hypergeometric hyge N, M, n hyper M, N /C0M, n
Negative binomialanbin r, p nbinom r, p
Poisson poiss μ pois μ
Betabbeta α,β beta α,β
Exponential exp 1/λ exp λ
Gamma gam α,β gamma α,1/β
Lognormal logn μ,σ lnorm μ,σ
Normal norm μ,σ norm μ,σ
Uniform unif A, B unif A, B
Weibull wbl β,α weibull α,β
aThe geometric and negative binomial commands in Matlab and R assume that the random variable counts only failures,
and not the total number of trials. See Sect. 2.6or the software documentation for details.
bThe beta distribution commands in Matlab and R assume a standard beta distribution; i.e., with A¼0 and B¼1.Appendix C: Important Probability Distributions 619
Answers to Odd-Numbered Exercises
Chapter 1
1. (a) A\B0(b)A[B(c) (A\B0)[(B\A0)
3. (a) S¼{1324, 1342, 1423, 2314, 2341, 2413, 2431, 3124, 3142, 4123, 4132, 3214, 3241, 4213, 4231}
(b)A¼{1324, 1342, 1423, 1432}
(c)B¼{2314, 2341, 2413, 2431, 3214, 3241, 4213, 4231}
(d)A[B¼{1324, 1342, 1423, 1432, 2314, 2341, 2413, 2431, 3214, 3241, 4213, 4231}
A\B¼Ø
A0¼{2314, 2341, 2413, 2431, 3124, 3142, 4123, 4132, 3214, 3241, 4213, 4231}
5. (a) A¼{SSF,SFS,FSS}
(b)B¼{SSS,SSF,SFS,SSS}
(c)C¼{SSS,SSF,SFS}
(d)C0¼{SFF,FSS,FSF,FFS,FFF}
A[C¼{SSS,SSF,SFS,FSS}
A\C¼{SSF,SFS}
B[C¼{SSS,SSF,SFS,FSS}
B\C¼{SSS,SSF,SFS}
7. (a) {111, 112, 113, 121, 122, 123, 131, 132, 133, 211, 212, 213, 221, 222, 223, 231, 232, 233, 311, 312, 313,
321, 322, 323, 331, 332, 333}
(b) {111, 222, 333}
(c) {123, 132, 213, 231, 312, 321}
(d) {111, 113, 131, 133, 311, 313, 331, 333}
9. (a) S¼{BBBAAAA ,BBABAAA ,BBAABAA ,BBAAABA ,BBAAAAB ,BABBAAA ,BABABAA ,BABAABA ,
BABAAAB ,BAABBAA ,BAABABA ,BAABAAB ,BAAABBA ,BAAABAB ,BAAAABB ,ABBBAAA ,ABBABAA ,
ABBAABA ,ABBAAAB ,ABABBAA ,ABABABA ,ABABAAB ,ABAABBA ,ABAABAB ,ABAAABB ,AABBBAA ,
AABBABA ,AABBAAB ,AABABBA ,AABABAB ,AABAABB ,AAABBBA ,AAABBAB ,AAABABB ,AAAABBB }
(b) {AAAABBB ,AAABABB ,AAABBAB ,AABAABB ,AABABAB }
13. (a) .07 (b) .30 (c) .57
15. (a) They are awarded at least one of the ﬁrst two projects, .36
(b) They are awarded neither of the ﬁrst two projects, .64
(c) They are awarded at least one of the projects, .53
(d) They are awarded none of the projects, .47
(e) They are awarded only the third project, .17
(f) Either they fail to get the ﬁrst two or they are awarded the third, .75
17. (a) .572 (b) .879
(continued)
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6621
19. (a) SAS and SPSS are not the only packages
(b) .7 (c) .8 (d) .2
21. (a) .8841 (b) .0435
23. (a) .10 (b) .18, .19 (c) .41 (d) .59 (e) .31 (f) .69
25. (a) 1/15 (b) 6/15 (c) 14/15 (d) 8/15
27. (a) .85 (b) .15 (c) .22 (d) .35
29. (a) 1/9 (b) 8/9 (c) 2/9
31. (a) 10,000 (b) .9876 (c) .03 (d) .0337
33. (a) 336 (b) 593,775 (c) 83,160 (d) .140 (e) .002
35. (a) 240 (b) 12 (c) 108 (d) 132 (e) .55, .413
37. (a) .0775 (b) .0082
39. (a) 8008 (b) 3300 (c) 5236 (d) .4121, .6538
41. .2
43. (a) .2967 (b) .0747 (c) .2637 (d) .042
45. (a) 369,600 (b) .00006494
47. (a) 1/15 (b) 1/3 (c) 2/3
51. P(A|B)>P(B|A)
53. (a) .50 (b) .0833 (c) .3571 (d) .8333
55. (a) .05 (b) .12 (c) .56, .44 (d) .49, .25 (e) .533 (f) .444, .556
57. .04
59. (a) .50 (b) .0455 (c) .682 (d) .0189
65. (a) 3/4 (b) 2/3
67. (a) .067 (b) .509
71. (a) .765 (b) .235
73. .087, .652, .261
75. .00329
77. .4657 for airline #1, .2877 for airline #2, .2466 for airline #3
81. A2andA3are independent
83. .1936, .3816
85. .1052
87. .99999969, .226
89. .9981
91. (a) Yes (b) No
93. (a) .343 (b) .657 (c) .189 (d) .216 (e) .3525
95. (a) P(A)¼P(B)¼.02,P(A\B)¼.039984, AandBare not independent
(b) .04, very little difference
(c)P(A\B)¼.0222, not close; P(A\B) is close to P(A)P(B) when the sample size is very small relative to
the population size
97. (a) Route #1 (b) .216
99. (a) 1 /C0(1/C01/N)n
(b)n¼3: .4212, 1/2; n¼6: .6651, 1; n¼10: .8385, 10/6; the answers are not close
(c) .1052, 1/9 ¼.1111; much closer
101. (a) Exact answer ¼.46 (b) se /C25.005
103. .8186 (answers will vary)
105. /C25.39,/C25.88 (answers will vary)
107. /C25.91 (answers will vary)
109. /C25.02 (answers will vary)
111. (b) /C25.37 (answers will vary) (c) /C25176,000,000 (answers will vary; exact ¼176,214,841)
113. (a) /C25.20 (b) /C25.56 (answers will vary)
115. (a) /C25.5177 (b) /C25.4914 (answers will vary)
(continued)622 Answers to Odd-Numbered Exercises
117. /C25.2 (answers will vary)
119. (b)π/C254/C1^PAðÞ(numerical answers will vary)
121. (a) 1140 (b) 969 (c) 1020 (d) .85
123. (a) .0762 (b) .143
125. (a) .512 (b) .608 (c) .7835
127. .1074
129. (a) 1014(b) 7.3719 /C210/C09
131. (a) .974 (b) .9754
133. .926
135. (a) .018 (b) .601
137. .156
139. (a) .0625 (b) .15625 (c) .34375 (d) .014
141. (a) .12, .88 (b) .18, .38
143. 1/4 ¼P(A1\A2\A3)6¼P(A1)/C1P(A2)/C1P(A3)¼1/8
145. (a) a0¼0,a5¼1 (b) a2¼(1/2)a1+ (1/2) a3(c)ai¼i/5 for i¼0, 1, 2, 3, 4, 5
149. (a) .6923 (b) .52
Chapter 2
1. x¼0 for FFF;x¼1 for SFF, FSF, andFFS;x¼2 for SSF, SFS, andFSS;x¼3 for SSS
3. Z¼average of the two numbers, with possible values 2/2, 3/2, ..., 12/2; W¼absolute value of the
difference, with possible values 0, 1, 2, 3, 4, 5
5. No. In Example 2.4, let Y¼1 if at most three batteries are examined and let Y¼0 otherwise. Then Yhas
only two values
7. (a) {0, 1, 2 ..., 12}; discrete (c) {1, 2, 3, ...}; discrete (e) {0, c,2c,..., 10000 c} where cis the royalty per
book; discrete (g) { x:m/C20x/C20M} where mandMare the minimum and maximum possible tension;
continuous
9. (a) {2, 4, 6, 8, ...}, that is, {2(1), 2(2), 2(3), 2(4), ...}, an inﬁnite sequence; discrete
11. (a) .10 (c) .45, .25
13. (a) .70 (b) .45 (c) .55 (d) .71 (e) .65 (f) .45
15. (a) (1,2), (1,3), (1,4), (1,5), (2,3), (2,4), (2,5), (3,4), (3,5), (4,5) (b) p(0)¼.3,p(1)¼.6,p(2)¼.1
(c)F(x)¼0 for x<0,¼.3 for 0 /C20x<1,¼.9 for 1 /C20x<2, and ¼1 for x/C212
17. (a) .81 (b) .162 (c) it is A;AUUUA, UAUUA, UUAUA, UUUAA ; .00324
19. p(0)¼.09,p(1)¼.40,p(2)¼.32,p(3)¼.19
21. (b) p(x)¼.301, .176, .125, .097, .079, .067, .058, .051, .046 for x¼1, 2, ...,9
(c)F(x)¼0 for x<1,¼.301 for 1 /C20x<2,¼.477 for 2 /C20x<3,...,¼.954 for 8 /C20x<9, and ¼1
forx/C219
(d) .602, .301
23. (a) .20 (b) .33 (c) .78 (d) .53
25. (a) p(y)¼(1/C0p)y/C1pfory¼0, 1, 2, 3, ...
27. (a) 1234, 1243, 1324, ..., 4321
(b)p(0)¼9/24, p(1)¼8/24, p(2)¼6/24, p(3)¼0,p(4)¼1/24
29. (a) 6.45 GB (b) 15.6475 (c) 3.96 GB (d) 15.6475
31. 4.49, 2.12, .68
33. (a) p(b)p(1/C0p) (c) p
35. E[h3(X)]¼$4.93, E[h4(X)]¼$5.33, so 4 copies is better
37. E(X)¼(n+ 1)/2, E(X2)¼(n+ 1)(2 n+ 1)/6, Var( X)¼(n2/C01)/12
39. (b) .61 (c) .47 (d) $2598 (e) $4064
(continued)Answers to Odd-Numbered Exercises 623
41. (a) μ¼/C0$2/38 for both methods (c) single number: σ¼$5.76; square: σ¼$2.76
45. E(X/C0c)¼E(X)/C0c,E(X/C0μ)¼0
47. (a) .25, .11, .06, .04, .01 (b) μ¼2.64, σ¼1.54; for k¼2, the probability is .04, and the bound of .25 is
much too conservative; for k¼3, 4, 5, 10, the probability is 0, and the bounds are again conservative
(c)μ¼$0,σ¼$d, 0 (d) 1/9, same as the Chebyshev bound (e) there are many, e.g., p(1)¼p(/C01)¼.02
andp(0)¼.96
49. (a) Yes, n¼10,p¼1/6 (b) Yes, n¼40,p¼1/4 (c) No (d) No (e) No (f) Yes, assuming the population is
very large; n¼15,p¼P(a randomly selected apple weighs >150 g)
51. (a) .515 (b) .218 (c) .011 (d) .480 (e) .965 (f) .000 (g) .595
53. (a) .354 (b) .115 (c) .918
55. (a) 5 (b) 1.94 (c) .017
57. (a) .403 (b) .787 (c) .774
59. .1478
61. .407, independence
63. (a) .010368 (c) the probability decreases, to .001970 (d) 1500, 259.2
65. (a) .017 (b) .811, .425 (c) .006, .902, .586
67. When p¼.9, the probability is .99 for Aand .9963 for B.I fp¼.5, the probabilities are .75 and .6875,
respectively
69. (a) 20, 16 (b) 70, 21
71. (a) p¼0 or 1 (b) p¼.5
73. P(|X/C0μ|/C212σ)¼.042 when p¼.5 and ¼.065 when p¼.75, compared to the upper bound of .25.
Using k¼3 in place of k¼2, these probabilities are .002 and .004, respectively, whereas the upper bound
is .11
75. (a) .932 (b) .065 (c) .068 (d) .492 (e) .251
77. (a) .011 (b) .441 (c) .554, .459 (d) .945
79. Poisson(5) (a) .492 (b) .133
81. .271, .857
83. (a) 2.9565, .948 (b) .726
85. (a) .122, .809, .283 (b) 12, 3.464 (c) .530, .011
87. (a) .221 (b) 6,800,000 (c) p(x; 20.106)
89. (a) 1/(1 /C0e/C0θ) (b) θ¼2; .981 (c) 1.26
91. (a) .114 (b) .879 (c) .121 (d) Use the binomial distribution with n¼15,p¼.10
93. (a) h(x; 15, 10, 20) for x¼5,..., 10 (b) .0325 (c) .697
95. (a) h(x; 10, 10, 20) (b) .033 (c) h(x;n, n, 2n)
97. (a) .2817 (b) .7513 (c) .4912, .9123
99. (a) nb(x; 2, .5) (b) .188 (c) .688 (d) 2, 4
101. nb(x; 6, .5), 6
103. nb(x; 5, 6/36), 30, 12.2
105. (a) 160, 21.9 (b) .6756
107. (a) .01 e9t+.05e10t+.16e11t+.78e12t(b)E(X)¼11.71, SD( X)¼0.605
109. MX(t)¼et/(2/C0et),E(X)¼2, SD XðÞ ¼ﬃﬃﬃ
2p
111. Skewness ¼/C02.20 (Ex. 107), +0.54 (Ex. 108), +2.12 (Ex. 109), 0 (Ex. 110)
113. E(X)¼0, Var( X)¼2
115. p(y)¼(.25)y/C01(.75) for y¼1, 2, 3, ...
117. MYtðÞ¼ et2=2,E(Y)¼0, Var( Y)¼1
121. E(X)¼5, Var( X)¼4
123. Mn/C0X(t)¼(p+( 1/C0p)et)n
125. MY(t)¼pr[1/C0(1/C0p)et]/C0r,E(Y)¼r(1/C0p)/p; Var( Y)¼r(1/C0p)/p2
129. mean /C250.5968, sd /C250.8548 (answers will vary)
(continued)624 Answers to Odd-Numbered Exercises
131. /C25.9090 (answers will vary)
133. (a) μ/C2513.5888, σ/C252.9381 (b) /C25.1562 (answers will vary)
135. mean /C253.4152, variance /C255.97 (answers will vary)
137. (b) 142 tickets
139. (a) /C25.2291 (b) /C25$8696 (c) /C25$7811 (d) /C25.2342, /C25$7,767, /C25$7,571 (answers will vary)
141. (b) probability /C25.9196, conﬁdence interval ¼(.9143, .9249) (answers will vary)
143. (b) 3.114, .405, .636
145. (a) b(x; 15, .75) (b) .686 (c) .313 (d) 11.25, 2.81 (e) .310
147. (a) .013 (b) 19 (c) .266 (d) Poisson with μ¼500
149. (a) p(x; 2.5) (b) .067 (c) .109
151. 1.813, 3.05
153. p(2)¼p2,p(3)¼(1/C0p)p2,p(4)¼(1/C0p)p2,p(x)¼[1/C0p(2)/C0.../C0p(x/C03)](1 /C0p)p2forx¼5,
6, 7, ...; .99950841
155. (a) .0029 (b) .0767, .9702
157. (a) .135 (b) .00144 (c)P1
x¼0[p(x; 2)]5
159. 3.590
161. (a) No (b) .0273
163. (b) .5 μ1+. 5 μ2(c) .25( μ1/C0μ2)2+ .5( μ1+μ2) (d) .6 and .4 replace .5 and .5, respectively
165. μ¼.5
167. 500p+ 750, 100ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1/C0pðÞp
169. (a) 2.50 (b) 3.1
Chapter 3
1. (b) .4625; the same (c) .5, .278125
3. (b) .5 (c) .6875 (d) .6328
5. (a) k¼3/8 (b) .125 (c) .296875 (d) .578125
7. (a) f(x)¼1/4.05 for .20 /C20x/C204.25 (b) .3086 (c) .4938 (d) 1/4.05
9. (a) .562 (b) .438, .438 (c) .071
11. (a) .25 (b) .1875 (c) .4375 (d) 1.414 h (e) f(x)¼x/2 for 0 /C20x<2
13. (a) k¼3 (b) F(x)¼1/C01/x3forx/C211 and ¼0 otherwise (c) .125, .088
15. (a) F(x)¼x3/8 for 0 /C20x/C202,¼0 for x<0,¼1 for x>2 (b) .015625 (c) .0137, .0137 (d) 1.817 min
17. (a) .597 (b) .369 (c) f(x)¼[ln(4) /C0ln(x)]/4 for 0 <x<4
19. (a) 1.333 h (b) .471 h (c) $2
21. (a) .8182 ft3(b) .3137
23. (a) A+(B/C0A)p(b) (A+B )/2 (c) ( Bn+1/C0An+1)/[(n+ 1)( B/C0A)]
25. 314.79 m2
27. 248/C14F, 3.6/C14F
29. 1/4 min, 1/4 min
31. (c) μR/C25v/20,σR/C25v/800 (d) ~100 π(e) ~80 π2
33. g(x)¼10x/C05,MY(t)¼(e5t/C0e/C05t)/10t,Y~ Unif[ /C05, 5]
35. (a) MX(t)¼.15e.5t/(.15/C0t),μ¼7.167, variance ¼44.444 (b) .15/(.15 /C0t),μ¼6.67,
variance ¼44.444 (c) MY(t)¼.15/(.15 /C0t)
39. (a) .4850 (b) .3413 (c) .4938 (d) .9876 (e) .9147 (f) .9599 (g) .9104 (h) .0791 (i) .0668 (j) .9876
41. (a) 1.34 (b) /C01.34 (c) .675 (d) /C0.675 (e) /C01.555
43. (a) .9664 (b) .2451 (c) .8664
45. (a) .4584 (b) 135.8 kph (c) .9265 (d) .3173 (e) .6844
47. (a) .9236 (b) .0021 (c) .1336
(continued)Answers to Odd-Numbered Exercises 625
49. .6826 <.9987 )the second machine
51. (a) .2514, ~0 (b) ~39.985 ksi
53. σ¼.0510
55. (a) .8664 (b) .0124 (c) .2718
57. (a) .7938 (b) 5.88 (c) 7.938 (d) .2651
59. (a) Φ(1.72) /C0Φ(.55) (b) Φ(.55)/C0[1/C0Φ(1.72)]
61. (a) .7286 (b) .8643, .8159
63. (a) .9932 (b) .9875 (c) .8064
65. (a) .0392 (b) ~1
69. (a) .1587 (b) .0013 (c) .999937 (d) .00000029
71. (a) 1 (b) 1 (c) .982 (d) .129
73. (a) .1481 (b) .0183
75. (a) 120 (b) 3 =4ðÞﬃﬃﬃπp(c) .371 (d) .735 (e) 0
77. (a) .424 (b) .567; median <24 (c) 60 weeks (d) 66 weeks
79. ηp¼/C0ln(1/C0p)/λ,η¼.693/ λ
81. (a) .5488 (b) .3119 (c) 7.667 s (d) 6.667 s
85. (a) .8257, .8257, .0636 (b) .6637 (c) 172.727 h
89. (a) .9295 (b) .2974 (c) 98.184 ksi
91. (a) μ¼9.164, σ¼.38525 (b) .8790 (c) .4247 (d) no
93. η¼eμ¼9547 kg/day/km
95. (a) 3.96, 1.99 months (b) .0375 (c) .7016 (d) 7.77 months (e) 13.75 months (f) 4.522
97. α¼β
99. (b) Γ(α+β)Γ(m+β)/[Γ(α+m+β)Γ(β)],β/(α+β)
101. Yes, since the pattern in the plot is quite linear
103. Yes
105. Yes
107. Plot ln( x) versus zpercentile. The pattern is somewhat straight, so a lognormal distribution is plausible
109. It is plausible that strength is normally distributed, because the pattern is reasonably linear
111. There is substantial curvature in the plot. λis a scale parameter (as is σfor the normal family)
113. fY(y)¼2/y3fory>1
115. fYyðÞ ¼ ye/C0y2=2fory>0
117. fY(y)¼1/16 for 0 <y<16
119. fY(y)¼1/[π(1 + y2)] for /C01 <y<1
121. Y¼g(X)¼X2/16
123. fYyðÞ ¼ 1=2ﬃﬃ ﬃyp/C2/C3
for 0 <y/C201
125.
fYyðÞ ¼1=4ﬃﬃ ﬃyp/C0/C1
0<y/C201
1=8ﬃﬃ ﬃyp/C0/C1
1<y/C209
0 otherwise8
><
>:
129. (a)FxðÞ ¼ x2=4,F/C01uðÞ ¼ 2ﬃﬃﬃup(c)μ¼1.333, σ¼0.4714, /C22xandswill vary
131. The inverse cdf is F/C01uðÞ ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1þ48up/C01/C2/C3
=3
133. (a) The inverse cdf is F/C01(u)¼τ/C1[1/C0(1/C0u)1/θ] (b) E(X)¼16, /C22xwill vary
135. (a)c¼1.5 (c) 15,000 (d) μ¼3/8, /C22xwill vary (e) ^PM <:1 ðÞ ¼ :8760 (answers will vary)
137. (a)x¼G/C01(u)¼/C0ln(1/C0u) (b)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2e=πp
/C251:3155 (c) ~13,155
141. (a) .4 (b) .6 (c) F(x)¼x/25 for 0 /C20x/C2025,¼0 for x<0,¼1 for x>25 (d) 12.5, 7.22
143. (b) F(x)¼1/C016/(x+4 )2forx>0,¼0 for x/C200 (c) .247 (d) 4 years (e) 16.67
145. (a) .6568 (b) 41.56 V (c) .3197
147. (a) .0003 (exact: .00086) (b) .0888 (exact: .0963)
(continued)626 Answers to Odd-Numbered Exercises
149. (a) 68.03 dB, 122.09 dB (b) .3204 (c) .7642, because the lognormal distribution is not symmetric
151. (a) F(x)¼1.5(1 /C01/x) for 1 /C20x/C203,¼0 for x<1,¼1 for x>3 (b) .9, .4 (c) 1.648 s (d) .553 s (e) .267 s
153. (a) 1.075, 1.075 (b) .0614, .333 (c) 2.476 mm
155. (b) $95,600, .3300
157. (b) F(x)¼.5e2xforx<0,¼1/C0.5e/C02xforx/C210 (c) .5, .665, .256, .670
159. (a) k¼(α/C01)5α/C01,α>1 (b) F(x)¼1/C0(5/x)α/C01forx/C215 (c) 5( α/C01)/(α/C02),α>2
161. (b) .4602, .3636 (c) .5950 (d) 140.178 MPa
163. (a) Weibull, with α¼2 and β¼ﬃﬃﬃ
2p
σ(b) .542
165. .5062
171. (a) 710, 84.423, .684 (b) .376
Chapter 4
1. (a) .20 (b) .42 (c) .70 (d) pX(x)¼.16, .34, .50 for x¼0, 1, 2; pY(y)¼.24, .38, .38 for y¼0, 1, 2; .50 (e) no
3. (a) .15 (b) .40 (c) .22 (d) .17, .46 (e) p1(x1)¼.19, .30, .25, .14, .12 for x1¼0, 1, 2, 3, 4 (f) p2(x2)¼.19, .30,
.28, .23 for x2¼0, 1, 2, 3 (g) no
5. (a) .0305 (b) .1829 (c) .1073
7. (a) .054 (b) .00018
9. (a) .030 (b) .120 (c) .300 (d) .380 (e) no
11. (a) k¼3/380,000 (b) .3024 (c) .3593 (d) fX(x)¼10kx2+ .05 for 20 /C20x/C2030 (e) no
13.(a)px;yðÞ ¼e/C0μ1/C0μ2μx
1μy
2
x!y!(b)e/C0μ1/C0μ21þμ1þμ2 ½/C138 (c)e/C0μ1/C0μ2
m!μ1þμ2 ðÞm
15. (a) f(x, y)¼e/C0x/C0yforx, y/C210 (b) .400 (c) .594 (d) .330
17. (a) F(y)¼(1/C0e/C0λy)+( 1 /C0e/C0λy)2/C0(1/C0e/C0λy)3fory>0,f(y)¼4λe/C02λy/C03λe/C03λyfory>0 (b) 2/3 λ
19.(a) .25 (b) 1/ π(c) 2/ π(d)fXxðÞ ¼2ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
r2/C0x2p
πr2for/C0r/C20x/C20r,fY(y)¼fX(y), no
21. 1/3
23. (a) .11 (b) pX(x)¼.78, .12, .07, .03 for x¼0, 1, 2, 3; pY(y)¼.77, .14, .09 for y¼0, 1, 2 (c) no (d) 0.35,
0.32 (e) 95.72
25. .15
27. L2
29. .25 h, or 15 min
31. /C02/3
33. (a) /C03.20 (b) /C0.207
35. (a) .238 (b) .51
37. (a) Var( h(X,Y))¼ÐÐ
[h(x,y)]2/C1f(x,y)dA/C0[ÐÐ
h(x,y)/C1f(x,y)dA]2(b) 13.34
43. (a) 87,850, 4370.37 (b) mean yes, variance no (c) .0027
45. .2877, .3686
47. .0314
49. (a) 45 min (b) 68.33 (c) /C01 min, 13.67 (d) /C05 min, 68.33
51. (a) 50, 10.308 (b) .0075 (c) 50 (d) 111.5625 (e) 131.25
53. (a) .9616 (b) .0623
55. (a) E(Yi)¼1/2,E(W)¼n(n+ 1)/4 (b) Var( Yi)¼1/4, Var( W)¼n(n+ 1)(2 n+ 1)/24
57. 10:52.76 a.m.
59. (a) mean ¼0, sd¼ﬃﬃﬃ
2p
61. (a) X~ Bin(10, 18/38) (b) Y~ Bin(15, 18/38) (c) X+Y ~ Bin(25, 18/38) (f) no
65. (a) α¼2,β¼1/λ(c) gamma, α¼n,β¼1/λ
67. (a) .5102 (b) .000000117
(continued)Answers to Odd-Numbered Exercises 627
69. (a) x2/2,x4/12 (b) f(x,y)¼1/x2for 0 <y<x2<1 (c) fYyðÞ ¼ 1=ﬃﬃ ﬃyp/C01 for 0 <y<1
71. (a) pX(x)¼1/10 for x¼0, 1, ...,9 ;p(y|x)¼1/9 for y¼0,..., 9 and y6¼x;p(x, y)¼1/90 for x, y¼0,
1,..., 9 and y6¼x(b) 5/C0x/9
73. (a) fX(x)¼2x,0<x<1 (b) f(y|x)¼1/x,0<y<x(c) .6 (d) no (e) x/2 (f) x2/12
75. (a)px;yðÞ ¼2!
x!y!2/C0x/C0y ðÞ !:3ðÞx:2ðÞy:5ðÞ2/C0x/C0y(b)X~ Bin(2, .3), Y~ Bin(2, .2) (c) YjX¼x~ Bin(2 /C0x, .2/.7)
(d) no (e) (4 /C02x)/7 (f) 10(2 /C0x)/49
77. (a) x/2,x2/12 (b) f(x, y)¼1/xfor 0 <y<x<1 (c) fY(y)¼/C0ln(y) for 0 <y<1
79. (a) .6 x, .24x(b) 60 (c) 60
81. 176 lbs, 12.68 lbs
83. (a) 1 + 4 p,4p(1/C0p) (b) $2598, 16,158,196 (c) 2598(1 + 4 p),ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
16518196 þ93071200 p/C026998416 p2p
(d) $2598 and $4064 for p¼0; $7794 and $7504 for p¼.5; $12,990 and $9088 for p¼1
85. (a) 12 cm, .01 cm (b) 12 cm, .005 cm (c) the larger sample
87. (a) .9772, .4772 (b) 10
89. 43.29 h
91. .9332
93. (a) .8357 (b) no
95. (a) .1894 (b) .1894 (c) 621.5 gallons
97. (a) .0968 (b) .8882
99. .9616
101. 1=/C22X
103.(a)fy1;y2ðÞ ¼1
4πe/C0y2
1þy2
2ðÞ =4(b)fY1y1ðÞ ¼1ﬃﬃﬃﬃ
4πpe/C0y2
1=4(c) yes
105. (a) y2for 0/C20y/C201 and y(2/C0y) for 1 <y/C202 (b) 2(1 /C0w) for 0 /C20w/C201
107. 4 y3[ln(y3)]2for 0 <y3<1
111. (a) N(984, 197.45) (b) .1379 (c) 1237
113. (a) N(158, 8.72) (b) N(170, 8.72) (c) .4090
115. (a) .8875 x+ 5.2125 (b) 111.5775 (c) 10.563 (d) .0951
117. (a) 2 x/C010 (b) 9 (c) 3 (d) .0228
119. (a) .1410 (b) .1165
121. (a)RtðÞ¼ e/C0t2(b) .1054 (c) 2 t(d) 0.886 thousand hours
123. (a) R(t)¼1/C0.125t3for 0/C20t/C202,¼0 for t>2 (b) 3 t2/(8/C0t3) (c) undeﬁned
125.
(a) parallel (b) RtðÞ¼ 1/C01/C0e/C0t2/C16/C174
(c)htðÞ¼8te/C0t21/C0e/C0t2/C0/C13
1/C01/C0e/C0t2ðÞ4
127. (a) [1 /C0(1/C0R1(t))(1/C0R2(t))][1/C0(1/C0R3(t))(1/C0R4(t))][1/C0(1/C0R5(t))(1/C0R6(t))] (b) 70 h
129.(a)RtðÞ¼ e/C0αt/C0t2=2β½/C138ðÞfort/C20β,¼e/C0αβ/2fort>β(b)ftðÞ¼ α1/C0t
β/C16/C17
e/C0αt/C0t2=2β½/C138ðÞ
133. (a) 5 y4/105for 0 <y<10, 8.33 min (b) 6.67 min (c) 5 min (d) 1.409 min
135. (a) .0238 (b) $2,025
137. n!Γiþ1=θ ðÞ
i/C01ðÞ !Γnþ1=θþ1 ðÞ,n!Γiþ2=θ ðÞ
i/C01ðÞ !Γnþ2=θþ1 ðÞ/C0n!Γiþ1=θ ðÞ
i/C01ðÞ !Γnþ1=θþ1 ðÞ/C20/C212
139. E(Yk+1)¼η
143. (b) ^PX/C201,Y/C201 ðÞ ¼:4154 (answers will vary), exact ¼.42 (c) mean /C250.4866, sd /C250.6438 (answers
will vary)
145. (b) 60,000 (c) 7.0873, 1.0180 (answers will vary) (d) .2080 (answers will vary)
147. (a) fX(x)¼12x(1/C0x2) for 0 /C20x/C201,f(y|x)¼2y/(1/C0x)2for 0/C20y/C201/C0x(c) we expect 16/9
candidates per accepted value, rather than 6
149. (a) pX(100) ¼.5 and pX(250) ¼.5 (b) p(y|100) ¼.4, .2, .4 for y¼0, 100, 200; p(y|250) ¼.1, .3, .6 for
y¼0, 100, 200
151. (a)N(μ1,σ1),N(μ2+ρσ2/σ1[(x/C0μ1)],σ2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1/C0ρ2p
)
153. (b) ^μ¼196:6193 h, standard error ¼1.045 h (answers will vary) (c) .9554, .0021 (answers will vary)
(continued)628 Answers to Odd-Numbered Exercises
155. fT(t)¼e/C0t/2/C0e/C0tfort>0
157. (a) k¼3/81,250 (b) fX(x)¼k(250x/C010x2) for 0 /C20x/C2020,¼k(450x/C030x2+. 5x3) for 20 /C20x/C2030;
fY(y)¼fX(y); not independent (c) .355 (d) 25.969 lb (e) /C032.19, /C0.894 (f) 7.66
159. t¼E(X+Y )¼1.167
163. (c) p¼1, because μ<1;p¼2/3<1, because μ>1
165. (a) F(b, d)/C0F(a, d)/C0F(b, c)+F(a, c)
(b)F(10,6) /C0F(4,6)/C0F(10,1) + F(4,1); F(b, d)/C0F(a/C01,d)/C0F(b, c/C01) + F(a/C01,c/C01)
(c) At each ( x*,y*),F(x*,y*) is the sum of the probabilities at points ( x,y) such that x/C20x* and y/C20y*.
The table of F(x,y) values is
x
100 250
200 :50 1
y100 :30 :50
0 :20 :25
(d)Fx;yðÞ ¼ :6x2yþ:4xy3,0/C20x/C201;0/C20y/C201;Fx;yðÞ ¼ 0,x/C200;
Fx;yðÞ ¼ 0,y/C200;
Fx;yðÞ ¼ :6x2þ:4x,0/C20x/C201,y>1;
Fx;yðÞ ¼ :6yþ:4y3,x>1, 0/C20y/C201;
Fx;yðÞ ¼ 1,x>1,y>1
P:25/C20x/C20:75,:25/C20y/C20:75 ðÞ ¼ :23125
(e)Fx;yðÞ ¼ 6x2y2,xþy/C201, 0/C20x/C201;0/C20y/C201,x/C210,y/C210
Fx;yðÞ ¼ 3x4/C08x3þ6x2þ3y4/C08y3þ6y2/C01,xþy>1,x/C201,y/C201
Fx;yðÞ ¼ 0,x/C200;Fx;yðÞ ¼ 0,y/C200;
Fx;yðÞ ¼ 3x4/C08x3þ6x2,0/C20x/C201,y>1
Fx;yðÞ ¼ 3y4/C08y3þ6y2,0/C20y/C201,x>1
Fx;yðÞ ¼ 1,x>1,y>1
167. (a) 2 x,x(b) 40 (c) 100
169. Undeﬁned, /C250
171. 2
1/C01000 t ðÞ 2/C01000 t ðÞ, 1500 h
173. Not valid for 75th percentile, but valid for 50th percentile;
sum of percentiles ¼(μ1+zσ1)+(μ2+zσ2)¼μ1+μ2+z(σ1+σ2),
percentile of sums ¼μ1þμ2 ðÞ þ zﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
1þσ2
2p
175. (a) 2360, 73.7 (b) .9713
177. .9686
179. .9099
181. .8340
183.(a)σ2
W
σ2
Wþσ2
E(b) .9999
185. 26, 1.64
187. (a) g(y1,yn)¼n(n/C01)[F(yn)/C0F(y1)]n/C02f(y1)f(yn) for y1<yn
(b)f(w1,w2)¼n(n/C01)[F(w1+w2)/C0F(w1)]n/C02f(w1)f(w1+w2),
fW2w2ðÞ ¼ nn/C01ðÞð1
/C01Fw 1þw2 ðÞ /C0 Fw 1ðÞ ½/C138n/C02fw 1ðÞfw 1þw2 ðÞ dw1
(c)n(n/C01)w2n/C02(1/C0w2) for 0 /C20w2/C201
191. (a) 10/9 (b) 10/8 (c) 1 + Y2+...+Y10, 29.29 boxes (d) 11.2 boxes
Chapter 5
1. (a) /C22x¼113:73 (b) ex¼113 (c) s¼12.74 (d) .9091 (e) s=/C22x¼11:2
3. (a) /C22x¼1:3481 (b) /C22x¼1:3481 (c) /C22xþ1:28s¼1:7814 (d) .6736
(continued)Answers to Odd-Numbered Exercises 629
5. ^θ1¼N/C22X¼1, 703, 000, ^θ2¼τ/C0N/C22D¼1, 591, 300, ^θ3¼τ/C1/C22X
/C22Y¼1, 601, 438 :281
7. (a) 120.6 (b) 1,206,000 (c) .80 (d) 120
9. (a) /C22X;/C22x¼2:11 (b)ﬃﬃﬃμp=ﬃﬃﬃnp, .119
11. (b) nλ/(n/C01) (c) n2λ2/(n/C01)2(n/C02)
13. (b)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p1q1=n1þp2q2=n2p
(c) with ^p1¼x1=n1and ^p2¼x2=n2,ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
^p1^q1=n1þ^p2^q2=n2p
(d)/C0.245 (e) .041
17. (a) ∑Xi2/2n(b) 74.505
19. (b) .444
21. (a) ^p¼2Y=n/C0:3; .2 (c) (10/7) Y/n/C09/70
23. (a)ﬃﬃnppþ1=2ﬃﬃnpþ1,p1/C0pðÞﬃﬃnpþ1 ðÞ2,1
4ﬃﬃnpþ1 ðÞ2; the MSE does not depend on p(b) when pis near .5, the MSE from part (a) is
smaller; when pis near 0 or 1, the usual estimator has lower MSE
25. (a) ^p¼x=n¼:15 (b) yes (c) .4437
27. /C22x,/C22y,/C22x/C0/C22y
29. ^p¼r=x¼:15, yes
31.(a) ^θ¼X
X2
i=2n¼74:505, yes (b) ^η¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ
/C02l n :5ðÞ ^θq
¼10:163
33.
(a) ^θ¼n
/C0Σln 1/C0xi=τ ðÞ(b)θ/C01ðÞXn
i¼1xi
τ/C0xi¼n, subject to τ>max( xi)
35. ^λ¼n Xn
i¼1Yi=tiðÞ
37. (a) 2.228 (b) 2.131 (c) 2.947 (d) 4.604 (e) 2.492 (f) ~2.715
39. (a) A normal probability plot of these 20 values is quite linear. (b) (23.79, 26.31) (c) yes
41. (a) (357.38, 384.01) (b) narrower
43. (a) Based on a normal probability plot, it is reasonable to assume the sample observations came from a
normal distribution. (b) (430.51, 446.08); 440 is plausible, 450 is not
45. Interval (c)
47. 26.14
49. (c) (12.10, 31.70)
51. (a) yes (b) no (c) no (d) yes (e) no (f) yes
53. Using Ha:μ<100 results in the welds being believed in conformance unless proved otherwise, so the
burden of proof is on the nonconformance claim
55. (a) reject H0(b) reject H0(c) don’t reject H0(d) reject H0(e) don’t reject H0
57. (a) .040 (b) .018 (c) .130 (d) .653 (e) <.005 (f) ~.000
59. (a) .0778 (b) .1841 (c) .0250 (d) .0066 (e) .5438
61. (a) H0:μ¼10 versus Ha:μ<10 (b) reject H0(c) don’t reject H0(d) reject H0
63. (a) no; no, because n¼49 (b) H0:μ¼1.0 versus Ha:μ<1.0,z¼/C05.79, reject H0, yes
65. H0:μ¼200 versus Ha:μ>200, t¼1.19 at ll df, P-value ¼.128, do not reject H0
67. H0:μ¼3 versus Ha:μ6¼3,t¼/C01.759, P-value ¼.082, reject H0atα¼.10 but not at α¼.05
69. H0:μ¼360 versus Ha:μ>360, t¼2.24 at 25 df, P-value ¼.018, reject H0, yes
71. H0:μ¼15 versus Ha:μ<15,z¼/C06.17, P-value /C250, reject H0, yes
73. H0:σ¼.05 versus Ha:σ<.05. Type I error: Conclude that the standard deviation is <.05 mm when it is
really equal to .05 mm. Type II error: Conclude that the standard deviation is .05 mm when it is really <.05
75. Type I: saying that the plant is not in compliance when in fact it is. Type II: conclude that the plant is in
compliance when in fact it isn’t
77. (.224, .278)
79. (.496, .631)
81. (.225, .275)
83. (b) 342 (c) 385
85. H0:p¼.15 versus Ha:p>.15,z¼0.69, P-value ¼.2451, fail to reject H0
(continued)630 Answers to Odd-Numbered Exercises
87. (a) H0:p¼.25 versus Ha:p<.25,z¼/C01.01, P-value ¼.1562, fail to reject H0: the winery should switch
to screw tops (b) Type I: conclude that less than 25% of all customers ﬁnd screw tops acceptable, when the
true percentage is 25%. Type II: fail to recognize that less than 25% of all customers ﬁnd screw tops
acceptable when that’s actually true. Type II
89. (a) H0:p¼.2 versus Ha:p>.2,z¼1.27, P-value ¼.1020, fail to reject H0(b) Type I: conclude that more
than 20% of the population of female workers is obese, when the true percentage is 20%. Type II: fail to
recognize that more than 20% of the population of female workers is obese when that’s actually true
91. H0:p¼.1,Ha:p>.1,z¼0.74, P-value /C25.23, fail to reject H0
93. H0:p¼.1 versus Ha:p>.1,z¼1.33, P-value ¼.0918, fail to reject H0; Type II
95. H0:p¼.25 versus Ha:p<.25,z¼/C06.09, P-value /C250, reject H0
97. (a) H0:p¼.2 versus Ha:p>.2,z¼0.97, P-value ¼.166, fail to reject H0, so no modiﬁcation appears
necessary (b) .9974
99. (a) Gamma(9, 5/3) (b) Gamma(145, 5/53) (c) (11.54, 15.99)
101. B(490, 455), the same posterior distribution found in the example
103. Gamma( α+Σxi, 1/(n+1 /β))
105. Beta( α+x,β+n/C0x)
107. n/∑kxk¼.0436
109. No:E^σ2ðÞ ¼ σ2=2
111. (a) expected payoff ¼0 (b) ^θ¼Σxiþ2y
Σxiþ2n
113. (a) The pattern of points in a normal probability plot (not shown) is reasonably linear, so, yes, normality is
plausible. (b) (33.53, 43.79)
115. (.1295, .2986)
117. (a) A normal probability plot lends support to the assumption that pulmonary compliance is normally
distributed. (b) (196.88, 222.62)
119. (a) (.539, .581) (b) 2401
121.(a)N(0, 1) (b)x1x2/C61:96ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x2
1þx2
2/C01:96ðÞ2p
x2
2/C01:96ðÞ2 provided x12+x22/C21(1.96)2
123. (a) 90.25% (b) at least 90% (c) at least 100(1 /C0kα)%
125. (a)H0:μ¼2150 versus Ha:μ>2150 (b) t¼ /C22x/C02150ðÞ =s=ﬃﬃﬃnpðÞ (c) 1.33 (d) .107 (e) fail to reject H0
127. H0:μ¼29.0 versus Ha:μ>29.0, t¼.7742, P-value ¼.232, fail to reject H0
129. H0:μ¼9.75 versus Ha:μ>9.75, t¼4.75, P-value /C250. The condition is not met.
131. H0:μ¼1.75 versus Ha:μ6¼1.75, t¼1.70, P-value ¼.102, do not reject H0; the data does not contradict
prior research
133. H0:p¼.75 versus Ha:p<.75,z¼/C03.28, P-value ¼.0005, reject H0
135. (a) H0:p/C20.02 versus Ha:p>.02; with X~ Bin(200, .02), P-value ¼P(X/C2117)¼7.5/C210/C07; reject H0
here and conclude that the NIST benchmark is notsatisﬁed (b) .2133
137. H0:μ¼4 versus Ha:μ>4,z¼1.33, P-value ¼.0918 >.02, fail to reject H0
Chapter 6
1. {cooperative, competitive}; with 1 ¼cooperative and 2 ¼competitive, p11¼.6,p12¼.4,p22¼.7,
p21¼.3
3. (a) {full, part, broken} (b) with 1 ¼ﬁll, 2 ¼part, 3 ¼broken, p11¼.7,p12¼.2,p13¼.1,p21¼0,
p22¼.6,p23¼.4,p31¼.8,p32¼0,p33¼.2
5. (a) X1¼2 with prob. pand¼0 with prob. 1 /C0p(b) 0, 2, 4
(c)PX nþ1¼2y/C12/C12Xn¼x/C0/C1
¼x
y/C18/C19
py1/C0pðÞx/C0yfory¼0, 1, ...,x
(continued)Answers to Odd-Numbered Exercises 631
7. (a) A son’s social status, given his father’s social status, has the same probability distribution as his social
status conditional on all family history; no
(b) The probabilities of social status changes (e.g., poor to middle class) are the same in every generation; no
9. (a) no (b) deﬁne a state space by pairs; probabilities from each pair into the next state
11.(a):90 :10
:11 :89/C20/C21
(b) .8210, .5460 (c) .8031, .5006
13. (a) Willow City: P(S!S)¼.988 >.776 (b) .9776, .9685 (c) .9529
15.(a):6:4
:3:7/C20/C21
(b) .52 (c) .524 (d) .606
17. (a) .2740, .7747 (b) .0380 (c) 2.1, 2.2
19.
(a):1439 :2790 :2704 :1747 :1320
:2201 :3332 :2522 :1272 :0674
:1481 :2829 :2701 :1719 :1269
:0874 :2129 :2596 :2109 :2292
:0319 :1099 :1893 :2174 :45162
666643
77775(b) .0730 (c) .1719
21. (a) .0608, .0646, .0658 (b) .0523, .0664, .0709, .0725 (c) they increase to .2710, .1320, .0926, .0798
23. (a) .525 (b) .4372
25.(a):96 :04
:05 :95/C20/C21
(b) .778 0’s, .222 1’s (c) .7081 0’s, .2919 1’s
27. (a) π¼[.80 .20] (b) P(X1¼G)¼.816, P(X1¼S)¼.184 (c) .8541
29. (a) π¼[0 1] (b) P(cooperative) ¼.3,P(competitive) ¼.7 (c) .39, .61
31. (a) no (b) yes
33. (a) (.3681, .2153, .4167) (b) .4167 (c) 2.72
35.
(a):7:2:1
0:6:4
:80 :22
643
75(b)P2has all nonzero entries (c) (8/15, 4/15, 1/5)
(d) 8/15 (e) 5
39. (a) π0¼β/(α+β),π1¼α/(α+β) (b) α¼β¼0)the chain is constant; α¼β¼1)the chain
alternates perfectly; α¼0,β¼1)the chain is always 0; α¼1,β¼0)the chain is always 1; α¼0,
0<β<1)the chain eventually gets stuck at 0; 0 <α<1,β¼0)the chain eventually gets stuck at 1;
0<α<1 and β¼1o rα¼1 and 0 <β<1)the chain is regular, and the answers to (a) still hold
41.
(a)00
01
10
111/C0αðÞ2α1/C0αðÞ α1/C0αðÞ α2
β1/C0αðÞ 1/C0αðÞ/C0
1/C0β/C1
αβ α 1/C0βðÞ
β1/C0αðÞ αβ 1/C0αðÞ/C0
1/C0β/C1
α1/C0βðÞ
β2β1/C0βðÞ β1/C0βðÞ 1/C0βðÞ22
666643
77775
(b)β2
αþβðÞ2,αβ
αþβðÞ2,αβ
αþβðÞ2,α2
αþβðÞ2(c)α2
αþβðÞ2
45.
(a):25 :75 0 0
0:25 :75 0
00 :25 :75
00012
666643
77775(b) .4219, .7383, .8965 (c) 4 (d) 1; no
47. (a) states 4 and 5
(b)k 1 2 3456789 1 0
P(T1£ k) 0 .46 .7108 .8302 .9089 .9474 .9713 .9837 .9910 .9949
(c)k 1 2 3456789 1 0
P(T1= k) 0 .46 .2508 .1194 .0787 .0385 .0239 .0124 .0073 .0039
μ/C253.1457
(d) 3.2084 (e) .3814, .6186
(continued)632 Answers to Odd-Numbered Exercises
49.
(a):5:5000
:50 :500
:500 :50
:5000 :5
000012
666643
77775, 4 is an absorbing state
(b)P(T0/C20k)¼0 for k¼1, 2, 3; the probabilities for k¼4,..., 15 are .0625, .0938, .1250, .1563, .1875,
.2168, .2451, .2725, .2988, .3242, .3487, .3723
(c) .2451
(d)P(T0¼k)¼0 for k¼1, 2, 3; the probabilities for k¼4,..., 15 are .0625, .03125, .03125, .03125,
.03125, .0293, .0283, .0273, .0264, .0254, .0245, .0236; μ/C253.2531,
σ/C253.9897 (e) 30
51. μcoop¼4.44, μcomp¼3.89; cooperative
53.
(a)0
1
2
3
4100 0 0
1/C0p 0 p 00
01 /C0p 0 p0
00 1 /C0p0p
000 0 12
666643
77775
(b) for x0¼$1, $2, $3:2p2þ1
2p2/C02pþ1,2
2p2/C02pþ1,2p2/C04pþ3
2p2/C02pþ1
(c) for x0¼$1, $2, $3:p3
2p2/C02pþ1,p2
2p2/C02pþ1,p3/C0p2þp
2p2/C02pþ1
55. 3.4825 generations
59. (c) (2069,0, 2079.8) (d) (.5993, .6185) (answers will vary)
61. (a) P(Xn+1¼10 |Xn¼x)¼.4,P(Xn+1¼2x|Xn¼x)¼.6 (b) mean /C25$47.2 billion, sd /C25$2.07 trillion
(c) ($6.53 billion, $87.7 billion) (d) ($618.32 million, $627.90 million); easier
63. (a) ($5586.60, $5632.3) (b) ($6695.50, $6773.80) (answers will vary)
65. (b) .9224 (answers will vary) (c) (6.89, 7.11) (answers will vary)
67.
(a)0:500 0 :5
:50 :50 0 0
0:50 :500
00 :50 :50
000 :50 :5
:50 0 0 :502
66666643
7777775(b) no (c) π¼1
61
61
61
61
61
6/C2/C3
(d) 6 (e) 9
69.
(a)010000
1
301
31
300
01
301
31
30
01
21
2000
001
2001
2
0000102
6666666666643
777777777775(b) all entries of P6are positive
(c) 1/12, 1/4, 1/4, 1/6, 1/6, 1/12 (d) 1/4 (e) 12
71.
(a)0
1
2
3
4
5000001
:3:70 0 0 0
0:3:700 0
00 :3:70 0
000 :3:70
0000 :3:72
66666643
7777775(b) .0566, .1887, .1887, .1887, .1887, .1887
(c) 17.67 weeks (including the one week of shipping)
73. (a) 2 seasons (b) .3613 (c) 15 seasons (d) 6.25 seasons
75. (a) p1¼[0.3168 0.1812 0.2761 0.1413 0.0846];
p2¼[0.3035 0.1266 0.2880 0.1643 0.1176];
p3¼[0.2908 0.0918 0.2770 0.1843 0.1561]
(b) 35.7 years, 11.9 years, 9.2 years, 4.3 years
(c) 16.6 years
(continued)Answers to Odd-Numbered Exercises 633
77.
(a)0
1
2
3
pd
tbr0 1 0000
00 :959 0 :041 0
00 0 :987 :013 0
00 0 0 :804 :196
0 0 0010
0 0 00012
66666643
7777775(c) 3.9055 weeks (d) .8145
(e) payments are always at least 1 week late; most payments are made at the end of 3 weeks
79.(a)P1¼P3¼:98 :02
:02 :98/C20/C21
,P2¼:97 :03
:03 :97/C20/C21
,P4¼P5¼:99 :01
:01 :99/C20/C21
(b) .916
81. (a) [3259 22,533 19,469 26,066 81,227 16,701 1511 211,486 171,820 56,916]
(b) [2683 24,119 21,980 27,015 86,100 15,117 1518 223,783 149,277 59,395];
[2261 25,213 24,221 27,526 89,397 13,926 1524 233,533 131,752 61,636];
/C044%, +24%, +46%, +12%, +20%, /C026%, +1.3%, +19%, +34%, +13.4%
(c) [920 23,202 51,593 21,697 78,402 8988 1445 266,505 65,073 93,160]
Chapter 7
1. (a) Continuous-time, continuous-space (b) continuous-time, discrete-space (c) discrete-time, continuous-
space (d) discrete-time, discrete-space
7. (b) No: at time t¼.25,x0(.25)¼/C0cos(π/2)¼0 and x1(.25)¼cos(π/2)¼0
(c)X(0)¼/C01 with probability .8 and +1 with probability .2; X(.5)¼+1 with probability .8 and /C01 with
probability .2
9. (a) discrete-space (c) Xn~ Bin( n, 18/38)
11. (a) 0 (b) 1/2
13. CXX(t,s)¼Var(A)cos( ω0t+θ0)cos( ω0s+θ0),RXX(t,s)¼v02+v0E[A][cos( ω0t+θ0) + cos
(ω0s+θ0)] +E[A2]cos( ω0t+θ0)cos( ω0s+θ0)
15. (b) N(s)>0, because covariance >0 (c) ρ¼e/C02(d) Gaussian, mean 0, variance 1.73
19. (a) μS(t)+μN(t) (b) RSS(t,s)+μS(t)μN(s)+μN(t)μS(s)+RNN(t,s)
(c)CSS(t,s)+CNN(t,s) (d) σ2
StðÞþ σ2
NtðÞ
23. (a) (1/2)sin( ω0(s/C0t)) (b) not orthogonal, not uncorrelated, not independent
25. (a) μV(b)E[V2]+(A02/2)cos( ω0τ) (c) yes
27. (a) yes (b) no (c) yes (d) yes
29. no
31. μA+μB,CAA(τ)+CAB(τ)+CBA(τ)+CBB(τ), yes
33. (a) yes, because its autocovariance has periodic components (b) /C042 (c) 50cos(100 πτ) + 8cos(600 πτ)+4 9
(d) 107 (e) 58
35. yes: both the time average and ensemble average are 0
37. CXX(τ)/CXX(0)
41. (a) .0062 (b) 75 þ25 sin2π
36559/C0150 ðÞ/C0/C1
(c) 16 δ[n/C0m] (d) no, and it shouldn’t be
43. (a) 18 n/38, 360 n/1444, 360 min( m, n)/1444, (360 min( m, n) + 324 mn)/1444
(b)/C010n/38, 36,000 n/1444, 36,000 min( m, n)/1444, (36,000 min( n, m) + 100 mn)/1444
(c) .3141
47. (a)μX(b)1
42CXXm/C0n½/C138 þ CXXm/C0nþ1 ½/C138 þ CXXm/C0n/C01 ½/C138 ðÞ (c) yes
(d) (CXX[0] + CXX[1])/2
49. (c)μ
1/C0α(d)αm/C0nσ2
1/C0α2(e) yes (f) αk
53. (a) .0993 (b) .1353 (c) 2
55. (a) .0516 (b) 1 /C0∑x¼075e/C05050x/x! (c) .9179 (d) 6 s (e) .8679
57. k/λ
59.(a) .0911 (b)e3t/C0sðÞ3t/C0sðÞ½/C138n/C0m
n/C0mðÞ !
61. e/C0pλt
63. fY(y)¼2λe/C0λy(1/C0e/C0λy) for y>0
67. (a) .0492 (b) .00255
(continued)634 Answers to Odd-Numbered Exercises
71. pmf: N(t)¼0 or 1 with probability 1/2 each for all t; mean ¼.5, variance ¼.25,CNN(τ)¼.25e/C02λ|τ|,
RNN(τ)¼.25 + .25 e/C02λ|τ|
73. (a) .0038 (b) .9535
75. (a) yes (b) .3174 (c) .3174 (d) .4778
77. (a)EX tðÞ½/C138 ¼ 80þ20 cosπ
12t/C015ðÞ/C0/C1
, Var( X(t))¼.2t(b) .1251 (c) .3372 (d) .1818
79. (a) .3078 (b) .1074
81. (a) .1171 (b) .6376 (c) .0181, .7410
83. (a) yes (b) E[X(t)]¼0,RXX(t, s)¼(N0/2)min( t, s), no
87. (a) 0 ¼empty, 1 ¼a person in stage 1, and 2 ¼a person in stage 2; q0¼λ,q1¼λ1,q2¼λ2;
q02¼q21¼q10¼0;q01¼λ,q12¼λ1,q20¼λ2(b)π¼(6/11, 2/11, 3/11)
(c)π¼(6/11, 3/11, 2/11) (d) π¼(1/7, 2/7, 4/7)
89. (a) q0¼λ,q1¼λ1,q2¼λ2;q02¼q10¼0;q01¼λ,q12¼λ1,q20¼.8λ2,q21¼.2λ2
(b)π¼(24/49, 10/49, 15/49) (c) π¼(24/49, 15/49, 10/49) (d) π¼(2/17, 5/17, 10/17)
(e) 1.25(1/ λ1+1 /λ2)
91. qi¼iβ,qi,i+1¼iβfori¼1,...,N/C01
93. qi,i+1¼λfori/C210,qi,i/C01¼iβfori/C211,qi¼λ+iβfori/C211
95.π00¼α1β1
Σ,π01¼α1β0
Σ,π10¼α0β1
Σ,π11¼α0β0
Σ, where Σ¼α1β1+α1β0+α0β1+α0β0;
1/C0π11¼1/C0α0β0
α1β1þα1β0þα0β1þα0β0
99. (a) 0 (b) CXX(t, s)¼1 if ﬂoor( t)¼ﬂoor( s),¼0 otherwise
101.
(a) 0, (1/3)cos( ωkτ) (b) 0,1
3Xn
k¼1cosωkτðÞ , yes
103.
(a) 0 (b)1
2Xn
k¼1cosωkτðÞ /C1 pk(c) yes
105. (a) Sndenotes the total lifetime of the machine through its use of the ﬁrst nrotors.
(b)μS[n]¼125n;σ2
S[n]¼15, 625 n;CSS[n, m]¼15,625 min( n, m);RSS[n, m]¼15,625[min( n, m)+nm]
(c) .5040
107. Yes
109. (a) e/C0λt(b)e/C0λt(1 + λt) (c) e/C0λε
111.(a)10 1/C0e/C0αt0 ðÞ
αt0(b)2λ
α1/C0e/C0αt0ðÞ
Chapter 8
1. F{RXX(τ)}¼sinc( f), which is not /C210 for all f
3.(a) 250 δfðÞ þﬃﬃπp
2exp/C0π2f2
4/C2106/C16/C17
(b) 240.37 W (c) 593.17 W
5. (a) 112,838 W (b) 108,839 W (c) RXXτðÞ¼200,000ﬃﬃπpexp/C01012τ2/C0/C1
7. (a) N0B(b)N0Bsinc(2 Bτ)
9.(a)A02e/C02λ|τ|(b)2λA2
0
λ2þ2πfðÞ2(c)A02(d)2A2
0
πarctan 2 πðÞ
11. (a) 100(1 + e/C01)/C25136.8 W (b)200
1þ2πfðÞ21þcos 2 πfðÞ ½/C138 (c) 126.34 W
13. μW(t)¼0,RWW(τ)¼2RXX(τ)/C0RXX(τ/C0d)/C0RXX(τ+d),SWW(f)¼2SXX(f)[1/C0cos(2 πfd)]
15. (a) Yes (b) Yes (c) SZZ(f)¼SXX(f)+SYY(f)
17. (b) SZZ(f)¼SXX(f)★SYY(f)
19. No, because PN¼1
21. (a) SXX(f)¼E[A2]SYY(f) (b) Scc
XX(f)¼E[A2]SYY(f)/C0(E[A]μY)2δ(f) (c) Yes; our “engineering
interpretation” of the elements of a psd are not valid for non-ergodic processes
23. (a) 2400sin(120,000 τ) (b) 2400 W (c) 40/(40 + j2πf) (d) 32/(1600 + (2 πf)2) for | f|/C2060 kHz (e) 0.399997 W
(continued)Answers to Odd-Numbered Exercises 635
25. (a) 0 (b) (1 /C0e/C0j2πf)/(j2πf) (c) ( N0/2)sinc2(f) (d) N0/2
27. (a) 100 δfðÞ þ50
1þ2πfðÞ2(b) 125 W (c)1
4þj2πf ðÞ2,1
16þ2πfðÞ2ðÞ2
(d) 100 δfðÞ þ50
1þ2πfðÞ2hi
/C11
16þ2πfðÞ2ðÞ2(e) 0.461 W
29. (a) (N0/2)e/C02α|f|(b)2N0α
4α2þ4π2τ2(c)N0/2α
33. (a) 2N0π2f2rect( f/2B) (b)N0
πτ32π2B2τ2sin 2 πBτðÞ þ 2πBτcos 2 πBτðÞ /C0 sin 2 πBτðÞ/C2/C3
(c) 4N0π2B3/3
35. (a) RXX(τ)/C0RXX(τ)★h(τ)/C0RXX(τ)★h(/C0τ)+RXX(τ)★h(τ)★h(/C0τ)
(b)SXX(f)|1/C0H(f)|2
37.(a) 1.17 MW (b) 250, 000 δfðÞ þ 60, 000 δf/C035, 000ðÞ þ δfþ35, 000ðÞ ½/C138 þ 8rectf
100,000/C16/C17
(c) same as part
(b) (d) 1.17 MW (e) 5000 W (f) 3000 W (g) SNR in¼234, SNR out¼390
41. 1/C0α2
1þα2/C02αcos 2 πFðÞ
43. 1/C0e/C020λ
1þe/C020λ/C02e/C010λcos 2 πFðÞ
45.1/C0π2
8þπ2
4tri 2FðÞ
47. (b) Psinc(k/2)
49.
(a)Yn¼(Xn/C0M+1+...+Xn)/M(b)1/C0e/C0j2πFM
M1/C0e/C0j2πF ðÞ(c)σ2M/C0/C12/C12k/C12/C12
M2for |k|¼0, 1, ...,M/C01 and zero
otherwise636 Answers to Odd-Numbered Exercises
References
Ambardar, Ashok, Analog and Digital Signal Processing (2nd ed.), Brooks/Cole Publishing, Paciﬁc Grove, CA, 1999.
A thorough treatment of the mathematics of signals and systems, including both discrete- and continuous-time
structures.
Bury, Karl, Statistical Distributions in Engineering , Cambridge University Press, Cambridge, England, 1999. A
readable and informative survey of distributions and their properties.
Crawley, Michael, The R Book (2nd ed.), Wiley, Hoboken, NJ, 2012. At more than 1000 pages, carrying it may give you
lower back pain, but it obviously contains a great deal of information about the R software.
Gorroochurn, Prakash, Classic Problems of Probability , Wiley, Hoboken, NJ, 2012. An entertaining excursion through
33 famous probability problems.
Davis, Timothy A., Matlab Primer (8th ed.), CRC Press, Boca Raton, FL, 2010. A good reference for basic Matlab
syntax, along with extensive catalogs of Matlab commands.
DeGroot, Morris and Mark Schervish, Probability and Statistics (4th ed.), Addison-Wesley, Upper Saddle River, NJ,
2012. Contains a nice exposition of subjective probability and an introduction to Bayesian methods of inference.
Devore, Jay and Ken Berk, Modern Mathematical Statistics with Applications (2nd ed.), Springer, New York, 2011. A
comprehensive text on statistical methodology designed for upper-level students.
Durrett, Richard, Elementary Probability for Applications , Cambridge Univ. Press, London, England, 2009. A very
brief (254 pp.) introduction that still ﬁnds room for some interesting examples.
Johnson, Norman, Samuel Kotz, and Adrienne Kemp, Univariate Discrete Distributions (3rd ed.), Wiley-Interscience,
New York, 2005. An encyclopedia of information on discrete distributions.
Johnson, Norman, Samuel Kotz, and N. Balakrishnan, Continuous Univariate Distributions , vols. 1–2, Wiley,
New York, 1993. These two volumes together present an exhaustive survey of various continuous distributions.
Law, Averill, Simulation Modeling and Analysis (4th ed.), McGraw-Hill, New York, 2006. An accessible and
comprehensive guide to many aspects of simulation.
Meys, Joris, and Andrie de Vries, R for Dummies, For Dummies (Wiley), New York, 2012. Need we say more?
Mosteller, Frederick, Robert Rourke, and George Thomas, Probability with Statistical Applications (2nd ed.), Addison-
Wesley, Reading, MA, 1970. A very good precalculus introduction to probability, with many entertaining examples;
especially good on counting rules and their application.
Nelson, Wayne, Applied Life Data Analysis , Wiley, New York, 1982. Gives a comprehensive discussion of distributions
and methods that are used in the analysis of lifetime data.
Olofsson, Peter, Probabilities :The Little Numbers That Rule Our Lives , Wiley, Hoboken, NJ, 2007. A very
non-technical and thoroughly charming introduction to the quantitative assessment of uncertainty.
Peebles, Peyton, Probability, Random Variables, and Random Signal Principles (4th ed.), McGraw-Hill, New York,
2001. Provides a short introduction to probability and distributions, then moves quickly into signal processing with
an emphasis on practical considerations. Includes some Matlab code.
Ross, Sheldon, Introduction to Probability Models (9th ed.), Academic Press, San Diego, CA, 2006. A good source of
material on the Poisson process and generalizations, Markov chains, and other topics in applied probability.
Ross, Sheldon, Simulation (5th ed.), Academic Press, San Diego, CA, 2012. A tight presentation of modern simulation
techniques and applications.
Taylor, Howard M. and Samuel Karlin, An Introduction to Stochastic Modeling (3rd ed.), Academic Press, San Diego,
CA, 1999. More sophisticated than our book, but with a wealth of information about discrete and continuous time
Markov chains, Poisson processes, Brownian motion, and queueing systems.
Winkler, Robert, Introduction to Bayesian Inference and Decision (2nd ed.), Probabilistic Publishing, Sugar Land,
Texas, 2003. A very good introduction to subjective probability.
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6637
Index
A
Accept–reject method, 224–227, 334
Ambardar, Ashok, 576, 583
Autocorrelation/autocovariance functions, 499–502
B
Band-limited white noise, 571
Bandpass ﬁlter, 581
Bandstop ﬁlter, 581
Bayesian inference
comments on, 413–414
conjugation, 414
credibility interval, 413
posterior distribution
inferences from, 413
of parameter, 410–412
prior distribution, 409–410
Bayes’ theorem, 35
Berk, Ken, 293, 381, 405, 414
Bernoulli random sequence, 518
Bernoulli random variable, 68
Beta distributions, 201–202
Binomial CDF, 597–601
Binomial distribution, 95–97
approximating, 180–182
computing, 99–101
mean and variance of, 101–102
negative, 117–120
poisson distribution comparing with, 108
random variable, 97–99
with software, 102
standard error, 102
Bivariate normal distribution, 309–311
conditional distributions, 311
multivariate normal distribution, 312
regression to mean, 312
simulating, 336–338
with software, 313
Brownian motion process, 493, 536–538
with drift, 541
geometric, 541
as limit, 538
properties of, 538–540
variations, 540C
Carlton, Matthew, xxiv
CDF SeeCumulative distribution function (CDF)
Central limit theorem (CLT), 293–297
applications of, 297–298
deﬁnition, 293
Chambers, John, 212
Chapman–Kolmogorov Equations, 431–436
Chebyshev, Pafnuty, 89–90
Chebyshev’s inequality, 89–90
CISeeConﬁdence interval (CI)
CLT SeeCentral limit theorem (CLT)
Combinations, 22–25
Conditional distributions, 277–279
bivariate normal distribution, 311
and independence, 279–280
Conditional expectation, 277–281
Conditional mean, 280
Conditional probability, 29–30
Bayes’ theorem, 35–36
deﬁnition, 30–32
density/mass function, 277
Law of Total Probability, 34–35
Markov chains, 427
multiplication rule, 32–34
Conﬁdence interval (CI)
bootstrap conﬁdence interval, 381
large-sample for μ, 380–381
forμwith conﬁdence level, 379
for normal population mean, 376–380
for population mean, 375–376
population proportion, 401–405
score, 401
software for calculation, 381–382
statistical inference, 351
tdistributions
family, 377
properties, 377
Continuous distribution, 619
beta distributions, 201–202
lognormal distributions, 199–201
mean value, 162
percentiles of, 156–157
variance, 164
#Springer International Publishing AG 2017
M.A. Carlton, J.L. Devore, Probability with Applications in Engineering, Science, and Technology ,
Springer Texts in Statistics, DOI 10.1007/978-3-319-52401-6639
Continuous distribution ( cont. )
Weibull distributions, 196–199
Continuous random variables, 221
accept–reject method, 224–226
built-in simulation packages for Matlab and R, 227
inverse CDF method, 221–224
PDF for, 148–152
precision of simulation results, 227
Correlation, 255–259
vs.causation, 262
coefﬁcient, 260
Counting processes, 603
Covariance, 255–259
matrix, 313
Crawley, Michael J., xviii
Credibility interval, 413
Cross-correlation/covariance function, 502
Cumulative distribution function (CDF), 75–79, 152–154
binomial, 597–602
inverse method, 133–134, 221–224
PDF and, 147–148
poisson, 602–603
probability density functions and, 147–148
standard normal, 603–604
step function, 76
Cuthbert, Daniel, 211
D
Davis, Timothy A., xviii
De Morgan, Augustus, 5
DeGroot, Morris, 486
Delta function, 610–611
De Morgan’s laws, 5
Devore, Jay L., xxiv, 293, 377, 381, 405, 414, 482
Diaconis, Persi, xxiv
Dirac delta function, 610
Discrete distributions, 615–616
Discrete random variable, 147
probability distributions for, 71–74
simulation of, 131–134
Discrete sequences, 518–519
Discrete-time Fourier transform (DTFT), 612–613
Discrete-time random processes, 516–519
Discrete-time signal processing
discrete-time Fourier transform, 612
random sequences
and LTI systems, 591–593
and sampling, 593–594
E
Engineering functions, 609–610
Error(s)
estimated standard, 357
in hypothesis testing, 392–395
simulation of random events, 55
standard, 101
of mean, 69, 291
point estimation, 357
Type I, 393–395Type II, 393–395
Estimated standard error, 357
Estimators, 354–356
Event(s)
complement, 4
compound, 3
deﬁnition, 3
De Morgan’s laws, 4
dependent, 43
disjoint, 5
independent, 43
intersection, 4
mutually exclusive, 5
probability of, 7
relations from set theory, 4–5
relative frequency, 10
simple, 3
simulation, 50
estimated/standard error, 55
precision, 55–56
RNG, 51–55
union, 4
venn diagrams, 6
Expected value, 83–85, 162–166, 255–256
of function, 86–87
linearity of, 87
properties, 256–257
Experiment
deﬁnition, 1
sample space of, 2–3
Exponential distribution, 187–190
and gamma distribution, 524–527
F
Fermat, Pierre de, xvii
Finite population correction factor, 117
Fisher, R.A., 366
Fourier transform, 611–612
discrete-time, 589, 612–613
G
Galton, Francis, 311
Gamma distribution, 190–192
calculations with software, 193
exponential and, 524–526
incomplete, 191
MGF, 193
standard, 191
Gates, Bill, 352
Gaussian/normal distribution, 172
binomial distribution, approximating, 183
calculations with software, 182
and discrete populations, 179–180
non-standardized, 175–178
normal MGF, 178–180
standard, 173–175
Gaussian processes, 535–536
Gaussian white noise, 570
Geometric distributions, 75640 Index
negative, 117––120
Geometric random variable, 119
Gosset, William Sealy, 378
Gosset’s theorem, 378
H
Highpass ﬁlter, 581
Hoaglin, David, 373
Hypergeometric distribution, 114–117
Hypothesis testing
about population mean, 386
alternative, 387
errors in, 392–395
null, 387
population proportion, 403–405
power of test, 392–395
P-values and one-sample ttest, 387–391
signiﬁcance level,
software for, 395–396
statistical, 386
test procedures, 386–388
about population mean μ, 388–389
test statistic, 389
Type I error, 392–395
Type II error, 392–395
I
Ideal ﬁlters, 580–583
Impulse function, 610
Inclusion-exclusion principle, 13
Incomplete gamma function, 603
Independence, 43–44
events, 44–47
mutually, 46
Interval estimate, 376
Inverse CDF method, 221–224
Inverse DTFT, 612
Inverse Fourier transform, 611
J
Jointly wide-sense stationary, 508
Joint probability density function, 241–245
Joint probability distributions, 239
bivariate normal distribution, 309–311
conditional distributions, 311–312
multivariate normal distribution, 312
regression to mean, 312
with software, 313
conditional distributions, 277–279
and independence, 279–280
conditional expectation, 277–279
and variance, 280–281
correlation, 255–256, 259–262
vs.causation, 262
coefﬁcient, 260
covariance, 255–259
dependent, 245
expected values, 255–256
properties, 256–257independent random variables, 245–246
joint PDF, 241–245
joint PMF, 240–241
joint probability table, 240
Law of Large Numbers, 299–300
Laws of Total Expectation and Variance, 281–286
limit theorems ( seeLimit theorems)
linear combinations, properties, 264–277
convolution, 268
moment generating functions, 270–272
PDF of sum, 268–270
theorem, 265
marginal probability density functions, 243
marginal probability mass functions, 241
multinomial distribution, 247
multinomial experiment, 247
order statistics, 326
distributions of Ynand Y1, 326–328
ith order statistic distribution, 328–329
joint distribution of norder statistics, 329–331
of random variables, 246–249
reliability ( seeReliability)
simulations methods ( seeSimulations methods)
transformations of variables, 302–307
Joint probability mass function, 240–241
K
Kahneman, Daniel, xxiii
Karlin, Samuel, 449, 450
L
Law of Large Numbers, 299–300
Law of Total Probability, 34–35
Laws of Total Expectation, 281–286
Laws of Variance, 281–286
Likelihood function, 368
Limit theorems
CLT, 293–297
applications of, 297–298
independent and identically distributed, 290
random samples, 290–293
standard error of mean, 291
Linear combinations, properties, 264–277
convolution, 268
moment generating functions, 270–272
PDF of sum, 268–270
theorem, 265
Linear, time-invariant (LTI) system, 576–577
butterworth ﬁlters, 583
ideal ﬁlters, 580–583
impulse response, 576
power signal-to-noise ratio, 584
random sequences and, 591–593
signal plus noise, 583–586
statistical properties of, 577–580
transfer function, 576
Lognormal distributions, 199–201
Lowpass ﬁlter, 581, 582
LTI system SeeLinear, time-invariant (LTI) systemIndex 641
M
Marginal probability density functions, 243
Marginal probability mass functions, 241
Markov, Andrey A., 423
Markov chains, 423
with absorbing states, 457–458
canonical form, 467
Chapman–Kolmogorov Equations, 431–436
conditional probabilities, 426
continuous-time, 425
discrete-space, 424
discrete-time, 425
eventual absorption probabilities, 466–469
ﬁnite-state, 424
initial distribution, specifying, 440–443
initial state, 424
irreducible chains, 453–454
mean ﬁrst passage times, 465–466
mean time to absorption, 461–465
one-step transition probabilities, 426
periodic chains, 453–454
process of, 544
birth and death process, 551
continuous-time, 544–546
explicit form of transition matrix, 554–555
generator matrix, 552
inﬁnitesimal parameters, 548
instantaneous transition rates, 547
long-run behavior, 552–554
sojourn times, transition and, 548–551
time homogeneous, 544
transition probabilities, 546
property, 423–428
regular, 446–448
simulation, 472–480
states, 424
state space, 424
steady-state distribution and, 450–451
Steady-State Theorem, 448–449
time-homogeneous, 425
time to absorption, 458–461
transition
matrix, 431–432
probabilities, 426, 432–436
Matlab
probability plots in, 213
and R commands, xviii, 619
simulation implemented in, 134–135
Maximum likelihood estimation (MLE), 366–373
Mean
and autocorrelation functions, 496–504
ﬁrst passage times, 465–466
recurrence time, 450
and variance functions, 496–499
Mean square
sense, 512
value, 509
Mean time to absorption (MTTA), 461–465
Mean value SeeExpected value
Memoryless property, 189Mendel, Gregor, 442
Minimum variance unbiased estimator (MVUE), 359
Moment generating functions (MGF), 125–126, 166–168
of common distributions, 128–129
gamma distributions, 193
normal, 178–179
obtaining moments from, 127–128
Moments, 123–125
from MGF, 127–128
skewness coefﬁcient, 124
MTTA SeeMean time to absorption (MTTA)
Multinomial distribution, 247
Multinomial experiment, 247
Multiplication rule, 32–34
Multivariate normal distribution, 312
N
Negative binomial distributions
alternative deﬁnition, 120
and geometric distributions, 117–120
Notch ﬁlter SeeBandstop ﬁlter
Nyquist rate, 594
Nyquist sampling theorem, 594
O
Olofsson, Peter, xxv
o(h) notation, 610
Order statistics, 326
distributions of Ynand Y1, 326–328
ith order statistic distribution, 328–329
joint distribution of the norder statistics, 329–331
P
Pascal, Blaise, xvii
PDF SeeProbability density function (PDF)
Peebles, Peyton, 583
Periodic chains, 453–454
Permutations, 20–22
PMF SeeProbability mass function (PMF)
Point estimation, 352
accuracy and precision, 357–359
estimated standard error, 357
estimates and estimators, 354–356
parameter, 352
sample mean, 353
sample median, 353
sample range, 353
sample standard deviation, 353
sample variance, 353
standard error, 357
statistic, 352
unbiased estimator, 357
Poisson cumulative distribution function, 600
Poisson distribution, 107
with binomial distributions, 108
as limit, 107–110
mean and variance, 110
poisson process, 110–111
with software, 111
Poisson process, 110–111, 522–524642 Index
alternative deﬁnition, 528–530
combining and decomposing, 526–528
exponential and gamma distributions, 524–526
independent increments, 522
intensity function, 530
non-homogeneous, 530–531
rate, 522
spatial, 530
stationary increments, 522
telegraphic process, 531–532
Population proportion
conﬁdence intervals, 401–403
hypothesis testing, 403–405
score conﬁdence interval, 401
software for inferences, 405
Power spectral density (PSD), 563
average/expected power, 564
cross-power spectral density, 572
in frequency band, 569–570
partitioning, 567–569
properties, 566–569
for two processes, 572–573
white noise processes, 570–572
Wiener–Khinchin Theorem, 565
Precision, 135–137
Principle of Unbiased Estimation, 359
Probability
Addition Rule, 12–13
application, xviii
to business, xix
to engineering and operations research, xx–xxii
to ﬁnance, xxii–xxiii
to life sciences, xix–xx
axioms, 7–9
Complement Rule, 11–12
conditional, 29–30
Bayes’ theorem, 35–37
deﬁnition, 30–32
Law of Total Probability, 34–35
multiplication rule, 32–34
counting methods
combinations, 22–25
fundamental principle, 18–19
k-tuple, 19
permutations, 20–22
tree diagrams, 19–20
coupon collector problem, xviii
deﬁnition, 1
De Morgan’s laws, 4
determining systematically, 13
development of, xvii
events, 3, 7
of eventual absorption, 466–469
in everyday life, xxiii–xxvii
experiment, 1
game theory, xvii
inclusion–exclusion principle, 13
independence, 43–44
events, 44–47
mutually, 46interpretations, 7–11
outcomes, 14
properties, 7–9, 11–13
relations from set theory, 4–5
sample spaces, 1–2
simulation of random events, 51
estimated/standard error, 55
precision, 55–56
RNG, 51–55
software in, xviii
transition, 432–436
vectorization, 54
Probability density function (PDF)
continuous distribution, percentiles of, 156–157
for continuous variables, 148–152
and cumulative distribution functions, 147–148
joint, 241–245, 334–336
marginal PDF, 243
median of, 156
obtaining f(x)from F(x), 155–156
symmetric, 157
uniform distribution, 150
using F(x) to compute probabilities, 154–155
Probability distributions
continuous distributions, 617–619
cumulative, 75–78
discrete distributions, 615–616
for discrete random variables, 71–74
family of, 74
geometric distribution, 75
Matlab and R commands, 619
parameter of, 74–75
Probability histogram, 74
Probability mass function (PMF), 72, 240–241
joint, 332–334
marginal PMF, 241
view of, 78–79
Probability plots, 205–209
beyond normality, 211–212
departures from normality, 209–211
location and scale parameters, 211
in Matlab and R, 213
normal, 208
sample percentiles, 205–206
shape parameter, 211
PSD SeePower spectral density (PSD)
P-values, 389–392
R
Random noise, 489
Random number generator (RNG), 51–55
Random process, 489
autocovariance/autocorrelation functions, 499–502
classiﬁcation, 493
continuous-space process, 493
continuous-time processes, 493
discrete sequences, 518–519
discrete-space process, 493
discrete-time, 493, 516–519
ensemble, 490Index 643
Random process ( cont. )
independent, 502
joint distribution of, 502
mean and variance functions, 496–499
orthogonal, 502
poisson process ( seePoisson process)
random sequence, 493
regarded as random variables, 493–494
sample function, 490
stationary processes, 504–508
types, 489–492
uncorrelated, 502
WSS ( seeWide-sense stationary (WSS) processes)
Random variable (RV), 67
Bernoulli, 68
binomial distribution, 97–99
continuous, 70
deﬁnition, 68
discrete, 70
transformations of, 216–220
types, 69–70
Random walk, 518
R commands
Matlab and, xviii, 619
probability plots in, 213
simulation implemented in, 134–135
Regular Markov chains, 446–448
Reliability, 315
function, 315–317
hazard functions, 321–323
mean time to failure, 320
series and parallel designs, 317–319
simulations methods for, 338–339
RNG SeeRandom number generator (RNG)
Ross, Sheldon, 227, 428
RVSeeRandom variable (RV)
S
Sample mean
deﬁnition, 136
point estimation, 353
Sample median, 205, 326, 353
Sample space, 1–2
Sample standard deviation, 166, 353
Sample variance, 353
Sampling
interval, 593
random sequences and, 593–594
rate, 594
Score conﬁdence interval, 401
SDSeeStandard deviation (SD)
Set theory, 4–5
Signal processing
discrete-time ( seeDiscrete-time signal processing)
LTI systems, random processes and, 576–577
ideal ﬁlters, 580–583
signal plus noise, 583–586
statistical properties of, 577–580
power spectral density, 563–566power in frequency band, 569–570
for processes, 572–573
properties, 566–569
white noise processes, 570–572
Simulation
bivariate normal distribution, 336–338
of discrete random variables, 131–134
implemented in R and Matlab, 134–135
of joint probability distributions/system reliability,
332–339
mean, standard deviation, and precision, 135–137
for reliability, 338–339
standard error of mean, 136
values from joint PDF, 334–336
values from joint PMF, 332–334
Standard deviation (SD), 135–137, 164
Chebyshev’s inequality, 89–90
deﬁnition, 88
function, 496
Standard error, 102
of mean, 136, 291
point estimation, 357
Standard normal CDF, 601–602
Standard normal random variable, 173–175
Stationary processes, 504–508
deﬁnition, 505
ergodic processes, 511–513
Statistical inference
Bayesian inference ( seeBayesian inference)
Bayesian method, 352
CI (seeConﬁdence interval (CI))
hypothesis testing ( seeHypothesis testing)
maximum likelihood estimation, 366–373
point estimation ( seePoint estimation)
population proportion, inferences for
conﬁdence intervals, 401–403
hypothesis testing, 403–405
score conﬁdence interval, 401
software for inferences, 405
Steady-state distribution, 449–451
Steady-state probabilities, 451–453
Steady-State Theorem, 448–449
Step function, 76
Stochastic processes SeeRandom process
T
Taylor, Howard M., 449, 450
tdistribution
critical values for, 604
family, 377
properties, 377
tail areas of, 605–607
Thorp, Edward O., xvii, xxii
Transformations of random variable, 216–220
Transition matrix, 431–432
Transition probability
multi-step, 432–436
one-step, 426
Tree diagrams, 19–20644 Index
Trigonometric identities, 609
ttest, one-sample, 389–392
Tversky, Amos, xxiii
U
Uncorrelated random processes, 502
Uniform distribution, 150
V
Variance
Chebyshev’s inequality, 89–90
conditional expectation and, 280–281
deﬁnition, 88
functions, 496–499
Laws of Total Expectation and, 281–286
mean-square value, 90
properties, 90–91
shortcut formula, 90–91
Venn diagram, 5
Volcker, Paul, xxiii
W
Weibull, Waloddi, 196
Weibull distributions, 196–199White noise processes, 570–572
Wide-sense stationary (WSS) processes, 504–508
autocorrelation ergodic, 513
dc power offset, 511
deﬁnition, 506
ergodic processes, 511–513
mean ergodic, 512
mean square
sense, 512
value, 508
properties, 508–511
time autocorrelation, 513
time average, 512–513
Wiener–Khinchin Theorem, 565, 572–573
Wiener process SeeBrownian motion process
Winkler, Robert, 11
Wood, Fred, 211
WSS processes SeeWide-sense stationary (WSS)
processes
Z
zinterval, one-proportion, 401
ztest, one-proportion, 403Index 645
