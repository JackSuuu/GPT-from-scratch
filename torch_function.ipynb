{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0048\n",
      "CPU times: user 1.18 ms, sys: 2.85 ms, total: 4.03 ms\n",
      "Wall time: 5.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "# matrix operation\n",
    "zeros = torch.zeros(1, 1)\n",
    "end_times = time.time()\n",
    "\n",
    "elapsed_time = end_times - start_time\n",
    "print(f'{elapsed_time: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2.95683312\n",
      "0.30720401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z0/t_vv9z0908vc5j6k_b12pwn40000gp/T/ipykernel_93332/2337744327.py:16: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  rand = np.multiply(np_rand1, np_rand2)\n"
     ]
    }
   ],
   "source": [
    "torch_rand1 = torch.rand (10000, 10000). to(device)\n",
    "torch_rand2 = torch.rand (10000, 10000). to(device)\n",
    "np_rand1 = torch.rand (10000, 10000)\n",
    "np_rand2 = torch.rand (10000, 10000)\n",
    "\n",
    "start_time - time.time()\n",
    "\n",
    "rand = (torch_rand1 @ torch_rand2)\n",
    "\n",
    "end_time = time. time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\" {elapsed_time:.8f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "end_time = time. time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{elapsed_time:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a probability tensor\n",
    "probabilities = torch.tensor([0.1, 0.9])\n",
    "# 10% or 01 => o, 90%Z or 0.9 >= 1, each probability points to the index of the probabilities\n",
    "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1, 2, 3, 4])\n",
    "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tril is making a triangular shape of matrix with 1 and 0\n",
    "out = torch.tril(torch.ones(5,5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.triu(torch.ones(5, 5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.zeros(5,5).masked_fill(torch.tril(torch.ones(5, 5)) == 0, float('-inf'))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.zeros(2, 3, 4)\n",
    "out = input.transpose(0, 2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "\n",
    "# Stack the tensors along a new dimension\n",
    "# for making a batch\n",
    "stacked_tensor = torch.stack([tensor1, tensor2, tensor3])\n",
    "stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.3072,  7.2185, 12.9880], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "sample = torch.tensor([10., 10., 10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "linear(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# creaet a tensor\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# apply softmax using torch.nn.functional.softmax()\n",
    "softmax_output = F.softmax(tensor1, dim=0)\n",
    "softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2337e+00, -2.0857e-01, -1.5413e+00, -4.8479e-02, -1.0044e+00,\n",
       "          2.6424e-01,  7.9775e-02,  1.2979e+00,  3.6312e-01, -1.0739e+00,\n",
       "         -1.1457e-01,  2.8456e-01, -3.5305e-01, -1.1540e+00,  5.7164e-01,\n",
       "         -1.2368e+00, -2.9870e-01, -7.5536e-01,  1.7297e-01,  1.7439e+00,\n",
       "          1.4931e+00, -9.5080e-01, -1.4705e+00,  1.1278e+00,  6.8075e-02,\n",
       "          1.1210e+00, -1.0760e+00,  2.9613e-01,  8.1302e-01,  2.0347e+00,\n",
       "          3.8288e-01, -1.3196e+00,  2.4115e-01,  1.0808e+00, -5.1086e-01,\n",
       "          7.5798e-01, -2.0893e+00, -2.1838e-01, -5.7255e-01,  1.1455e+00,\n",
       "          1.1028e+00, -9.4393e-01, -3.6049e-02,  2.2471e-01,  6.6789e-01,\n",
       "         -8.9218e-01, -7.1831e-01, -1.1316e+00, -6.7330e-01, -9.4591e-01,\n",
       "          1.5059e+00, -3.6210e-01, -3.6247e-01, -5.4153e-01, -6.4393e-01,\n",
       "          8.0983e-02, -5.4664e-01, -1.4545e-01,  1.0338e+00, -2.1298e+00,\n",
       "          6.5314e-01,  8.2233e-01,  8.5134e-01,  4.2356e-01, -4.0796e-01,\n",
       "         -1.1059e+00,  8.1191e-02,  8.6082e-01,  1.1133e+00, -7.3632e-01,\n",
       "         -5.2437e-01, -2.0442e-01,  9.9811e-01,  1.7678e+00, -7.8758e-01,\n",
       "         -6.0314e-01,  2.1128e+00, -4.9444e-01,  5.6440e-01,  4.3356e-02,\n",
       "         -3.4568e-01,  2.5172e+00, -7.6624e-01,  4.8464e-01,  1.3377e+00,\n",
       "          4.5959e-01, -1.4990e+00, -3.5425e-01, -1.4372e+00,  1.7350e+00,\n",
       "          1.8416e-01,  5.1028e-01, -1.4702e+00,  3.2288e-01, -2.2901e+00,\n",
       "          2.9968e-01,  3.2541e-01,  1.6223e+00, -3.4332e-01, -1.3442e-01],\n",
       "        [-1.0757e+00,  1.2284e+00, -1.4528e+00,  2.7357e-01,  6.1455e-01,\n",
       "         -6.7646e-01, -4.6659e-01,  4.1438e-01, -1.0536e+00,  4.6515e-01,\n",
       "          4.3342e-01,  6.4940e-01, -1.2522e-01,  5.2872e-01, -9.5301e-01,\n",
       "         -6.9762e-01, -1.5377e+00,  7.3760e-01,  5.6770e-01, -1.0334e+00,\n",
       "          1.2159e+00, -2.3060e+00, -1.5669e+00, -2.2549e-01, -2.0910e-01,\n",
       "          2.1019e-01, -1.1747e+00,  4.2705e-02,  1.2746e+00,  1.3303e+00,\n",
       "          6.7268e-02, -7.5480e-01, -3.0514e-01,  1.7739e+00,  3.3536e-01,\n",
       "         -7.5460e-01, -1.4662e+00,  1.5380e+00,  3.0627e-01, -5.0107e-01,\n",
       "         -4.3301e-01, -4.1019e-01, -1.1703e+00,  3.7633e-01, -1.6019e+00,\n",
       "         -9.7545e-01,  4.5880e-01, -5.2547e-01,  2.7925e-01, -7.6360e-01,\n",
       "          1.5951e-01, -1.2022e+00,  6.5313e-01, -8.4012e-01,  8.4640e-01,\n",
       "          1.9377e-01,  2.0286e-01, -2.0188e-01, -1.4273e+00, -2.1700e+00,\n",
       "         -2.1628e+00,  2.7110e-01,  1.3602e+00,  4.2941e-02, -1.0981e-01,\n",
       "          1.8354e-01,  4.1180e-01,  3.5569e-01, -1.3149e+00, -8.6823e-02,\n",
       "          1.6849e+00,  2.5307e-01,  8.7394e-01,  1.3141e+00, -8.8267e-01,\n",
       "         -5.7521e-01, -1.5001e-01, -1.5406e+00,  1.2331e+00,  8.3676e-01,\n",
       "          5.1599e-01, -3.6956e-01, -2.3426e-01,  5.7320e-01,  4.1719e-01,\n",
       "          5.1828e-01, -1.5148e+00,  5.7986e-01, -4.7581e-01, -3.8869e-01,\n",
       "         -1.4434e+00, -6.2921e-01, -3.9984e-02, -1.0660e+00, -6.4031e-01,\n",
       "         -1.1523e-01, -1.1671e+00,  1.1140e+00,  1.1377e+00,  1.1525e+00],\n",
       "        [-1.0380e+00,  8.4572e-01,  1.2163e+00,  2.0176e+00, -5.3589e-01,\n",
       "          2.1945e+00,  1.0480e+00, -3.1132e-01, -1.1586e+00, -5.6927e-01,\n",
       "         -1.1989e-01, -1.0112e+00, -1.0848e+00, -6.9229e-01,  1.3297e-03,\n",
       "          4.3798e-01,  5.6824e-01,  5.7005e-01, -2.4540e-01, -1.7616e-01,\n",
       "         -1.0953e-01, -3.7794e-01,  3.6005e-01,  1.6135e+00,  2.5661e+00,\n",
       "          2.5560e+00, -7.5854e-01, -4.9620e-01, -9.5847e-01,  2.3410e-01,\n",
       "          1.6539e-01,  5.6730e-01, -8.8804e-01,  1.0864e+00, -3.0647e-01,\n",
       "         -2.4278e+00, -1.0707e+00,  6.4254e-02, -1.8397e-01,  4.6683e-01,\n",
       "         -5.2870e-01, -1.6702e-01, -1.7634e+00,  1.6815e+00,  1.3798e-01,\n",
       "         -7.9195e-01,  1.4333e+00,  1.5451e-01, -2.6027e-01, -1.8545e-01,\n",
       "         -1.6014e-01, -2.9929e-01,  1.1876e+00,  3.8661e-02,  6.9440e-01,\n",
       "          3.5043e-01, -6.6373e-01,  2.7217e-01,  4.1165e-01, -9.4966e-01,\n",
       "          3.4470e-01,  2.8557e-01,  5.7545e-01,  6.4509e-01, -5.0870e-01,\n",
       "          2.5653e-01,  8.2838e-01,  4.9367e-01,  1.5316e+00,  1.4881e-04,\n",
       "         -2.1531e-01, -1.4729e+00, -1.6372e+00,  2.8092e-01, -2.8652e-01,\n",
       "          1.6417e+00, -3.3090e-01,  1.2230e+00,  8.2571e-01,  3.4917e-01,\n",
       "         -6.7572e-01, -6.3107e-01,  1.4541e+00, -9.6347e-03, -8.7778e-02,\n",
       "          3.5278e-01,  1.0973e+00, -1.8287e+00,  2.5717e-01, -5.4701e-01,\n",
       "         -3.7933e-01, -1.7957e+00,  5.4108e-02, -1.2710e+00, -1.6972e-01,\n",
       "          7.3574e-01, -9.1428e-01,  1.4016e+00, -8.2049e-02,  2.8751e-01],\n",
       "        [-1.1958e+00,  2.6421e-01,  5.2880e-01,  2.9358e-01,  1.0447e+00,\n",
       "         -2.4269e-01,  8.1927e-01,  2.7617e+00, -3.3692e-01, -1.5692e+00,\n",
       "          6.1300e-01,  1.1677e+00,  2.2847e-01, -1.3757e-01,  2.5139e+00,\n",
       "          1.7481e-01, -7.5703e-01, -7.5339e-01, -1.7224e+00,  1.0800e+00,\n",
       "         -1.4423e-01, -1.0898e+00,  9.2705e-01, -1.4770e+00,  7.4951e-01,\n",
       "         -3.8634e-01, -1.3244e-01, -2.0797e+00, -4.3369e-01,  9.5357e-01,\n",
       "         -1.0431e+00,  1.8265e-01, -2.4201e-01, -3.4705e-01,  8.3474e-01,\n",
       "          7.4098e-01, -8.8053e-02, -4.3772e-01, -5.5612e-01, -1.2033e+00,\n",
       "          1.3715e+00,  3.1496e-01, -1.1503e+00, -7.0021e-01, -5.3704e-01,\n",
       "         -1.5656e+00, -1.3859e-02,  7.7169e-01,  1.3953e-01, -1.8361e-01,\n",
       "         -1.0358e+00, -4.6727e-01, -9.1750e-01, -1.2968e-01,  9.3041e-01,\n",
       "         -7.6354e-01,  4.8663e-01,  1.5730e+00, -1.4720e+00,  2.5988e-01,\n",
       "         -1.0945e+00, -4.5042e-01,  5.9192e-01,  5.7177e-01,  9.0105e-02,\n",
       "         -9.0369e-01,  9.2919e-01,  6.2222e-01, -6.8721e-01, -8.3695e-01,\n",
       "          1.7694e-01, -4.7805e-01, -5.7679e-01,  3.3519e-01,  9.7039e-01,\n",
       "          3.2463e-01, -4.8404e-01,  3.8451e-01, -1.6592e+00, -2.1574e-01,\n",
       "         -3.9542e-01, -1.2825e+00, -1.1320e+00, -1.0866e+00, -1.7559e+00,\n",
       "         -2.7172e-01, -9.3242e-01, -1.4557e-01,  2.0853e-01,  6.5773e-01,\n",
       "         -2.4381e-01,  1.8484e+00,  1.8118e-01, -1.0246e+00, -1.0049e-01,\n",
       "         -1.3139e+00,  1.7419e-01, -5.8874e-01,  1.5606e+00,  1.0461e+00]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Init an e,bedding layer\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create some input indices\n",
    "input_indices = torch.LongTensor([1, 5, 3, 2])\n",
    "\n",
    "# Apply the embedding layer\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "embedded_output.shape\n",
    "embedded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  30,  33],\n",
       "        [ 61,  68,  75],\n",
       "        [ 95, 106, 117]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "a = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "b = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "# dot product\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type int64\n",
    "int_64 = torch.randint(1, (3, 2)).float()\n",
    "#type float32\n",
    "float_32 = torch.rand(2, 3)\n",
    "int_64.dtype\n",
    "result = torch.matmul(int_64, float_32)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3, 5)\n",
    "# unpack the torch tensor\n",
    "x, y, z = a.shape\n",
    "a = a.view(x, y, z)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
